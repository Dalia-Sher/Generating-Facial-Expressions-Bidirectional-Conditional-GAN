{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "6_Bidirectional_Conditional_GAN_6emo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "9d2b5c38-07bd-49cf-ba6b-12e2011393d8"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data = data[data.emotion != 1]\n",
        "data.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHR9swxs5P0w"
      },
      "source": [
        "data['emotion'] = data.emotion.replace(6, 1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQj2Szg75fm5",
        "outputId": "0206acd8-d945-409c-c8b4-50e25f09f75b"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yxZyZWONmc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "7456535a-5fb0-4cd5-9043-57b288b1fddb"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sONoLUCKEOZR",
        "outputId": "679d021c-59ee-4579-96c2-55c7b8ac46d8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=20000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 48, 48, 128)  3328        input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 2304)      13824       input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 100)          0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 2304)         0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3556)         0           flatten_6[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            3557        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.706232, acc.: 60.55%] [G loss: 1.430347]\n",
            "20 [D loss: 1.217101, acc.: 22.27%] [G loss: 0.723359]\n",
            "40 [D loss: 1.885723, acc.: 0.78%] [G loss: 0.538788]\n",
            "60 [D loss: 0.477225, acc.: 83.98%] [G loss: 3.369669]\n",
            "80 [D loss: 0.401443, acc.: 98.05%] [G loss: 2.041670]\n",
            "100 [D loss: 0.563599, acc.: 61.33%] [G loss: 2.548039]\n",
            "120 [D loss: 0.498267, acc.: 86.33%] [G loss: 2.071423]\n",
            "140 [D loss: 0.541494, acc.: 80.47%] [G loss: 2.482299]\n",
            "160 [D loss: 0.734743, acc.: 38.28%] [G loss: 2.858089]\n",
            "180 [D loss: 0.592435, acc.: 72.66%] [G loss: 2.540502]\n",
            "200 [D loss: 0.152034, acc.: 100.00%] [G loss: 5.895642]\n",
            "220 [D loss: 1.017967, acc.: 52.34%] [G loss: 1.768340]\n",
            "240 [D loss: 0.753762, acc.: 67.19%] [G loss: 2.004522]\n",
            "260 [D loss: 0.265279, acc.: 89.06%] [G loss: 7.230158]\n",
            "280 [D loss: 0.346930, acc.: 90.23%] [G loss: 3.946693]\n",
            "300 [D loss: 0.313647, acc.: 93.75%] [G loss: 4.567370]\n",
            "320 [D loss: 1.314217, acc.: 42.19%] [G loss: 2.523178]\n",
            "340 [D loss: 0.275414, acc.: 93.36%] [G loss: 6.436322]\n",
            "360 [D loss: 0.335555, acc.: 90.62%] [G loss: 5.498212]\n",
            "380 [D loss: 0.526623, acc.: 79.30%] [G loss: 3.274611]\n",
            "400 [D loss: 0.681489, acc.: 51.17%] [G loss: 2.559381]\n",
            "420 [D loss: 0.415291, acc.: 85.94%] [G loss: 4.398726]\n",
            "440 [D loss: 0.567251, acc.: 80.47%] [G loss: 1.936352]\n",
            "460 [D loss: 0.439860, acc.: 86.72%] [G loss: 3.011594]\n",
            "480 [D loss: 0.359850, acc.: 85.55%] [G loss: 5.366264]\n",
            "500 [D loss: 0.787960, acc.: 52.73%] [G loss: 2.068174]\n",
            "520 [D loss: 0.640141, acc.: 60.16%] [G loss: 1.844001]\n",
            "540 [D loss: 0.669491, acc.: 55.47%] [G loss: 1.944935]\n",
            "560 [D loss: 0.547158, acc.: 74.22%] [G loss: 2.790805]\n",
            "580 [D loss: 0.605015, acc.: 66.80%] [G loss: 2.743867]\n",
            "600 [D loss: 0.610373, acc.: 67.97%] [G loss: 1.972929]\n",
            "620 [D loss: 0.644601, acc.: 61.33%] [G loss: 2.175435]\n",
            "640 [D loss: 0.619036, acc.: 66.02%] [G loss: 2.286901]\n",
            "660 [D loss: 0.293911, acc.: 95.70%] [G loss: 1.806792]\n",
            "680 [D loss: 0.300280, acc.: 96.88%] [G loss: 1.916419]\n",
            "700 [D loss: 1.089808, acc.: 38.28%] [G loss: 1.589358]\n",
            "720 [D loss: 0.403344, acc.: 92.58%] [G loss: 2.780918]\n",
            "740 [D loss: 0.740005, acc.: 55.08%] [G loss: 2.156884]\n",
            "760 [D loss: 0.684439, acc.: 64.06%] [G loss: 2.908791]\n",
            "780 [D loss: 0.453448, acc.: 85.16%] [G loss: 2.750512]\n",
            "800 [D loss: 0.630594, acc.: 60.55%] [G loss: 2.348054]\n",
            "820 [D loss: 0.563374, acc.: 70.70%] [G loss: 3.088882]\n",
            "840 [D loss: 0.532340, acc.: 73.83%] [G loss: 3.002697]\n",
            "860 [D loss: 0.480982, acc.: 79.30%] [G loss: 3.471669]\n",
            "880 [D loss: 0.586878, acc.: 71.88%] [G loss: 2.742713]\n",
            "900 [D loss: 0.584276, acc.: 71.88%] [G loss: 2.435167]\n",
            "920 [D loss: 0.530624, acc.: 72.66%] [G loss: 3.061721]\n",
            "940 [D loss: 0.523850, acc.: 75.78%] [G loss: 4.102468]\n",
            "960 [D loss: 0.482206, acc.: 78.52%] [G loss: 2.997077]\n",
            "980 [D loss: 0.530967, acc.: 75.39%] [G loss: 3.098781]\n",
            "1000 [D loss: 0.565757, acc.: 72.66%] [G loss: 2.751681]\n",
            "1020 [D loss: 0.539624, acc.: 74.61%] [G loss: 3.120546]\n",
            "1040 [D loss: 0.451201, acc.: 83.59%] [G loss: 2.974205]\n",
            "1060 [D loss: 0.567019, acc.: 71.48%] [G loss: 2.794724]\n",
            "1080 [D loss: 0.506902, acc.: 76.95%] [G loss: 3.115929]\n",
            "1100 [D loss: 0.570056, acc.: 69.92%] [G loss: 2.776104]\n",
            "1120 [D loss: 0.539332, acc.: 71.88%] [G loss: 2.831180]\n",
            "1140 [D loss: 0.460577, acc.: 80.47%] [G loss: 3.175192]\n",
            "1160 [D loss: 0.507112, acc.: 75.00%] [G loss: 3.101369]\n",
            "1180 [D loss: 0.567937, acc.: 72.27%] [G loss: 2.721071]\n",
            "1200 [D loss: 0.522029, acc.: 75.00%] [G loss: 3.204654]\n",
            "1220 [D loss: 0.542677, acc.: 73.44%] [G loss: 2.922536]\n",
            "1240 [D loss: 0.516903, acc.: 72.27%] [G loss: 2.846246]\n",
            "1260 [D loss: 0.472351, acc.: 80.86%] [G loss: 3.460250]\n",
            "1280 [D loss: 0.597562, acc.: 70.31%] [G loss: 3.052846]\n",
            "1300 [D loss: 0.434356, acc.: 82.42%] [G loss: 3.407623]\n",
            "1320 [D loss: 0.515246, acc.: 76.56%] [G loss: 3.076324]\n",
            "1340 [D loss: 0.428481, acc.: 82.81%] [G loss: 3.862190]\n",
            "1360 [D loss: 0.560600, acc.: 74.22%] [G loss: 2.863063]\n",
            "1380 [D loss: 0.487883, acc.: 74.61%] [G loss: 3.282668]\n",
            "1400 [D loss: 0.490378, acc.: 75.00%] [G loss: 3.334760]\n",
            "1420 [D loss: 0.459529, acc.: 78.52%] [G loss: 3.659270]\n",
            "1440 [D loss: 0.514360, acc.: 71.48%] [G loss: 3.280240]\n",
            "1460 [D loss: 0.483433, acc.: 76.95%] [G loss: 3.212197]\n",
            "1480 [D loss: 0.553476, acc.: 68.75%] [G loss: 2.959445]\n",
            "1500 [D loss: 0.446189, acc.: 77.73%] [G loss: 3.802277]\n",
            "1520 [D loss: 0.464531, acc.: 77.73%] [G loss: 2.825715]\n",
            "1540 [D loss: 0.514584, acc.: 74.22%] [G loss: 2.618604]\n",
            "1560 [D loss: 0.555133, acc.: 71.88%] [G loss: 3.639329]\n",
            "1580 [D loss: 0.369495, acc.: 86.72%] [G loss: 3.196917]\n",
            "1600 [D loss: 0.492778, acc.: 78.12%] [G loss: 3.091559]\n",
            "1620 [D loss: 0.518815, acc.: 72.66%] [G loss: 3.399104]\n",
            "1640 [D loss: 0.424026, acc.: 82.42%] [G loss: 4.058899]\n",
            "1660 [D loss: 0.622853, acc.: 70.70%] [G loss: 2.186899]\n",
            "1680 [D loss: 1.611887, acc.: 32.03%] [G loss: 3.535844]\n",
            "1700 [D loss: 0.615001, acc.: 66.80%] [G loss: 3.694600]\n",
            "1720 [D loss: 0.367553, acc.: 86.33%] [G loss: 3.651321]\n",
            "1740 [D loss: 0.728708, acc.: 64.45%] [G loss: 2.800281]\n",
            "1760 [D loss: 0.570370, acc.: 70.70%] [G loss: 3.855214]\n",
            "1780 [D loss: 0.589539, acc.: 68.75%] [G loss: 3.922290]\n",
            "1800 [D loss: 0.498909, acc.: 78.52%] [G loss: 3.145550]\n",
            "1820 [D loss: 0.602429, acc.: 66.80%] [G loss: 3.219541]\n",
            "1840 [D loss: 0.523017, acc.: 74.22%] [G loss: 3.351976]\n",
            "1860 [D loss: 0.452661, acc.: 80.47%] [G loss: 3.546785]\n",
            "1880 [D loss: 0.569467, acc.: 67.97%] [G loss: 3.275994]\n",
            "1900 [D loss: 0.592782, acc.: 69.92%] [G loss: 3.057335]\n",
            "1920 [D loss: 0.531864, acc.: 75.39%] [G loss: 3.175184]\n",
            "1940 [D loss: 0.505334, acc.: 77.73%] [G loss: 3.340090]\n",
            "1960 [D loss: 0.422431, acc.: 80.86%] [G loss: 3.571739]\n",
            "1980 [D loss: 0.511136, acc.: 73.44%] [G loss: 3.114657]\n",
            "2000 [D loss: 0.465089, acc.: 78.91%] [G loss: 3.362786]\n",
            "2020 [D loss: 0.559446, acc.: 72.66%] [G loss: 2.503114]\n",
            "2040 [D loss: 0.685939, acc.: 62.11%] [G loss: 2.473320]\n",
            "2060 [D loss: 0.548992, acc.: 72.66%] [G loss: 3.383912]\n",
            "2080 [D loss: 0.557958, acc.: 73.05%] [G loss: 3.030600]\n",
            "2100 [D loss: 0.545500, acc.: 70.70%] [G loss: 3.220004]\n",
            "2120 [D loss: 0.512643, acc.: 72.66%] [G loss: 3.406450]\n",
            "2140 [D loss: 0.572731, acc.: 70.31%] [G loss: 2.672393]\n",
            "2160 [D loss: 0.409135, acc.: 85.55%] [G loss: 3.588869]\n",
            "2180 [D loss: 0.594717, acc.: 69.14%] [G loss: 2.914464]\n",
            "2200 [D loss: 0.580630, acc.: 69.14%] [G loss: 3.038337]\n",
            "2220 [D loss: 0.564404, acc.: 71.09%] [G loss: 3.005021]\n",
            "2240 [D loss: 0.439591, acc.: 79.30%] [G loss: 3.675675]\n",
            "2260 [D loss: 0.478575, acc.: 76.17%] [G loss: 3.230572]\n",
            "2280 [D loss: 0.527328, acc.: 76.95%] [G loss: 3.436522]\n",
            "2300 [D loss: 0.503462, acc.: 76.56%] [G loss: 3.114594]\n",
            "2320 [D loss: 0.491291, acc.: 75.00%] [G loss: 3.125184]\n",
            "2340 [D loss: 0.506762, acc.: 77.73%] [G loss: 2.903718]\n",
            "2360 [D loss: 0.476701, acc.: 75.78%] [G loss: 3.466147]\n",
            "2380 [D loss: 0.514535, acc.: 75.39%] [G loss: 3.192154]\n",
            "2400 [D loss: 0.436918, acc.: 81.64%] [G loss: 3.586831]\n",
            "2420 [D loss: 0.457726, acc.: 78.12%] [G loss: 3.519226]\n",
            "2440 [D loss: 0.500112, acc.: 75.00%] [G loss: 3.319220]\n",
            "2460 [D loss: 0.455483, acc.: 78.52%] [G loss: 3.298305]\n",
            "2480 [D loss: 0.452864, acc.: 78.52%] [G loss: 3.465660]\n",
            "2500 [D loss: 0.572330, acc.: 69.92%] [G loss: 3.205936]\n",
            "2520 [D loss: 0.428926, acc.: 80.86%] [G loss: 3.473191]\n",
            "2540 [D loss: 0.882393, acc.: 48.83%] [G loss: 2.917379]\n",
            "2560 [D loss: 0.457030, acc.: 79.69%] [G loss: 3.383259]\n",
            "2580 [D loss: 0.475530, acc.: 77.34%] [G loss: 3.335738]\n",
            "2600 [D loss: 0.575878, acc.: 72.27%] [G loss: 3.034939]\n",
            "2620 [D loss: 0.458638, acc.: 77.73%] [G loss: 3.355735]\n",
            "2640 [D loss: 0.526665, acc.: 72.27%] [G loss: 3.228882]\n",
            "2660 [D loss: 0.453338, acc.: 79.69%] [G loss: 3.226248]\n",
            "2680 [D loss: 0.529972, acc.: 73.83%] [G loss: 3.098207]\n",
            "2700 [D loss: 0.456305, acc.: 76.17%] [G loss: 3.476497]\n",
            "2720 [D loss: 0.519117, acc.: 77.73%] [G loss: 3.254436]\n",
            "2740 [D loss: 0.537344, acc.: 73.83%] [G loss: 3.211457]\n",
            "2760 [D loss: 0.478923, acc.: 76.95%] [G loss: 3.722591]\n",
            "2780 [D loss: 0.549374, acc.: 68.75%] [G loss: 3.105783]\n",
            "2800 [D loss: 0.447425, acc.: 80.86%] [G loss: 3.510050]\n",
            "2820 [D loss: 0.502626, acc.: 73.83%] [G loss: 3.425640]\n",
            "2840 [D loss: 0.505731, acc.: 72.27%] [G loss: 3.143507]\n",
            "2860 [D loss: 0.530211, acc.: 71.09%] [G loss: 3.044604]\n",
            "2880 [D loss: 0.531367, acc.: 76.17%] [G loss: 3.149966]\n",
            "2900 [D loss: 0.499100, acc.: 75.78%] [G loss: 3.184056]\n",
            "2920 [D loss: 0.500902, acc.: 77.73%] [G loss: 3.208908]\n",
            "2940 [D loss: 0.558023, acc.: 71.88%] [G loss: 3.359692]\n",
            "2960 [D loss: 0.502390, acc.: 77.73%] [G loss: 3.294577]\n",
            "2980 [D loss: 0.490926, acc.: 78.52%] [G loss: 3.350224]\n",
            "3000 [D loss: 0.460174, acc.: 78.52%] [G loss: 3.555957]\n",
            "3020 [D loss: 0.488155, acc.: 79.30%] [G loss: 3.458911]\n",
            "3040 [D loss: 0.514793, acc.: 75.39%] [G loss: 3.274413]\n",
            "3060 [D loss: 0.430796, acc.: 84.77%] [G loss: 3.823357]\n",
            "3080 [D loss: 0.492467, acc.: 75.78%] [G loss: 3.257968]\n",
            "3100 [D loss: 0.574420, acc.: 70.31%] [G loss: 3.209862]\n",
            "3120 [D loss: 0.631562, acc.: 66.80%] [G loss: 3.103356]\n",
            "3140 [D loss: 0.549057, acc.: 73.05%] [G loss: 3.111012]\n",
            "3160 [D loss: 0.556561, acc.: 71.48%] [G loss: 3.051562]\n",
            "3180 [D loss: 0.531821, acc.: 72.27%] [G loss: 3.612314]\n",
            "3200 [D loss: 0.508988, acc.: 73.83%] [G loss: 3.215456]\n",
            "3220 [D loss: 0.510109, acc.: 73.44%] [G loss: 3.378584]\n",
            "3240 [D loss: 0.433939, acc.: 82.03%] [G loss: 3.381417]\n",
            "3260 [D loss: 0.494823, acc.: 75.78%] [G loss: 3.161434]\n",
            "3280 [D loss: 0.441719, acc.: 81.64%] [G loss: 3.368750]\n",
            "3300 [D loss: 0.571123, acc.: 68.36%] [G loss: 3.073169]\n",
            "3320 [D loss: 0.519015, acc.: 75.00%] [G loss: 3.266238]\n",
            "3340 [D loss: 0.462349, acc.: 78.12%] [G loss: 3.804005]\n",
            "3360 [D loss: 0.551414, acc.: 68.36%] [G loss: 3.218886]\n",
            "3380 [D loss: 0.520315, acc.: 75.39%] [G loss: 3.556564]\n",
            "3400 [D loss: 0.526983, acc.: 71.48%] [G loss: 3.322884]\n",
            "3420 [D loss: 0.525456, acc.: 73.05%] [G loss: 3.331654]\n",
            "3440 [D loss: 0.461047, acc.: 78.91%] [G loss: 3.173903]\n",
            "3460 [D loss: 0.449816, acc.: 80.86%] [G loss: 3.505243]\n",
            "3480 [D loss: 0.515801, acc.: 77.34%] [G loss: 3.205610]\n",
            "3500 [D loss: 0.455348, acc.: 76.56%] [G loss: 3.478307]\n",
            "3520 [D loss: 0.484456, acc.: 77.34%] [G loss: 3.375330]\n",
            "3540 [D loss: 0.457285, acc.: 78.91%] [G loss: 3.407244]\n",
            "3560 [D loss: 0.519641, acc.: 72.27%] [G loss: 3.151236]\n",
            "3580 [D loss: 0.476833, acc.: 78.52%] [G loss: 3.252203]\n",
            "3600 [D loss: 0.467673, acc.: 77.34%] [G loss: 3.240648]\n",
            "3620 [D loss: 0.567453, acc.: 71.88%] [G loss: 3.292992]\n",
            "3640 [D loss: 0.440880, acc.: 79.69%] [G loss: 3.486482]\n",
            "3660 [D loss: 0.473317, acc.: 75.78%] [G loss: 3.272520]\n",
            "3680 [D loss: 0.559021, acc.: 71.48%] [G loss: 3.034760]\n",
            "3700 [D loss: 0.529308, acc.: 72.27%] [G loss: 3.069001]\n",
            "3720 [D loss: 0.433033, acc.: 80.47%] [G loss: 3.310915]\n",
            "3740 [D loss: 0.507702, acc.: 77.34%] [G loss: 3.400394]\n",
            "3760 [D loss: 0.387427, acc.: 85.55%] [G loss: 1.981348]\n",
            "3780 [D loss: 1.445725, acc.: 30.86%] [G loss: 2.362212]\n",
            "3800 [D loss: 0.707435, acc.: 64.06%] [G loss: 3.657179]\n",
            "3820 [D loss: 0.493977, acc.: 76.95%] [G loss: 2.973856]\n",
            "3840 [D loss: 0.487007, acc.: 78.91%] [G loss: 4.092357]\n",
            "3860 [D loss: 0.661897, acc.: 63.67%] [G loss: 2.909097]\n",
            "3880 [D loss: 0.513526, acc.: 74.61%] [G loss: 3.134341]\n",
            "3900 [D loss: 0.458451, acc.: 77.73%] [G loss: 3.136073]\n",
            "3920 [D loss: 0.597325, acc.: 71.09%] [G loss: 3.352078]\n",
            "3940 [D loss: 0.416200, acc.: 82.03%] [G loss: 3.768098]\n",
            "3960 [D loss: 0.590739, acc.: 68.36%] [G loss: 3.641544]\n",
            "3980 [D loss: 0.551367, acc.: 68.36%] [G loss: 3.545583]\n",
            "4000 [D loss: 0.530982, acc.: 72.66%] [G loss: 3.731053]\n",
            "4020 [D loss: 0.488810, acc.: 77.34%] [G loss: 3.596047]\n",
            "4040 [D loss: 0.454663, acc.: 79.69%] [G loss: 3.612244]\n",
            "4060 [D loss: 0.585538, acc.: 68.75%] [G loss: 3.492934]\n",
            "4080 [D loss: 0.557120, acc.: 72.27%] [G loss: 3.179292]\n",
            "4100 [D loss: 0.549027, acc.: 74.22%] [G loss: 3.387897]\n",
            "4120 [D loss: 0.544202, acc.: 73.44%] [G loss: 3.239805]\n",
            "4140 [D loss: 0.498729, acc.: 73.44%] [G loss: 3.547421]\n",
            "4160 [D loss: 0.503648, acc.: 76.17%] [G loss: 3.319017]\n",
            "4180 [D loss: 0.477589, acc.: 78.91%] [G loss: 3.705745]\n",
            "4200 [D loss: 0.501391, acc.: 74.22%] [G loss: 3.648453]\n",
            "4220 [D loss: 0.513649, acc.: 75.39%] [G loss: 3.239741]\n",
            "4240 [D loss: 0.514978, acc.: 76.95%] [G loss: 3.609402]\n",
            "4260 [D loss: 0.454050, acc.: 80.08%] [G loss: 3.514418]\n",
            "4280 [D loss: 0.504794, acc.: 75.39%] [G loss: 3.388720]\n",
            "4300 [D loss: 0.479397, acc.: 79.69%] [G loss: 3.503524]\n",
            "4320 [D loss: 0.479186, acc.: 78.12%] [G loss: 3.529054]\n",
            "4340 [D loss: 0.426376, acc.: 81.64%] [G loss: 3.674338]\n",
            "4360 [D loss: 0.482229, acc.: 77.73%] [G loss: 3.446977]\n",
            "4380 [D loss: 0.452854, acc.: 79.30%] [G loss: 3.642946]\n",
            "4400 [D loss: 0.475008, acc.: 76.17%] [G loss: 3.457497]\n",
            "4420 [D loss: 0.546464, acc.: 73.05%] [G loss: 3.295317]\n",
            "4440 [D loss: 0.465210, acc.: 80.47%] [G loss: 3.504657]\n",
            "4460 [D loss: 0.512249, acc.: 75.00%] [G loss: 3.555539]\n",
            "4480 [D loss: 0.478299, acc.: 76.17%] [G loss: 3.567965]\n",
            "4500 [D loss: 0.534088, acc.: 75.00%] [G loss: 3.320988]\n",
            "4520 [D loss: 0.577139, acc.: 69.53%] [G loss: 3.346489]\n",
            "4540 [D loss: 0.506246, acc.: 77.34%] [G loss: 3.828055]\n",
            "4560 [D loss: 0.496534, acc.: 76.56%] [G loss: 3.257156]\n",
            "4580 [D loss: 0.559614, acc.: 70.70%] [G loss: 3.111984]\n",
            "4600 [D loss: 0.472632, acc.: 78.52%] [G loss: 2.776975]\n",
            "4620 [D loss: 0.852350, acc.: 51.95%] [G loss: 2.515236]\n",
            "4640 [D loss: 0.572372, acc.: 69.53%] [G loss: 3.483664]\n",
            "4660 [D loss: 0.607908, acc.: 69.14%] [G loss: 3.302951]\n",
            "4680 [D loss: 0.540460, acc.: 73.83%] [G loss: 3.603069]\n",
            "4700 [D loss: 0.506278, acc.: 73.83%] [G loss: 3.788809]\n",
            "4720 [D loss: 0.503246, acc.: 76.17%] [G loss: 3.396763]\n",
            "4740 [D loss: 0.569788, acc.: 72.27%] [G loss: 3.479475]\n",
            "4760 [D loss: 0.426972, acc.: 79.69%] [G loss: 3.471421]\n",
            "4780 [D loss: 0.568035, acc.: 69.14%] [G loss: 3.558135]\n",
            "4800 [D loss: 0.535782, acc.: 70.31%] [G loss: 4.204315]\n",
            "4820 [D loss: 0.445834, acc.: 80.47%] [G loss: 3.682072]\n",
            "4840 [D loss: 0.500359, acc.: 76.95%] [G loss: 3.763301]\n",
            "4860 [D loss: 0.530634, acc.: 75.00%] [G loss: 3.500263]\n",
            "4880 [D loss: 0.547760, acc.: 73.05%] [G loss: 3.571451]\n",
            "4900 [D loss: 0.544523, acc.: 76.17%] [G loss: 3.401505]\n",
            "4920 [D loss: 0.505009, acc.: 73.83%] [G loss: 3.680705]\n",
            "4940 [D loss: 0.554264, acc.: 75.39%] [G loss: 3.126825]\n",
            "4960 [D loss: 0.441138, acc.: 81.64%] [G loss: 3.494090]\n",
            "4980 [D loss: 0.539350, acc.: 71.88%] [G loss: 3.306271]\n",
            "5000 [D loss: 0.563999, acc.: 69.14%] [G loss: 3.440122]\n",
            "5020 [D loss: 0.517082, acc.: 71.48%] [G loss: 3.392577]\n",
            "5040 [D loss: 0.576493, acc.: 67.19%] [G loss: 3.339834]\n",
            "5060 [D loss: 0.410535, acc.: 83.20%] [G loss: 4.086168]\n",
            "5080 [D loss: 0.482936, acc.: 78.52%] [G loss: 3.529166]\n",
            "5100 [D loss: 0.497494, acc.: 76.95%] [G loss: 3.610767]\n",
            "5120 [D loss: 0.481097, acc.: 78.91%] [G loss: 3.583129]\n",
            "5140 [D loss: 0.464439, acc.: 76.17%] [G loss: 3.641556]\n",
            "5160 [D loss: 0.481703, acc.: 78.52%] [G loss: 3.516496]\n",
            "5180 [D loss: 0.489958, acc.: 79.69%] [G loss: 3.653242]\n",
            "5200 [D loss: 0.538902, acc.: 70.31%] [G loss: 3.444652]\n",
            "5220 [D loss: 0.427026, acc.: 80.47%] [G loss: 3.779132]\n",
            "5240 [D loss: 0.538522, acc.: 75.78%] [G loss: 3.650506]\n",
            "5260 [D loss: 0.533970, acc.: 73.05%] [G loss: 3.247943]\n",
            "5280 [D loss: 0.505322, acc.: 77.34%] [G loss: 3.766088]\n",
            "5300 [D loss: 0.525477, acc.: 76.17%] [G loss: 3.470484]\n",
            "5320 [D loss: 0.519591, acc.: 75.78%] [G loss: 3.516977]\n",
            "5340 [D loss: 0.559033, acc.: 73.83%] [G loss: 3.408518]\n",
            "5360 [D loss: 0.535132, acc.: 73.05%] [G loss: 3.452179]\n",
            "5380 [D loss: 0.514604, acc.: 75.00%] [G loss: 3.495547]\n",
            "5400 [D loss: 0.542915, acc.: 72.27%] [G loss: 4.092391]\n",
            "5420 [D loss: 0.449359, acc.: 81.64%] [G loss: 3.762217]\n",
            "5440 [D loss: 0.502787, acc.: 76.56%] [G loss: 3.435275]\n",
            "5460 [D loss: 0.435030, acc.: 82.03%] [G loss: 3.711686]\n",
            "5480 [D loss: 0.470904, acc.: 78.12%] [G loss: 3.633735]\n",
            "5500 [D loss: 0.574962, acc.: 69.92%] [G loss: 3.128076]\n",
            "5520 [D loss: 0.520531, acc.: 72.27%] [G loss: 3.414007]\n",
            "5540 [D loss: 0.549634, acc.: 71.88%] [G loss: 3.524128]\n",
            "5560 [D loss: 0.428685, acc.: 82.42%] [G loss: 3.431865]\n",
            "5580 [D loss: 0.348923, acc.: 87.89%] [G loss: 2.478498]\n",
            "5600 [D loss: 0.230693, acc.: 95.31%] [G loss: 2.282694]\n",
            "5620 [D loss: 0.661783, acc.: 70.70%] [G loss: 2.316643]\n",
            "5640 [D loss: 1.121781, acc.: 38.67%] [G loss: 3.216094]\n",
            "5660 [D loss: 0.819887, acc.: 45.70%] [G loss: 2.521322]\n",
            "5680 [D loss: 0.865053, acc.: 42.58%] [G loss: 1.586817]\n",
            "5700 [D loss: 0.894971, acc.: 44.14%] [G loss: 2.250525]\n",
            "5720 [D loss: 0.581451, acc.: 69.14%] [G loss: 2.827217]\n",
            "5740 [D loss: 0.715584, acc.: 58.20%] [G loss: 2.703974]\n",
            "5760 [D loss: 0.582714, acc.: 64.45%] [G loss: 2.735703]\n",
            "5780 [D loss: 0.560395, acc.: 72.66%] [G loss: 2.841208]\n",
            "5800 [D loss: 0.637753, acc.: 61.33%] [G loss: 2.542631]\n",
            "5820 [D loss: 0.654983, acc.: 64.84%] [G loss: 2.491463]\n",
            "5840 [D loss: 0.641076, acc.: 66.02%] [G loss: 2.422629]\n",
            "5860 [D loss: 0.609501, acc.: 66.80%] [G loss: 2.315421]\n",
            "5880 [D loss: 0.587190, acc.: 69.53%] [G loss: 2.451125]\n",
            "5900 [D loss: 0.526571, acc.: 73.83%] [G loss: 2.553148]\n",
            "5920 [D loss: 0.561268, acc.: 73.44%] [G loss: 2.369669]\n",
            "5940 [D loss: 0.656079, acc.: 59.77%] [G loss: 2.409783]\n",
            "5960 [D loss: 0.553083, acc.: 71.09%] [G loss: 2.559157]\n",
            "5980 [D loss: 0.584644, acc.: 69.92%] [G loss: 2.169463]\n",
            "6000 [D loss: 0.528173, acc.: 71.88%] [G loss: 2.566104]\n",
            "6020 [D loss: 0.617161, acc.: 63.28%] [G loss: 2.292866]\n",
            "6040 [D loss: 0.608939, acc.: 70.31%] [G loss: 2.193375]\n",
            "6060 [D loss: 0.557393, acc.: 71.48%] [G loss: 2.440521]\n",
            "6080 [D loss: 0.562165, acc.: 69.14%] [G loss: 2.383747]\n",
            "6100 [D loss: 0.562349, acc.: 75.00%] [G loss: 2.559956]\n",
            "6120 [D loss: 0.582058, acc.: 70.70%] [G loss: 2.290657]\n",
            "6140 [D loss: 0.554426, acc.: 73.05%] [G loss: 2.222058]\n",
            "6160 [D loss: 0.536546, acc.: 75.78%] [G loss: 2.448485]\n",
            "6180 [D loss: 0.565481, acc.: 69.92%] [G loss: 2.353347]\n",
            "6200 [D loss: 0.607848, acc.: 66.41%] [G loss: 2.352953]\n",
            "6220 [D loss: 0.619084, acc.: 63.67%] [G loss: 2.369138]\n",
            "6240 [D loss: 0.525115, acc.: 74.61%] [G loss: 2.573746]\n",
            "6260 [D loss: 0.541591, acc.: 74.61%] [G loss: 2.562327]\n",
            "6280 [D loss: 0.625935, acc.: 64.45%] [G loss: 2.453801]\n",
            "6300 [D loss: 0.557293, acc.: 68.36%] [G loss: 2.638930]\n",
            "6320 [D loss: 0.531207, acc.: 74.22%] [G loss: 2.611478]\n",
            "6340 [D loss: 0.545340, acc.: 74.22%] [G loss: 2.598910]\n",
            "6360 [D loss: 0.546211, acc.: 74.61%] [G loss: 2.672058]\n",
            "6380 [D loss: 0.561036, acc.: 70.70%] [G loss: 2.620870]\n",
            "6400 [D loss: 0.609995, acc.: 69.92%] [G loss: 2.456125]\n",
            "6420 [D loss: 0.527131, acc.: 74.22%] [G loss: 2.615963]\n",
            "6440 [D loss: 0.598524, acc.: 67.97%] [G loss: 2.365952]\n",
            "6460 [D loss: 0.571096, acc.: 73.05%] [G loss: 2.431806]\n",
            "6480 [D loss: 0.590535, acc.: 66.41%] [G loss: 2.395086]\n",
            "6500 [D loss: 0.561346, acc.: 73.44%] [G loss: 2.579642]\n",
            "6520 [D loss: 0.554368, acc.: 74.22%] [G loss: 2.719513]\n",
            "6540 [D loss: 0.572872, acc.: 69.92%] [G loss: 2.614459]\n",
            "6560 [D loss: 0.591477, acc.: 69.92%] [G loss: 2.673647]\n",
            "6580 [D loss: 0.565162, acc.: 73.05%] [G loss: 2.499106]\n",
            "6600 [D loss: 0.593836, acc.: 66.80%] [G loss: 2.647215]\n",
            "6620 [D loss: 0.550401, acc.: 73.44%] [G loss: 2.716001]\n",
            "6640 [D loss: 0.505439, acc.: 74.61%] [G loss: 2.756197]\n",
            "6660 [D loss: 0.507963, acc.: 76.56%] [G loss: 2.810943]\n",
            "6680 [D loss: 0.540095, acc.: 71.88%] [G loss: 2.605380]\n",
            "6700 [D loss: 0.558492, acc.: 69.92%] [G loss: 2.609943]\n",
            "6720 [D loss: 0.592412, acc.: 69.14%] [G loss: 2.501903]\n",
            "6740 [D loss: 0.593385, acc.: 67.58%] [G loss: 2.716796]\n",
            "6760 [D loss: 0.526088, acc.: 75.00%] [G loss: 2.711235]\n",
            "6780 [D loss: 0.625293, acc.: 66.80%] [G loss: 2.564932]\n",
            "6800 [D loss: 0.532832, acc.: 73.05%] [G loss: 2.849829]\n",
            "6820 [D loss: 0.495112, acc.: 78.12%] [G loss: 2.630175]\n",
            "6840 [D loss: 0.547509, acc.: 72.66%] [G loss: 2.617497]\n",
            "6860 [D loss: 0.518663, acc.: 75.39%] [G loss: 2.775481]\n",
            "6880 [D loss: 0.568615, acc.: 70.70%] [G loss: 2.624435]\n",
            "6900 [D loss: 0.520783, acc.: 75.39%] [G loss: 2.751020]\n",
            "6920 [D loss: 0.549524, acc.: 73.83%] [G loss: 2.649670]\n",
            "6940 [D loss: 0.532696, acc.: 73.44%] [G loss: 2.590271]\n",
            "6960 [D loss: 0.570584, acc.: 69.14%] [G loss: 2.508890]\n",
            "6980 [D loss: 0.499083, acc.: 76.95%] [G loss: 2.446463]\n",
            "7000 [D loss: 0.348365, acc.: 89.84%] [G loss: 2.058556]\n",
            "7020 [D loss: 0.493000, acc.: 75.00%] [G loss: 1.740290]\n",
            "7040 [D loss: 0.994517, acc.: 40.62%] [G loss: 2.186551]\n",
            "7060 [D loss: 1.026503, acc.: 42.97%] [G loss: 3.026409]\n",
            "7080 [D loss: 0.553228, acc.: 72.66%] [G loss: 2.941264]\n",
            "7100 [D loss: 0.520227, acc.: 75.39%] [G loss: 2.736366]\n",
            "7120 [D loss: 0.689857, acc.: 59.77%] [G loss: 3.030543]\n",
            "7140 [D loss: 0.627019, acc.: 66.02%] [G loss: 2.679882]\n",
            "7160 [D loss: 0.550046, acc.: 72.66%] [G loss: 3.001352]\n",
            "7180 [D loss: 0.576926, acc.: 69.92%] [G loss: 3.014247]\n",
            "7200 [D loss: 0.486399, acc.: 78.12%] [G loss: 2.960817]\n",
            "7220 [D loss: 0.584620, acc.: 66.41%] [G loss: 3.018108]\n",
            "7240 [D loss: 0.618607, acc.: 68.75%] [G loss: 2.818521]\n",
            "7260 [D loss: 0.468227, acc.: 80.08%] [G loss: 3.166610]\n",
            "7280 [D loss: 0.563566, acc.: 72.27%] [G loss: 2.711900]\n",
            "7300 [D loss: 0.553284, acc.: 69.14%] [G loss: 2.840956]\n",
            "7320 [D loss: 0.546200, acc.: 75.39%] [G loss: 2.855443]\n",
            "7340 [D loss: 0.538529, acc.: 73.83%] [G loss: 3.095094]\n",
            "7360 [D loss: 0.497388, acc.: 78.91%] [G loss: 3.056880]\n",
            "7380 [D loss: 0.511776, acc.: 76.95%] [G loss: 2.978170]\n",
            "7400 [D loss: 0.521207, acc.: 75.78%] [G loss: 2.929000]\n",
            "7420 [D loss: 0.493441, acc.: 76.95%] [G loss: 2.996004]\n",
            "7440 [D loss: 0.507673, acc.: 76.95%] [G loss: 3.092633]\n",
            "7460 [D loss: 0.520464, acc.: 73.05%] [G loss: 2.999928]\n",
            "7480 [D loss: 0.503068, acc.: 72.27%] [G loss: 2.983502]\n",
            "7500 [D loss: 0.566425, acc.: 70.31%] [G loss: 2.896176]\n",
            "7520 [D loss: 0.543047, acc.: 73.44%] [G loss: 2.768458]\n",
            "7540 [D loss: 0.487804, acc.: 75.39%] [G loss: 3.042640]\n",
            "7560 [D loss: 0.515366, acc.: 75.78%] [G loss: 3.104166]\n",
            "7580 [D loss: 0.539797, acc.: 72.66%] [G loss: 3.121668]\n",
            "7600 [D loss: 0.527969, acc.: 75.39%] [G loss: 2.955673]\n",
            "7620 [D loss: 0.535823, acc.: 73.44%] [G loss: 2.826960]\n",
            "7640 [D loss: 0.488402, acc.: 77.34%] [G loss: 3.043752]\n",
            "7660 [D loss: 0.555149, acc.: 71.48%] [G loss: 2.979108]\n",
            "7680 [D loss: 0.512355, acc.: 76.17%] [G loss: 3.027466]\n",
            "7700 [D loss: 0.534213, acc.: 73.05%] [G loss: 3.030695]\n",
            "7720 [D loss: 0.505953, acc.: 76.56%] [G loss: 3.269614]\n",
            "7740 [D loss: 0.550100, acc.: 73.44%] [G loss: 3.276233]\n",
            "7760 [D loss: 0.499797, acc.: 75.78%] [G loss: 3.093486]\n",
            "7780 [D loss: 0.464147, acc.: 81.64%] [G loss: 3.250412]\n",
            "7800 [D loss: 0.518949, acc.: 75.39%] [G loss: 3.277881]\n",
            "7820 [D loss: 0.482907, acc.: 79.30%] [G loss: 3.307514]\n",
            "7840 [D loss: 0.528995, acc.: 70.31%] [G loss: 3.109260]\n",
            "7860 [D loss: 0.538619, acc.: 71.48%] [G loss: 3.085762]\n",
            "7880 [D loss: 0.543275, acc.: 71.09%] [G loss: 2.947448]\n",
            "7900 [D loss: 0.515760, acc.: 75.78%] [G loss: 3.019404]\n",
            "7920 [D loss: 0.542315, acc.: 73.83%] [G loss: 3.064797]\n",
            "7940 [D loss: 0.422087, acc.: 83.59%] [G loss: 3.459311]\n",
            "7960 [D loss: 0.486077, acc.: 79.69%] [G loss: 3.187304]\n",
            "7980 [D loss: 0.570458, acc.: 70.31%] [G loss: 3.120438]\n",
            "8000 [D loss: 0.447505, acc.: 81.25%] [G loss: 3.259807]\n",
            "8020 [D loss: 0.739604, acc.: 57.81%] [G loss: 2.327905]\n",
            "8040 [D loss: 0.497306, acc.: 77.34%] [G loss: 3.536372]\n",
            "8060 [D loss: 0.600397, acc.: 66.80%] [G loss: 3.192383]\n",
            "8080 [D loss: 0.503148, acc.: 75.00%] [G loss: 3.388122]\n",
            "8100 [D loss: 0.606689, acc.: 68.36%] [G loss: 3.141438]\n",
            "8120 [D loss: 0.477060, acc.: 77.34%] [G loss: 3.157726]\n",
            "8140 [D loss: 0.617782, acc.: 67.97%] [G loss: 2.921834]\n",
            "8160 [D loss: 0.540469, acc.: 71.88%] [G loss: 3.633448]\n",
            "8180 [D loss: 0.514954, acc.: 75.78%] [G loss: 3.393314]\n",
            "8200 [D loss: 0.501203, acc.: 76.95%] [G loss: 3.230327]\n",
            "8220 [D loss: 0.539101, acc.: 71.48%] [G loss: 3.180819]\n",
            "8240 [D loss: 0.389493, acc.: 83.20%] [G loss: 3.741533]\n",
            "8260 [D loss: 0.512504, acc.: 76.95%] [G loss: 3.128437]\n",
            "8280 [D loss: 0.422143, acc.: 81.64%] [G loss: 3.545429]\n",
            "8300 [D loss: 0.579619, acc.: 69.53%] [G loss: 3.200247]\n",
            "8320 [D loss: 0.484383, acc.: 77.34%] [G loss: 3.603167]\n",
            "8340 [D loss: 0.532555, acc.: 76.56%] [G loss: 3.215329]\n",
            "8360 [D loss: 0.453693, acc.: 77.73%] [G loss: 3.416160]\n",
            "8380 [D loss: 0.449719, acc.: 79.30%] [G loss: 3.445146]\n",
            "8400 [D loss: 0.542236, acc.: 73.05%] [G loss: 3.359929]\n",
            "8420 [D loss: 0.438429, acc.: 79.69%] [G loss: 3.408146]\n",
            "8440 [D loss: 0.510872, acc.: 74.22%] [G loss: 3.268120]\n",
            "8460 [D loss: 0.421065, acc.: 82.42%] [G loss: 3.583038]\n",
            "8480 [D loss: 0.472467, acc.: 78.91%] [G loss: 3.170739]\n",
            "8500 [D loss: 0.540313, acc.: 73.05%] [G loss: 3.163605]\n",
            "8520 [D loss: 0.423417, acc.: 80.86%] [G loss: 3.568910]\n",
            "8540 [D loss: 0.492828, acc.: 76.17%] [G loss: 3.155205]\n",
            "8560 [D loss: 0.487121, acc.: 79.69%] [G loss: 3.289920]\n",
            "8580 [D loss: 0.438548, acc.: 82.42%] [G loss: 3.411406]\n",
            "8600 [D loss: 0.524356, acc.: 71.88%] [G loss: 3.167904]\n",
            "8620 [D loss: 0.406425, acc.: 77.73%] [G loss: 3.764222]\n",
            "8640 [D loss: 0.499420, acc.: 79.30%] [G loss: 2.911081]\n",
            "8660 [D loss: 0.193064, acc.: 96.09%] [G loss: 4.515199]\n",
            "8680 [D loss: 0.674510, acc.: 63.28%] [G loss: 3.277832]\n",
            "8700 [D loss: 0.602259, acc.: 65.23%] [G loss: 3.607296]\n",
            "8720 [D loss: 0.524961, acc.: 76.17%] [G loss: 3.467923]\n",
            "8740 [D loss: 0.509469, acc.: 72.66%] [G loss: 3.449835]\n",
            "8760 [D loss: 0.478093, acc.: 76.95%] [G loss: 3.232409]\n",
            "8780 [D loss: 0.472626, acc.: 79.69%] [G loss: 3.422409]\n",
            "8800 [D loss: 0.525532, acc.: 74.61%] [G loss: 3.935421]\n",
            "8820 [D loss: 0.584817, acc.: 69.92%] [G loss: 3.279616]\n",
            "8840 [D loss: 0.586611, acc.: 72.66%] [G loss: 3.434999]\n",
            "8860 [D loss: 0.506516, acc.: 77.34%] [G loss: 3.535484]\n",
            "8880 [D loss: 0.495314, acc.: 75.39%] [G loss: 3.629237]\n",
            "8900 [D loss: 0.398855, acc.: 85.16%] [G loss: 3.510157]\n",
            "8920 [D loss: 0.546182, acc.: 72.66%] [G loss: 3.610223]\n",
            "8940 [D loss: 0.432134, acc.: 83.98%] [G loss: 3.729788]\n",
            "8960 [D loss: 0.536143, acc.: 71.48%] [G loss: 3.097433]\n",
            "8980 [D loss: 0.415871, acc.: 82.81%] [G loss: 3.659496]\n",
            "9000 [D loss: 0.546095, acc.: 75.00%] [G loss: 3.363401]\n",
            "9020 [D loss: 0.404268, acc.: 84.77%] [G loss: 3.686739]\n",
            "9040 [D loss: 0.502631, acc.: 76.95%] [G loss: 3.296489]\n",
            "9060 [D loss: 0.485798, acc.: 78.12%] [G loss: 3.623417]\n",
            "9080 [D loss: 0.490440, acc.: 75.39%] [G loss: 3.366158]\n",
            "9100 [D loss: 0.470563, acc.: 76.17%] [G loss: 3.502312]\n",
            "9120 [D loss: 0.489323, acc.: 76.56%] [G loss: 3.646985]\n",
            "9140 [D loss: 0.564466, acc.: 70.70%] [G loss: 3.316487]\n",
            "9160 [D loss: 0.505670, acc.: 73.83%] [G loss: 3.584737]\n",
            "9180 [D loss: 0.510042, acc.: 75.00%] [G loss: 3.582575]\n",
            "9200 [D loss: 0.533226, acc.: 72.27%] [G loss: 3.503407]\n",
            "9220 [D loss: 0.480363, acc.: 79.30%] [G loss: 3.555978]\n",
            "9240 [D loss: 0.498463, acc.: 73.83%] [G loss: 3.706298]\n",
            "9260 [D loss: 0.469741, acc.: 79.30%] [G loss: 3.531924]\n",
            "9280 [D loss: 0.444434, acc.: 79.30%] [G loss: 3.637574]\n",
            "9300 [D loss: 0.423451, acc.: 82.42%] [G loss: 3.541385]\n",
            "9320 [D loss: 0.499963, acc.: 76.56%] [G loss: 3.559286]\n",
            "9340 [D loss: 0.416240, acc.: 80.47%] [G loss: 3.797525]\n",
            "9360 [D loss: 0.417302, acc.: 84.38%] [G loss: 3.846467]\n",
            "9380 [D loss: 0.410308, acc.: 82.81%] [G loss: 3.794281]\n",
            "9400 [D loss: 0.476877, acc.: 76.56%] [G loss: 3.473633]\n",
            "9420 [D loss: 0.480294, acc.: 78.91%] [G loss: 3.886507]\n",
            "9440 [D loss: 0.541873, acc.: 75.78%] [G loss: 3.353763]\n",
            "9460 [D loss: 0.441027, acc.: 79.30%] [G loss: 3.885249]\n",
            "9480 [D loss: 0.424450, acc.: 82.42%] [G loss: 3.640411]\n",
            "9500 [D loss: 0.442000, acc.: 80.86%] [G loss: 3.742791]\n",
            "9520 [D loss: 0.480615, acc.: 75.39%] [G loss: 3.614785]\n",
            "9540 [D loss: 0.468502, acc.: 76.17%] [G loss: 3.505013]\n",
            "9560 [D loss: 0.529961, acc.: 74.22%] [G loss: 3.481314]\n",
            "9580 [D loss: 0.453475, acc.: 78.52%] [G loss: 3.720847]\n",
            "9600 [D loss: 0.417320, acc.: 82.81%] [G loss: 3.812128]\n",
            "9620 [D loss: 0.480710, acc.: 76.95%] [G loss: 3.492843]\n",
            "9640 [D loss: 0.413084, acc.: 82.03%] [G loss: 3.597260]\n",
            "9660 [D loss: 0.490834, acc.: 76.56%] [G loss: 3.754131]\n",
            "9680 [D loss: 0.420533, acc.: 82.42%] [G loss: 3.902893]\n",
            "9700 [D loss: 0.444246, acc.: 78.12%] [G loss: 3.635737]\n",
            "9720 [D loss: 0.463330, acc.: 76.17%] [G loss: 3.492645]\n",
            "9740 [D loss: 0.538957, acc.: 72.27%] [G loss: 3.601542]\n",
            "9760 [D loss: 0.075690, acc.: 99.61%] [G loss: 2.887484]\n",
            "9780 [D loss: 5.692799, acc.: 50.00%] [G loss: 20.929974]\n",
            "9800 [D loss: 1.648191, acc.: 28.91%] [G loss: 2.829832]\n",
            "9820 [D loss: 0.828910, acc.: 44.53%] [G loss: 1.650008]\n",
            "9840 [D loss: 1.092260, acc.: 33.20%] [G loss: 1.970044]\n",
            "9860 [D loss: 1.004513, acc.: 41.02%] [G loss: 2.084742]\n",
            "9880 [D loss: 0.539169, acc.: 71.09%] [G loss: 2.834425]\n",
            "9900 [D loss: 0.707695, acc.: 62.89%] [G loss: 2.336858]\n",
            "9920 [D loss: 0.658625, acc.: 63.67%] [G loss: 2.340637]\n",
            "9940 [D loss: 0.647891, acc.: 64.45%] [G loss: 2.158009]\n",
            "9960 [D loss: 0.685804, acc.: 61.33%] [G loss: 2.473566]\n",
            "9980 [D loss: 0.602388, acc.: 70.31%] [G loss: 2.429510]\n",
            "10000 [D loss: 0.654145, acc.: 64.06%] [G loss: 2.247046]\n",
            "10020 [D loss: 0.494766, acc.: 77.34%] [G loss: 2.727576]\n",
            "10040 [D loss: 0.662677, acc.: 63.28%] [G loss: 2.258549]\n",
            "10060 [D loss: 0.609546, acc.: 68.75%] [G loss: 2.300612]\n",
            "10080 [D loss: 0.569990, acc.: 71.09%] [G loss: 2.450339]\n",
            "10100 [D loss: 0.547706, acc.: 75.39%] [G loss: 2.387949]\n",
            "10120 [D loss: 0.569095, acc.: 71.09%] [G loss: 2.487194]\n",
            "10140 [D loss: 0.561028, acc.: 70.31%] [G loss: 2.486979]\n",
            "10160 [D loss: 0.552195, acc.: 71.09%] [G loss: 2.554817]\n",
            "10180 [D loss: 0.593082, acc.: 67.19%] [G loss: 2.529120]\n",
            "10200 [D loss: 0.576316, acc.: 72.27%] [G loss: 2.559456]\n",
            "10220 [D loss: 0.558676, acc.: 69.92%] [G loss: 2.721836]\n",
            "10240 [D loss: 0.597175, acc.: 68.75%] [G loss: 2.482705]\n",
            "10260 [D loss: 0.529595, acc.: 75.00%] [G loss: 2.593625]\n",
            "10280 [D loss: 0.518496, acc.: 76.95%] [G loss: 2.791225]\n",
            "10300 [D loss: 0.522853, acc.: 78.12%] [G loss: 2.676011]\n",
            "10320 [D loss: 0.512153, acc.: 71.88%] [G loss: 2.777105]\n",
            "10340 [D loss: 0.606712, acc.: 70.70%] [G loss: 2.554832]\n",
            "10360 [D loss: 0.530074, acc.: 73.44%] [G loss: 2.646863]\n",
            "10380 [D loss: 0.593403, acc.: 69.53%] [G loss: 2.687000]\n",
            "10400 [D loss: 0.532258, acc.: 76.17%] [G loss: 2.745467]\n",
            "10420 [D loss: 0.534076, acc.: 74.61%] [G loss: 2.780966]\n",
            "10440 [D loss: 0.534047, acc.: 73.44%] [G loss: 2.760104]\n",
            "10460 [D loss: 0.505410, acc.: 75.78%] [G loss: 2.753617]\n",
            "10480 [D loss: 0.642675, acc.: 63.67%] [G loss: 2.492256]\n",
            "10500 [D loss: 0.516558, acc.: 74.22%] [G loss: 3.033243]\n",
            "10520 [D loss: 0.516903, acc.: 76.17%] [G loss: 2.678423]\n",
            "10540 [D loss: 0.496295, acc.: 75.00%] [G loss: 2.817688]\n",
            "10560 [D loss: 0.598280, acc.: 67.58%] [G loss: 2.559930]\n",
            "10580 [D loss: 0.553124, acc.: 70.31%] [G loss: 2.761294]\n",
            "10600 [D loss: 0.525467, acc.: 76.95%] [G loss: 2.840541]\n",
            "10620 [D loss: 0.540733, acc.: 71.09%] [G loss: 2.639056]\n",
            "10640 [D loss: 0.584479, acc.: 72.66%] [G loss: 2.768480]\n",
            "10660 [D loss: 0.567955, acc.: 69.14%] [G loss: 2.720109]\n",
            "10680 [D loss: 0.515030, acc.: 72.66%] [G loss: 2.952698]\n",
            "10700 [D loss: 0.563594, acc.: 71.88%] [G loss: 2.791551]\n",
            "10720 [D loss: 0.478938, acc.: 79.30%] [G loss: 2.990060]\n",
            "10740 [D loss: 0.508863, acc.: 74.22%] [G loss: 2.961750]\n",
            "10760 [D loss: 0.505835, acc.: 71.48%] [G loss: 3.186454]\n",
            "10780 [D loss: 0.610738, acc.: 68.36%] [G loss: 2.757973]\n",
            "10800 [D loss: 0.560568, acc.: 71.48%] [G loss: 2.789414]\n",
            "10820 [D loss: 0.511078, acc.: 76.95%] [G loss: 3.160106]\n",
            "10840 [D loss: 0.592436, acc.: 67.97%] [G loss: 2.605905]\n",
            "10860 [D loss: 0.506564, acc.: 74.61%] [G loss: 2.964876]\n",
            "10880 [D loss: 0.542257, acc.: 73.44%] [G loss: 3.069125]\n",
            "10900 [D loss: 0.573360, acc.: 73.44%] [G loss: 2.879078]\n",
            "10920 [D loss: 0.600196, acc.: 69.92%] [G loss: 2.719522]\n",
            "10940 [D loss: 0.461892, acc.: 80.47%] [G loss: 3.380392]\n",
            "10960 [D loss: 0.513548, acc.: 74.22%] [G loss: 3.103308]\n",
            "10980 [D loss: 0.499782, acc.: 74.61%] [G loss: 3.020046]\n",
            "11000 [D loss: 0.504384, acc.: 73.44%] [G loss: 3.214390]\n",
            "11020 [D loss: 0.457019, acc.: 79.69%] [G loss: 2.937595]\n",
            "11040 [D loss: 0.577278, acc.: 70.31%] [G loss: 2.798201]\n",
            "11060 [D loss: 0.428710, acc.: 84.38%] [G loss: 2.586783]\n",
            "11080 [D loss: 0.438822, acc.: 80.08%] [G loss: 2.275178]\n",
            "11100 [D loss: 0.400906, acc.: 83.20%] [G loss: 1.972153]\n",
            "11120 [D loss: 0.825539, acc.: 54.69%] [G loss: 1.610461]\n",
            "11140 [D loss: 1.028281, acc.: 43.75%] [G loss: 2.108430]\n",
            "11160 [D loss: 0.553679, acc.: 72.66%] [G loss: 2.796394]\n",
            "11180 [D loss: 0.584088, acc.: 71.09%] [G loss: 3.584604]\n",
            "11200 [D loss: 0.502085, acc.: 73.83%] [G loss: 3.694035]\n",
            "11220 [D loss: 0.513685, acc.: 76.17%] [G loss: 3.040958]\n",
            "11240 [D loss: 0.537918, acc.: 71.09%] [G loss: 3.219967]\n",
            "11260 [D loss: 0.558961, acc.: 73.05%] [G loss: 3.074571]\n",
            "11280 [D loss: 0.498791, acc.: 77.34%] [G loss: 3.483498]\n",
            "11300 [D loss: 0.514491, acc.: 75.39%] [G loss: 3.159360]\n",
            "11320 [D loss: 0.447644, acc.: 82.03%] [G loss: 3.413441]\n",
            "11340 [D loss: 0.495441, acc.: 78.12%] [G loss: 3.310582]\n",
            "11360 [D loss: 0.562187, acc.: 72.66%] [G loss: 3.049346]\n",
            "11380 [D loss: 0.496492, acc.: 76.17%] [G loss: 3.484582]\n",
            "11400 [D loss: 0.502658, acc.: 78.52%] [G loss: 3.452566]\n",
            "11420 [D loss: 0.484848, acc.: 76.56%] [G loss: 3.459033]\n",
            "11440 [D loss: 0.458199, acc.: 81.25%] [G loss: 3.335121]\n",
            "11460 [D loss: 0.478068, acc.: 77.73%] [G loss: 3.167995]\n",
            "11480 [D loss: 0.540754, acc.: 74.22%] [G loss: 3.012886]\n",
            "11500 [D loss: 0.433257, acc.: 82.42%] [G loss: 3.197979]\n",
            "11520 [D loss: 0.503593, acc.: 75.00%] [G loss: 3.385596]\n",
            "11540 [D loss: 0.477698, acc.: 80.08%] [G loss: 3.538779]\n",
            "11560 [D loss: 0.470015, acc.: 78.52%] [G loss: 3.408746]\n",
            "11580 [D loss: 0.491108, acc.: 79.30%] [G loss: 3.475717]\n",
            "11600 [D loss: 0.513197, acc.: 75.00%] [G loss: 3.385620]\n",
            "11620 [D loss: 0.531967, acc.: 74.22%] [G loss: 3.338406]\n",
            "11640 [D loss: 0.553247, acc.: 68.75%] [G loss: 3.357586]\n",
            "11660 [D loss: 0.586373, acc.: 66.02%] [G loss: 3.200933]\n",
            "11680 [D loss: 0.523785, acc.: 72.27%] [G loss: 3.304983]\n",
            "11700 [D loss: 0.489436, acc.: 76.56%] [G loss: 3.281634]\n",
            "11720 [D loss: 0.406117, acc.: 80.47%] [G loss: 3.316993]\n",
            "11740 [D loss: 0.555237, acc.: 70.31%] [G loss: 3.343490]\n",
            "11760 [D loss: 0.634877, acc.: 67.58%] [G loss: 2.272147]\n",
            "11780 [D loss: 0.482355, acc.: 76.56%] [G loss: 3.667951]\n",
            "11800 [D loss: 0.467907, acc.: 78.12%] [G loss: 3.605880]\n",
            "11820 [D loss: 0.376903, acc.: 82.42%] [G loss: 3.831793]\n",
            "11840 [D loss: 0.377580, acc.: 84.38%] [G loss: 3.968134]\n",
            "11860 [D loss: 0.558823, acc.: 72.27%] [G loss: 3.308629]\n",
            "11880 [D loss: 0.527454, acc.: 75.39%] [G loss: 3.437979]\n",
            "11900 [D loss: 0.558592, acc.: 72.66%] [G loss: 3.170155]\n",
            "11920 [D loss: 0.445097, acc.: 80.08%] [G loss: 3.802868]\n",
            "11940 [D loss: 0.448828, acc.: 81.64%] [G loss: 3.771658]\n",
            "11960 [D loss: 0.582232, acc.: 68.75%] [G loss: 3.417302]\n",
            "11980 [D loss: 0.458710, acc.: 77.34%] [G loss: 3.597616]\n",
            "12000 [D loss: 0.601176, acc.: 67.19%] [G loss: 3.312770]\n",
            "12020 [D loss: 0.550594, acc.: 73.05%] [G loss: 3.575200]\n",
            "12040 [D loss: 0.481182, acc.: 78.12%] [G loss: 3.749516]\n",
            "12060 [D loss: 0.574483, acc.: 68.36%] [G loss: 3.514136]\n",
            "12080 [D loss: 0.456344, acc.: 80.86%] [G loss: 3.504912]\n",
            "12100 [D loss: 0.454615, acc.: 81.25%] [G loss: 3.602323]\n",
            "12120 [D loss: 0.460269, acc.: 80.08%] [G loss: 3.532269]\n",
            "12140 [D loss: 0.436744, acc.: 82.03%] [G loss: 3.648187]\n",
            "12160 [D loss: 0.471882, acc.: 79.30%] [G loss: 3.604219]\n",
            "12180 [D loss: 0.528623, acc.: 75.39%] [G loss: 3.391794]\n",
            "12200 [D loss: 0.521739, acc.: 73.05%] [G loss: 3.528623]\n",
            "12220 [D loss: 0.518787, acc.: 73.05%] [G loss: 3.711117]\n",
            "12240 [D loss: 0.524915, acc.: 74.61%] [G loss: 3.406554]\n",
            "12260 [D loss: 0.492490, acc.: 77.34%] [G loss: 3.784742]\n",
            "12280 [D loss: 0.481922, acc.: 78.91%] [G loss: 3.637299]\n",
            "12300 [D loss: 0.483391, acc.: 76.17%] [G loss: 3.937090]\n",
            "12320 [D loss: 0.405121, acc.: 83.20%] [G loss: 4.044754]\n",
            "12340 [D loss: 0.433446, acc.: 82.42%] [G loss: 3.908506]\n",
            "12360 [D loss: 0.468021, acc.: 76.56%] [G loss: 3.823442]\n",
            "12380 [D loss: 0.495495, acc.: 74.61%] [G loss: 3.682726]\n",
            "12400 [D loss: 0.473442, acc.: 78.12%] [G loss: 3.674625]\n",
            "12420 [D loss: 0.487721, acc.: 76.56%] [G loss: 3.745276]\n",
            "12440 [D loss: 0.458314, acc.: 80.47%] [G loss: 3.641819]\n",
            "12460 [D loss: 0.468982, acc.: 80.47%] [G loss: 3.564899]\n",
            "12480 [D loss: 0.471799, acc.: 78.52%] [G loss: 3.478067]\n",
            "12500 [D loss: 0.443356, acc.: 81.25%] [G loss: 3.782730]\n",
            "12520 [D loss: 0.267552, acc.: 92.19%] [G loss: 2.733687]\n",
            "12540 [D loss: 0.279462, acc.: 92.19%] [G loss: 2.474970]\n",
            "12560 [D loss: 0.418643, acc.: 83.98%] [G loss: 2.232884]\n",
            "12580 [D loss: 2.127001, acc.: 22.27%] [G loss: 2.805514]\n",
            "12600 [D loss: 0.610160, acc.: 69.14%] [G loss: 5.086184]\n",
            "12620 [D loss: 0.854765, acc.: 55.86%] [G loss: 4.086056]\n",
            "12640 [D loss: 0.533412, acc.: 75.00%] [G loss: 4.196053]\n",
            "12660 [D loss: 0.550413, acc.: 74.61%] [G loss: 3.780637]\n",
            "12680 [D loss: 0.460364, acc.: 76.56%] [G loss: 4.107439]\n",
            "12700 [D loss: 0.497505, acc.: 78.52%] [G loss: 3.905221]\n",
            "12720 [D loss: 0.406507, acc.: 82.42%] [G loss: 3.758475]\n",
            "12740 [D loss: 0.519526, acc.: 71.48%] [G loss: 3.724998]\n",
            "12760 [D loss: 0.409895, acc.: 79.69%] [G loss: 4.196256]\n",
            "12780 [D loss: 0.556780, acc.: 69.92%] [G loss: 3.428181]\n",
            "12800 [D loss: 0.438670, acc.: 79.30%] [G loss: 3.833969]\n",
            "12820 [D loss: 0.485493, acc.: 75.00%] [G loss: 3.582854]\n",
            "12840 [D loss: 0.397947, acc.: 83.59%] [G loss: 4.024882]\n",
            "12860 [D loss: 0.521532, acc.: 77.34%] [G loss: 3.943151]\n",
            "12880 [D loss: 0.418696, acc.: 82.81%] [G loss: 3.792245]\n",
            "12900 [D loss: 0.531393, acc.: 73.05%] [G loss: 3.825617]\n",
            "12920 [D loss: 0.443522, acc.: 79.69%] [G loss: 3.751585]\n",
            "12940 [D loss: 0.431672, acc.: 80.47%] [G loss: 4.389043]\n",
            "12960 [D loss: 0.491633, acc.: 75.39%] [G loss: 3.902747]\n",
            "12980 [D loss: 0.399342, acc.: 80.86%] [G loss: 4.212011]\n",
            "13000 [D loss: 0.507801, acc.: 77.73%] [G loss: 3.810790]\n",
            "13020 [D loss: 0.480246, acc.: 77.73%] [G loss: 3.697788]\n",
            "13040 [D loss: 0.370388, acc.: 83.59%] [G loss: 4.049839]\n",
            "13060 [D loss: 0.431218, acc.: 80.08%] [G loss: 3.692760]\n",
            "13080 [D loss: 0.410952, acc.: 79.69%] [G loss: 3.679608]\n",
            "13100 [D loss: 0.455696, acc.: 80.08%] [G loss: 3.866432]\n",
            "13120 [D loss: 0.483504, acc.: 76.17%] [G loss: 3.841898]\n",
            "13140 [D loss: 0.459208, acc.: 79.30%] [G loss: 3.776829]\n",
            "13160 [D loss: 0.446884, acc.: 76.56%] [G loss: 3.771619]\n",
            "13180 [D loss: 0.438652, acc.: 80.47%] [G loss: 4.107807]\n",
            "13200 [D loss: 0.452252, acc.: 76.17%] [G loss: 3.822512]\n",
            "13220 [D loss: 0.471437, acc.: 76.95%] [G loss: 3.641901]\n",
            "13240 [D loss: 0.396133, acc.: 82.03%] [G loss: 3.986142]\n",
            "13260 [D loss: 0.524639, acc.: 74.22%] [G loss: 3.575221]\n",
            "13280 [D loss: 0.408376, acc.: 82.42%] [G loss: 4.154051]\n",
            "13300 [D loss: 0.524124, acc.: 73.83%] [G loss: 3.651845]\n",
            "13320 [D loss: 0.458929, acc.: 78.12%] [G loss: 4.144563]\n",
            "13340 [D loss: 0.476265, acc.: 77.73%] [G loss: 3.788085]\n",
            "13360 [D loss: 0.523560, acc.: 71.88%] [G loss: 3.804731]\n",
            "13380 [D loss: 0.464960, acc.: 76.95%] [G loss: 4.016467]\n",
            "13400 [D loss: 0.375022, acc.: 84.38%] [G loss: 4.237887]\n",
            "13420 [D loss: 0.403924, acc.: 83.20%] [G loss: 4.041124]\n",
            "13440 [D loss: 0.526987, acc.: 74.61%] [G loss: 3.559883]\n",
            "13460 [D loss: 0.460677, acc.: 79.30%] [G loss: 4.015133]\n",
            "13480 [D loss: 0.497991, acc.: 75.39%] [G loss: 3.621460]\n",
            "13500 [D loss: 0.431094, acc.: 80.08%] [G loss: 4.390000]\n",
            "13520 [D loss: 0.415991, acc.: 84.38%] [G loss: 4.171952]\n",
            "13540 [D loss: 0.377993, acc.: 85.16%] [G loss: 4.033911]\n",
            "13560 [D loss: 0.392260, acc.: 83.20%] [G loss: 4.561698]\n",
            "13580 [D loss: 0.408987, acc.: 82.42%] [G loss: 4.263513]\n",
            "13600 [D loss: 0.420773, acc.: 80.47%] [G loss: 3.553739]\n",
            "13620 [D loss: 0.453506, acc.: 78.91%] [G loss: 2.382793]\n",
            "13640 [D loss: 0.389968, acc.: 83.20%] [G loss: 2.091710]\n",
            "13660 [D loss: 0.636574, acc.: 67.19%] [G loss: 4.395144]\n",
            "13680 [D loss: 0.962973, acc.: 49.22%] [G loss: 2.917119]\n",
            "13700 [D loss: 0.855389, acc.: 61.33%] [G loss: 3.769411]\n",
            "13720 [D loss: 0.519163, acc.: 76.17%] [G loss: 3.571222]\n",
            "13740 [D loss: 0.626328, acc.: 68.75%] [G loss: 4.528101]\n",
            "13760 [D loss: 0.428059, acc.: 78.52%] [G loss: 4.294826]\n",
            "13780 [D loss: 0.480818, acc.: 76.95%] [G loss: 3.835447]\n",
            "13800 [D loss: 0.417025, acc.: 80.47%] [G loss: 4.370798]\n",
            "13820 [D loss: 0.388842, acc.: 82.03%] [G loss: 4.373123]\n",
            "13840 [D loss: 0.459603, acc.: 79.30%] [G loss: 4.120314]\n",
            "13860 [D loss: 0.409253, acc.: 82.42%] [G loss: 4.211740]\n",
            "13880 [D loss: 0.467829, acc.: 76.56%] [G loss: 4.271527]\n",
            "13900 [D loss: 0.391466, acc.: 82.42%] [G loss: 4.592598]\n",
            "13920 [D loss: 0.456753, acc.: 77.73%] [G loss: 4.559702]\n",
            "13940 [D loss: 0.550370, acc.: 70.70%] [G loss: 4.036580]\n",
            "13960 [D loss: 0.451895, acc.: 81.64%] [G loss: 4.060725]\n",
            "13980 [D loss: 0.458239, acc.: 80.47%] [G loss: 4.304169]\n",
            "14000 [D loss: 0.411589, acc.: 80.47%] [G loss: 4.516422]\n",
            "14020 [D loss: 0.393971, acc.: 83.59%] [G loss: 4.454391]\n",
            "14040 [D loss: 0.444666, acc.: 80.47%] [G loss: 4.120130]\n",
            "14060 [D loss: 0.355202, acc.: 85.16%] [G loss: 4.485117]\n",
            "14080 [D loss: 0.454826, acc.: 78.12%] [G loss: 4.442460]\n",
            "14100 [D loss: 0.410516, acc.: 82.03%] [G loss: 4.475756]\n",
            "14120 [D loss: 0.472778, acc.: 78.12%] [G loss: 3.931965]\n",
            "14140 [D loss: 0.466029, acc.: 77.73%] [G loss: 4.563457]\n",
            "14160 [D loss: 0.413485, acc.: 80.08%] [G loss: 4.799966]\n",
            "14180 [D loss: 0.403529, acc.: 82.42%] [G loss: 4.347845]\n",
            "14200 [D loss: 0.454473, acc.: 78.12%] [G loss: 4.087527]\n",
            "14220 [D loss: 0.382325, acc.: 83.59%] [G loss: 4.307010]\n",
            "14240 [D loss: 0.450149, acc.: 80.47%] [G loss: 4.313926]\n",
            "14260 [D loss: 0.413304, acc.: 80.86%] [G loss: 4.449327]\n",
            "14280 [D loss: 0.382710, acc.: 83.98%] [G loss: 4.414242]\n",
            "14300 [D loss: 0.467978, acc.: 76.95%] [G loss: 4.779026]\n",
            "14320 [D loss: 0.344738, acc.: 87.89%] [G loss: 4.542480]\n",
            "14340 [D loss: 0.432543, acc.: 78.12%] [G loss: 4.478060]\n",
            "14360 [D loss: 0.408496, acc.: 83.98%] [G loss: 4.632874]\n",
            "14380 [D loss: 0.484531, acc.: 77.34%] [G loss: 4.199874]\n",
            "14400 [D loss: 0.404921, acc.: 80.86%] [G loss: 4.388925]\n",
            "14420 [D loss: 0.317959, acc.: 87.11%] [G loss: 4.637698]\n",
            "14440 [D loss: 0.398399, acc.: 80.08%] [G loss: 4.384537]\n",
            "14460 [D loss: 0.407728, acc.: 80.47%] [G loss: 4.207868]\n",
            "14480 [D loss: 0.410080, acc.: 82.42%] [G loss: 4.652514]\n",
            "14500 [D loss: 0.452174, acc.: 77.34%] [G loss: 4.646966]\n",
            "14520 [D loss: 0.460235, acc.: 78.52%] [G loss: 4.154096]\n",
            "14540 [D loss: 0.327524, acc.: 88.67%] [G loss: 5.008009]\n",
            "14560 [D loss: 0.438357, acc.: 80.47%] [G loss: 4.326036]\n",
            "14580 [D loss: 0.389894, acc.: 85.55%] [G loss: 4.468609]\n",
            "14600 [D loss: 0.446763, acc.: 80.86%] [G loss: 4.078653]\n",
            "14620 [D loss: 0.377946, acc.: 82.81%] [G loss: 4.476561]\n",
            "14640 [D loss: 0.384009, acc.: 83.20%] [G loss: 4.634953]\n",
            "14660 [D loss: 0.400583, acc.: 80.47%] [G loss: 4.536296]\n",
            "14680 [D loss: 0.356729, acc.: 84.77%] [G loss: 4.704243]\n",
            "14700 [D loss: 0.385321, acc.: 82.81%] [G loss: 4.518253]\n",
            "14720 [D loss: 0.350126, acc.: 87.11%] [G loss: 4.723076]\n",
            "14740 [D loss: 0.494040, acc.: 75.78%] [G loss: 4.197428]\n",
            "14760 [D loss: 0.412424, acc.: 81.25%] [G loss: 4.558945]\n",
            "14780 [D loss: 0.379332, acc.: 85.55%] [G loss: 4.530576]\n",
            "14800 [D loss: 0.420614, acc.: 79.30%] [G loss: 5.033054]\n",
            "14820 [D loss: 0.423683, acc.: 77.73%] [G loss: 4.824743]\n",
            "14840 [D loss: 0.467983, acc.: 76.17%] [G loss: 4.450193]\n",
            "14860 [D loss: 0.358038, acc.: 82.03%] [G loss: 4.587168]\n",
            "14880 [D loss: 0.405634, acc.: 81.64%] [G loss: 4.847033]\n",
            "14900 [D loss: 0.381700, acc.: 83.20%] [G loss: 4.657835]\n",
            "14920 [D loss: 0.401079, acc.: 81.25%] [G loss: 4.489391]\n",
            "14940 [D loss: 0.443734, acc.: 78.91%] [G loss: 4.617713]\n",
            "14960 [D loss: 0.372071, acc.: 82.81%] [G loss: 4.999284]\n",
            "14980 [D loss: 0.301213, acc.: 86.72%] [G loss: 5.016478]\n",
            "15000 [D loss: 0.405135, acc.: 82.81%] [G loss: 4.759830]\n",
            "15020 [D loss: 0.446537, acc.: 78.52%] [G loss: 4.809000]\n",
            "15040 [D loss: 0.333094, acc.: 87.11%] [G loss: 4.914985]\n",
            "15060 [D loss: 0.445180, acc.: 78.52%] [G loss: 4.528184]\n",
            "15080 [D loss: 0.407463, acc.: 81.25%] [G loss: 4.510050]\n",
            "15100 [D loss: 0.448690, acc.: 81.64%] [G loss: 4.380229]\n",
            "15120 [D loss: 0.316365, acc.: 85.94%] [G loss: 5.132348]\n",
            "15140 [D loss: 0.415522, acc.: 81.64%] [G loss: 4.816623]\n",
            "15160 [D loss: 0.397213, acc.: 80.47%] [G loss: 4.571545]\n",
            "15180 [D loss: 0.392911, acc.: 82.81%] [G loss: 5.178292]\n",
            "15200 [D loss: 0.484244, acc.: 81.25%] [G loss: 4.215808]\n",
            "15220 [D loss: 0.425695, acc.: 81.25%] [G loss: 4.168875]\n",
            "15240 [D loss: 0.283574, acc.: 89.45%] [G loss: 5.103979]\n",
            "15260 [D loss: 0.331579, acc.: 87.11%] [G loss: 3.839583]\n",
            "15280 [D loss: 0.281808, acc.: 87.89%] [G loss: 3.469316]\n",
            "15300 [D loss: 0.180982, acc.: 95.31%] [G loss: 2.936918]\n",
            "15320 [D loss: 1.973832, acc.: 26.17%] [G loss: 3.336789]\n",
            "15340 [D loss: 0.355037, acc.: 87.50%] [G loss: 22.760210]\n",
            "15360 [D loss: 0.114469, acc.: 96.48%] [G loss: 8.720812]\n",
            "15380 [D loss: 0.039592, acc.: 98.44%] [G loss: 24.643204]\n",
            "15400 [D loss: 0.045145, acc.: 99.22%] [G loss: 26.758783]\n",
            "15420 [D loss: 0.013849, acc.: 99.61%] [G loss: 25.276173]\n",
            "15440 [D loss: 0.148065, acc.: 96.09%] [G loss: 21.821688]\n",
            "15460 [D loss: 0.263325, acc.: 91.80%] [G loss: 12.789566]\n",
            "15480 [D loss: 0.672538, acc.: 76.95%] [G loss: 10.205158]\n",
            "15500 [D loss: 0.356496, acc.: 83.59%] [G loss: 7.494583]\n",
            "15520 [D loss: 0.483212, acc.: 76.56%] [G loss: 5.564304]\n",
            "15540 [D loss: 0.566284, acc.: 78.12%] [G loss: 5.392528]\n",
            "15560 [D loss: 0.632458, acc.: 70.31%] [G loss: 4.831096]\n",
            "15580 [D loss: 0.565801, acc.: 73.05%] [G loss: 4.522705]\n",
            "15600 [D loss: 0.528564, acc.: 76.56%] [G loss: 4.318461]\n",
            "15620 [D loss: 0.600787, acc.: 75.00%] [G loss: 4.668165]\n",
            "15640 [D loss: 0.499380, acc.: 75.39%] [G loss: 4.677675]\n",
            "15660 [D loss: 0.305748, acc.: 87.50%] [G loss: 5.234561]\n",
            "15680 [D loss: 0.446193, acc.: 78.52%] [G loss: 4.888513]\n",
            "15700 [D loss: 0.422889, acc.: 80.86%] [G loss: 4.305991]\n",
            "15720 [D loss: 0.377699, acc.: 85.16%] [G loss: 4.577327]\n",
            "15740 [D loss: 0.476847, acc.: 77.34%] [G loss: 4.347926]\n",
            "15760 [D loss: 0.409478, acc.: 82.03%] [G loss: 4.605901]\n",
            "15780 [D loss: 0.476504, acc.: 77.73%] [G loss: 4.237090]\n",
            "15800 [D loss: 0.468352, acc.: 83.59%] [G loss: 4.230949]\n",
            "15820 [D loss: 0.413374, acc.: 82.03%] [G loss: 4.534577]\n",
            "15840 [D loss: 0.365337, acc.: 85.55%] [G loss: 4.633649]\n",
            "15860 [D loss: 0.464491, acc.: 77.34%] [G loss: 4.144034]\n",
            "15880 [D loss: 0.362495, acc.: 85.94%] [G loss: 4.593099]\n",
            "15900 [D loss: 0.403127, acc.: 82.81%] [G loss: 4.054631]\n",
            "15920 [D loss: 0.470624, acc.: 78.52%] [G loss: 4.096116]\n",
            "15940 [D loss: 0.456184, acc.: 76.17%] [G loss: 4.078807]\n",
            "15960 [D loss: 0.472979, acc.: 79.69%] [G loss: 3.838728]\n",
            "15980 [D loss: 0.394791, acc.: 81.64%] [G loss: 4.227736]\n",
            "16000 [D loss: 0.410426, acc.: 80.47%] [G loss: 4.118171]\n",
            "16020 [D loss: 0.403907, acc.: 83.59%] [G loss: 3.834717]\n",
            "16040 [D loss: 0.396157, acc.: 83.98%] [G loss: 4.013658]\n",
            "16060 [D loss: 0.458667, acc.: 78.52%] [G loss: 4.104383]\n",
            "16080 [D loss: 0.470980, acc.: 77.73%] [G loss: 3.744132]\n",
            "16100 [D loss: 0.419118, acc.: 82.42%] [G loss: 3.929239]\n",
            "16120 [D loss: 0.389899, acc.: 83.98%] [G loss: 4.078700]\n",
            "16140 [D loss: 0.472173, acc.: 77.34%] [G loss: 3.796878]\n",
            "16160 [D loss: 0.396144, acc.: 82.03%] [G loss: 3.977724]\n",
            "16180 [D loss: 0.461760, acc.: 78.12%] [G loss: 3.691078]\n",
            "16200 [D loss: 0.470975, acc.: 77.34%] [G loss: 3.641277]\n",
            "16220 [D loss: 0.417363, acc.: 83.20%] [G loss: 4.022133]\n",
            "16240 [D loss: 0.502100, acc.: 75.39%] [G loss: 3.536461]\n",
            "16260 [D loss: 0.440649, acc.: 75.78%] [G loss: 4.004407]\n",
            "16280 [D loss: 0.403196, acc.: 81.64%] [G loss: 4.128658]\n",
            "16300 [D loss: 0.350340, acc.: 87.89%] [G loss: 4.180054]\n",
            "16320 [D loss: 0.432894, acc.: 81.25%] [G loss: 3.983349]\n",
            "16340 [D loss: 0.428293, acc.: 79.30%] [G loss: 4.033486]\n",
            "16360 [D loss: 0.431206, acc.: 79.69%] [G loss: 4.166407]\n",
            "16380 [D loss: 0.396502, acc.: 82.03%] [G loss: 4.265548]\n",
            "16400 [D loss: 0.394901, acc.: 81.25%] [G loss: 4.149632]\n",
            "16420 [D loss: 0.465461, acc.: 81.64%] [G loss: 4.062462]\n",
            "16440 [D loss: 0.407385, acc.: 82.03%] [G loss: 3.997718]\n",
            "16460 [D loss: 0.369773, acc.: 84.77%] [G loss: 4.051058]\n",
            "16480 [D loss: 0.368304, acc.: 84.38%] [G loss: 4.245140]\n",
            "16500 [D loss: 0.396917, acc.: 82.03%] [G loss: 4.277501]\n",
            "16520 [D loss: 0.422002, acc.: 82.81%] [G loss: 4.220487]\n",
            "16540 [D loss: 0.370488, acc.: 87.11%] [G loss: 4.362805]\n",
            "16560 [D loss: 0.411729, acc.: 80.86%] [G loss: 4.398872]\n",
            "16580 [D loss: 0.523147, acc.: 74.61%] [G loss: 3.765893]\n",
            "16600 [D loss: 0.372000, acc.: 85.16%] [G loss: 4.469404]\n",
            "16620 [D loss: 0.461578, acc.: 81.25%] [G loss: 3.902834]\n",
            "16640 [D loss: 0.438316, acc.: 76.56%] [G loss: 4.166046]\n",
            "16660 [D loss: 0.377919, acc.: 81.25%] [G loss: 4.026049]\n",
            "16680 [D loss: 0.426993, acc.: 79.30%] [G loss: 3.937635]\n",
            "16700 [D loss: 0.426392, acc.: 81.64%] [G loss: 4.113935]\n",
            "16720 [D loss: 0.447690, acc.: 81.64%] [G loss: 4.111579]\n",
            "16740 [D loss: 0.448698, acc.: 75.00%] [G loss: 4.232028]\n",
            "16760 [D loss: 0.376937, acc.: 84.77%] [G loss: 4.472915]\n",
            "16780 [D loss: 0.314462, acc.: 85.94%] [G loss: 4.622740]\n",
            "16800 [D loss: 0.465161, acc.: 78.52%] [G loss: 4.191881]\n",
            "16820 [D loss: 0.347665, acc.: 85.16%] [G loss: 3.494469]\n",
            "16840 [D loss: 0.275750, acc.: 89.84%] [G loss: 4.684105]\n",
            "16860 [D loss: 0.467895, acc.: 81.25%] [G loss: 4.010959]\n",
            "16880 [D loss: 0.375039, acc.: 85.16%] [G loss: 4.684514]\n",
            "16900 [D loss: 0.564022, acc.: 74.61%] [G loss: 4.355430]\n",
            "16920 [D loss: 0.497490, acc.: 75.39%] [G loss: 3.775622]\n",
            "16940 [D loss: 0.338178, acc.: 89.45%] [G loss: 4.210439]\n",
            "16960 [D loss: 0.332088, acc.: 86.33%] [G loss: 3.825176]\n",
            "16980 [D loss: 0.267838, acc.: 91.02%] [G loss: 4.194290]\n",
            "17000 [D loss: 0.303272, acc.: 86.72%] [G loss: 4.212486]\n",
            "17020 [D loss: 0.238156, acc.: 90.62%] [G loss: 4.574352]\n",
            "17040 [D loss: 0.345303, acc.: 86.33%] [G loss: 2.711638]\n",
            "17060 [D loss: 0.443289, acc.: 78.91%] [G loss: 3.408890]\n",
            "17080 [D loss: 0.469691, acc.: 77.34%] [G loss: 3.509233]\n",
            "17100 [D loss: 0.337558, acc.: 86.72%] [G loss: 2.554925]\n",
            "17120 [D loss: 0.390174, acc.: 82.81%] [G loss: 2.524773]\n",
            "17140 [D loss: 0.660303, acc.: 66.80%] [G loss: 4.898804]\n",
            "17160 [D loss: 0.454139, acc.: 79.30%] [G loss: 4.579131]\n",
            "17180 [D loss: 0.309417, acc.: 87.11%] [G loss: 5.342544]\n",
            "17200 [D loss: 0.375127, acc.: 83.20%] [G loss: 4.647982]\n",
            "17220 [D loss: 0.415903, acc.: 81.25%] [G loss: 4.575088]\n",
            "17240 [D loss: 0.376270, acc.: 82.03%] [G loss: 4.697979]\n",
            "17260 [D loss: 0.402311, acc.: 80.08%] [G loss: 4.412364]\n",
            "17280 [D loss: 0.436198, acc.: 80.86%] [G loss: 4.492314]\n",
            "17300 [D loss: 0.385610, acc.: 82.03%] [G loss: 4.628658]\n",
            "17320 [D loss: 0.471901, acc.: 77.34%] [G loss: 4.172570]\n",
            "17340 [D loss: 0.356042, acc.: 84.38%] [G loss: 4.400015]\n",
            "17360 [D loss: 0.334195, acc.: 86.33%] [G loss: 4.764472]\n",
            "17380 [D loss: 0.446743, acc.: 77.34%] [G loss: 4.303770]\n",
            "17400 [D loss: 0.361925, acc.: 83.59%] [G loss: 4.612856]\n",
            "17420 [D loss: 0.371195, acc.: 84.77%] [G loss: 4.595983]\n",
            "17440 [D loss: 0.355086, acc.: 83.59%] [G loss: 4.632392]\n",
            "17460 [D loss: 0.475611, acc.: 79.69%] [G loss: 4.229058]\n",
            "17480 [D loss: 0.352738, acc.: 86.72%] [G loss: 4.687152]\n",
            "17500 [D loss: 0.435411, acc.: 78.91%] [G loss: 4.629483]\n",
            "17520 [D loss: 0.381297, acc.: 81.25%] [G loss: 4.983884]\n",
            "17540 [D loss: 0.482103, acc.: 79.69%] [G loss: 4.196619]\n",
            "17560 [D loss: 0.369670, acc.: 84.77%] [G loss: 4.496827]\n",
            "17580 [D loss: 0.333047, acc.: 85.94%] [G loss: 4.997783]\n",
            "17600 [D loss: 0.343837, acc.: 85.16%] [G loss: 4.920817]\n",
            "17620 [D loss: 0.427327, acc.: 80.08%] [G loss: 4.626046]\n",
            "17640 [D loss: 0.352310, acc.: 83.20%] [G loss: 4.878919]\n",
            "17660 [D loss: 0.409480, acc.: 82.42%] [G loss: 4.790177]\n",
            "17680 [D loss: 0.333181, acc.: 85.55%] [G loss: 5.087419]\n",
            "17700 [D loss: 0.408203, acc.: 80.86%] [G loss: 5.017757]\n",
            "17720 [D loss: 0.415932, acc.: 80.47%] [G loss: 4.724635]\n",
            "17740 [D loss: 0.368175, acc.: 83.20%] [G loss: 4.682936]\n",
            "17760 [D loss: 0.376353, acc.: 84.38%] [G loss: 4.769293]\n",
            "17780 [D loss: 0.429929, acc.: 76.95%] [G loss: 4.796968]\n",
            "17800 [D loss: 0.473236, acc.: 78.52%] [G loss: 4.813396]\n",
            "17820 [D loss: 0.417735, acc.: 82.03%] [G loss: 4.430888]\n",
            "17840 [D loss: 0.373402, acc.: 83.98%] [G loss: 4.528617]\n",
            "17860 [D loss: 0.345770, acc.: 85.94%] [G loss: 4.956656]\n",
            "17880 [D loss: 0.338973, acc.: 86.33%] [G loss: 5.045628]\n",
            "17900 [D loss: 0.423791, acc.: 79.69%] [G loss: 4.744628]\n",
            "17920 [D loss: 0.372426, acc.: 84.77%] [G loss: 5.049957]\n",
            "17940 [D loss: 0.339453, acc.: 83.59%] [G loss: 4.938129]\n",
            "17960 [D loss: 0.363632, acc.: 84.38%] [G loss: 4.684995]\n",
            "17980 [D loss: 0.379499, acc.: 84.38%] [G loss: 5.128147]\n",
            "18000 [D loss: 0.336774, acc.: 85.55%] [G loss: 5.053657]\n",
            "18020 [D loss: 0.506990, acc.: 75.39%] [G loss: 4.810893]\n",
            "18040 [D loss: 0.302509, acc.: 88.67%] [G loss: 5.096052]\n",
            "18060 [D loss: 0.393373, acc.: 82.03%] [G loss: 5.065316]\n",
            "18080 [D loss: 0.344667, acc.: 85.55%] [G loss: 5.073158]\n",
            "18100 [D loss: 0.388187, acc.: 82.03%] [G loss: 4.646010]\n",
            "18120 [D loss: 0.410525, acc.: 83.20%] [G loss: 4.900289]\n",
            "18140 [D loss: 0.343291, acc.: 83.98%] [G loss: 5.509571]\n",
            "18160 [D loss: 0.388012, acc.: 79.30%] [G loss: 4.658406]\n",
            "18180 [D loss: 0.343937, acc.: 85.16%] [G loss: 5.183447]\n",
            "18200 [D loss: 0.417071, acc.: 84.77%] [G loss: 4.942852]\n",
            "18220 [D loss: 0.359865, acc.: 82.42%] [G loss: 5.142763]\n",
            "18240 [D loss: 0.284724, acc.: 89.45%] [G loss: 5.647150]\n",
            "18260 [D loss: 0.403953, acc.: 82.03%] [G loss: 5.077285]\n",
            "18280 [D loss: 0.543199, acc.: 72.27%] [G loss: 4.641413]\n",
            "18300 [D loss: 0.329372, acc.: 87.50%] [G loss: 5.351993]\n",
            "18320 [D loss: 0.418976, acc.: 80.08%] [G loss: 5.186482]\n",
            "18340 [D loss: 0.333733, acc.: 85.94%] [G loss: 5.363000]\n",
            "18360 [D loss: 0.282334, acc.: 88.67%] [G loss: 5.339027]\n",
            "18380 [D loss: 0.392878, acc.: 81.64%] [G loss: 5.158883]\n",
            "18400 [D loss: 0.349308, acc.: 84.38%] [G loss: 5.485016]\n",
            "18420 [D loss: 0.383218, acc.: 84.38%] [G loss: 5.024354]\n",
            "18440 [D loss: 0.356120, acc.: 82.81%] [G loss: 4.991132]\n",
            "18460 [D loss: 0.368678, acc.: 82.42%] [G loss: 5.072590]\n",
            "18480 [D loss: 0.331462, acc.: 84.77%] [G loss: 5.355572]\n",
            "18500 [D loss: 0.351496, acc.: 81.64%] [G loss: 5.309839]\n",
            "18520 [D loss: 0.355765, acc.: 85.94%] [G loss: 5.163082]\n",
            "18540 [D loss: 0.389700, acc.: 83.59%] [G loss: 5.202000]\n",
            "18560 [D loss: 0.343966, acc.: 83.98%] [G loss: 5.355763]\n",
            "18580 [D loss: 0.297284, acc.: 87.50%] [G loss: 5.327144]\n",
            "18600 [D loss: 0.408938, acc.: 83.20%] [G loss: 5.028829]\n",
            "18620 [D loss: 0.375919, acc.: 81.64%] [G loss: 5.408983]\n",
            "18640 [D loss: 0.305664, acc.: 87.50%] [G loss: 5.433725]\n",
            "18660 [D loss: 0.381210, acc.: 84.77%] [G loss: 5.358838]\n",
            "18680 [D loss: 0.360368, acc.: 83.20%] [G loss: 5.086848]\n",
            "18700 [D loss: 0.347371, acc.: 83.59%] [G loss: 5.428819]\n",
            "18720 [D loss: 0.378408, acc.: 84.38%] [G loss: 4.609510]\n",
            "18740 [D loss: 0.341120, acc.: 83.98%] [G loss: 4.633071]\n",
            "18760 [D loss: 0.480039, acc.: 78.91%] [G loss: 6.019763]\n",
            "18780 [D loss: 0.447178, acc.: 80.47%] [G loss: 4.685143]\n",
            "18800 [D loss: 0.273395, acc.: 90.23%] [G loss: 4.710984]\n",
            "18820 [D loss: 0.354176, acc.: 84.77%] [G loss: 4.707300]\n",
            "18840 [D loss: 0.389484, acc.: 82.03%] [G loss: 5.090184]\n",
            "18860 [D loss: 0.715890, acc.: 67.58%] [G loss: 4.274816]\n",
            "18880 [D loss: 0.371055, acc.: 82.42%] [G loss: 6.331087]\n",
            "18900 [D loss: 0.464095, acc.: 80.47%] [G loss: 5.519727]\n",
            "18920 [D loss: 0.365281, acc.: 86.33%] [G loss: 6.115959]\n",
            "18940 [D loss: 0.220807, acc.: 94.14%] [G loss: 6.005662]\n",
            "18960 [D loss: 0.329749, acc.: 85.94%] [G loss: 5.519472]\n",
            "18980 [D loss: 0.343070, acc.: 85.55%] [G loss: 5.627805]\n",
            "19000 [D loss: 0.335882, acc.: 85.16%] [G loss: 5.499103]\n",
            "19020 [D loss: 0.315857, acc.: 86.72%] [G loss: 5.712449]\n",
            "19040 [D loss: 0.351919, acc.: 83.98%] [G loss: 5.611444]\n",
            "19060 [D loss: 0.338344, acc.: 85.94%] [G loss: 5.981630]\n",
            "19080 [D loss: 0.399530, acc.: 83.20%] [G loss: 5.165501]\n",
            "19100 [D loss: 0.315560, acc.: 86.33%] [G loss: 5.916917]\n",
            "19120 [D loss: 0.324799, acc.: 86.33%] [G loss: 5.785566]\n",
            "19140 [D loss: 0.308823, acc.: 89.45%] [G loss: 5.409524]\n",
            "19160 [D loss: 0.396729, acc.: 81.25%] [G loss: 6.231946]\n",
            "19180 [D loss: 0.331192, acc.: 86.72%] [G loss: 5.616512]\n",
            "19200 [D loss: 0.361870, acc.: 84.38%] [G loss: 5.604869]\n",
            "19220 [D loss: 0.321393, acc.: 85.55%] [G loss: 5.549469]\n",
            "19240 [D loss: 0.330566, acc.: 85.94%] [G loss: 5.742742]\n",
            "19260 [D loss: 0.366735, acc.: 83.59%] [G loss: 5.117493]\n",
            "19280 [D loss: 0.378818, acc.: 81.64%] [G loss: 5.336864]\n",
            "19300 [D loss: 0.315701, acc.: 86.33%] [G loss: 5.532146]\n",
            "19320 [D loss: 0.315825, acc.: 86.72%] [G loss: 5.684618]\n",
            "19340 [D loss: 0.305784, acc.: 86.72%] [G loss: 5.818166]\n",
            "19360 [D loss: 0.392378, acc.: 84.38%] [G loss: 5.630092]\n",
            "19380 [D loss: 0.352400, acc.: 83.59%] [G loss: 5.644225]\n",
            "19400 [D loss: 0.370721, acc.: 83.98%] [G loss: 5.539282]\n",
            "19420 [D loss: 0.271473, acc.: 89.06%] [G loss: 5.780265]\n",
            "19440 [D loss: 0.376457, acc.: 81.64%] [G loss: 5.628087]\n",
            "19460 [D loss: 0.304724, acc.: 88.28%] [G loss: 5.617555]\n",
            "19480 [D loss: 0.280020, acc.: 90.23%] [G loss: 6.238982]\n",
            "19500 [D loss: 0.338884, acc.: 85.55%] [G loss: 6.099956]\n",
            "19520 [D loss: 0.343331, acc.: 85.94%] [G loss: 5.563714]\n",
            "19540 [D loss: 0.331784, acc.: 85.55%] [G loss: 5.591521]\n",
            "19560 [D loss: 0.402533, acc.: 82.81%] [G loss: 5.287437]\n",
            "19580 [D loss: 0.333936, acc.: 86.72%] [G loss: 5.676760]\n",
            "19600 [D loss: 0.321474, acc.: 84.38%] [G loss: 5.573879]\n",
            "19620 [D loss: 0.305258, acc.: 87.11%] [G loss: 6.335135]\n",
            "19640 [D loss: 0.336393, acc.: 84.38%] [G loss: 5.837489]\n",
            "19660 [D loss: 0.393416, acc.: 79.30%] [G loss: 5.341193]\n",
            "19680 [D loss: 0.301961, acc.: 89.06%] [G loss: 6.078883]\n",
            "19700 [D loss: 0.404923, acc.: 82.42%] [G loss: 5.566249]\n",
            "19720 [D loss: 0.315245, acc.: 87.50%] [G loss: 6.138768]\n",
            "19740 [D loss: 0.360647, acc.: 83.98%] [G loss: 5.629126]\n",
            "19760 [D loss: 0.339654, acc.: 86.72%] [G loss: 5.500765]\n",
            "19780 [D loss: 0.366257, acc.: 84.77%] [G loss: 5.519613]\n",
            "19800 [D loss: 0.339945, acc.: 85.16%] [G loss: 6.291703]\n",
            "19820 [D loss: 0.362322, acc.: 85.16%] [G loss: 5.726030]\n",
            "19840 [D loss: 0.392342, acc.: 83.59%] [G loss: 5.698299]\n",
            "19860 [D loss: 0.251272, acc.: 90.62%] [G loss: 6.732767]\n",
            "19880 [D loss: 0.251159, acc.: 89.45%] [G loss: 5.951423]\n",
            "19900 [D loss: 0.356037, acc.: 83.59%] [G loss: 6.022497]\n",
            "19920 [D loss: 0.357246, acc.: 83.20%] [G loss: 5.826564]\n",
            "19940 [D loss: 0.323184, acc.: 83.59%] [G loss: 5.969613]\n",
            "19960 [D loss: 0.282350, acc.: 89.45%] [G loss: 6.393616]\n",
            "19980 [D loss: 0.352903, acc.: 84.38%] [G loss: 5.818161]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7OvsALJTfRi",
        "outputId": "12a355d3-9d4b-49ca-ad2b-aba5aa6f7fad"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 48, 48, 128)  3328        input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 100)          0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          51712       flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 2304)      16128       input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 1152)         0           leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 2304)         0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3968)         0           dropout_2[0][0]                  \n",
            "                                                                 flatten_10[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 1)            3969        concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 4608)              465408    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)   (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 12, 12, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 48, 48, 1)         4609      \n",
            "=================================================================\n",
            "Total params: 1,256,833\n",
            "Trainable params: 1,256,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.692794, acc.: 41.80%] [G loss: 1.363244]\n",
            "20 [D loss: 0.997228, acc.: 49.22%] [G loss: 1.079654]\n",
            "40 [D loss: 10.192451, acc.: 38.28%] [G loss: 16.920563]\n",
            "60 [D loss: 4.623682, acc.: 29.30%] [G loss: 7.144755]\n",
            "80 [D loss: 0.145405, acc.: 96.88%] [G loss: 3.568455]\n",
            "100 [D loss: 0.171819, acc.: 96.09%] [G loss: 4.876417]\n",
            "120 [D loss: 0.059815, acc.: 100.00%] [G loss: 8.702084]\n",
            "140 [D loss: 0.277375, acc.: 89.45%] [G loss: 4.059882]\n",
            "160 [D loss: 0.139765, acc.: 94.53%] [G loss: 6.267387]\n",
            "180 [D loss: 0.098693, acc.: 96.48%] [G loss: 7.450905]\n",
            "200 [D loss: 0.075423, acc.: 96.88%] [G loss: 9.491319]\n",
            "220 [D loss: 0.860310, acc.: 76.17%] [G loss: 7.120339]\n",
            "240 [D loss: 0.021870, acc.: 100.00%] [G loss: 10.763273]\n",
            "260 [D loss: 0.283057, acc.: 98.83%] [G loss: 4.523032]\n",
            "280 [D loss: 0.064968, acc.: 99.22%] [G loss: 8.937262]\n",
            "300 [D loss: 0.257237, acc.: 98.05%] [G loss: 5.262499]\n",
            "320 [D loss: 0.638951, acc.: 46.88%] [G loss: 5.962174]\n",
            "340 [D loss: 0.419628, acc.: 89.84%] [G loss: 4.417611]\n",
            "360 [D loss: 0.347575, acc.: 93.36%] [G loss: 3.552187]\n",
            "380 [D loss: 0.969137, acc.: 64.84%] [G loss: 2.598330]\n",
            "400 [D loss: 0.632267, acc.: 78.52%] [G loss: 3.372662]\n",
            "420 [D loss: 0.525285, acc.: 83.98%] [G loss: 2.757557]\n",
            "440 [D loss: 0.400338, acc.: 88.28%] [G loss: 5.184896]\n",
            "460 [D loss: 0.484532, acc.: 89.84%] [G loss: 5.033338]\n",
            "480 [D loss: 0.526991, acc.: 77.34%] [G loss: 3.353721]\n",
            "500 [D loss: 0.484876, acc.: 87.11%] [G loss: 3.852768]\n",
            "520 [D loss: 0.238678, acc.: 94.14%] [G loss: 6.053322]\n",
            "540 [D loss: 0.310701, acc.: 91.41%] [G loss: 4.480524]\n",
            "560 [D loss: 0.386147, acc.: 92.19%] [G loss: 5.068025]\n",
            "580 [D loss: 0.441447, acc.: 82.42%] [G loss: 8.714972]\n",
            "600 [D loss: 2.068088, acc.: 35.16%] [G loss: 5.693320]\n",
            "620 [D loss: 0.076214, acc.: 96.88%] [G loss: 15.504116]\n",
            "640 [D loss: 0.402508, acc.: 89.84%] [G loss: 5.015415]\n",
            "660 [D loss: 0.291974, acc.: 86.33%] [G loss: 6.800432]\n",
            "680 [D loss: 0.365668, acc.: 85.94%] [G loss: 4.809776]\n",
            "700 [D loss: 0.375825, acc.: 87.89%] [G loss: 7.323451]\n",
            "720 [D loss: 0.584250, acc.: 77.34%] [G loss: 3.898723]\n",
            "740 [D loss: 0.480237, acc.: 77.34%] [G loss: 3.939720]\n",
            "760 [D loss: 0.615871, acc.: 68.75%] [G loss: 3.039476]\n",
            "780 [D loss: 0.518439, acc.: 75.39%] [G loss: 3.012846]\n",
            "800 [D loss: 0.591483, acc.: 67.58%] [G loss: 2.936316]\n",
            "820 [D loss: 0.580957, acc.: 68.75%] [G loss: 3.117126]\n",
            "840 [D loss: 0.453159, acc.: 84.77%] [G loss: 3.259461]\n",
            "860 [D loss: 0.419408, acc.: 83.20%] [G loss: 4.634182]\n",
            "880 [D loss: 0.313598, acc.: 90.62%] [G loss: 4.173212]\n",
            "900 [D loss: 0.333613, acc.: 86.33%] [G loss: 5.602916]\n",
            "920 [D loss: 0.195415, acc.: 96.48%] [G loss: 6.447292]\n",
            "940 [D loss: 0.307183, acc.: 89.45%] [G loss: 5.055152]\n",
            "960 [D loss: 0.452245, acc.: 81.64%] [G loss: 4.614908]\n",
            "980 [D loss: 0.375906, acc.: 84.38%] [G loss: 4.065977]\n",
            "1000 [D loss: 0.322876, acc.: 87.50%] [G loss: 4.418130]\n",
            "1020 [D loss: 0.586964, acc.: 73.05%] [G loss: 3.883324]\n",
            "1040 [D loss: 0.422864, acc.: 80.08%] [G loss: 3.679060]\n",
            "1060 [D loss: 0.452724, acc.: 79.30%] [G loss: 4.292919]\n",
            "1080 [D loss: 0.270108, acc.: 92.19%] [G loss: 5.464712]\n",
            "1100 [D loss: 0.379701, acc.: 83.98%] [G loss: 5.048302]\n",
            "1120 [D loss: 0.425299, acc.: 85.55%] [G loss: 4.107570]\n",
            "1140 [D loss: 0.346484, acc.: 86.33%] [G loss: 4.969882]\n",
            "1160 [D loss: 0.475160, acc.: 74.61%] [G loss: 4.161832]\n",
            "1180 [D loss: 0.556998, acc.: 73.83%] [G loss: 3.082120]\n",
            "1200 [D loss: 0.484912, acc.: 76.56%] [G loss: 3.433202]\n",
            "1220 [D loss: 0.389804, acc.: 84.77%] [G loss: 3.793457]\n",
            "1240 [D loss: 0.383044, acc.: 83.20%] [G loss: 4.044407]\n",
            "1260 [D loss: 0.557780, acc.: 74.61%] [G loss: 3.227710]\n",
            "1280 [D loss: 0.454065, acc.: 78.52%] [G loss: 3.505677]\n",
            "1300 [D loss: 0.447838, acc.: 81.25%] [G loss: 3.583960]\n",
            "1320 [D loss: 0.614067, acc.: 71.88%] [G loss: 3.790469]\n",
            "1340 [D loss: 0.397807, acc.: 83.59%] [G loss: 3.764839]\n",
            "1360 [D loss: 0.807834, acc.: 53.91%] [G loss: 3.551185]\n",
            "1380 [D loss: 0.439475, acc.: 81.64%] [G loss: 3.677790]\n",
            "1400 [D loss: 0.483154, acc.: 77.34%] [G loss: 3.529718]\n",
            "1420 [D loss: 0.475700, acc.: 76.56%] [G loss: 3.799642]\n",
            "1440 [D loss: 0.545951, acc.: 74.61%] [G loss: 3.470526]\n",
            "1460 [D loss: 0.441273, acc.: 81.64%] [G loss: 3.969632]\n",
            "1480 [D loss: 0.387720, acc.: 82.81%] [G loss: 4.333671]\n",
            "1500 [D loss: 0.557913, acc.: 71.88%] [G loss: 3.855313]\n",
            "1520 [D loss: 0.368076, acc.: 82.81%] [G loss: 4.123768]\n",
            "1540 [D loss: 0.410299, acc.: 82.42%] [G loss: 4.277707]\n",
            "1560 [D loss: 0.400476, acc.: 82.81%] [G loss: 3.751390]\n",
            "1580 [D loss: 0.449101, acc.: 79.30%] [G loss: 3.734212]\n",
            "1600 [D loss: 0.415674, acc.: 81.25%] [G loss: 3.873059]\n",
            "1620 [D loss: 0.265750, acc.: 92.19%] [G loss: 2.964638]\n",
            "1640 [D loss: 0.489255, acc.: 76.17%] [G loss: 3.519682]\n",
            "1660 [D loss: 0.477191, acc.: 78.91%] [G loss: 4.055447]\n",
            "1680 [D loss: 0.386211, acc.: 82.03%] [G loss: 4.089319]\n",
            "1700 [D loss: 0.438694, acc.: 80.86%] [G loss: 3.858937]\n",
            "1720 [D loss: 0.324894, acc.: 87.11%] [G loss: 4.758214]\n",
            "1740 [D loss: 0.508147, acc.: 73.83%] [G loss: 3.894225]\n",
            "1760 [D loss: 0.466030, acc.: 81.25%] [G loss: 4.541659]\n",
            "1780 [D loss: 0.499243, acc.: 77.34%] [G loss: 3.977340]\n",
            "1800 [D loss: 0.441533, acc.: 81.64%] [G loss: 3.896728]\n",
            "1820 [D loss: 0.476543, acc.: 77.34%] [G loss: 4.024343]\n",
            "1840 [D loss: 0.380871, acc.: 82.81%] [G loss: 4.468583]\n",
            "1860 [D loss: 0.521969, acc.: 74.22%] [G loss: 4.363077]\n",
            "1880 [D loss: 0.398990, acc.: 85.16%] [G loss: 4.545360]\n",
            "1900 [D loss: 0.352253, acc.: 84.77%] [G loss: 4.854161]\n",
            "1920 [D loss: 0.411604, acc.: 80.86%] [G loss: 4.394051]\n",
            "1940 [D loss: 0.348949, acc.: 86.72%] [G loss: 4.684855]\n",
            "1960 [D loss: 0.408454, acc.: 83.59%] [G loss: 4.370540]\n",
            "1980 [D loss: 0.445645, acc.: 78.12%] [G loss: 4.173965]\n",
            "2000 [D loss: 0.382557, acc.: 83.59%] [G loss: 4.587084]\n",
            "2020 [D loss: 0.528059, acc.: 77.73%] [G loss: 4.250171]\n",
            "2040 [D loss: 0.434418, acc.: 81.64%] [G loss: 3.984667]\n",
            "2060 [D loss: 0.396934, acc.: 83.59%] [G loss: 4.381786]\n",
            "2080 [D loss: 0.406715, acc.: 81.64%] [G loss: 4.359241]\n",
            "2100 [D loss: 0.498469, acc.: 76.17%] [G loss: 4.309250]\n",
            "2120 [D loss: 0.392552, acc.: 81.64%] [G loss: 4.542624]\n",
            "2140 [D loss: 0.497858, acc.: 75.39%] [G loss: 4.256995]\n",
            "2160 [D loss: 0.431669, acc.: 78.52%] [G loss: 4.374342]\n",
            "2180 [D loss: 0.386353, acc.: 82.03%] [G loss: 4.030497]\n",
            "2200 [D loss: 0.393525, acc.: 84.38%] [G loss: 4.327544]\n",
            "2220 [D loss: 0.471047, acc.: 76.17%] [G loss: 4.148203]\n",
            "2240 [D loss: 0.431076, acc.: 80.86%] [G loss: 4.362601]\n",
            "2260 [D loss: 0.376126, acc.: 82.81%] [G loss: 4.311936]\n",
            "2280 [D loss: 0.435135, acc.: 82.03%] [G loss: 4.216527]\n",
            "2300 [D loss: 0.412959, acc.: 80.47%] [G loss: 4.111159]\n",
            "2320 [D loss: 0.550280, acc.: 71.88%] [G loss: 3.931458]\n",
            "2340 [D loss: 0.335698, acc.: 87.50%] [G loss: 4.733627]\n",
            "2360 [D loss: 0.425331, acc.: 80.86%] [G loss: 4.688098]\n",
            "2380 [D loss: 0.424558, acc.: 82.81%] [G loss: 4.332030]\n",
            "2400 [D loss: 0.427221, acc.: 78.91%] [G loss: 4.370893]\n",
            "2420 [D loss: 0.318654, acc.: 88.28%] [G loss: 4.803578]\n",
            "2440 [D loss: 0.413908, acc.: 83.59%] [G loss: 4.504460]\n",
            "2460 [D loss: 0.421730, acc.: 79.69%] [G loss: 4.413567]\n",
            "2480 [D loss: 0.419884, acc.: 81.64%] [G loss: 4.405695]\n",
            "2500 [D loss: 0.424117, acc.: 82.42%] [G loss: 4.691920]\n",
            "2520 [D loss: 0.453394, acc.: 80.47%] [G loss: 4.406740]\n",
            "2540 [D loss: 0.374935, acc.: 84.77%] [G loss: 4.736284]\n",
            "2560 [D loss: 0.321794, acc.: 87.50%] [G loss: 5.267558]\n",
            "2580 [D loss: 0.518656, acc.: 72.27%] [G loss: 4.482098]\n",
            "2600 [D loss: 0.388319, acc.: 83.98%] [G loss: 4.813517]\n",
            "2620 [D loss: 0.328663, acc.: 86.33%] [G loss: 4.836767]\n",
            "2640 [D loss: 0.318099, acc.: 85.55%] [G loss: 5.641196]\n",
            "2660 [D loss: 0.326650, acc.: 87.50%] [G loss: 5.069518]\n",
            "2680 [D loss: 0.447819, acc.: 77.34%] [G loss: 5.226392]\n",
            "2700 [D loss: 0.347934, acc.: 87.50%] [G loss: 4.671406]\n",
            "2720 [D loss: 0.453368, acc.: 76.56%] [G loss: 4.273150]\n",
            "2740 [D loss: 0.432023, acc.: 78.12%] [G loss: 4.159589]\n",
            "2760 [D loss: 0.420880, acc.: 80.47%] [G loss: 4.599861]\n",
            "2780 [D loss: 0.566641, acc.: 69.92%] [G loss: 4.225712]\n",
            "2800 [D loss: 0.463328, acc.: 76.95%] [G loss: 4.600976]\n",
            "2820 [D loss: 0.350038, acc.: 83.98%] [G loss: 4.546372]\n",
            "2840 [D loss: 0.403359, acc.: 81.25%] [G loss: 5.237429]\n",
            "2860 [D loss: 0.345739, acc.: 85.94%] [G loss: 4.741522]\n",
            "2880 [D loss: 0.354460, acc.: 85.16%] [G loss: 4.596492]\n",
            "2900 [D loss: 0.375729, acc.: 80.86%] [G loss: 4.914245]\n",
            "2920 [D loss: 0.322705, acc.: 86.33%] [G loss: 5.045888]\n",
            "2940 [D loss: 0.247954, acc.: 91.41%] [G loss: 5.091659]\n",
            "2960 [D loss: 0.557924, acc.: 72.27%] [G loss: 4.150468]\n",
            "2980 [D loss: 0.327608, acc.: 89.06%] [G loss: 4.811913]\n",
            "3000 [D loss: 0.412486, acc.: 79.30%] [G loss: 4.731018]\n",
            "3020 [D loss: 0.344638, acc.: 87.11%] [G loss: 5.435119]\n",
            "3040 [D loss: 0.331938, acc.: 83.59%] [G loss: 4.884459]\n",
            "3060 [D loss: 0.328882, acc.: 83.98%] [G loss: 5.597978]\n",
            "3080 [D loss: 0.380475, acc.: 81.25%] [G loss: 4.764737]\n",
            "3100 [D loss: 0.393132, acc.: 81.64%] [G loss: 4.988447]\n",
            "3120 [D loss: 0.417215, acc.: 81.64%] [G loss: 4.992435]\n",
            "3140 [D loss: 0.358083, acc.: 83.59%] [G loss: 5.373530]\n",
            "3160 [D loss: 0.343837, acc.: 85.16%] [G loss: 5.502664]\n",
            "3180 [D loss: 0.242313, acc.: 91.41%] [G loss: 5.834956]\n",
            "3200 [D loss: 0.299378, acc.: 90.23%] [G loss: 5.601436]\n",
            "3220 [D loss: 0.500149, acc.: 76.17%] [G loss: 5.214738]\n",
            "3240 [D loss: 0.355832, acc.: 85.16%] [G loss: 5.202427]\n",
            "3260 [D loss: 0.290968, acc.: 88.28%] [G loss: 6.063551]\n",
            "3280 [D loss: 0.327741, acc.: 87.89%] [G loss: 5.059925]\n",
            "3300 [D loss: 0.451675, acc.: 79.30%] [G loss: 3.925858]\n",
            "3320 [D loss: 0.313318, acc.: 86.33%] [G loss: 5.141754]\n",
            "3340 [D loss: 0.371400, acc.: 84.77%] [G loss: 5.368063]\n",
            "3360 [D loss: 0.335157, acc.: 87.89%] [G loss: 5.477335]\n",
            "3380 [D loss: 0.408369, acc.: 82.81%] [G loss: 4.978508]\n",
            "3400 [D loss: 0.377747, acc.: 82.03%] [G loss: 5.000229]\n",
            "3420 [D loss: 0.411301, acc.: 80.08%] [G loss: 4.702883]\n",
            "3440 [D loss: 0.389751, acc.: 81.25%] [G loss: 5.163847]\n",
            "3460 [D loss: 0.364691, acc.: 84.38%] [G loss: 4.934480]\n",
            "3480 [D loss: 0.313698, acc.: 87.89%] [G loss: 5.449514]\n",
            "3500 [D loss: 0.315051, acc.: 88.67%] [G loss: 5.151597]\n",
            "3520 [D loss: 0.294548, acc.: 88.28%] [G loss: 5.393147]\n",
            "3540 [D loss: 0.360445, acc.: 83.20%] [G loss: 5.641194]\n",
            "3560 [D loss: 0.415940, acc.: 80.47%] [G loss: 5.054404]\n",
            "3580 [D loss: 0.353312, acc.: 84.38%] [G loss: 5.222958]\n",
            "3600 [D loss: 0.288212, acc.: 88.28%] [G loss: 5.577487]\n",
            "3620 [D loss: 0.343285, acc.: 85.55%] [G loss: 5.528058]\n",
            "3640 [D loss: 0.379959, acc.: 82.42%] [G loss: 4.946277]\n",
            "3660 [D loss: 0.299427, acc.: 85.94%] [G loss: 5.427139]\n",
            "3680 [D loss: 0.308425, acc.: 86.72%] [G loss: 5.204938]\n",
            "3700 [D loss: 0.379670, acc.: 83.98%] [G loss: 4.750484]\n",
            "3720 [D loss: 0.323518, acc.: 89.06%] [G loss: 6.012178]\n",
            "3740 [D loss: 0.311610, acc.: 88.67%] [G loss: 5.523414]\n",
            "3760 [D loss: 0.424161, acc.: 83.98%] [G loss: 5.111194]\n",
            "3780 [D loss: 0.404188, acc.: 80.47%] [G loss: 4.824166]\n",
            "3800 [D loss: 0.348751, acc.: 83.98%] [G loss: 5.565083]\n",
            "3820 [D loss: 0.362564, acc.: 82.03%] [G loss: 5.672406]\n",
            "3840 [D loss: 0.274154, acc.: 90.23%] [G loss: 5.396473]\n",
            "3860 [D loss: 0.397751, acc.: 82.42%] [G loss: 5.311002]\n",
            "3880 [D loss: 0.281521, acc.: 89.84%] [G loss: 5.293602]\n",
            "3900 [D loss: 0.239948, acc.: 93.36%] [G loss: 4.203655]\n",
            "3920 [D loss: 0.388629, acc.: 83.20%] [G loss: 5.298659]\n",
            "3940 [D loss: 0.416746, acc.: 82.42%] [G loss: 5.064167]\n",
            "3960 [D loss: 0.326796, acc.: 89.06%] [G loss: 5.618279]\n",
            "3980 [D loss: 0.406631, acc.: 81.64%] [G loss: 5.007459]\n",
            "4000 [D loss: 0.359046, acc.: 83.98%] [G loss: 5.233293]\n",
            "4020 [D loss: 0.309346, acc.: 87.11%] [G loss: 5.397418]\n",
            "4040 [D loss: 0.397237, acc.: 83.59%] [G loss: 5.136081]\n",
            "4060 [D loss: 0.263437, acc.: 89.45%] [G loss: 5.792728]\n",
            "4080 [D loss: 0.375356, acc.: 83.59%] [G loss: 5.597792]\n",
            "4100 [D loss: 0.425198, acc.: 77.73%] [G loss: 5.603860]\n",
            "4120 [D loss: 0.355144, acc.: 85.16%] [G loss: 4.942945]\n",
            "4140 [D loss: 0.310467, acc.: 87.50%] [G loss: 5.567299]\n",
            "4160 [D loss: 0.415152, acc.: 81.64%] [G loss: 5.416082]\n",
            "4180 [D loss: 0.411281, acc.: 82.42%] [G loss: 4.888762]\n",
            "4200 [D loss: 0.355689, acc.: 84.77%] [G loss: 5.024950]\n",
            "4220 [D loss: 0.364526, acc.: 83.59%] [G loss: 5.475099]\n",
            "4240 [D loss: 0.337517, acc.: 87.89%] [G loss: 5.470329]\n",
            "4260 [D loss: 0.354351, acc.: 84.77%] [G loss: 5.294339]\n",
            "4280 [D loss: 0.390846, acc.: 82.42%] [G loss: 5.764386]\n",
            "4300 [D loss: 0.399027, acc.: 82.81%] [G loss: 4.830018]\n",
            "4320 [D loss: 0.411984, acc.: 79.69%] [G loss: 5.266260]\n",
            "4340 [D loss: 0.371327, acc.: 83.98%] [G loss: 4.919018]\n",
            "4360 [D loss: 0.377299, acc.: 86.72%] [G loss: 5.610697]\n",
            "4380 [D loss: 0.404603, acc.: 83.20%] [G loss: 4.994227]\n",
            "4400 [D loss: 0.330091, acc.: 87.89%] [G loss: 5.244478]\n",
            "4420 [D loss: 0.374008, acc.: 82.81%] [G loss: 5.226240]\n",
            "4440 [D loss: 0.343358, acc.: 86.33%] [G loss: 5.445497]\n",
            "4460 [D loss: 0.418696, acc.: 81.64%] [G loss: 4.754268]\n",
            "4480 [D loss: 0.319797, acc.: 86.72%] [G loss: 5.319033]\n",
            "4500 [D loss: 0.287284, acc.: 88.67%] [G loss: 5.353791]\n",
            "4520 [D loss: 0.341780, acc.: 87.11%] [G loss: 4.987555]\n",
            "4540 [D loss: 0.390417, acc.: 85.16%] [G loss: 5.194599]\n",
            "4560 [D loss: 0.325254, acc.: 89.06%] [G loss: 5.506325]\n",
            "4580 [D loss: 0.397132, acc.: 81.64%] [G loss: 5.209028]\n",
            "4600 [D loss: 0.335165, acc.: 86.72%] [G loss: 5.436874]\n",
            "4620 [D loss: 0.194968, acc.: 94.53%] [G loss: 3.942992]\n",
            "4640 [D loss: 0.283390, acc.: 89.06%] [G loss: 5.332844]\n",
            "4660 [D loss: 0.310230, acc.: 87.89%] [G loss: 6.026187]\n",
            "4680 [D loss: 0.429914, acc.: 79.30%] [G loss: 5.205091]\n",
            "4700 [D loss: 0.346587, acc.: 86.33%] [G loss: 5.363153]\n",
            "4720 [D loss: 0.315344, acc.: 87.11%] [G loss: 5.414298]\n",
            "4740 [D loss: 0.379832, acc.: 81.64%] [G loss: 4.749491]\n",
            "4760 [D loss: 0.429490, acc.: 81.25%] [G loss: 5.200512]\n",
            "4780 [D loss: 0.391730, acc.: 85.16%] [G loss: 5.007379]\n",
            "4800 [D loss: 0.334838, acc.: 85.16%] [G loss: 5.388161]\n",
            "4820 [D loss: 0.325584, acc.: 88.67%] [G loss: 5.573364]\n",
            "4840 [D loss: 0.339214, acc.: 83.20%] [G loss: 5.809021]\n",
            "4860 [D loss: 0.338332, acc.: 84.77%] [G loss: 5.561607]\n",
            "4880 [D loss: 0.374262, acc.: 85.16%] [G loss: 5.383989]\n",
            "4900 [D loss: 0.379931, acc.: 86.33%] [G loss: 5.251416]\n",
            "4920 [D loss: 0.331883, acc.: 87.11%] [G loss: 5.405425]\n",
            "4940 [D loss: 0.334563, acc.: 84.38%] [G loss: 5.640156]\n",
            "4960 [D loss: 0.439441, acc.: 81.25%] [G loss: 5.522693]\n",
            "4980 [D loss: 0.289872, acc.: 89.84%] [G loss: 5.592683]\n",
            "5000 [D loss: 0.394560, acc.: 84.38%] [G loss: 5.213978]\n",
            "5020 [D loss: 0.306190, acc.: 87.11%] [G loss: 5.533834]\n",
            "5040 [D loss: 0.349813, acc.: 86.72%] [G loss: 5.110530]\n",
            "5060 [D loss: 0.300289, acc.: 91.02%] [G loss: 5.372245]\n",
            "5080 [D loss: 0.360505, acc.: 83.20%] [G loss: 5.911191]\n",
            "5100 [D loss: 0.319047, acc.: 86.72%] [G loss: 5.747505]\n",
            "5120 [D loss: 0.363069, acc.: 84.77%] [G loss: 5.584892]\n",
            "5140 [D loss: 0.385717, acc.: 83.59%] [G loss: 5.609719]\n",
            "5160 [D loss: 0.362564, acc.: 83.98%] [G loss: 5.778864]\n",
            "5180 [D loss: 0.317650, acc.: 85.55%] [G loss: 6.104589]\n",
            "5200 [D loss: 0.251684, acc.: 91.02%] [G loss: 5.762933]\n",
            "5220 [D loss: 0.301229, acc.: 87.89%] [G loss: 5.769100]\n",
            "5240 [D loss: 0.306466, acc.: 88.28%] [G loss: 5.430788]\n",
            "5260 [D loss: 0.298505, acc.: 87.50%] [G loss: 6.293819]\n",
            "5280 [D loss: 0.282071, acc.: 86.72%] [G loss: 5.871499]\n",
            "5300 [D loss: 0.314756, acc.: 87.11%] [G loss: 6.444508]\n",
            "5320 [D loss: 0.384698, acc.: 82.42%] [G loss: 5.484086]\n",
            "5340 [D loss: 0.300356, acc.: 88.28%] [G loss: 5.790524]\n",
            "5360 [D loss: 0.294371, acc.: 85.94%] [G loss: 5.791577]\n",
            "5380 [D loss: 0.342749, acc.: 85.16%] [G loss: 4.439356]\n",
            "5400 [D loss: 0.235910, acc.: 91.80%] [G loss: 5.700068]\n",
            "5420 [D loss: 0.365386, acc.: 83.98%] [G loss: 5.132436]\n",
            "5440 [D loss: 0.331750, acc.: 87.11%] [G loss: 5.891356]\n",
            "5460 [D loss: 0.262088, acc.: 89.06%] [G loss: 5.571550]\n",
            "5480 [D loss: 0.314808, acc.: 87.11%] [G loss: 5.646025]\n",
            "5500 [D loss: 0.233844, acc.: 92.19%] [G loss: 6.248126]\n",
            "5520 [D loss: 0.347749, acc.: 85.55%] [G loss: 5.817762]\n",
            "5540 [D loss: 0.362213, acc.: 84.77%] [G loss: 5.883071]\n",
            "5560 [D loss: 0.292542, acc.: 88.67%] [G loss: 6.394390]\n",
            "5580 [D loss: 0.268725, acc.: 89.45%] [G loss: 6.440895]\n",
            "5600 [D loss: 0.316412, acc.: 85.55%] [G loss: 6.040981]\n",
            "5620 [D loss: 0.222469, acc.: 92.19%] [G loss: 6.744613]\n",
            "5640 [D loss: 0.263532, acc.: 87.89%] [G loss: 6.116303]\n",
            "5660 [D loss: 0.286192, acc.: 89.45%] [G loss: 6.010793]\n",
            "5680 [D loss: 0.296819, acc.: 88.28%] [G loss: 5.903666]\n",
            "5700 [D loss: 0.282366, acc.: 87.89%] [G loss: 5.365356]\n",
            "5720 [D loss: 0.395449, acc.: 80.08%] [G loss: 5.242566]\n",
            "5740 [D loss: 0.326615, acc.: 85.55%] [G loss: 5.571678]\n",
            "5760 [D loss: 0.332601, acc.: 84.38%] [G loss: 5.675895]\n",
            "5780 [D loss: 0.385176, acc.: 81.25%] [G loss: 5.950945]\n",
            "5800 [D loss: 0.382811, acc.: 83.20%] [G loss: 5.254795]\n",
            "5820 [D loss: 0.333592, acc.: 87.50%] [G loss: 5.504701]\n",
            "5840 [D loss: 0.419590, acc.: 82.42%] [G loss: 5.453451]\n",
            "5860 [D loss: 0.273522, acc.: 91.80%] [G loss: 5.945286]\n",
            "5880 [D loss: 0.345261, acc.: 86.33%] [G loss: 5.456802]\n",
            "5900 [D loss: 0.327052, acc.: 83.59%] [G loss: 5.362304]\n",
            "5920 [D loss: 0.342705, acc.: 83.59%] [G loss: 5.935745]\n",
            "5940 [D loss: 0.264386, acc.: 89.06%] [G loss: 3.793453]\n",
            "5960 [D loss: 0.301584, acc.: 87.89%] [G loss: 5.095552]\n",
            "5980 [D loss: 0.363940, acc.: 84.77%] [G loss: 5.450327]\n",
            "6000 [D loss: 0.402290, acc.: 80.47%] [G loss: 4.837795]\n",
            "6020 [D loss: 0.348144, acc.: 83.98%] [G loss: 6.261810]\n",
            "6040 [D loss: 0.287552, acc.: 88.67%] [G loss: 5.299012]\n",
            "6060 [D loss: 0.445677, acc.: 77.34%] [G loss: 5.946408]\n",
            "6080 [D loss: 0.227243, acc.: 90.62%] [G loss: 6.162244]\n",
            "6100 [D loss: 0.353216, acc.: 83.20%] [G loss: 6.010668]\n",
            "6120 [D loss: 0.375577, acc.: 82.81%] [G loss: 5.656889]\n",
            "6140 [D loss: 0.345884, acc.: 86.33%] [G loss: 5.737312]\n",
            "6160 [D loss: 0.334394, acc.: 86.72%] [G loss: 5.291653]\n",
            "6180 [D loss: 0.300782, acc.: 85.94%] [G loss: 5.146754]\n",
            "6200 [D loss: 0.281913, acc.: 90.62%] [G loss: 5.748967]\n",
            "6220 [D loss: 0.334877, acc.: 85.94%] [G loss: 5.493914]\n",
            "6240 [D loss: 0.337611, acc.: 85.55%] [G loss: 5.100847]\n",
            "6260 [D loss: 0.324709, acc.: 84.77%] [G loss: 5.309240]\n",
            "6280 [D loss: 0.383528, acc.: 83.20%] [G loss: 5.204239]\n",
            "6300 [D loss: 0.456260, acc.: 78.12%] [G loss: 4.921412]\n",
            "6320 [D loss: 0.319257, acc.: 88.28%] [G loss: 4.922379]\n",
            "6340 [D loss: 0.347562, acc.: 85.16%] [G loss: 5.244738]\n",
            "6360 [D loss: 0.268125, acc.: 89.84%] [G loss: 5.325434]\n",
            "6380 [D loss: 0.284629, acc.: 87.50%] [G loss: 5.339823]\n",
            "6400 [D loss: 0.453858, acc.: 76.56%] [G loss: 5.390445]\n",
            "6420 [D loss: 0.307804, acc.: 86.72%] [G loss: 5.089771]\n",
            "6440 [D loss: 0.370477, acc.: 85.55%] [G loss: 5.368149]\n",
            "6460 [D loss: 0.339559, acc.: 85.16%] [G loss: 5.092026]\n",
            "6480 [D loss: 0.411319, acc.: 81.25%] [G loss: 4.833236]\n",
            "6500 [D loss: 0.332855, acc.: 87.50%] [G loss: 5.338369]\n",
            "6520 [D loss: 0.360680, acc.: 84.77%] [G loss: 5.107018]\n",
            "6540 [D loss: 0.360588, acc.: 82.81%] [G loss: 5.363896]\n",
            "6560 [D loss: 0.259306, acc.: 88.67%] [G loss: 5.369337]\n",
            "6580 [D loss: 0.339890, acc.: 83.20%] [G loss: 4.808709]\n",
            "6600 [D loss: 0.140767, acc.: 94.53%] [G loss: 5.386733]\n",
            "6620 [D loss: 0.347659, acc.: 87.50%] [G loss: 5.324479]\n",
            "6640 [D loss: 0.380079, acc.: 83.98%] [G loss: 4.730723]\n",
            "6660 [D loss: 0.366442, acc.: 83.59%] [G loss: 6.020202]\n",
            "6680 [D loss: 0.331211, acc.: 86.72%] [G loss: 5.635776]\n",
            "6700 [D loss: 0.316687, acc.: 87.11%] [G loss: 5.261915]\n",
            "6720 [D loss: 0.263666, acc.: 87.50%] [G loss: 5.789947]\n",
            "6740 [D loss: 0.450168, acc.: 77.34%] [G loss: 5.162899]\n",
            "6760 [D loss: 0.289071, acc.: 90.62%] [G loss: 5.552494]\n",
            "6780 [D loss: 0.316516, acc.: 86.33%] [G loss: 5.380622]\n",
            "6800 [D loss: 0.350055, acc.: 86.72%] [G loss: 5.656824]\n",
            "6820 [D loss: 0.322616, acc.: 87.50%] [G loss: 5.498057]\n",
            "6840 [D loss: 0.322218, acc.: 86.33%] [G loss: 5.597958]\n",
            "6860 [D loss: 0.340773, acc.: 84.77%] [G loss: 5.283210]\n",
            "6880 [D loss: 0.350483, acc.: 87.89%] [G loss: 5.216579]\n",
            "6900 [D loss: 0.244983, acc.: 89.84%] [G loss: 5.237999]\n",
            "6920 [D loss: 0.318166, acc.: 87.11%] [G loss: 5.810130]\n",
            "6940 [D loss: 0.416369, acc.: 79.69%] [G loss: 5.419020]\n",
            "6960 [D loss: 0.433403, acc.: 82.81%] [G loss: 6.122477]\n",
            "6980 [D loss: 0.337892, acc.: 86.33%] [G loss: 5.497224]\n",
            "7000 [D loss: 0.373318, acc.: 83.20%] [G loss: 5.608348]\n",
            "7020 [D loss: 0.269202, acc.: 88.67%] [G loss: 5.577579]\n",
            "7040 [D loss: 0.088703, acc.: 98.44%] [G loss: 3.912928]\n",
            "7060 [D loss: 0.328243, acc.: 85.94%] [G loss: 4.819490]\n",
            "7080 [D loss: 0.420681, acc.: 78.12%] [G loss: 5.446097]\n",
            "7100 [D loss: 0.448072, acc.: 79.69%] [G loss: 4.985517]\n",
            "7120 [D loss: 0.429468, acc.: 79.30%] [G loss: 5.316717]\n",
            "7140 [D loss: 0.312819, acc.: 87.89%] [G loss: 5.414249]\n",
            "7160 [D loss: 0.332111, acc.: 86.72%] [G loss: 5.827503]\n",
            "7180 [D loss: 0.238801, acc.: 89.45%] [G loss: 5.157704]\n",
            "7200 [D loss: 0.307728, acc.: 85.55%] [G loss: 5.356025]\n",
            "7220 [D loss: 0.321008, acc.: 83.98%] [G loss: 5.497461]\n",
            "7240 [D loss: 0.357018, acc.: 84.77%] [G loss: 5.168530]\n",
            "7260 [D loss: 0.404600, acc.: 82.03%] [G loss: 5.734508]\n",
            "7280 [D loss: 0.351773, acc.: 81.64%] [G loss: 5.576539]\n",
            "7300 [D loss: 0.271966, acc.: 88.67%] [G loss: 5.642989]\n",
            "7320 [D loss: 0.271960, acc.: 87.50%] [G loss: 5.433709]\n",
            "7340 [D loss: 0.310674, acc.: 85.55%] [G loss: 5.472804]\n",
            "7360 [D loss: 0.297896, acc.: 87.89%] [G loss: 5.940438]\n",
            "7380 [D loss: 0.315322, acc.: 85.55%] [G loss: 5.497225]\n",
            "7400 [D loss: 0.260705, acc.: 90.62%] [G loss: 5.451283]\n",
            "7420 [D loss: 0.347332, acc.: 83.20%] [G loss: 5.647743]\n",
            "7440 [D loss: 0.437393, acc.: 80.86%] [G loss: 5.248051]\n",
            "7460 [D loss: 0.309130, acc.: 87.89%] [G loss: 5.564451]\n",
            "7480 [D loss: 0.258613, acc.: 90.23%] [G loss: 5.398925]\n",
            "7500 [D loss: 0.373239, acc.: 85.55%] [G loss: 5.228782]\n",
            "7520 [D loss: 0.362169, acc.: 83.59%] [G loss: 5.316082]\n",
            "7540 [D loss: 0.237251, acc.: 89.45%] [G loss: 5.718632]\n",
            "7560 [D loss: 0.389103, acc.: 81.64%] [G loss: 5.932398]\n",
            "7580 [D loss: 0.286184, acc.: 89.06%] [G loss: 5.214523]\n",
            "7600 [D loss: 0.367061, acc.: 85.55%] [G loss: 5.895893]\n",
            "7620 [D loss: 0.338533, acc.: 85.94%] [G loss: 5.206831]\n",
            "7640 [D loss: 0.390912, acc.: 82.42%] [G loss: 5.079202]\n",
            "7660 [D loss: 0.307353, acc.: 89.45%] [G loss: 4.937565]\n",
            "7680 [D loss: 0.323087, acc.: 88.28%] [G loss: 5.009768]\n",
            "7700 [D loss: 0.308532, acc.: 89.06%] [G loss: 5.077505]\n",
            "7720 [D loss: 0.298557, acc.: 89.06%] [G loss: 5.603384]\n",
            "7740 [D loss: 0.280219, acc.: 88.28%] [G loss: 5.810865]\n",
            "7760 [D loss: 0.355162, acc.: 83.98%] [G loss: 5.248317]\n",
            "7780 [D loss: 0.324720, acc.: 87.50%] [G loss: 5.954989]\n",
            "7800 [D loss: 0.315485, acc.: 87.50%] [G loss: 5.386941]\n",
            "7820 [D loss: 0.510927, acc.: 75.00%] [G loss: 5.247611]\n",
            "7840 [D loss: 0.834379, acc.: 64.45%] [G loss: 4.532631]\n",
            "7860 [D loss: 0.543879, acc.: 72.27%] [G loss: 3.511292]\n",
            "7880 [D loss: 0.712902, acc.: 58.20%] [G loss: 2.758089]\n",
            "7900 [D loss: 0.697044, acc.: 58.98%] [G loss: 2.727075]\n",
            "7920 [D loss: 0.569789, acc.: 72.66%] [G loss: 3.217299]\n",
            "7940 [D loss: 0.593531, acc.: 68.36%] [G loss: 3.060927]\n",
            "7960 [D loss: 0.450351, acc.: 81.64%] [G loss: 3.634347]\n",
            "7980 [D loss: 0.427071, acc.: 76.95%] [G loss: 4.088515]\n",
            "8000 [D loss: 0.403474, acc.: 81.64%] [G loss: 4.158008]\n",
            "8020 [D loss: 0.264738, acc.: 89.06%] [G loss: 4.936428]\n",
            "8040 [D loss: 0.573446, acc.: 75.39%] [G loss: 3.190813]\n",
            "8060 [D loss: 0.286453, acc.: 86.33%] [G loss: 5.337687]\n",
            "8080 [D loss: 0.353029, acc.: 86.33%] [G loss: 5.581841]\n",
            "8100 [D loss: 0.340519, acc.: 84.38%] [G loss: 6.170039]\n",
            "8120 [D loss: 0.395155, acc.: 82.03%] [G loss: 5.883152]\n",
            "8140 [D loss: 0.319650, acc.: 83.98%] [G loss: 5.560670]\n",
            "8160 [D loss: 0.368742, acc.: 85.16%] [G loss: 5.223540]\n",
            "8180 [D loss: 0.317306, acc.: 86.72%] [G loss: 6.455637]\n",
            "8200 [D loss: 0.360140, acc.: 87.11%] [G loss: 5.370908]\n",
            "8220 [D loss: 0.297327, acc.: 88.67%] [G loss: 6.195693]\n",
            "8240 [D loss: 0.468766, acc.: 77.73%] [G loss: 4.452073]\n",
            "8260 [D loss: 0.342802, acc.: 83.98%] [G loss: 5.458196]\n",
            "8280 [D loss: 0.330584, acc.: 85.55%] [G loss: 4.843324]\n",
            "8300 [D loss: 0.348265, acc.: 86.33%] [G loss: 4.559250]\n",
            "8320 [D loss: 0.306799, acc.: 86.72%] [G loss: 5.830043]\n",
            "8340 [D loss: 0.432454, acc.: 82.03%] [G loss: 4.482040]\n",
            "8360 [D loss: 0.376301, acc.: 83.98%] [G loss: 5.358859]\n",
            "8380 [D loss: 0.282868, acc.: 88.67%] [G loss: 5.618600]\n",
            "8400 [D loss: 0.336804, acc.: 82.81%] [G loss: 5.274965]\n",
            "8420 [D loss: 0.363905, acc.: 83.20%] [G loss: 5.295593]\n",
            "8440 [D loss: 0.443801, acc.: 77.34%] [G loss: 4.648054]\n",
            "8460 [D loss: 0.347363, acc.: 86.72%] [G loss: 4.994941]\n",
            "8480 [D loss: 0.335500, acc.: 85.94%] [G loss: 4.914373]\n",
            "8500 [D loss: 0.316940, acc.: 87.89%] [G loss: 4.927286]\n",
            "8520 [D loss: 0.316858, acc.: 85.55%] [G loss: 4.926901]\n",
            "8540 [D loss: 0.358397, acc.: 84.38%] [G loss: 4.686515]\n",
            "8560 [D loss: 0.340775, acc.: 86.33%] [G loss: 4.704080]\n",
            "8580 [D loss: 0.328553, acc.: 84.77%] [G loss: 4.910675]\n",
            "8600 [D loss: 0.311557, acc.: 89.84%] [G loss: 4.430941]\n",
            "8620 [D loss: 0.300677, acc.: 88.28%] [G loss: 5.097423]\n",
            "8640 [D loss: 0.379789, acc.: 85.16%] [G loss: 4.683819]\n",
            "8660 [D loss: 0.383746, acc.: 85.55%] [G loss: 4.533529]\n",
            "8680 [D loss: 0.366701, acc.: 83.20%] [G loss: 4.618659]\n",
            "8700 [D loss: 0.438025, acc.: 81.25%] [G loss: 4.483653]\n",
            "8720 [D loss: 0.291162, acc.: 89.84%] [G loss: 4.547482]\n",
            "8740 [D loss: 0.272211, acc.: 89.45%] [G loss: 5.259771]\n",
            "8760 [D loss: 0.300338, acc.: 89.06%] [G loss: 4.994818]\n",
            "8780 [D loss: 0.165917, acc.: 95.70%] [G loss: 4.161804]\n",
            "8800 [D loss: 0.262181, acc.: 89.06%] [G loss: 4.381589]\n",
            "8820 [D loss: 0.366845, acc.: 84.77%] [G loss: 4.678288]\n",
            "8840 [D loss: 0.356838, acc.: 84.38%] [G loss: 4.391065]\n",
            "8860 [D loss: 0.317773, acc.: 86.72%] [G loss: 5.087899]\n",
            "8880 [D loss: 0.301357, acc.: 89.06%] [G loss: 5.207426]\n",
            "8900 [D loss: 0.415138, acc.: 81.25%] [G loss: 4.783024]\n",
            "8920 [D loss: 0.261798, acc.: 89.45%] [G loss: 4.700616]\n",
            "8940 [D loss: 0.290867, acc.: 87.89%] [G loss: 5.362225]\n",
            "8960 [D loss: 0.302861, acc.: 89.45%] [G loss: 5.130871]\n",
            "8980 [D loss: 0.405977, acc.: 82.42%] [G loss: 4.672053]\n",
            "9000 [D loss: 0.301594, acc.: 86.33%] [G loss: 4.924905]\n",
            "9020 [D loss: 0.374966, acc.: 85.16%] [G loss: 4.598363]\n",
            "9040 [D loss: 0.356573, acc.: 83.98%] [G loss: 4.320249]\n",
            "9060 [D loss: 0.291078, acc.: 89.84%] [G loss: 5.025998]\n",
            "9080 [D loss: 0.284955, acc.: 89.06%] [G loss: 4.873401]\n",
            "9100 [D loss: 0.340587, acc.: 84.38%] [G loss: 4.727383]\n",
            "9120 [D loss: 0.409597, acc.: 82.42%] [G loss: 4.274794]\n",
            "9140 [D loss: 0.340841, acc.: 85.94%] [G loss: 4.420088]\n",
            "9160 [D loss: 0.367653, acc.: 87.11%] [G loss: 4.632758]\n",
            "9180 [D loss: 0.343595, acc.: 85.55%] [G loss: 4.362648]\n",
            "9200 [D loss: 0.402040, acc.: 83.98%] [G loss: 4.331769]\n",
            "9220 [D loss: 0.338936, acc.: 85.94%] [G loss: 4.614621]\n",
            "9240 [D loss: 0.314944, acc.: 85.94%] [G loss: 4.928287]\n",
            "9260 [D loss: 0.313025, acc.: 87.11%] [G loss: 4.394022]\n",
            "9280 [D loss: 0.340353, acc.: 87.11%] [G loss: 4.729609]\n",
            "9300 [D loss: 0.388792, acc.: 82.81%] [G loss: 4.618552]\n",
            "9320 [D loss: 0.323104, acc.: 84.38%] [G loss: 4.993255]\n",
            "9340 [D loss: 0.388894, acc.: 85.94%] [G loss: 4.203055]\n",
            "9360 [D loss: 0.432078, acc.: 81.64%] [G loss: 4.226387]\n",
            "9380 [D loss: 0.400364, acc.: 81.25%] [G loss: 4.625824]\n",
            "9400 [D loss: 0.405556, acc.: 83.59%] [G loss: 4.488070]\n",
            "9420 [D loss: 0.323865, acc.: 85.94%] [G loss: 4.650316]\n",
            "9440 [D loss: 0.354810, acc.: 85.55%] [G loss: 4.565337]\n",
            "9460 [D loss: 0.413705, acc.: 81.64%] [G loss: 4.067022]\n",
            "9480 [D loss: 0.342281, acc.: 84.77%] [G loss: 4.250812]\n",
            "9500 [D loss: 0.310325, acc.: 87.50%] [G loss: 4.318416]\n",
            "9520 [D loss: 0.323131, acc.: 88.67%] [G loss: 4.726884]\n",
            "9540 [D loss: 0.360582, acc.: 84.38%] [G loss: 4.409189]\n",
            "9560 [D loss: 0.355458, acc.: 82.81%] [G loss: 4.455582]\n",
            "9580 [D loss: 0.338105, acc.: 85.55%] [G loss: 4.128982]\n",
            "9600 [D loss: 0.347263, acc.: 87.89%] [G loss: 4.534519]\n",
            "9620 [D loss: 0.298550, acc.: 87.89%] [G loss: 4.690908]\n",
            "9640 [D loss: 0.349257, acc.: 84.77%] [G loss: 4.295678]\n",
            "9660 [D loss: 0.279677, acc.: 88.67%] [G loss: 4.817104]\n",
            "9680 [D loss: 0.357021, acc.: 84.77%] [G loss: 4.612180]\n",
            "9700 [D loss: 0.225340, acc.: 91.02%] [G loss: 3.468170]\n",
            "9720 [D loss: 0.408100, acc.: 80.86%] [G loss: 4.110803]\n",
            "9740 [D loss: 0.354475, acc.: 87.89%] [G loss: 4.343664]\n",
            "9760 [D loss: 0.454871, acc.: 81.64%] [G loss: 4.114688]\n",
            "9780 [D loss: 0.265703, acc.: 89.84%] [G loss: 4.728211]\n",
            "9800 [D loss: 0.389367, acc.: 84.38%] [G loss: 4.563790]\n",
            "9820 [D loss: 0.375536, acc.: 83.20%] [G loss: 4.234245]\n",
            "9840 [D loss: 0.290670, acc.: 88.28%] [G loss: 4.791508]\n",
            "9860 [D loss: 0.300152, acc.: 87.89%] [G loss: 4.726147]\n",
            "9880 [D loss: 0.333102, acc.: 87.89%] [G loss: 4.696382]\n",
            "9900 [D loss: 0.356743, acc.: 85.55%] [G loss: 4.687780]\n",
            "9920 [D loss: 0.362884, acc.: 84.38%] [G loss: 4.338201]\n",
            "9940 [D loss: 0.310876, acc.: 86.72%] [G loss: 4.624803]\n",
            "9960 [D loss: 0.325089, acc.: 87.11%] [G loss: 5.075215]\n",
            "9980 [D loss: 0.354865, acc.: 85.55%] [G loss: 4.868052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKyOoPom5Zz",
        "outputId": "c8d3af2f-3729-4904-f9a0-9cb8c6d9a8f8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 48, 48, 128)  3328        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 100)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          51712       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 2304)      16128       input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1152)         0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2304)         0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_15[0][0]                 \n",
            "                                                                 flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            3969        concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.775621, acc.: 20.31%] [G loss: 1.527493]\n",
            "20 [D loss: 1.136945, acc.: 31.64%] [G loss: 1.106470]\n",
            "40 [D loss: 0.436015, acc.: 49.61%] [G loss: 17.784233]\n",
            "60 [D loss: 0.537491, acc.: 73.83%] [G loss: 4.473651]\n",
            "80 [D loss: 0.834621, acc.: 22.66%] [G loss: 1.869293]\n",
            "100 [D loss: 0.741704, acc.: 39.45%] [G loss: 1.638891]\n",
            "120 [D loss: 0.575083, acc.: 82.81%] [G loss: 2.042169]\n",
            "140 [D loss: 0.455398, acc.: 78.12%] [G loss: 3.064530]\n",
            "160 [D loss: 1.021226, acc.: 2.73%] [G loss: 1.166114]\n",
            "180 [D loss: 0.558926, acc.: 76.17%] [G loss: 2.472870]\n",
            "200 [D loss: 0.770760, acc.: 41.80%] [G loss: 1.451331]\n",
            "220 [D loss: 0.876846, acc.: 49.22%] [G loss: 3.595408]\n",
            "240 [D loss: 0.186598, acc.: 100.00%] [G loss: 7.332498]\n",
            "260 [D loss: 0.682056, acc.: 68.75%] [G loss: 1.890666]\n",
            "280 [D loss: 0.209264, acc.: 95.31%] [G loss: 10.532845]\n",
            "300 [D loss: 0.691736, acc.: 60.55%] [G loss: 1.672787]\n",
            "320 [D loss: 0.672510, acc.: 71.48%] [G loss: 1.888943]\n",
            "340 [D loss: 0.385103, acc.: 89.06%] [G loss: 4.332753]\n",
            "360 [D loss: 0.380074, acc.: 84.77%] [G loss: 4.375402]\n",
            "380 [D loss: 0.762896, acc.: 57.81%] [G loss: 1.722982]\n",
            "400 [D loss: 0.730825, acc.: 46.09%] [G loss: 2.234101]\n",
            "420 [D loss: 0.644904, acc.: 57.81%] [G loss: 2.380552]\n",
            "440 [D loss: 0.686306, acc.: 59.77%] [G loss: 2.222342]\n",
            "460 [D loss: 0.746045, acc.: 54.69%] [G loss: 1.957719]\n",
            "480 [D loss: 0.601959, acc.: 67.58%] [G loss: 2.125723]\n",
            "500 [D loss: 0.650282, acc.: 62.11%] [G loss: 2.268615]\n",
            "520 [D loss: 0.609233, acc.: 67.58%] [G loss: 2.602067]\n",
            "540 [D loss: 0.572285, acc.: 69.53%] [G loss: 2.928072]\n",
            "560 [D loss: 0.618016, acc.: 66.41%] [G loss: 2.138952]\n",
            "580 [D loss: 0.671801, acc.: 58.20%] [G loss: 1.966664]\n",
            "600 [D loss: 0.712011, acc.: 48.83%] [G loss: 1.836228]\n",
            "620 [D loss: 0.744234, acc.: 47.27%] [G loss: 1.852621]\n",
            "640 [D loss: 0.625981, acc.: 65.23%] [G loss: 2.130571]\n",
            "660 [D loss: 0.605573, acc.: 68.75%] [G loss: 2.238631]\n",
            "680 [D loss: 0.745590, acc.: 51.17%] [G loss: 1.863794]\n",
            "700 [D loss: 0.541753, acc.: 73.05%] [G loss: 2.539672]\n",
            "720 [D loss: 0.546152, acc.: 75.78%] [G loss: 2.414199]\n",
            "740 [D loss: 0.617598, acc.: 65.62%] [G loss: 2.347521]\n",
            "760 [D loss: 0.445802, acc.: 83.98%] [G loss: 3.243727]\n",
            "780 [D loss: 0.495237, acc.: 78.12%] [G loss: 2.732445]\n",
            "800 [D loss: 0.560501, acc.: 75.39%] [G loss: 2.522155]\n",
            "820 [D loss: 0.555277, acc.: 67.97%] [G loss: 2.861588]\n",
            "840 [D loss: 0.536461, acc.: 76.95%] [G loss: 2.788723]\n",
            "860 [D loss: 0.578135, acc.: 70.70%] [G loss: 3.652322]\n",
            "880 [D loss: 0.623166, acc.: 65.23%] [G loss: 2.995444]\n",
            "900 [D loss: 0.660175, acc.: 62.89%] [G loss: 2.713311]\n",
            "920 [D loss: 0.448838, acc.: 81.25%] [G loss: 3.552639]\n",
            "940 [D loss: 0.507613, acc.: 76.56%] [G loss: 3.297152]\n",
            "960 [D loss: 0.538144, acc.: 73.05%] [G loss: 3.129484]\n",
            "980 [D loss: 0.657594, acc.: 61.72%] [G loss: 3.054131]\n",
            "1000 [D loss: 0.435557, acc.: 84.77%] [G loss: 3.602971]\n",
            "1020 [D loss: 0.389269, acc.: 85.16%] [G loss: 4.215716]\n",
            "1040 [D loss: 0.441645, acc.: 80.47%] [G loss: 3.759854]\n",
            "1060 [D loss: 0.417450, acc.: 81.64%] [G loss: 3.896385]\n",
            "1080 [D loss: 0.648562, acc.: 64.84%] [G loss: 3.626264]\n",
            "1100 [D loss: 0.402189, acc.: 83.98%] [G loss: 3.678422]\n",
            "1120 [D loss: 0.418486, acc.: 82.81%] [G loss: 3.632628]\n",
            "1140 [D loss: 0.467013, acc.: 76.95%] [G loss: 3.824034]\n",
            "1160 [D loss: 0.570863, acc.: 70.31%] [G loss: 3.044311]\n",
            "1180 [D loss: 0.511243, acc.: 76.95%] [G loss: 3.353204]\n",
            "1200 [D loss: 0.478253, acc.: 78.12%] [G loss: 3.580424]\n",
            "1220 [D loss: 0.470860, acc.: 77.73%] [G loss: 3.401241]\n",
            "1240 [D loss: 0.509738, acc.: 75.00%] [G loss: 3.294276]\n",
            "1260 [D loss: 0.510494, acc.: 78.91%] [G loss: 3.284930]\n",
            "1280 [D loss: 0.589470, acc.: 67.19%] [G loss: 3.138620]\n",
            "1300 [D loss: 0.555125, acc.: 73.05%] [G loss: 2.883266]\n",
            "1320 [D loss: 0.479775, acc.: 79.30%] [G loss: 3.127534]\n",
            "1340 [D loss: 0.498140, acc.: 75.78%] [G loss: 3.167541]\n",
            "1360 [D loss: 0.530504, acc.: 76.56%] [G loss: 2.911742]\n",
            "1380 [D loss: 0.482003, acc.: 77.34%] [G loss: 3.266805]\n",
            "1400 [D loss: 0.470014, acc.: 80.47%] [G loss: 3.325715]\n",
            "1420 [D loss: 0.546466, acc.: 73.44%] [G loss: 3.093244]\n",
            "1440 [D loss: 0.548517, acc.: 73.83%] [G loss: 3.152225]\n",
            "1460 [D loss: 0.533497, acc.: 71.88%] [G loss: 3.297659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1-QlEvelsO",
        "outputId": "d5e6db9b-1b83-4118-fa9f-1173864d140b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      16128       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_73 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_33 (Flatten)            (None, 2304)         0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_32 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4708)         0           input_73[0][0]                   \n",
            "                                                                 flatten_33[0][0]                 \n",
            "                                                                 flatten_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 1024)         4822016     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, 1024)         0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 1024)         0           leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1024)         1049600     dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1024)         0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 1024)         0           leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 1024)         1049600     dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1024)         0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 1024)         0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 1)            1025        dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.849750, acc.: 30.47%] [G loss: 5.367685]\n",
            "20 [D loss: 0.341891, acc.: 91.41%] [G loss: 8.554846]\n",
            "40 [D loss: 0.710111, acc.: 44.14%] [G loss: 2.083147]\n",
            "60 [D loss: 0.715512, acc.: 46.48%] [G loss: 1.738583]\n",
            "80 [D loss: 0.648917, acc.: 56.25%] [G loss: 1.863081]\n",
            "100 [D loss: 0.701115, acc.: 44.92%] [G loss: 1.767049]\n",
            "120 [D loss: 0.732758, acc.: 40.23%] [G loss: 1.580268]\n",
            "140 [D loss: 0.706723, acc.: 48.05%] [G loss: 1.537512]\n",
            "160 [D loss: 0.717456, acc.: 40.23%] [G loss: 1.437199]\n",
            "180 [D loss: 0.710523, acc.: 42.97%] [G loss: 1.439812]\n",
            "200 [D loss: 0.711228, acc.: 45.70%] [G loss: 1.451416]\n",
            "220 [D loss: 0.721137, acc.: 35.55%] [G loss: 1.414149]\n",
            "240 [D loss: 0.714687, acc.: 42.58%] [G loss: 1.432792]\n",
            "260 [D loss: 0.703062, acc.: 46.09%] [G loss: 1.423483]\n",
            "280 [D loss: 0.724828, acc.: 36.72%] [G loss: 1.404998]\n",
            "300 [D loss: 0.716795, acc.: 42.58%] [G loss: 1.396966]\n",
            "320 [D loss: 0.720376, acc.: 42.97%] [G loss: 1.444355]\n",
            "340 [D loss: 0.706069, acc.: 46.88%] [G loss: 1.423882]\n",
            "360 [D loss: 0.709865, acc.: 39.06%] [G loss: 1.404603]\n",
            "380 [D loss: 0.714929, acc.: 43.75%] [G loss: 1.404289]\n",
            "400 [D loss: 0.711042, acc.: 42.97%] [G loss: 1.414505]\n",
            "420 [D loss: 0.717568, acc.: 40.23%] [G loss: 1.403865]\n",
            "440 [D loss: 0.711510, acc.: 40.23%] [G loss: 1.410104]\n",
            "460 [D loss: 0.714272, acc.: 41.41%] [G loss: 1.419285]\n",
            "480 [D loss: 0.708251, acc.: 41.02%] [G loss: 1.413074]\n",
            "500 [D loss: 0.706564, acc.: 40.62%] [G loss: 1.414759]\n",
            "520 [D loss: 0.711570, acc.: 39.84%] [G loss: 1.416901]\n",
            "540 [D loss: 0.707966, acc.: 44.14%] [G loss: 1.396562]\n",
            "560 [D loss: 0.714514, acc.: 40.62%] [G loss: 1.400588]\n",
            "580 [D loss: 0.708786, acc.: 39.06%] [G loss: 1.401240]\n",
            "600 [D loss: 0.715925, acc.: 37.50%] [G loss: 1.401176]\n",
            "620 [D loss: 0.700743, acc.: 42.19%] [G loss: 1.417309]\n",
            "640 [D loss: 0.706829, acc.: 40.23%] [G loss: 1.403668]\n",
            "660 [D loss: 0.707380, acc.: 39.84%] [G loss: 1.389831]\n",
            "680 [D loss: 0.707225, acc.: 39.84%] [G loss: 1.398783]\n",
            "700 [D loss: 0.703080, acc.: 40.62%] [G loss: 1.396986]\n",
            "720 [D loss: 0.718166, acc.: 34.38%] [G loss: 1.389264]\n",
            "740 [D loss: 0.713579, acc.: 40.23%] [G loss: 1.403226]\n",
            "760 [D loss: 0.705897, acc.: 45.31%] [G loss: 1.437489]\n",
            "780 [D loss: 0.707634, acc.: 40.62%] [G loss: 1.401700]\n",
            "800 [D loss: 0.701959, acc.: 39.06%] [G loss: 1.398977]\n",
            "820 [D loss: 0.706529, acc.: 42.19%] [G loss: 1.400484]\n",
            "840 [D loss: 0.700337, acc.: 41.02%] [G loss: 1.400969]\n",
            "860 [D loss: 0.706927, acc.: 38.67%] [G loss: 1.405506]\n",
            "880 [D loss: 0.708806, acc.: 38.28%] [G loss: 1.387069]\n",
            "900 [D loss: 0.706434, acc.: 41.41%] [G loss: 1.398114]\n",
            "920 [D loss: 0.704907, acc.: 35.94%] [G loss: 1.406425]\n",
            "940 [D loss: 0.696471, acc.: 47.66%] [G loss: 1.392628]\n",
            "960 [D loss: 0.705585, acc.: 41.02%] [G loss: 1.396953]\n",
            "980 [D loss: 0.705313, acc.: 39.84%] [G loss: 1.393413]\n",
            "1000 [D loss: 0.693701, acc.: 48.44%] [G loss: 1.410988]\n",
            "1020 [D loss: 0.707825, acc.: 38.28%] [G loss: 1.393575]\n",
            "1040 [D loss: 0.700687, acc.: 44.53%] [G loss: 1.400779]\n",
            "1060 [D loss: 0.703935, acc.: 39.84%] [G loss: 1.387593]\n",
            "1080 [D loss: 0.706629, acc.: 39.84%] [G loss: 1.389481]\n",
            "1100 [D loss: 0.707315, acc.: 37.89%] [G loss: 1.378371]\n",
            "1120 [D loss: 0.707380, acc.: 42.97%] [G loss: 1.383276]\n",
            "1140 [D loss: 0.701792, acc.: 45.31%] [G loss: 1.392983]\n",
            "1160 [D loss: 0.698018, acc.: 41.41%] [G loss: 1.396971]\n",
            "1180 [D loss: 0.710089, acc.: 35.94%] [G loss: 1.389826]\n",
            "1200 [D loss: 0.704007, acc.: 40.23%] [G loss: 1.392146]\n",
            "1220 [D loss: 0.704774, acc.: 42.97%] [G loss: 1.406959]\n",
            "1240 [D loss: 0.699959, acc.: 41.41%] [G loss: 1.412235]\n",
            "1260 [D loss: 0.702670, acc.: 44.53%] [G loss: 1.408520]\n",
            "1280 [D loss: 0.704078, acc.: 41.41%] [G loss: 1.384988]\n",
            "1300 [D loss: 0.707281, acc.: 40.62%] [G loss: 1.399330]\n",
            "1320 [D loss: 0.709640, acc.: 39.06%] [G loss: 1.400736]\n",
            "1340 [D loss: 0.697176, acc.: 46.88%] [G loss: 1.408559]\n",
            "1360 [D loss: 0.690890, acc.: 46.09%] [G loss: 1.407980]\n",
            "1380 [D loss: 0.695711, acc.: 48.44%] [G loss: 1.384939]\n",
            "1400 [D loss: 0.696284, acc.: 41.41%] [G loss: 1.391898]\n",
            "1420 [D loss: 0.702413, acc.: 41.02%] [G loss: 1.391516]\n",
            "1440 [D loss: 0.702305, acc.: 43.75%] [G loss: 1.404577]\n",
            "1460 [D loss: 0.702467, acc.: 40.62%] [G loss: 1.401769]\n",
            "1480 [D loss: 0.700135, acc.: 39.84%] [G loss: 1.391395]\n",
            "1500 [D loss: 0.701792, acc.: 41.80%] [G loss: 1.390833]\n",
            "1520 [D loss: 0.700585, acc.: 42.19%] [G loss: 1.394488]\n",
            "1540 [D loss: 0.704487, acc.: 41.41%] [G loss: 1.397046]\n",
            "1560 [D loss: 0.695279, acc.: 46.48%] [G loss: 1.408600]\n",
            "1580 [D loss: 0.695438, acc.: 48.83%] [G loss: 1.392757]\n",
            "1600 [D loss: 0.704662, acc.: 38.67%] [G loss: 1.390574]\n",
            "1620 [D loss: 0.699533, acc.: 46.09%] [G loss: 1.401004]\n",
            "1640 [D loss: 0.702735, acc.: 43.75%] [G loss: 1.398700]\n",
            "1660 [D loss: 0.697723, acc.: 43.75%] [G loss: 1.401226]\n",
            "1680 [D loss: 0.697773, acc.: 41.80%] [G loss: 1.410201]\n",
            "1700 [D loss: 0.696773, acc.: 43.36%] [G loss: 1.407255]\n",
            "1720 [D loss: 0.701552, acc.: 39.45%] [G loss: 1.395131]\n",
            "1740 [D loss: 0.696195, acc.: 48.44%] [G loss: 1.396674]\n",
            "1760 [D loss: 0.702511, acc.: 43.36%] [G loss: 1.383245]\n",
            "1780 [D loss: 0.704975, acc.: 40.62%] [G loss: 1.399527]\n",
            "1800 [D loss: 0.698895, acc.: 41.41%] [G loss: 1.391627]\n",
            "1820 [D loss: 0.702525, acc.: 37.50%] [G loss: 1.391825]\n",
            "1840 [D loss: 0.698295, acc.: 46.48%] [G loss: 1.401908]\n",
            "1860 [D loss: 0.699764, acc.: 42.97%] [G loss: 1.399934]\n",
            "1880 [D loss: 0.702305, acc.: 41.80%] [G loss: 1.396282]\n",
            "1900 [D loss: 0.705898, acc.: 42.19%] [G loss: 1.384895]\n",
            "1920 [D loss: 0.700531, acc.: 44.92%] [G loss: 1.391573]\n",
            "1940 [D loss: 0.707957, acc.: 41.02%] [G loss: 1.386891]\n",
            "1960 [D loss: 0.700286, acc.: 41.41%] [G loss: 1.396966]\n",
            "1980 [D loss: 0.697184, acc.: 42.19%] [G loss: 1.398432]\n",
            "2000 [D loss: 0.701151, acc.: 46.09%] [G loss: 1.408123]\n",
            "2020 [D loss: 0.707549, acc.: 41.80%] [G loss: 1.400710]\n",
            "2040 [D loss: 0.706370, acc.: 41.80%] [G loss: 1.382454]\n",
            "2060 [D loss: 0.696820, acc.: 44.14%] [G loss: 1.416820]\n",
            "2080 [D loss: 0.704532, acc.: 39.84%] [G loss: 1.388172]\n",
            "2100 [D loss: 0.698797, acc.: 41.80%] [G loss: 1.401481]\n",
            "2120 [D loss: 0.701321, acc.: 42.58%] [G loss: 1.405037]\n",
            "2140 [D loss: 0.700277, acc.: 42.58%] [G loss: 1.388465]\n",
            "2160 [D loss: 0.706183, acc.: 34.77%] [G loss: 1.395349]\n",
            "2180 [D loss: 0.700379, acc.: 44.53%] [G loss: 1.399946]\n",
            "2200 [D loss: 0.706776, acc.: 35.55%] [G loss: 1.397158]\n",
            "2220 [D loss: 0.690010, acc.: 49.61%] [G loss: 1.415059]\n",
            "2240 [D loss: 0.700388, acc.: 45.31%] [G loss: 1.396531]\n",
            "2260 [D loss: 0.698710, acc.: 41.41%] [G loss: 1.387498]\n",
            "2280 [D loss: 0.699097, acc.: 44.53%] [G loss: 1.400677]\n",
            "2300 [D loss: 0.698183, acc.: 41.41%] [G loss: 1.393741]\n",
            "2320 [D loss: 0.700072, acc.: 44.92%] [G loss: 1.387109]\n",
            "2340 [D loss: 0.702045, acc.: 43.75%] [G loss: 1.396456]\n",
            "2360 [D loss: 0.698545, acc.: 42.97%] [G loss: 1.386749]\n",
            "2380 [D loss: 0.701191, acc.: 44.53%] [G loss: 1.390651]\n",
            "2400 [D loss: 0.702897, acc.: 41.02%] [G loss: 1.385156]\n",
            "2420 [D loss: 0.705316, acc.: 41.02%] [G loss: 1.401502]\n",
            "2440 [D loss: 0.706111, acc.: 37.11%] [G loss: 1.388617]\n",
            "2460 [D loss: 0.702543, acc.: 41.02%] [G loss: 1.407373]\n",
            "2480 [D loss: 0.705135, acc.: 44.92%] [G loss: 1.386979]\n",
            "2500 [D loss: 0.699529, acc.: 43.75%] [G loss: 1.433604]\n",
            "2520 [D loss: 0.701719, acc.: 43.36%] [G loss: 1.395393]\n",
            "2540 [D loss: 0.699508, acc.: 42.58%] [G loss: 1.396002]\n",
            "2560 [D loss: 0.703406, acc.: 38.67%] [G loss: 1.390184]\n",
            "2580 [D loss: 0.702974, acc.: 38.67%] [G loss: 1.384311]\n",
            "2600 [D loss: 0.697291, acc.: 44.92%] [G loss: 1.391137]\n",
            "2620 [D loss: 0.698857, acc.: 46.48%] [G loss: 1.397241]\n",
            "2640 [D loss: 0.702192, acc.: 41.02%] [G loss: 1.391943]\n",
            "2660 [D loss: 0.703083, acc.: 35.94%] [G loss: 1.392532]\n",
            "2680 [D loss: 0.704199, acc.: 41.80%] [G loss: 1.383410]\n",
            "2700 [D loss: 0.705399, acc.: 40.62%] [G loss: 1.386579]\n",
            "2720 [D loss: 0.702500, acc.: 42.19%] [G loss: 1.398633]\n",
            "2740 [D loss: 0.701238, acc.: 41.41%] [G loss: 1.387710]\n",
            "2760 [D loss: 0.695434, acc.: 47.66%] [G loss: 1.398785]\n",
            "2780 [D loss: 0.698381, acc.: 46.48%] [G loss: 1.409770]\n",
            "2800 [D loss: 0.703224, acc.: 41.02%] [G loss: 1.398382]\n",
            "2820 [D loss: 0.704637, acc.: 42.97%] [G loss: 1.391324]\n",
            "2840 [D loss: 0.695273, acc.: 46.48%] [G loss: 1.403218]\n",
            "2860 [D loss: 0.703234, acc.: 45.31%] [G loss: 1.395308]\n",
            "2880 [D loss: 0.699889, acc.: 41.80%] [G loss: 1.389093]\n",
            "2900 [D loss: 0.704404, acc.: 42.19%] [G loss: 1.391987]\n",
            "2920 [D loss: 0.700413, acc.: 43.36%] [G loss: 1.394257]\n",
            "2940 [D loss: 0.701555, acc.: 45.31%] [G loss: 1.386916]\n",
            "2960 [D loss: 0.704574, acc.: 41.02%] [G loss: 1.397935]\n",
            "2980 [D loss: 0.701216, acc.: 38.28%] [G loss: 1.397118]\n",
            "3000 [D loss: 0.703859, acc.: 43.75%] [G loss: 1.386564]\n",
            "3020 [D loss: 0.697224, acc.: 47.27%] [G loss: 1.404107]\n",
            "3040 [D loss: 0.700284, acc.: 44.14%] [G loss: 1.408972]\n",
            "3060 [D loss: 0.710935, acc.: 39.45%] [G loss: 1.398036]\n",
            "3080 [D loss: 0.701705, acc.: 43.75%] [G loss: 1.385907]\n",
            "3100 [D loss: 0.703351, acc.: 42.19%] [G loss: 1.400969]\n",
            "3120 [D loss: 0.705543, acc.: 35.55%] [G loss: 1.395355]\n",
            "3140 [D loss: 0.705566, acc.: 41.02%] [G loss: 1.389833]\n",
            "3160 [D loss: 0.694014, acc.: 50.78%] [G loss: 1.399073]\n",
            "3180 [D loss: 0.696958, acc.: 47.27%] [G loss: 1.398360]\n",
            "3200 [D loss: 0.697163, acc.: 45.31%] [G loss: 1.396299]\n",
            "3220 [D loss: 0.704172, acc.: 39.45%] [G loss: 1.401600]\n",
            "3240 [D loss: 0.706146, acc.: 35.55%] [G loss: 1.386641]\n",
            "3260 [D loss: 0.693955, acc.: 46.88%] [G loss: 1.392742]\n",
            "3280 [D loss: 0.694101, acc.: 48.83%] [G loss: 1.400743]\n",
            "3300 [D loss: 0.699886, acc.: 42.58%] [G loss: 1.380824]\n",
            "3320 [D loss: 0.709387, acc.: 47.27%] [G loss: 1.398697]\n",
            "3340 [D loss: 0.699752, acc.: 48.44%] [G loss: 1.394596]\n",
            "3360 [D loss: 0.706852, acc.: 40.62%] [G loss: 1.405188]\n",
            "3380 [D loss: 0.699275, acc.: 44.53%] [G loss: 1.398795]\n",
            "3400 [D loss: 0.696006, acc.: 44.92%] [G loss: 1.421854]\n",
            "3420 [D loss: 0.696535, acc.: 48.05%] [G loss: 1.411503]\n",
            "3440 [D loss: 0.702331, acc.: 43.36%] [G loss: 1.399069]\n",
            "3460 [D loss: 0.705177, acc.: 42.19%] [G loss: 1.388886]\n",
            "3480 [D loss: 0.703387, acc.: 36.33%] [G loss: 1.393602]\n",
            "3500 [D loss: 0.701317, acc.: 38.67%] [G loss: 1.405229]\n",
            "3520 [D loss: 0.700779, acc.: 36.33%] [G loss: 1.394291]\n",
            "3540 [D loss: 0.704368, acc.: 43.75%] [G loss: 1.388223]\n",
            "3560 [D loss: 0.698320, acc.: 44.92%] [G loss: 1.408260]\n",
            "3580 [D loss: 0.699920, acc.: 41.80%] [G loss: 1.396066]\n",
            "3600 [D loss: 0.703215, acc.: 42.97%] [G loss: 1.393963]\n",
            "3620 [D loss: 0.704095, acc.: 42.58%] [G loss: 1.389001]\n",
            "3640 [D loss: 0.696503, acc.: 46.48%] [G loss: 1.391987]\n",
            "3660 [D loss: 0.706459, acc.: 39.45%] [G loss: 1.403823]\n",
            "3680 [D loss: 0.701097, acc.: 45.70%] [G loss: 1.391359]\n",
            "3700 [D loss: 0.700797, acc.: 40.62%] [G loss: 1.401428]\n",
            "3720 [D loss: 0.706842, acc.: 39.45%] [G loss: 1.401338]\n",
            "3740 [D loss: 0.695790, acc.: 48.44%] [G loss: 1.410640]\n",
            "3760 [D loss: 0.700871, acc.: 41.02%] [G loss: 1.389013]\n",
            "3780 [D loss: 0.698214, acc.: 45.31%] [G loss: 1.391579]\n",
            "3800 [D loss: 0.702306, acc.: 41.02%] [G loss: 1.390742]\n",
            "3820 [D loss: 0.699549, acc.: 44.92%] [G loss: 1.388456]\n",
            "3840 [D loss: 0.726881, acc.: 41.02%] [G loss: 1.427503]\n",
            "3860 [D loss: 0.706836, acc.: 40.23%] [G loss: 1.408166]\n",
            "3880 [D loss: 0.708650, acc.: 37.89%] [G loss: 1.388743]\n",
            "3900 [D loss: 0.703004, acc.: 42.19%] [G loss: 1.413317]\n",
            "3920 [D loss: 0.701894, acc.: 43.75%] [G loss: 1.391886]\n",
            "3940 [D loss: 0.695446, acc.: 49.22%] [G loss: 1.405801]\n",
            "3960 [D loss: 0.704857, acc.: 41.02%] [G loss: 1.401155]\n",
            "3980 [D loss: 0.699231, acc.: 42.58%] [G loss: 1.404975]\n",
            "4000 [D loss: 0.699956, acc.: 39.84%] [G loss: 1.397395]\n",
            "4020 [D loss: 0.693868, acc.: 48.83%] [G loss: 1.414370]\n",
            "4040 [D loss: 0.699592, acc.: 46.48%] [G loss: 1.402936]\n",
            "4060 [D loss: 0.697715, acc.: 45.31%] [G loss: 1.424391]\n",
            "4080 [D loss: 0.708574, acc.: 40.23%] [G loss: 1.381411]\n",
            "4100 [D loss: 1.072704, acc.: 84.38%] [G loss: 26.749468]\n",
            "4120 [D loss: 0.717382, acc.: 67.97%] [G loss: 6.265586]\n",
            "4140 [D loss: 1.001233, acc.: 50.00%] [G loss: 3.090046]\n",
            "4160 [D loss: 0.768635, acc.: 51.56%] [G loss: 2.125261]\n",
            "4180 [D loss: 0.740759, acc.: 46.48%] [G loss: 1.783273]\n",
            "4200 [D loss: 0.722959, acc.: 47.66%] [G loss: 1.520224]\n",
            "4220 [D loss: 0.734547, acc.: 45.31%] [G loss: 1.466105]\n",
            "4240 [D loss: 0.688544, acc.: 57.03%] [G loss: 1.615793]\n",
            "4260 [D loss: 0.738717, acc.: 43.75%] [G loss: 1.405132]\n",
            "4280 [D loss: 0.722575, acc.: 42.19%] [G loss: 1.447261]\n",
            "4300 [D loss: 0.690146, acc.: 49.61%] [G loss: 1.476141]\n",
            "4320 [D loss: 0.719582, acc.: 43.75%] [G loss: 1.429009]\n",
            "4340 [D loss: 0.720578, acc.: 45.70%] [G loss: 1.434604]\n",
            "4360 [D loss: 0.706739, acc.: 45.70%] [G loss: 1.414366]\n",
            "4380 [D loss: 0.702524, acc.: 48.44%] [G loss: 1.432420]\n",
            "4400 [D loss: 0.710964, acc.: 48.83%] [G loss: 1.441298]\n",
            "4420 [D loss: 0.711855, acc.: 46.48%] [G loss: 1.427701]\n",
            "4440 [D loss: 0.710840, acc.: 47.66%] [G loss: 1.458087]\n",
            "4460 [D loss: 0.705373, acc.: 44.53%] [G loss: 1.425750]\n",
            "4480 [D loss: 0.704472, acc.: 48.83%] [G loss: 1.433243]\n",
            "4500 [D loss: 0.698071, acc.: 51.17%] [G loss: 1.431295]\n",
            "4520 [D loss: 0.716092, acc.: 41.80%] [G loss: 1.414632]\n",
            "4540 [D loss: 0.709937, acc.: 44.53%] [G loss: 1.402517]\n",
            "4560 [D loss: 0.706339, acc.: 43.75%] [G loss: 1.417696]\n",
            "4580 [D loss: 0.701099, acc.: 49.61%] [G loss: 1.398619]\n",
            "4600 [D loss: 0.705118, acc.: 43.75%] [G loss: 1.420572]\n",
            "4620 [D loss: 0.706447, acc.: 43.75%] [G loss: 1.411010]\n",
            "4640 [D loss: 0.701780, acc.: 48.44%] [G loss: 1.398860]\n",
            "4660 [D loss: 0.705210, acc.: 48.05%] [G loss: 1.383090]\n",
            "4680 [D loss: 0.708879, acc.: 46.09%] [G loss: 1.397547]\n",
            "4700 [D loss: 0.710725, acc.: 42.97%] [G loss: 1.417038]\n",
            "4720 [D loss: 0.704634, acc.: 45.70%] [G loss: 1.393745]\n",
            "4740 [D loss: 0.696728, acc.: 51.56%] [G loss: 1.395010]\n",
            "4760 [D loss: 0.702551, acc.: 48.83%] [G loss: 1.404335]\n",
            "4780 [D loss: 0.699379, acc.: 46.48%] [G loss: 1.415344]\n",
            "4800 [D loss: 0.700585, acc.: 49.22%] [G loss: 1.406814]\n",
            "4820 [D loss: 0.698388, acc.: 46.88%] [G loss: 1.390556]\n",
            "4840 [D loss: 0.694551, acc.: 51.56%] [G loss: 1.385368]\n",
            "4860 [D loss: 0.694786, acc.: 53.12%] [G loss: 1.396678]\n",
            "4880 [D loss: 0.703839, acc.: 44.53%] [G loss: 1.405728]\n",
            "4900 [D loss: 0.705474, acc.: 44.53%] [G loss: 1.391244]\n",
            "4920 [D loss: 0.698867, acc.: 46.09%] [G loss: 1.391430]\n",
            "4940 [D loss: 0.705874, acc.: 39.45%] [G loss: 1.395215]\n",
            "4960 [D loss: 0.697893, acc.: 49.61%] [G loss: 1.396267]\n",
            "4980 [D loss: 0.709537, acc.: 39.84%] [G loss: 1.394031]\n",
            "5000 [D loss: 0.704992, acc.: 42.19%] [G loss: 1.393940]\n",
            "5020 [D loss: 0.702913, acc.: 48.05%] [G loss: 1.390844]\n",
            "5040 [D loss: 0.697565, acc.: 48.83%] [G loss: 1.388696]\n",
            "5060 [D loss: 0.700234, acc.: 42.58%] [G loss: 1.404058]\n",
            "5080 [D loss: 0.703479, acc.: 44.92%] [G loss: 1.406952]\n",
            "5100 [D loss: 0.693664, acc.: 49.22%] [G loss: 1.407932]\n",
            "5120 [D loss: 0.706131, acc.: 42.97%] [G loss: 1.404334]\n",
            "5140 [D loss: 0.700269, acc.: 48.44%] [G loss: 1.391555]\n",
            "5160 [D loss: 0.696768, acc.: 45.31%] [G loss: 1.396595]\n",
            "5180 [D loss: 0.699592, acc.: 46.09%] [G loss: 1.397417]\n",
            "5200 [D loss: 0.695383, acc.: 47.27%] [G loss: 1.387535]\n",
            "5220 [D loss: 0.693621, acc.: 47.27%] [G loss: 1.408859]\n",
            "5240 [D loss: 0.695037, acc.: 47.66%] [G loss: 1.403044]\n",
            "5260 [D loss: 0.700985, acc.: 41.41%] [G loss: 1.395171]\n",
            "5280 [D loss: 0.695701, acc.: 47.66%] [G loss: 1.403005]\n",
            "5300 [D loss: 0.700270, acc.: 42.19%] [G loss: 1.394384]\n",
            "5320 [D loss: 0.698988, acc.: 44.92%] [G loss: 1.401334]\n",
            "5340 [D loss: 0.694417, acc.: 53.12%] [G loss: 1.388224]\n",
            "5360 [D loss: 0.696847, acc.: 50.78%] [G loss: 1.402042]\n",
            "5380 [D loss: 0.699440, acc.: 46.48%] [G loss: 1.400354]\n",
            "5400 [D loss: 0.695333, acc.: 46.88%] [G loss: 1.389589]\n",
            "5420 [D loss: 0.693278, acc.: 52.34%] [G loss: 1.395435]\n",
            "5440 [D loss: 0.697515, acc.: 52.73%] [G loss: 1.403365]\n",
            "5460 [D loss: 0.700039, acc.: 44.92%] [G loss: 1.385313]\n",
            "5480 [D loss: 0.701342, acc.: 41.80%] [G loss: 1.400771]\n",
            "5500 [D loss: 0.691950, acc.: 52.73%] [G loss: 1.405167]\n",
            "5520 [D loss: 0.697928, acc.: 47.66%] [G loss: 1.407117]\n",
            "5540 [D loss: 0.694162, acc.: 50.39%] [G loss: 1.412841]\n",
            "5560 [D loss: 0.696665, acc.: 49.22%] [G loss: 1.398718]\n",
            "5580 [D loss: 0.700371, acc.: 45.31%] [G loss: 1.393484]\n",
            "5600 [D loss: 0.702304, acc.: 42.19%] [G loss: 1.393507]\n",
            "5620 [D loss: 0.694285, acc.: 50.39%] [G loss: 1.394578]\n",
            "5640 [D loss: 0.697163, acc.: 46.88%] [G loss: 1.395253]\n",
            "5660 [D loss: 0.693129, acc.: 50.78%] [G loss: 1.392592]\n",
            "5680 [D loss: 0.696335, acc.: 45.31%] [G loss: 1.386523]\n",
            "5700 [D loss: 0.697125, acc.: 50.00%] [G loss: 1.412799]\n",
            "5720 [D loss: 0.697893, acc.: 47.27%] [G loss: 1.399107]\n",
            "5740 [D loss: 0.695130, acc.: 49.22%] [G loss: 1.400753]\n",
            "5760 [D loss: 0.698657, acc.: 44.14%] [G loss: 1.395851]\n",
            "5780 [D loss: 0.697089, acc.: 47.66%] [G loss: 1.392132]\n",
            "5800 [D loss: 0.700125, acc.: 46.09%] [G loss: 1.401990]\n",
            "5820 [D loss: 0.697709, acc.: 47.27%] [G loss: 1.394505]\n",
            "5840 [D loss: 0.693451, acc.: 50.39%] [G loss: 1.400309]\n",
            "5860 [D loss: 0.697274, acc.: 45.31%] [G loss: 1.386672]\n",
            "5880 [D loss: 0.700695, acc.: 44.53%] [G loss: 1.398656]\n",
            "5900 [D loss: 0.697150, acc.: 47.27%] [G loss: 1.390164]\n",
            "5920 [D loss: 0.696779, acc.: 44.14%] [G loss: 1.396117]\n",
            "5940 [D loss: 0.693782, acc.: 50.00%] [G loss: 1.390036]\n",
            "5960 [D loss: 0.700256, acc.: 51.17%] [G loss: 1.398869]\n",
            "5980 [D loss: 0.696114, acc.: 49.22%] [G loss: 1.395630]\n",
            "6000 [D loss: 0.697736, acc.: 45.70%] [G loss: 1.393245]\n",
            "6020 [D loss: 0.699556, acc.: 43.75%] [G loss: 1.396108]\n",
            "6040 [D loss: 0.693481, acc.: 50.39%] [G loss: 1.397763]\n",
            "6060 [D loss: 0.703812, acc.: 44.14%] [G loss: 1.388477]\n",
            "6080 [D loss: 0.697451, acc.: 43.36%] [G loss: 1.389992]\n",
            "6100 [D loss: 0.692298, acc.: 46.88%] [G loss: 1.387400]\n",
            "6120 [D loss: 0.699595, acc.: 45.70%] [G loss: 1.405494]\n",
            "6140 [D loss: 0.697563, acc.: 45.70%] [G loss: 1.396657]\n",
            "6160 [D loss: 0.695493, acc.: 48.83%] [G loss: 1.383792]\n",
            "6180 [D loss: 0.697858, acc.: 43.75%] [G loss: 1.381215]\n",
            "6200 [D loss: 0.695829, acc.: 48.83%] [G loss: 1.407797]\n",
            "6220 [D loss: 0.717301, acc.: 37.11%] [G loss: 1.387314]\n",
            "6240 [D loss: 0.697440, acc.: 46.88%] [G loss: 1.404925]\n",
            "6260 [D loss: 0.693353, acc.: 48.05%] [G loss: 1.386393]\n",
            "6280 [D loss: 0.691351, acc.: 53.52%] [G loss: 1.411113]\n",
            "6300 [D loss: 0.697193, acc.: 48.44%] [G loss: 1.393546]\n",
            "6320 [D loss: 0.693712, acc.: 48.05%] [G loss: 1.386686]\n",
            "6340 [D loss: 0.701024, acc.: 43.75%] [G loss: 1.396646]\n",
            "6360 [D loss: 0.701343, acc.: 46.09%] [G loss: 1.392262]\n",
            "6380 [D loss: 0.696558, acc.: 48.05%] [G loss: 1.396049]\n",
            "6400 [D loss: 0.699096, acc.: 45.31%] [G loss: 1.396192]\n",
            "6420 [D loss: 0.695062, acc.: 48.44%] [G loss: 1.393358]\n",
            "6440 [D loss: 0.699274, acc.: 41.80%] [G loss: 1.398998]\n",
            "6460 [D loss: 0.698745, acc.: 42.97%] [G loss: 1.413985]\n",
            "6480 [D loss: 0.696595, acc.: 49.22%] [G loss: 1.407619]\n",
            "6500 [D loss: 0.699104, acc.: 44.53%] [G loss: 1.392327]\n",
            "6520 [D loss: 0.696238, acc.: 47.66%] [G loss: 1.400739]\n",
            "6540 [D loss: 0.694688, acc.: 49.61%] [G loss: 1.409534]\n",
            "6560 [D loss: 0.700760, acc.: 41.80%] [G loss: 1.394047]\n",
            "6580 [D loss: 0.690933, acc.: 51.56%] [G loss: 1.394747]\n",
            "6600 [D loss: 0.698814, acc.: 42.58%] [G loss: 1.389279]\n",
            "6620 [D loss: 0.700194, acc.: 44.53%] [G loss: 1.398422]\n",
            "6640 [D loss: 0.698942, acc.: 47.27%] [G loss: 1.386726]\n",
            "6660 [D loss: 0.700201, acc.: 48.05%] [G loss: 1.389932]\n",
            "6680 [D loss: 0.697296, acc.: 46.88%] [G loss: 1.393817]\n",
            "6700 [D loss: 0.700214, acc.: 42.97%] [G loss: 1.383901]\n",
            "6720 [D loss: 0.697019, acc.: 46.88%] [G loss: 1.392673]\n",
            "6740 [D loss: 0.701964, acc.: 48.05%] [G loss: 1.392654]\n",
            "6760 [D loss: 0.696323, acc.: 52.34%] [G loss: 1.397271]\n",
            "6780 [D loss: 0.699229, acc.: 43.75%] [G loss: 1.387532]\n",
            "6800 [D loss: 0.693560, acc.: 50.78%] [G loss: 1.384406]\n",
            "6820 [D loss: 0.699900, acc.: 46.48%] [G loss: 1.396095]\n",
            "6840 [D loss: 0.700351, acc.: 46.09%] [G loss: 1.400817]\n",
            "6860 [D loss: 0.696565, acc.: 50.39%] [G loss: 1.393807]\n",
            "6880 [D loss: 0.694871, acc.: 49.22%] [G loss: 1.389832]\n",
            "6900 [D loss: 0.700529, acc.: 46.88%] [G loss: 1.386945]\n",
            "6920 [D loss: 0.695679, acc.: 47.27%] [G loss: 1.394230]\n",
            "6940 [D loss: 0.697947, acc.: 43.75%] [G loss: 1.383168]\n",
            "6960 [D loss: 0.697765, acc.: 47.27%] [G loss: 1.401471]\n",
            "6980 [D loss: 0.699596, acc.: 42.97%] [G loss: 1.397855]\n",
            "7000 [D loss: 0.701152, acc.: 42.19%] [G loss: 1.395025]\n",
            "7020 [D loss: 0.697597, acc.: 44.53%] [G loss: 1.392945]\n",
            "7040 [D loss: 0.696670, acc.: 44.14%] [G loss: 1.395268]\n",
            "7060 [D loss: 0.701981, acc.: 41.80%] [G loss: 1.394562]\n",
            "7080 [D loss: 0.698433, acc.: 48.83%] [G loss: 1.398969]\n",
            "7100 [D loss: 0.699508, acc.: 44.92%] [G loss: 1.388082]\n",
            "7120 [D loss: 0.698022, acc.: 46.88%] [G loss: 1.393650]\n",
            "7140 [D loss: 0.694677, acc.: 44.53%] [G loss: 1.384647]\n",
            "7160 [D loss: 0.698689, acc.: 47.27%] [G loss: 1.392577]\n",
            "7180 [D loss: 0.698242, acc.: 42.58%] [G loss: 1.391645]\n",
            "7200 [D loss: 0.700687, acc.: 43.75%] [G loss: 1.396249]\n",
            "7220 [D loss: 0.696790, acc.: 44.14%] [G loss: 1.388211]\n",
            "7240 [D loss: 0.692794, acc.: 46.48%] [G loss: 1.397785]\n",
            "7260 [D loss: 0.698412, acc.: 43.36%] [G loss: 1.383541]\n",
            "7280 [D loss: 0.696848, acc.: 45.70%] [G loss: 1.394438]\n",
            "7300 [D loss: 0.694678, acc.: 45.70%] [G loss: 1.395237]\n",
            "7320 [D loss: 0.693550, acc.: 47.66%] [G loss: 1.395023]\n",
            "7340 [D loss: 0.697784, acc.: 46.09%] [G loss: 1.393231]\n",
            "7360 [D loss: 0.699125, acc.: 43.36%] [G loss: 1.386775]\n",
            "7380 [D loss: 0.698780, acc.: 42.97%] [G loss: 1.399514]\n",
            "7400 [D loss: 0.693878, acc.: 49.61%] [G loss: 1.394288]\n",
            "7420 [D loss: 0.694396, acc.: 50.39%] [G loss: 1.387681]\n",
            "7440 [D loss: 0.699487, acc.: 42.58%] [G loss: 1.391767]\n",
            "7460 [D loss: 0.695520, acc.: 47.27%] [G loss: 1.400790]\n",
            "7480 [D loss: 0.696784, acc.: 43.75%] [G loss: 1.389204]\n",
            "7500 [D loss: 0.705090, acc.: 40.23%] [G loss: 1.392731]\n",
            "7520 [D loss: 0.698871, acc.: 41.02%] [G loss: 1.388768]\n",
            "7540 [D loss: 0.697078, acc.: 46.09%] [G loss: 1.396813]\n",
            "7560 [D loss: 0.696970, acc.: 43.36%] [G loss: 1.395193]\n",
            "7580 [D loss: 0.696497, acc.: 46.09%] [G loss: 1.390957]\n",
            "7600 [D loss: 0.702041, acc.: 41.02%] [G loss: 1.395289]\n",
            "7620 [D loss: 0.697056, acc.: 43.75%] [G loss: 1.393222]\n",
            "7640 [D loss: 0.696136, acc.: 47.27%] [G loss: 1.394576]\n",
            "7660 [D loss: 0.694996, acc.: 47.27%] [G loss: 1.393033]\n",
            "7680 [D loss: 0.695782, acc.: 46.48%] [G loss: 1.385630]\n",
            "7700 [D loss: 0.693979, acc.: 50.39%] [G loss: 1.398277]\n",
            "7720 [D loss: 0.702364, acc.: 45.70%] [G loss: 1.391881]\n",
            "7740 [D loss: 0.699747, acc.: 38.67%] [G loss: 1.395062]\n",
            "7760 [D loss: 0.697299, acc.: 44.53%] [G loss: 1.393300]\n",
            "7780 [D loss: 0.697575, acc.: 44.14%] [G loss: 1.388515]\n",
            "7800 [D loss: 0.696327, acc.: 47.27%] [G loss: 1.399309]\n",
            "7820 [D loss: 0.699796, acc.: 39.45%] [G loss: 1.394260]\n",
            "7840 [D loss: 0.697553, acc.: 44.92%] [G loss: 1.389583]\n",
            "7860 [D loss: 0.698274, acc.: 40.62%] [G loss: 1.385870]\n",
            "7880 [D loss: 0.695591, acc.: 50.39%] [G loss: 1.394403]\n",
            "7900 [D loss: 0.697667, acc.: 42.97%] [G loss: 1.399807]\n",
            "7920 [D loss: 0.699197, acc.: 43.36%] [G loss: 1.389332]\n",
            "7940 [D loss: 0.695461, acc.: 43.75%] [G loss: 1.395153]\n",
            "7960 [D loss: 0.693985, acc.: 50.78%] [G loss: 1.396758]\n",
            "7980 [D loss: 0.694616, acc.: 49.61%] [G loss: 1.394727]\n",
            "8000 [D loss: 0.698273, acc.: 47.66%] [G loss: 1.394009]\n",
            "8020 [D loss: 0.697546, acc.: 46.09%] [G loss: 1.393506]\n",
            "8040 [D loss: 0.696341, acc.: 41.80%] [G loss: 1.394150]\n",
            "8060 [D loss: 0.697245, acc.: 48.44%] [G loss: 1.392660]\n",
            "8080 [D loss: 0.694503, acc.: 49.22%] [G loss: 1.399790]\n",
            "8100 [D loss: 0.699422, acc.: 40.23%] [G loss: 1.391258]\n",
            "8120 [D loss: 0.694864, acc.: 47.66%] [G loss: 1.396652]\n",
            "8140 [D loss: 0.694395, acc.: 45.70%] [G loss: 1.396786]\n",
            "8160 [D loss: 0.696896, acc.: 41.02%] [G loss: 1.390924]\n",
            "8180 [D loss: 0.697071, acc.: 45.31%] [G loss: 1.389972]\n",
            "8200 [D loss: 0.698414, acc.: 41.02%] [G loss: 1.395693]\n",
            "8220 [D loss: 0.696968, acc.: 42.97%] [G loss: 1.391768]\n",
            "8240 [D loss: 0.695341, acc.: 49.61%] [G loss: 1.391918]\n",
            "8260 [D loss: 0.696258, acc.: 42.97%] [G loss: 1.389055]\n",
            "8280 [D loss: 0.694510, acc.: 44.92%] [G loss: 1.391949]\n",
            "8300 [D loss: 0.693706, acc.: 51.17%] [G loss: 1.391831]\n",
            "8320 [D loss: 0.692619, acc.: 50.78%] [G loss: 1.394634]\n",
            "8340 [D loss: 0.692181, acc.: 45.31%] [G loss: 1.398439]\n",
            "8360 [D loss: 0.696840, acc.: 50.00%] [G loss: 1.386354]\n",
            "8380 [D loss: 0.696040, acc.: 45.70%] [G loss: 1.388847]\n",
            "8400 [D loss: 0.697098, acc.: 38.28%] [G loss: 1.385808]\n",
            "8420 [D loss: 0.696205, acc.: 48.44%] [G loss: 1.395315]\n",
            "8440 [D loss: 0.696577, acc.: 44.53%] [G loss: 1.395890]\n",
            "8460 [D loss: 0.698952, acc.: 40.23%] [G loss: 1.392299]\n",
            "8480 [D loss: 0.705718, acc.: 39.84%] [G loss: 1.425954]\n",
            "8500 [D loss: 0.692237, acc.: 47.66%] [G loss: 1.392538]\n",
            "8520 [D loss: 0.694271, acc.: 43.36%] [G loss: 1.400621]\n",
            "8540 [D loss: 0.697283, acc.: 43.36%] [G loss: 1.392912]\n",
            "8560 [D loss: 0.693932, acc.: 46.88%] [G loss: 1.389800]\n",
            "8580 [D loss: 0.698320, acc.: 41.02%] [G loss: 1.390708]\n",
            "8600 [D loss: 0.696863, acc.: 44.53%] [G loss: 1.393254]\n",
            "8620 [D loss: 0.693541, acc.: 48.83%] [G loss: 1.392359]\n",
            "8640 [D loss: 0.698448, acc.: 46.09%] [G loss: 1.390194]\n",
            "8660 [D loss: 0.698225, acc.: 43.36%] [G loss: 1.391702]\n",
            "8680 [D loss: 0.696173, acc.: 43.36%] [G loss: 1.393579]\n",
            "8700 [D loss: 0.698311, acc.: 39.84%] [G loss: 1.393775]\n",
            "8720 [D loss: 0.695746, acc.: 42.97%] [G loss: 1.389348]\n",
            "8740 [D loss: 0.697773, acc.: 45.31%] [G loss: 1.388839]\n",
            "8760 [D loss: 0.698051, acc.: 41.02%] [G loss: 1.387564]\n",
            "8780 [D loss: 0.695869, acc.: 41.80%] [G loss: 1.393906]\n",
            "8800 [D loss: 0.695220, acc.: 43.36%] [G loss: 1.384415]\n",
            "8820 [D loss: 0.694940, acc.: 43.36%] [G loss: 1.385797]\n",
            "8840 [D loss: 0.699507, acc.: 44.53%] [G loss: 1.392140]\n",
            "8860 [D loss: 0.698243, acc.: 41.02%] [G loss: 1.383003]\n",
            "8880 [D loss: 0.694200, acc.: 51.17%] [G loss: 1.389239]\n",
            "8900 [D loss: 0.700332, acc.: 39.45%] [G loss: 1.392415]\n",
            "8920 [D loss: 0.694468, acc.: 43.75%] [G loss: 1.398754]\n",
            "8940 [D loss: 0.693868, acc.: 46.09%] [G loss: 1.401220]\n",
            "8960 [D loss: 0.696201, acc.: 46.09%] [G loss: 1.390292]\n",
            "8980 [D loss: 0.700917, acc.: 37.11%] [G loss: 1.384135]\n",
            "9000 [D loss: 0.529419, acc.: 60.16%] [G loss: 3.451092]\n",
            "9020 [D loss: 0.714005, acc.: 34.77%] [G loss: 1.418368]\n",
            "9040 [D loss: 0.708870, acc.: 40.62%] [G loss: 1.395880]\n",
            "9060 [D loss: 0.705619, acc.: 41.80%] [G loss: 1.390461]\n",
            "9080 [D loss: 0.698528, acc.: 45.31%] [G loss: 1.404152]\n",
            "9100 [D loss: 0.699169, acc.: 39.06%] [G loss: 1.402751]\n",
            "9120 [D loss: 0.696621, acc.: 41.41%] [G loss: 1.406112]\n",
            "9140 [D loss: 0.698882, acc.: 44.92%] [G loss: 1.396840]\n",
            "9160 [D loss: 0.696500, acc.: 44.53%] [G loss: 1.399978]\n",
            "9180 [D loss: 0.694397, acc.: 50.39%] [G loss: 1.403586]\n",
            "9200 [D loss: 0.694435, acc.: 49.22%] [G loss: 1.399590]\n",
            "9220 [D loss: 0.696431, acc.: 43.36%] [G loss: 1.405650]\n",
            "9240 [D loss: 0.695898, acc.: 47.27%] [G loss: 1.392597]\n",
            "9260 [D loss: 0.698319, acc.: 42.58%] [G loss: 1.398521]\n",
            "9280 [D loss: 0.700153, acc.: 39.06%] [G loss: 1.388895]\n",
            "9300 [D loss: 0.693785, acc.: 47.27%] [G loss: 1.391394]\n",
            "9320 [D loss: 0.691352, acc.: 50.78%] [G loss: 1.394004]\n",
            "9340 [D loss: 0.694901, acc.: 43.36%] [G loss: 1.396414]\n",
            "9360 [D loss: 0.696790, acc.: 42.58%] [G loss: 1.395674]\n",
            "9380 [D loss: 0.692653, acc.: 48.83%] [G loss: 1.399105]\n",
            "9400 [D loss: 0.697056, acc.: 44.14%] [G loss: 1.392526]\n",
            "9420 [D loss: 0.699672, acc.: 42.97%] [G loss: 1.388630]\n",
            "9440 [D loss: 0.696520, acc.: 47.66%] [G loss: 1.388295]\n",
            "9460 [D loss: 0.692791, acc.: 49.22%] [G loss: 1.395600]\n",
            "9480 [D loss: 0.692895, acc.: 45.70%] [G loss: 1.389935]\n",
            "9500 [D loss: 0.697345, acc.: 44.92%] [G loss: 1.405686]\n",
            "9520 [D loss: 0.695038, acc.: 48.83%] [G loss: 1.398619]\n",
            "9540 [D loss: 0.693577, acc.: 49.22%] [G loss: 1.392874]\n",
            "9560 [D loss: 0.696086, acc.: 44.53%] [G loss: 1.395670]\n",
            "9580 [D loss: 0.694003, acc.: 47.66%] [G loss: 1.405685]\n",
            "9600 [D loss: 0.691519, acc.: 48.44%] [G loss: 1.388005]\n",
            "9620 [D loss: 0.701189, acc.: 41.41%] [G loss: 1.402854]\n",
            "9640 [D loss: 0.692627, acc.: 50.39%] [G loss: 1.407890]\n",
            "9660 [D loss: 0.699078, acc.: 45.31%] [G loss: 1.403761]\n",
            "9680 [D loss: 0.692119, acc.: 46.48%] [G loss: 1.422443]\n",
            "9700 [D loss: 0.694870, acc.: 47.27%] [G loss: 1.402087]\n",
            "9720 [D loss: 0.694653, acc.: 47.66%] [G loss: 1.401800]\n",
            "9740 [D loss: 0.691276, acc.: 53.52%] [G loss: 1.412094]\n",
            "9760 [D loss: 0.697540, acc.: 41.41%] [G loss: 1.401138]\n",
            "9780 [D loss: 0.697595, acc.: 46.09%] [G loss: 1.409874]\n",
            "9800 [D loss: 0.693005, acc.: 46.48%] [G loss: 1.400883]\n",
            "9820 [D loss: 0.692952, acc.: 54.30%] [G loss: 1.406455]\n",
            "9840 [D loss: 0.691773, acc.: 48.05%] [G loss: 1.401276]\n",
            "9860 [D loss: 0.691053, acc.: 51.17%] [G loss: 1.397547]\n",
            "9880 [D loss: 0.687381, acc.: 53.12%] [G loss: 1.407923]\n",
            "9900 [D loss: 0.692056, acc.: 47.27%] [G loss: 1.400826]\n",
            "9920 [D loss: 0.694512, acc.: 43.75%] [G loss: 1.410615]\n",
            "9940 [D loss: 0.693986, acc.: 45.70%] [G loss: 1.404719]\n",
            "9960 [D loss: 0.691524, acc.: 52.73%] [G loss: 1.402085]\n",
            "9980 [D loss: 0.695208, acc.: 48.05%] [G loss: 1.405380]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
