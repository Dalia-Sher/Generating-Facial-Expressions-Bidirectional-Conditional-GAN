{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "6_Bidirectional_Conditional_GAN_6emo_with_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dalia-Sher/Generating-Facial-Expressions-Bidirectional-Conditional-GAN/blob/Shir/6_Bidirectional_Conditional_GAN_6emo_with_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt_ZQaFJO9F5"
      },
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "aae92774-5374-4c6a-fe2f-cb71af42df66"
      },
      "source": [
        "data = pd.read_csv('fer2013.csv')\n",
        "data = data[data.emotion != 1]\n",
        "data.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIL-PyaBP_oP",
        "outputId": "5d21ffa9-1f9b-454a-d5b3-fe6ae2004a1e"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35340, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHR9swxs5P0w"
      },
      "source": [
        "data['emotion'] = data.emotion.replace(6, 1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQj2Szg75fm5",
        "outputId": "a81e4c31-1982-4e38-d363-c781e244c5dc"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yxZyZWONmc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "bb0db007-d2b1-4ec1-cabc-d78897a4dba7"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tX-9YaZPDyx"
      },
      "source": [
        "Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoYxTe66PY9N",
        "outputId": "d05961f8-ceb3-4a96-8dbd-9c54a85b70a9"
      },
      "source": [
        "#flipping image horizontally\n",
        "iterations_per_image = 1\n",
        "for k in range(len(X_train)):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  flipped_images = []\n",
        "  emotions_list = []\n",
        "  for i in range(iterations_per_image):\n",
        "    flip_hr=iaa.Fliplr(p=1.0)\n",
        "    flip_hr_image= flip_hr.augment_image(img)\n",
        "    flipped_images.append(flip_hr_image)\n",
        "  flipped_images = np.array(flipped_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train = np.concatenate((X_train, flipped_images), axis=0)\n",
        "  emotions_list = [emotion]*iterations_per_image\n",
        "  y_train = np.concatenate((y_train, emotions_list), axis=0)\n",
        "  print(\"iteration:\" , k ,\"train shape:\",X_train.shape)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration: 30338 train shape: (65679, 48, 48, 1)\n",
            "iteration: 30339 train shape: (65680, 48, 48, 1)\n",
            "iteration: 30340 train shape: (65681, 48, 48, 1)\n",
            "iteration: 30341 train shape: (65682, 48, 48, 1)\n",
            "iteration: 30342 train shape: (65683, 48, 48, 1)\n",
            "iteration: 30343 train shape: (65684, 48, 48, 1)\n",
            "iteration: 30344 train shape: (65685, 48, 48, 1)\n",
            "iteration: 30345 train shape: (65686, 48, 48, 1)\n",
            "iteration: 30346 train shape: (65687, 48, 48, 1)\n",
            "iteration: 30347 train shape: (65688, 48, 48, 1)\n",
            "iteration: 30348 train shape: (65689, 48, 48, 1)\n",
            "iteration: 30349 train shape: (65690, 48, 48, 1)\n",
            "iteration: 30350 train shape: (65691, 48, 48, 1)\n",
            "iteration: 30351 train shape: (65692, 48, 48, 1)\n",
            "iteration: 30352 train shape: (65693, 48, 48, 1)\n",
            "iteration: 30353 train shape: (65694, 48, 48, 1)\n",
            "iteration: 30354 train shape: (65695, 48, 48, 1)\n",
            "iteration: 30355 train shape: (65696, 48, 48, 1)\n",
            "iteration: 30356 train shape: (65697, 48, 48, 1)\n",
            "iteration: 30357 train shape: (65698, 48, 48, 1)\n",
            "iteration: 30358 train shape: (65699, 48, 48, 1)\n",
            "iteration: 30359 train shape: (65700, 48, 48, 1)\n",
            "iteration: 30360 train shape: (65701, 48, 48, 1)\n",
            "iteration: 30361 train shape: (65702, 48, 48, 1)\n",
            "iteration: 30362 train shape: (65703, 48, 48, 1)\n",
            "iteration: 30363 train shape: (65704, 48, 48, 1)\n",
            "iteration: 30364 train shape: (65705, 48, 48, 1)\n",
            "iteration: 30365 train shape: (65706, 48, 48, 1)\n",
            "iteration: 30366 train shape: (65707, 48, 48, 1)\n",
            "iteration: 30367 train shape: (65708, 48, 48, 1)\n",
            "iteration: 30368 train shape: (65709, 48, 48, 1)\n",
            "iteration: 30369 train shape: (65710, 48, 48, 1)\n",
            "iteration: 30370 train shape: (65711, 48, 48, 1)\n",
            "iteration: 30371 train shape: (65712, 48, 48, 1)\n",
            "iteration: 30372 train shape: (65713, 48, 48, 1)\n",
            "iteration: 30373 train shape: (65714, 48, 48, 1)\n",
            "iteration: 30374 train shape: (65715, 48, 48, 1)\n",
            "iteration: 30375 train shape: (65716, 48, 48, 1)\n",
            "iteration: 30376 train shape: (65717, 48, 48, 1)\n",
            "iteration: 30377 train shape: (65718, 48, 48, 1)\n",
            "iteration: 30378 train shape: (65719, 48, 48, 1)\n",
            "iteration: 30379 train shape: (65720, 48, 48, 1)\n",
            "iteration: 30380 train shape: (65721, 48, 48, 1)\n",
            "iteration: 30381 train shape: (65722, 48, 48, 1)\n",
            "iteration: 30382 train shape: (65723, 48, 48, 1)\n",
            "iteration: 30383 train shape: (65724, 48, 48, 1)\n",
            "iteration: 30384 train shape: (65725, 48, 48, 1)\n",
            "iteration: 30385 train shape: (65726, 48, 48, 1)\n",
            "iteration: 30386 train shape: (65727, 48, 48, 1)\n",
            "iteration: 30387 train shape: (65728, 48, 48, 1)\n",
            "iteration: 30388 train shape: (65729, 48, 48, 1)\n",
            "iteration: 30389 train shape: (65730, 48, 48, 1)\n",
            "iteration: 30390 train shape: (65731, 48, 48, 1)\n",
            "iteration: 30391 train shape: (65732, 48, 48, 1)\n",
            "iteration: 30392 train shape: (65733, 48, 48, 1)\n",
            "iteration: 30393 train shape: (65734, 48, 48, 1)\n",
            "iteration: 30394 train shape: (65735, 48, 48, 1)\n",
            "iteration: 30395 train shape: (65736, 48, 48, 1)\n",
            "iteration: 30396 train shape: (65737, 48, 48, 1)\n",
            "iteration: 30397 train shape: (65738, 48, 48, 1)\n",
            "iteration: 30398 train shape: (65739, 48, 48, 1)\n",
            "iteration: 30399 train shape: (65740, 48, 48, 1)\n",
            "iteration: 30400 train shape: (65741, 48, 48, 1)\n",
            "iteration: 30401 train shape: (65742, 48, 48, 1)\n",
            "iteration: 30402 train shape: (65743, 48, 48, 1)\n",
            "iteration: 30403 train shape: (65744, 48, 48, 1)\n",
            "iteration: 30404 train shape: (65745, 48, 48, 1)\n",
            "iteration: 30405 train shape: (65746, 48, 48, 1)\n",
            "iteration: 30406 train shape: (65747, 48, 48, 1)\n",
            "iteration: 30407 train shape: (65748, 48, 48, 1)\n",
            "iteration: 30408 train shape: (65749, 48, 48, 1)\n",
            "iteration: 30409 train shape: (65750, 48, 48, 1)\n",
            "iteration: 30410 train shape: (65751, 48, 48, 1)\n",
            "iteration: 30411 train shape: (65752, 48, 48, 1)\n",
            "iteration: 30412 train shape: (65753, 48, 48, 1)\n",
            "iteration: 30413 train shape: (65754, 48, 48, 1)\n",
            "iteration: 30414 train shape: (65755, 48, 48, 1)\n",
            "iteration: 30415 train shape: (65756, 48, 48, 1)\n",
            "iteration: 30416 train shape: (65757, 48, 48, 1)\n",
            "iteration: 30417 train shape: (65758, 48, 48, 1)\n",
            "iteration: 30418 train shape: (65759, 48, 48, 1)\n",
            "iteration: 30419 train shape: (65760, 48, 48, 1)\n",
            "iteration: 30420 train shape: (65761, 48, 48, 1)\n",
            "iteration: 30421 train shape: (65762, 48, 48, 1)\n",
            "iteration: 30422 train shape: (65763, 48, 48, 1)\n",
            "iteration: 30423 train shape: (65764, 48, 48, 1)\n",
            "iteration: 30424 train shape: (65765, 48, 48, 1)\n",
            "iteration: 30425 train shape: (65766, 48, 48, 1)\n",
            "iteration: 30426 train shape: (65767, 48, 48, 1)\n",
            "iteration: 30427 train shape: (65768, 48, 48, 1)\n",
            "iteration: 30428 train shape: (65769, 48, 48, 1)\n",
            "iteration: 30429 train shape: (65770, 48, 48, 1)\n",
            "iteration: 30430 train shape: (65771, 48, 48, 1)\n",
            "iteration: 30431 train shape: (65772, 48, 48, 1)\n",
            "iteration: 30432 train shape: (65773, 48, 48, 1)\n",
            "iteration: 30433 train shape: (65774, 48, 48, 1)\n",
            "iteration: 30434 train shape: (65775, 48, 48, 1)\n",
            "iteration: 30435 train shape: (65776, 48, 48, 1)\n",
            "iteration: 30436 train shape: (65777, 48, 48, 1)\n",
            "iteration: 30437 train shape: (65778, 48, 48, 1)\n",
            "iteration: 30438 train shape: (65779, 48, 48, 1)\n",
            "iteration: 30439 train shape: (65780, 48, 48, 1)\n",
            "iteration: 30440 train shape: (65781, 48, 48, 1)\n",
            "iteration: 30441 train shape: (65782, 48, 48, 1)\n",
            "iteration: 30442 train shape: (65783, 48, 48, 1)\n",
            "iteration: 30443 train shape: (65784, 48, 48, 1)\n",
            "iteration: 30444 train shape: (65785, 48, 48, 1)\n",
            "iteration: 30445 train shape: (65786, 48, 48, 1)\n",
            "iteration: 30446 train shape: (65787, 48, 48, 1)\n",
            "iteration: 30447 train shape: (65788, 48, 48, 1)\n",
            "iteration: 30448 train shape: (65789, 48, 48, 1)\n",
            "iteration: 30449 train shape: (65790, 48, 48, 1)\n",
            "iteration: 30450 train shape: (65791, 48, 48, 1)\n",
            "iteration: 30451 train shape: (65792, 48, 48, 1)\n",
            "iteration: 30452 train shape: (65793, 48, 48, 1)\n",
            "iteration: 30453 train shape: (65794, 48, 48, 1)\n",
            "iteration: 30454 train shape: (65795, 48, 48, 1)\n",
            "iteration: 30455 train shape: (65796, 48, 48, 1)\n",
            "iteration: 30456 train shape: (65797, 48, 48, 1)\n",
            "iteration: 30457 train shape: (65798, 48, 48, 1)\n",
            "iteration: 30458 train shape: (65799, 48, 48, 1)\n",
            "iteration: 30459 train shape: (65800, 48, 48, 1)\n",
            "iteration: 30460 train shape: (65801, 48, 48, 1)\n",
            "iteration: 30461 train shape: (65802, 48, 48, 1)\n",
            "iteration: 30462 train shape: (65803, 48, 48, 1)\n",
            "iteration: 30463 train shape: (65804, 48, 48, 1)\n",
            "iteration: 30464 train shape: (65805, 48, 48, 1)\n",
            "iteration: 30465 train shape: (65806, 48, 48, 1)\n",
            "iteration: 30466 train shape: (65807, 48, 48, 1)\n",
            "iteration: 30467 train shape: (65808, 48, 48, 1)\n",
            "iteration: 30468 train shape: (65809, 48, 48, 1)\n",
            "iteration: 30469 train shape: (65810, 48, 48, 1)\n",
            "iteration: 30470 train shape: (65811, 48, 48, 1)\n",
            "iteration: 30471 train shape: (65812, 48, 48, 1)\n",
            "iteration: 30472 train shape: (65813, 48, 48, 1)\n",
            "iteration: 30473 train shape: (65814, 48, 48, 1)\n",
            "iteration: 30474 train shape: (65815, 48, 48, 1)\n",
            "iteration: 30475 train shape: (65816, 48, 48, 1)\n",
            "iteration: 30476 train shape: (65817, 48, 48, 1)\n",
            "iteration: 30477 train shape: (65818, 48, 48, 1)\n",
            "iteration: 30478 train shape: (65819, 48, 48, 1)\n",
            "iteration: 30479 train shape: (65820, 48, 48, 1)\n",
            "iteration: 30480 train shape: (65821, 48, 48, 1)\n",
            "iteration: 30481 train shape: (65822, 48, 48, 1)\n",
            "iteration: 30482 train shape: (65823, 48, 48, 1)\n",
            "iteration: 30483 train shape: (65824, 48, 48, 1)\n",
            "iteration: 30484 train shape: (65825, 48, 48, 1)\n",
            "iteration: 30485 train shape: (65826, 48, 48, 1)\n",
            "iteration: 30486 train shape: (65827, 48, 48, 1)\n",
            "iteration: 30487 train shape: (65828, 48, 48, 1)\n",
            "iteration: 30488 train shape: (65829, 48, 48, 1)\n",
            "iteration: 30489 train shape: (65830, 48, 48, 1)\n",
            "iteration: 30490 train shape: (65831, 48, 48, 1)\n",
            "iteration: 30491 train shape: (65832, 48, 48, 1)\n",
            "iteration: 30492 train shape: (65833, 48, 48, 1)\n",
            "iteration: 30493 train shape: (65834, 48, 48, 1)\n",
            "iteration: 30494 train shape: (65835, 48, 48, 1)\n",
            "iteration: 30495 train shape: (65836, 48, 48, 1)\n",
            "iteration: 30496 train shape: (65837, 48, 48, 1)\n",
            "iteration: 30497 train shape: (65838, 48, 48, 1)\n",
            "iteration: 30498 train shape: (65839, 48, 48, 1)\n",
            "iteration: 30499 train shape: (65840, 48, 48, 1)\n",
            "iteration: 30500 train shape: (65841, 48, 48, 1)\n",
            "iteration: 30501 train shape: (65842, 48, 48, 1)\n",
            "iteration: 30502 train shape: (65843, 48, 48, 1)\n",
            "iteration: 30503 train shape: (65844, 48, 48, 1)\n",
            "iteration: 30504 train shape: (65845, 48, 48, 1)\n",
            "iteration: 30505 train shape: (65846, 48, 48, 1)\n",
            "iteration: 30506 train shape: (65847, 48, 48, 1)\n",
            "iteration: 30507 train shape: (65848, 48, 48, 1)\n",
            "iteration: 30508 train shape: (65849, 48, 48, 1)\n",
            "iteration: 30509 train shape: (65850, 48, 48, 1)\n",
            "iteration: 30510 train shape: (65851, 48, 48, 1)\n",
            "iteration: 30511 train shape: (65852, 48, 48, 1)\n",
            "iteration: 30512 train shape: (65853, 48, 48, 1)\n",
            "iteration: 30513 train shape: (65854, 48, 48, 1)\n",
            "iteration: 30514 train shape: (65855, 48, 48, 1)\n",
            "iteration: 30515 train shape: (65856, 48, 48, 1)\n",
            "iteration: 30516 train shape: (65857, 48, 48, 1)\n",
            "iteration: 30517 train shape: (65858, 48, 48, 1)\n",
            "iteration: 30518 train shape: (65859, 48, 48, 1)\n",
            "iteration: 30519 train shape: (65860, 48, 48, 1)\n",
            "iteration: 30520 train shape: (65861, 48, 48, 1)\n",
            "iteration: 30521 train shape: (65862, 48, 48, 1)\n",
            "iteration: 30522 train shape: (65863, 48, 48, 1)\n",
            "iteration: 30523 train shape: (65864, 48, 48, 1)\n",
            "iteration: 30524 train shape: (65865, 48, 48, 1)\n",
            "iteration: 30525 train shape: (65866, 48, 48, 1)\n",
            "iteration: 30526 train shape: (65867, 48, 48, 1)\n",
            "iteration: 30527 train shape: (65868, 48, 48, 1)\n",
            "iteration: 30528 train shape: (65869, 48, 48, 1)\n",
            "iteration: 30529 train shape: (65870, 48, 48, 1)\n",
            "iteration: 30530 train shape: (65871, 48, 48, 1)\n",
            "iteration: 30531 train shape: (65872, 48, 48, 1)\n",
            "iteration: 30532 train shape: (65873, 48, 48, 1)\n",
            "iteration: 30533 train shape: (65874, 48, 48, 1)\n",
            "iteration: 30534 train shape: (65875, 48, 48, 1)\n",
            "iteration: 30535 train shape: (65876, 48, 48, 1)\n",
            "iteration: 30536 train shape: (65877, 48, 48, 1)\n",
            "iteration: 30537 train shape: (65878, 48, 48, 1)\n",
            "iteration: 30538 train shape: (65879, 48, 48, 1)\n",
            "iteration: 30539 train shape: (65880, 48, 48, 1)\n",
            "iteration: 30540 train shape: (65881, 48, 48, 1)\n",
            "iteration: 30541 train shape: (65882, 48, 48, 1)\n",
            "iteration: 30542 train shape: (65883, 48, 48, 1)\n",
            "iteration: 30543 train shape: (65884, 48, 48, 1)\n",
            "iteration: 30544 train shape: (65885, 48, 48, 1)\n",
            "iteration: 30545 train shape: (65886, 48, 48, 1)\n",
            "iteration: 30546 train shape: (65887, 48, 48, 1)\n",
            "iteration: 30547 train shape: (65888, 48, 48, 1)\n",
            "iteration: 30548 train shape: (65889, 48, 48, 1)\n",
            "iteration: 30549 train shape: (65890, 48, 48, 1)\n",
            "iteration: 30550 train shape: (65891, 48, 48, 1)\n",
            "iteration: 30551 train shape: (65892, 48, 48, 1)\n",
            "iteration: 30552 train shape: (65893, 48, 48, 1)\n",
            "iteration: 30553 train shape: (65894, 48, 48, 1)\n",
            "iteration: 30554 train shape: (65895, 48, 48, 1)\n",
            "iteration: 30555 train shape: (65896, 48, 48, 1)\n",
            "iteration: 30556 train shape: (65897, 48, 48, 1)\n",
            "iteration: 30557 train shape: (65898, 48, 48, 1)\n",
            "iteration: 30558 train shape: (65899, 48, 48, 1)\n",
            "iteration: 30559 train shape: (65900, 48, 48, 1)\n",
            "iteration: 30560 train shape: (65901, 48, 48, 1)\n",
            "iteration: 30561 train shape: (65902, 48, 48, 1)\n",
            "iteration: 30562 train shape: (65903, 48, 48, 1)\n",
            "iteration: 30563 train shape: (65904, 48, 48, 1)\n",
            "iteration: 30564 train shape: (65905, 48, 48, 1)\n",
            "iteration: 30565 train shape: (65906, 48, 48, 1)\n",
            "iteration: 30566 train shape: (65907, 48, 48, 1)\n",
            "iteration: 30567 train shape: (65908, 48, 48, 1)\n",
            "iteration: 30568 train shape: (65909, 48, 48, 1)\n",
            "iteration: 30569 train shape: (65910, 48, 48, 1)\n",
            "iteration: 30570 train shape: (65911, 48, 48, 1)\n",
            "iteration: 30571 train shape: (65912, 48, 48, 1)\n",
            "iteration: 30572 train shape: (65913, 48, 48, 1)\n",
            "iteration: 30573 train shape: (65914, 48, 48, 1)\n",
            "iteration: 30574 train shape: (65915, 48, 48, 1)\n",
            "iteration: 30575 train shape: (65916, 48, 48, 1)\n",
            "iteration: 30576 train shape: (65917, 48, 48, 1)\n",
            "iteration: 30577 train shape: (65918, 48, 48, 1)\n",
            "iteration: 30578 train shape: (65919, 48, 48, 1)\n",
            "iteration: 30579 train shape: (65920, 48, 48, 1)\n",
            "iteration: 30580 train shape: (65921, 48, 48, 1)\n",
            "iteration: 30581 train shape: (65922, 48, 48, 1)\n",
            "iteration: 30582 train shape: (65923, 48, 48, 1)\n",
            "iteration: 30583 train shape: (65924, 48, 48, 1)\n",
            "iteration: 30584 train shape: (65925, 48, 48, 1)\n",
            "iteration: 30585 train shape: (65926, 48, 48, 1)\n",
            "iteration: 30586 train shape: (65927, 48, 48, 1)\n",
            "iteration: 30587 train shape: (65928, 48, 48, 1)\n",
            "iteration: 30588 train shape: (65929, 48, 48, 1)\n",
            "iteration: 30589 train shape: (65930, 48, 48, 1)\n",
            "iteration: 30590 train shape: (65931, 48, 48, 1)\n",
            "iteration: 30591 train shape: (65932, 48, 48, 1)\n",
            "iteration: 30592 train shape: (65933, 48, 48, 1)\n",
            "iteration: 30593 train shape: (65934, 48, 48, 1)\n",
            "iteration: 30594 train shape: (65935, 48, 48, 1)\n",
            "iteration: 30595 train shape: (65936, 48, 48, 1)\n",
            "iteration: 30596 train shape: (65937, 48, 48, 1)\n",
            "iteration: 30597 train shape: (65938, 48, 48, 1)\n",
            "iteration: 30598 train shape: (65939, 48, 48, 1)\n",
            "iteration: 30599 train shape: (65940, 48, 48, 1)\n",
            "iteration: 30600 train shape: (65941, 48, 48, 1)\n",
            "iteration: 30601 train shape: (65942, 48, 48, 1)\n",
            "iteration: 30602 train shape: (65943, 48, 48, 1)\n",
            "iteration: 30603 train shape: (65944, 48, 48, 1)\n",
            "iteration: 30604 train shape: (65945, 48, 48, 1)\n",
            "iteration: 30605 train shape: (65946, 48, 48, 1)\n",
            "iteration: 30606 train shape: (65947, 48, 48, 1)\n",
            "iteration: 30607 train shape: (65948, 48, 48, 1)\n",
            "iteration: 30608 train shape: (65949, 48, 48, 1)\n",
            "iteration: 30609 train shape: (65950, 48, 48, 1)\n",
            "iteration: 30610 train shape: (65951, 48, 48, 1)\n",
            "iteration: 30611 train shape: (65952, 48, 48, 1)\n",
            "iteration: 30612 train shape: (65953, 48, 48, 1)\n",
            "iteration: 30613 train shape: (65954, 48, 48, 1)\n",
            "iteration: 30614 train shape: (65955, 48, 48, 1)\n",
            "iteration: 30615 train shape: (65956, 48, 48, 1)\n",
            "iteration: 30616 train shape: (65957, 48, 48, 1)\n",
            "iteration: 30617 train shape: (65958, 48, 48, 1)\n",
            "iteration: 30618 train shape: (65959, 48, 48, 1)\n",
            "iteration: 30619 train shape: (65960, 48, 48, 1)\n",
            "iteration: 30620 train shape: (65961, 48, 48, 1)\n",
            "iteration: 30621 train shape: (65962, 48, 48, 1)\n",
            "iteration: 30622 train shape: (65963, 48, 48, 1)\n",
            "iteration: 30623 train shape: (65964, 48, 48, 1)\n",
            "iteration: 30624 train shape: (65965, 48, 48, 1)\n",
            "iteration: 30625 train shape: (65966, 48, 48, 1)\n",
            "iteration: 30626 train shape: (65967, 48, 48, 1)\n",
            "iteration: 30627 train shape: (65968, 48, 48, 1)\n",
            "iteration: 30628 train shape: (65969, 48, 48, 1)\n",
            "iteration: 30629 train shape: (65970, 48, 48, 1)\n",
            "iteration: 30630 train shape: (65971, 48, 48, 1)\n",
            "iteration: 30631 train shape: (65972, 48, 48, 1)\n",
            "iteration: 30632 train shape: (65973, 48, 48, 1)\n",
            "iteration: 30633 train shape: (65974, 48, 48, 1)\n",
            "iteration: 30634 train shape: (65975, 48, 48, 1)\n",
            "iteration: 30635 train shape: (65976, 48, 48, 1)\n",
            "iteration: 30636 train shape: (65977, 48, 48, 1)\n",
            "iteration: 30637 train shape: (65978, 48, 48, 1)\n",
            "iteration: 30638 train shape: (65979, 48, 48, 1)\n",
            "iteration: 30639 train shape: (65980, 48, 48, 1)\n",
            "iteration: 30640 train shape: (65981, 48, 48, 1)\n",
            "iteration: 30641 train shape: (65982, 48, 48, 1)\n",
            "iteration: 30642 train shape: (65983, 48, 48, 1)\n",
            "iteration: 30643 train shape: (65984, 48, 48, 1)\n",
            "iteration: 30644 train shape: (65985, 48, 48, 1)\n",
            "iteration: 30645 train shape: (65986, 48, 48, 1)\n",
            "iteration: 30646 train shape: (65987, 48, 48, 1)\n",
            "iteration: 30647 train shape: (65988, 48, 48, 1)\n",
            "iteration: 30648 train shape: (65989, 48, 48, 1)\n",
            "iteration: 30649 train shape: (65990, 48, 48, 1)\n",
            "iteration: 30650 train shape: (65991, 48, 48, 1)\n",
            "iteration: 30651 train shape: (65992, 48, 48, 1)\n",
            "iteration: 30652 train shape: (65993, 48, 48, 1)\n",
            "iteration: 30653 train shape: (65994, 48, 48, 1)\n",
            "iteration: 30654 train shape: (65995, 48, 48, 1)\n",
            "iteration: 30655 train shape: (65996, 48, 48, 1)\n",
            "iteration: 30656 train shape: (65997, 48, 48, 1)\n",
            "iteration: 30657 train shape: (65998, 48, 48, 1)\n",
            "iteration: 30658 train shape: (65999, 48, 48, 1)\n",
            "iteration: 30659 train shape: (66000, 48, 48, 1)\n",
            "iteration: 30660 train shape: (66001, 48, 48, 1)\n",
            "iteration: 30661 train shape: (66002, 48, 48, 1)\n",
            "iteration: 30662 train shape: (66003, 48, 48, 1)\n",
            "iteration: 30663 train shape: (66004, 48, 48, 1)\n",
            "iteration: 30664 train shape: (66005, 48, 48, 1)\n",
            "iteration: 30665 train shape: (66006, 48, 48, 1)\n",
            "iteration: 30666 train shape: (66007, 48, 48, 1)\n",
            "iteration: 30667 train shape: (66008, 48, 48, 1)\n",
            "iteration: 30668 train shape: (66009, 48, 48, 1)\n",
            "iteration: 30669 train shape: (66010, 48, 48, 1)\n",
            "iteration: 30670 train shape: (66011, 48, 48, 1)\n",
            "iteration: 30671 train shape: (66012, 48, 48, 1)\n",
            "iteration: 30672 train shape: (66013, 48, 48, 1)\n",
            "iteration: 30673 train shape: (66014, 48, 48, 1)\n",
            "iteration: 30674 train shape: (66015, 48, 48, 1)\n",
            "iteration: 30675 train shape: (66016, 48, 48, 1)\n",
            "iteration: 30676 train shape: (66017, 48, 48, 1)\n",
            "iteration: 30677 train shape: (66018, 48, 48, 1)\n",
            "iteration: 30678 train shape: (66019, 48, 48, 1)\n",
            "iteration: 30679 train shape: (66020, 48, 48, 1)\n",
            "iteration: 30680 train shape: (66021, 48, 48, 1)\n",
            "iteration: 30681 train shape: (66022, 48, 48, 1)\n",
            "iteration: 30682 train shape: (66023, 48, 48, 1)\n",
            "iteration: 30683 train shape: (66024, 48, 48, 1)\n",
            "iteration: 30684 train shape: (66025, 48, 48, 1)\n",
            "iteration: 30685 train shape: (66026, 48, 48, 1)\n",
            "iteration: 30686 train shape: (66027, 48, 48, 1)\n",
            "iteration: 30687 train shape: (66028, 48, 48, 1)\n",
            "iteration: 30688 train shape: (66029, 48, 48, 1)\n",
            "iteration: 30689 train shape: (66030, 48, 48, 1)\n",
            "iteration: 30690 train shape: (66031, 48, 48, 1)\n",
            "iteration: 30691 train shape: (66032, 48, 48, 1)\n",
            "iteration: 30692 train shape: (66033, 48, 48, 1)\n",
            "iteration: 30693 train shape: (66034, 48, 48, 1)\n",
            "iteration: 30694 train shape: (66035, 48, 48, 1)\n",
            "iteration: 30695 train shape: (66036, 48, 48, 1)\n",
            "iteration: 30696 train shape: (66037, 48, 48, 1)\n",
            "iteration: 30697 train shape: (66038, 48, 48, 1)\n",
            "iteration: 30698 train shape: (66039, 48, 48, 1)\n",
            "iteration: 30699 train shape: (66040, 48, 48, 1)\n",
            "iteration: 30700 train shape: (66041, 48, 48, 1)\n",
            "iteration: 30701 train shape: (66042, 48, 48, 1)\n",
            "iteration: 30702 train shape: (66043, 48, 48, 1)\n",
            "iteration: 30703 train shape: (66044, 48, 48, 1)\n",
            "iteration: 30704 train shape: (66045, 48, 48, 1)\n",
            "iteration: 30705 train shape: (66046, 48, 48, 1)\n",
            "iteration: 30706 train shape: (66047, 48, 48, 1)\n",
            "iteration: 30707 train shape: (66048, 48, 48, 1)\n",
            "iteration: 30708 train shape: (66049, 48, 48, 1)\n",
            "iteration: 30709 train shape: (66050, 48, 48, 1)\n",
            "iteration: 30710 train shape: (66051, 48, 48, 1)\n",
            "iteration: 30711 train shape: (66052, 48, 48, 1)\n",
            "iteration: 30712 train shape: (66053, 48, 48, 1)\n",
            "iteration: 30713 train shape: (66054, 48, 48, 1)\n",
            "iteration: 30714 train shape: (66055, 48, 48, 1)\n",
            "iteration: 30715 train shape: (66056, 48, 48, 1)\n",
            "iteration: 30716 train shape: (66057, 48, 48, 1)\n",
            "iteration: 30717 train shape: (66058, 48, 48, 1)\n",
            "iteration: 30718 train shape: (66059, 48, 48, 1)\n",
            "iteration: 30719 train shape: (66060, 48, 48, 1)\n",
            "iteration: 30720 train shape: (66061, 48, 48, 1)\n",
            "iteration: 30721 train shape: (66062, 48, 48, 1)\n",
            "iteration: 30722 train shape: (66063, 48, 48, 1)\n",
            "iteration: 30723 train shape: (66064, 48, 48, 1)\n",
            "iteration: 30724 train shape: (66065, 48, 48, 1)\n",
            "iteration: 30725 train shape: (66066, 48, 48, 1)\n",
            "iteration: 30726 train shape: (66067, 48, 48, 1)\n",
            "iteration: 30727 train shape: (66068, 48, 48, 1)\n",
            "iteration: 30728 train shape: (66069, 48, 48, 1)\n",
            "iteration: 30729 train shape: (66070, 48, 48, 1)\n",
            "iteration: 30730 train shape: (66071, 48, 48, 1)\n",
            "iteration: 30731 train shape: (66072, 48, 48, 1)\n",
            "iteration: 30732 train shape: (66073, 48, 48, 1)\n",
            "iteration: 30733 train shape: (66074, 48, 48, 1)\n",
            "iteration: 30734 train shape: (66075, 48, 48, 1)\n",
            "iteration: 30735 train shape: (66076, 48, 48, 1)\n",
            "iteration: 30736 train shape: (66077, 48, 48, 1)\n",
            "iteration: 30737 train shape: (66078, 48, 48, 1)\n",
            "iteration: 30738 train shape: (66079, 48, 48, 1)\n",
            "iteration: 30739 train shape: (66080, 48, 48, 1)\n",
            "iteration: 30740 train shape: (66081, 48, 48, 1)\n",
            "iteration: 30741 train shape: (66082, 48, 48, 1)\n",
            "iteration: 30742 train shape: (66083, 48, 48, 1)\n",
            "iteration: 30743 train shape: (66084, 48, 48, 1)\n",
            "iteration: 30744 train shape: (66085, 48, 48, 1)\n",
            "iteration: 30745 train shape: (66086, 48, 48, 1)\n",
            "iteration: 30746 train shape: (66087, 48, 48, 1)\n",
            "iteration: 30747 train shape: (66088, 48, 48, 1)\n",
            "iteration: 30748 train shape: (66089, 48, 48, 1)\n",
            "iteration: 30749 train shape: (66090, 48, 48, 1)\n",
            "iteration: 30750 train shape: (66091, 48, 48, 1)\n",
            "iteration: 30751 train shape: (66092, 48, 48, 1)\n",
            "iteration: 30752 train shape: (66093, 48, 48, 1)\n",
            "iteration: 30753 train shape: (66094, 48, 48, 1)\n",
            "iteration: 30754 train shape: (66095, 48, 48, 1)\n",
            "iteration: 30755 train shape: (66096, 48, 48, 1)\n",
            "iteration: 30756 train shape: (66097, 48, 48, 1)\n",
            "iteration: 30757 train shape: (66098, 48, 48, 1)\n",
            "iteration: 30758 train shape: (66099, 48, 48, 1)\n",
            "iteration: 30759 train shape: (66100, 48, 48, 1)\n",
            "iteration: 30760 train shape: (66101, 48, 48, 1)\n",
            "iteration: 30761 train shape: (66102, 48, 48, 1)\n",
            "iteration: 30762 train shape: (66103, 48, 48, 1)\n",
            "iteration: 30763 train shape: (66104, 48, 48, 1)\n",
            "iteration: 30764 train shape: (66105, 48, 48, 1)\n",
            "iteration: 30765 train shape: (66106, 48, 48, 1)\n",
            "iteration: 30766 train shape: (66107, 48, 48, 1)\n",
            "iteration: 30767 train shape: (66108, 48, 48, 1)\n",
            "iteration: 30768 train shape: (66109, 48, 48, 1)\n",
            "iteration: 30769 train shape: (66110, 48, 48, 1)\n",
            "iteration: 30770 train shape: (66111, 48, 48, 1)\n",
            "iteration: 30771 train shape: (66112, 48, 48, 1)\n",
            "iteration: 30772 train shape: (66113, 48, 48, 1)\n",
            "iteration: 30773 train shape: (66114, 48, 48, 1)\n",
            "iteration: 30774 train shape: (66115, 48, 48, 1)\n",
            "iteration: 30775 train shape: (66116, 48, 48, 1)\n",
            "iteration: 30776 train shape: (66117, 48, 48, 1)\n",
            "iteration: 30777 train shape: (66118, 48, 48, 1)\n",
            "iteration: 30778 train shape: (66119, 48, 48, 1)\n",
            "iteration: 30779 train shape: (66120, 48, 48, 1)\n",
            "iteration: 30780 train shape: (66121, 48, 48, 1)\n",
            "iteration: 30781 train shape: (66122, 48, 48, 1)\n",
            "iteration: 30782 train shape: (66123, 48, 48, 1)\n",
            "iteration: 30783 train shape: (66124, 48, 48, 1)\n",
            "iteration: 30784 train shape: (66125, 48, 48, 1)\n",
            "iteration: 30785 train shape: (66126, 48, 48, 1)\n",
            "iteration: 30786 train shape: (66127, 48, 48, 1)\n",
            "iteration: 30787 train shape: (66128, 48, 48, 1)\n",
            "iteration: 30788 train shape: (66129, 48, 48, 1)\n",
            "iteration: 30789 train shape: (66130, 48, 48, 1)\n",
            "iteration: 30790 train shape: (66131, 48, 48, 1)\n",
            "iteration: 30791 train shape: (66132, 48, 48, 1)\n",
            "iteration: 30792 train shape: (66133, 48, 48, 1)\n",
            "iteration: 30793 train shape: (66134, 48, 48, 1)\n",
            "iteration: 30794 train shape: (66135, 48, 48, 1)\n",
            "iteration: 30795 train shape: (66136, 48, 48, 1)\n",
            "iteration: 30796 train shape: (66137, 48, 48, 1)\n",
            "iteration: 30797 train shape: (66138, 48, 48, 1)\n",
            "iteration: 30798 train shape: (66139, 48, 48, 1)\n",
            "iteration: 30799 train shape: (66140, 48, 48, 1)\n",
            "iteration: 30800 train shape: (66141, 48, 48, 1)\n",
            "iteration: 30801 train shape: (66142, 48, 48, 1)\n",
            "iteration: 30802 train shape: (66143, 48, 48, 1)\n",
            "iteration: 30803 train shape: (66144, 48, 48, 1)\n",
            "iteration: 30804 train shape: (66145, 48, 48, 1)\n",
            "iteration: 30805 train shape: (66146, 48, 48, 1)\n",
            "iteration: 30806 train shape: (66147, 48, 48, 1)\n",
            "iteration: 30807 train shape: (66148, 48, 48, 1)\n",
            "iteration: 30808 train shape: (66149, 48, 48, 1)\n",
            "iteration: 30809 train shape: (66150, 48, 48, 1)\n",
            "iteration: 30810 train shape: (66151, 48, 48, 1)\n",
            "iteration: 30811 train shape: (66152, 48, 48, 1)\n",
            "iteration: 30812 train shape: (66153, 48, 48, 1)\n",
            "iteration: 30813 train shape: (66154, 48, 48, 1)\n",
            "iteration: 30814 train shape: (66155, 48, 48, 1)\n",
            "iteration: 30815 train shape: (66156, 48, 48, 1)\n",
            "iteration: 30816 train shape: (66157, 48, 48, 1)\n",
            "iteration: 30817 train shape: (66158, 48, 48, 1)\n",
            "iteration: 30818 train shape: (66159, 48, 48, 1)\n",
            "iteration: 30819 train shape: (66160, 48, 48, 1)\n",
            "iteration: 30820 train shape: (66161, 48, 48, 1)\n",
            "iteration: 30821 train shape: (66162, 48, 48, 1)\n",
            "iteration: 30822 train shape: (66163, 48, 48, 1)\n",
            "iteration: 30823 train shape: (66164, 48, 48, 1)\n",
            "iteration: 30824 train shape: (66165, 48, 48, 1)\n",
            "iteration: 30825 train shape: (66166, 48, 48, 1)\n",
            "iteration: 30826 train shape: (66167, 48, 48, 1)\n",
            "iteration: 30827 train shape: (66168, 48, 48, 1)\n",
            "iteration: 30828 train shape: (66169, 48, 48, 1)\n",
            "iteration: 30829 train shape: (66170, 48, 48, 1)\n",
            "iteration: 30830 train shape: (66171, 48, 48, 1)\n",
            "iteration: 30831 train shape: (66172, 48, 48, 1)\n",
            "iteration: 30832 train shape: (66173, 48, 48, 1)\n",
            "iteration: 30833 train shape: (66174, 48, 48, 1)\n",
            "iteration: 30834 train shape: (66175, 48, 48, 1)\n",
            "iteration: 30835 train shape: (66176, 48, 48, 1)\n",
            "iteration: 30836 train shape: (66177, 48, 48, 1)\n",
            "iteration: 30837 train shape: (66178, 48, 48, 1)\n",
            "iteration: 30838 train shape: (66179, 48, 48, 1)\n",
            "iteration: 30839 train shape: (66180, 48, 48, 1)\n",
            "iteration: 30840 train shape: (66181, 48, 48, 1)\n",
            "iteration: 30841 train shape: (66182, 48, 48, 1)\n",
            "iteration: 30842 train shape: (66183, 48, 48, 1)\n",
            "iteration: 30843 train shape: (66184, 48, 48, 1)\n",
            "iteration: 30844 train shape: (66185, 48, 48, 1)\n",
            "iteration: 30845 train shape: (66186, 48, 48, 1)\n",
            "iteration: 30846 train shape: (66187, 48, 48, 1)\n",
            "iteration: 30847 train shape: (66188, 48, 48, 1)\n",
            "iteration: 30848 train shape: (66189, 48, 48, 1)\n",
            "iteration: 30849 train shape: (66190, 48, 48, 1)\n",
            "iteration: 30850 train shape: (66191, 48, 48, 1)\n",
            "iteration: 30851 train shape: (66192, 48, 48, 1)\n",
            "iteration: 30852 train shape: (66193, 48, 48, 1)\n",
            "iteration: 30853 train shape: (66194, 48, 48, 1)\n",
            "iteration: 30854 train shape: (66195, 48, 48, 1)\n",
            "iteration: 30855 train shape: (66196, 48, 48, 1)\n",
            "iteration: 30856 train shape: (66197, 48, 48, 1)\n",
            "iteration: 30857 train shape: (66198, 48, 48, 1)\n",
            "iteration: 30858 train shape: (66199, 48, 48, 1)\n",
            "iteration: 30859 train shape: (66200, 48, 48, 1)\n",
            "iteration: 30860 train shape: (66201, 48, 48, 1)\n",
            "iteration: 30861 train shape: (66202, 48, 48, 1)\n",
            "iteration: 30862 train shape: (66203, 48, 48, 1)\n",
            "iteration: 30863 train shape: (66204, 48, 48, 1)\n",
            "iteration: 30864 train shape: (66205, 48, 48, 1)\n",
            "iteration: 30865 train shape: (66206, 48, 48, 1)\n",
            "iteration: 30866 train shape: (66207, 48, 48, 1)\n",
            "iteration: 30867 train shape: (66208, 48, 48, 1)\n",
            "iteration: 30868 train shape: (66209, 48, 48, 1)\n",
            "iteration: 30869 train shape: (66210, 48, 48, 1)\n",
            "iteration: 30870 train shape: (66211, 48, 48, 1)\n",
            "iteration: 30871 train shape: (66212, 48, 48, 1)\n",
            "iteration: 30872 train shape: (66213, 48, 48, 1)\n",
            "iteration: 30873 train shape: (66214, 48, 48, 1)\n",
            "iteration: 30874 train shape: (66215, 48, 48, 1)\n",
            "iteration: 30875 train shape: (66216, 48, 48, 1)\n",
            "iteration: 30876 train shape: (66217, 48, 48, 1)\n",
            "iteration: 30877 train shape: (66218, 48, 48, 1)\n",
            "iteration: 30878 train shape: (66219, 48, 48, 1)\n",
            "iteration: 30879 train shape: (66220, 48, 48, 1)\n",
            "iteration: 30880 train shape: (66221, 48, 48, 1)\n",
            "iteration: 30881 train shape: (66222, 48, 48, 1)\n",
            "iteration: 30882 train shape: (66223, 48, 48, 1)\n",
            "iteration: 30883 train shape: (66224, 48, 48, 1)\n",
            "iteration: 30884 train shape: (66225, 48, 48, 1)\n",
            "iteration: 30885 train shape: (66226, 48, 48, 1)\n",
            "iteration: 30886 train shape: (66227, 48, 48, 1)\n",
            "iteration: 30887 train shape: (66228, 48, 48, 1)\n",
            "iteration: 30888 train shape: (66229, 48, 48, 1)\n",
            "iteration: 30889 train shape: (66230, 48, 48, 1)\n",
            "iteration: 30890 train shape: (66231, 48, 48, 1)\n",
            "iteration: 30891 train shape: (66232, 48, 48, 1)\n",
            "iteration: 30892 train shape: (66233, 48, 48, 1)\n",
            "iteration: 30893 train shape: (66234, 48, 48, 1)\n",
            "iteration: 30894 train shape: (66235, 48, 48, 1)\n",
            "iteration: 30895 train shape: (66236, 48, 48, 1)\n",
            "iteration: 30896 train shape: (66237, 48, 48, 1)\n",
            "iteration: 30897 train shape: (66238, 48, 48, 1)\n",
            "iteration: 30898 train shape: (66239, 48, 48, 1)\n",
            "iteration: 30899 train shape: (66240, 48, 48, 1)\n",
            "iteration: 30900 train shape: (66241, 48, 48, 1)\n",
            "iteration: 30901 train shape: (66242, 48, 48, 1)\n",
            "iteration: 30902 train shape: (66243, 48, 48, 1)\n",
            "iteration: 30903 train shape: (66244, 48, 48, 1)\n",
            "iteration: 30904 train shape: (66245, 48, 48, 1)\n",
            "iteration: 30905 train shape: (66246, 48, 48, 1)\n",
            "iteration: 30906 train shape: (66247, 48, 48, 1)\n",
            "iteration: 30907 train shape: (66248, 48, 48, 1)\n",
            "iteration: 30908 train shape: (66249, 48, 48, 1)\n",
            "iteration: 30909 train shape: (66250, 48, 48, 1)\n",
            "iteration: 30910 train shape: (66251, 48, 48, 1)\n",
            "iteration: 30911 train shape: (66252, 48, 48, 1)\n",
            "iteration: 30912 train shape: (66253, 48, 48, 1)\n",
            "iteration: 30913 train shape: (66254, 48, 48, 1)\n",
            "iteration: 30914 train shape: (66255, 48, 48, 1)\n",
            "iteration: 30915 train shape: (66256, 48, 48, 1)\n",
            "iteration: 30916 train shape: (66257, 48, 48, 1)\n",
            "iteration: 30917 train shape: (66258, 48, 48, 1)\n",
            "iteration: 30918 train shape: (66259, 48, 48, 1)\n",
            "iteration: 30919 train shape: (66260, 48, 48, 1)\n",
            "iteration: 30920 train shape: (66261, 48, 48, 1)\n",
            "iteration: 30921 train shape: (66262, 48, 48, 1)\n",
            "iteration: 30922 train shape: (66263, 48, 48, 1)\n",
            "iteration: 30923 train shape: (66264, 48, 48, 1)\n",
            "iteration: 30924 train shape: (66265, 48, 48, 1)\n",
            "iteration: 30925 train shape: (66266, 48, 48, 1)\n",
            "iteration: 30926 train shape: (66267, 48, 48, 1)\n",
            "iteration: 30927 train shape: (66268, 48, 48, 1)\n",
            "iteration: 30928 train shape: (66269, 48, 48, 1)\n",
            "iteration: 30929 train shape: (66270, 48, 48, 1)\n",
            "iteration: 30930 train shape: (66271, 48, 48, 1)\n",
            "iteration: 30931 train shape: (66272, 48, 48, 1)\n",
            "iteration: 30932 train shape: (66273, 48, 48, 1)\n",
            "iteration: 30933 train shape: (66274, 48, 48, 1)\n",
            "iteration: 30934 train shape: (66275, 48, 48, 1)\n",
            "iteration: 30935 train shape: (66276, 48, 48, 1)\n",
            "iteration: 30936 train shape: (66277, 48, 48, 1)\n",
            "iteration: 30937 train shape: (66278, 48, 48, 1)\n",
            "iteration: 30938 train shape: (66279, 48, 48, 1)\n",
            "iteration: 30939 train shape: (66280, 48, 48, 1)\n",
            "iteration: 30940 train shape: (66281, 48, 48, 1)\n",
            "iteration: 30941 train shape: (66282, 48, 48, 1)\n",
            "iteration: 30942 train shape: (66283, 48, 48, 1)\n",
            "iteration: 30943 train shape: (66284, 48, 48, 1)\n",
            "iteration: 30944 train shape: (66285, 48, 48, 1)\n",
            "iteration: 30945 train shape: (66286, 48, 48, 1)\n",
            "iteration: 30946 train shape: (66287, 48, 48, 1)\n",
            "iteration: 30947 train shape: (66288, 48, 48, 1)\n",
            "iteration: 30948 train shape: (66289, 48, 48, 1)\n",
            "iteration: 30949 train shape: (66290, 48, 48, 1)\n",
            "iteration: 30950 train shape: (66291, 48, 48, 1)\n",
            "iteration: 30951 train shape: (66292, 48, 48, 1)\n",
            "iteration: 30952 train shape: (66293, 48, 48, 1)\n",
            "iteration: 30953 train shape: (66294, 48, 48, 1)\n",
            "iteration: 30954 train shape: (66295, 48, 48, 1)\n",
            "iteration: 30955 train shape: (66296, 48, 48, 1)\n",
            "iteration: 30956 train shape: (66297, 48, 48, 1)\n",
            "iteration: 30957 train shape: (66298, 48, 48, 1)\n",
            "iteration: 30958 train shape: (66299, 48, 48, 1)\n",
            "iteration: 30959 train shape: (66300, 48, 48, 1)\n",
            "iteration: 30960 train shape: (66301, 48, 48, 1)\n",
            "iteration: 30961 train shape: (66302, 48, 48, 1)\n",
            "iteration: 30962 train shape: (66303, 48, 48, 1)\n",
            "iteration: 30963 train shape: (66304, 48, 48, 1)\n",
            "iteration: 30964 train shape: (66305, 48, 48, 1)\n",
            "iteration: 30965 train shape: (66306, 48, 48, 1)\n",
            "iteration: 30966 train shape: (66307, 48, 48, 1)\n",
            "iteration: 30967 train shape: (66308, 48, 48, 1)\n",
            "iteration: 30968 train shape: (66309, 48, 48, 1)\n",
            "iteration: 30969 train shape: (66310, 48, 48, 1)\n",
            "iteration: 30970 train shape: (66311, 48, 48, 1)\n",
            "iteration: 30971 train shape: (66312, 48, 48, 1)\n",
            "iteration: 30972 train shape: (66313, 48, 48, 1)\n",
            "iteration: 30973 train shape: (66314, 48, 48, 1)\n",
            "iteration: 30974 train shape: (66315, 48, 48, 1)\n",
            "iteration: 30975 train shape: (66316, 48, 48, 1)\n",
            "iteration: 30976 train shape: (66317, 48, 48, 1)\n",
            "iteration: 30977 train shape: (66318, 48, 48, 1)\n",
            "iteration: 30978 train shape: (66319, 48, 48, 1)\n",
            "iteration: 30979 train shape: (66320, 48, 48, 1)\n",
            "iteration: 30980 train shape: (66321, 48, 48, 1)\n",
            "iteration: 30981 train shape: (66322, 48, 48, 1)\n",
            "iteration: 30982 train shape: (66323, 48, 48, 1)\n",
            "iteration: 30983 train shape: (66324, 48, 48, 1)\n",
            "iteration: 30984 train shape: (66325, 48, 48, 1)\n",
            "iteration: 30985 train shape: (66326, 48, 48, 1)\n",
            "iteration: 30986 train shape: (66327, 48, 48, 1)\n",
            "iteration: 30987 train shape: (66328, 48, 48, 1)\n",
            "iteration: 30988 train shape: (66329, 48, 48, 1)\n",
            "iteration: 30989 train shape: (66330, 48, 48, 1)\n",
            "iteration: 30990 train shape: (66331, 48, 48, 1)\n",
            "iteration: 30991 train shape: (66332, 48, 48, 1)\n",
            "iteration: 30992 train shape: (66333, 48, 48, 1)\n",
            "iteration: 30993 train shape: (66334, 48, 48, 1)\n",
            "iteration: 30994 train shape: (66335, 48, 48, 1)\n",
            "iteration: 30995 train shape: (66336, 48, 48, 1)\n",
            "iteration: 30996 train shape: (66337, 48, 48, 1)\n",
            "iteration: 30997 train shape: (66338, 48, 48, 1)\n",
            "iteration: 30998 train shape: (66339, 48, 48, 1)\n",
            "iteration: 30999 train shape: (66340, 48, 48, 1)\n",
            "iteration: 31000 train shape: (66341, 48, 48, 1)\n",
            "iteration: 31001 train shape: (66342, 48, 48, 1)\n",
            "iteration: 31002 train shape: (66343, 48, 48, 1)\n",
            "iteration: 31003 train shape: (66344, 48, 48, 1)\n",
            "iteration: 31004 train shape: (66345, 48, 48, 1)\n",
            "iteration: 31005 train shape: (66346, 48, 48, 1)\n",
            "iteration: 31006 train shape: (66347, 48, 48, 1)\n",
            "iteration: 31007 train shape: (66348, 48, 48, 1)\n",
            "iteration: 31008 train shape: (66349, 48, 48, 1)\n",
            "iteration: 31009 train shape: (66350, 48, 48, 1)\n",
            "iteration: 31010 train shape: (66351, 48, 48, 1)\n",
            "iteration: 31011 train shape: (66352, 48, 48, 1)\n",
            "iteration: 31012 train shape: (66353, 48, 48, 1)\n",
            "iteration: 31013 train shape: (66354, 48, 48, 1)\n",
            "iteration: 31014 train shape: (66355, 48, 48, 1)\n",
            "iteration: 31015 train shape: (66356, 48, 48, 1)\n",
            "iteration: 31016 train shape: (66357, 48, 48, 1)\n",
            "iteration: 31017 train shape: (66358, 48, 48, 1)\n",
            "iteration: 31018 train shape: (66359, 48, 48, 1)\n",
            "iteration: 31019 train shape: (66360, 48, 48, 1)\n",
            "iteration: 31020 train shape: (66361, 48, 48, 1)\n",
            "iteration: 31021 train shape: (66362, 48, 48, 1)\n",
            "iteration: 31022 train shape: (66363, 48, 48, 1)\n",
            "iteration: 31023 train shape: (66364, 48, 48, 1)\n",
            "iteration: 31024 train shape: (66365, 48, 48, 1)\n",
            "iteration: 31025 train shape: (66366, 48, 48, 1)\n",
            "iteration: 31026 train shape: (66367, 48, 48, 1)\n",
            "iteration: 31027 train shape: (66368, 48, 48, 1)\n",
            "iteration: 31028 train shape: (66369, 48, 48, 1)\n",
            "iteration: 31029 train shape: (66370, 48, 48, 1)\n",
            "iteration: 31030 train shape: (66371, 48, 48, 1)\n",
            "iteration: 31031 train shape: (66372, 48, 48, 1)\n",
            "iteration: 31032 train shape: (66373, 48, 48, 1)\n",
            "iteration: 31033 train shape: (66374, 48, 48, 1)\n",
            "iteration: 31034 train shape: (66375, 48, 48, 1)\n",
            "iteration: 31035 train shape: (66376, 48, 48, 1)\n",
            "iteration: 31036 train shape: (66377, 48, 48, 1)\n",
            "iteration: 31037 train shape: (66378, 48, 48, 1)\n",
            "iteration: 31038 train shape: (66379, 48, 48, 1)\n",
            "iteration: 31039 train shape: (66380, 48, 48, 1)\n",
            "iteration: 31040 train shape: (66381, 48, 48, 1)\n",
            "iteration: 31041 train shape: (66382, 48, 48, 1)\n",
            "iteration: 31042 train shape: (66383, 48, 48, 1)\n",
            "iteration: 31043 train shape: (66384, 48, 48, 1)\n",
            "iteration: 31044 train shape: (66385, 48, 48, 1)\n",
            "iteration: 31045 train shape: (66386, 48, 48, 1)\n",
            "iteration: 31046 train shape: (66387, 48, 48, 1)\n",
            "iteration: 31047 train shape: (66388, 48, 48, 1)\n",
            "iteration: 31048 train shape: (66389, 48, 48, 1)\n",
            "iteration: 31049 train shape: (66390, 48, 48, 1)\n",
            "iteration: 31050 train shape: (66391, 48, 48, 1)\n",
            "iteration: 31051 train shape: (66392, 48, 48, 1)\n",
            "iteration: 31052 train shape: (66393, 48, 48, 1)\n",
            "iteration: 31053 train shape: (66394, 48, 48, 1)\n",
            "iteration: 31054 train shape: (66395, 48, 48, 1)\n",
            "iteration: 31055 train shape: (66396, 48, 48, 1)\n",
            "iteration: 31056 train shape: (66397, 48, 48, 1)\n",
            "iteration: 31057 train shape: (66398, 48, 48, 1)\n",
            "iteration: 31058 train shape: (66399, 48, 48, 1)\n",
            "iteration: 31059 train shape: (66400, 48, 48, 1)\n",
            "iteration: 31060 train shape: (66401, 48, 48, 1)\n",
            "iteration: 31061 train shape: (66402, 48, 48, 1)\n",
            "iteration: 31062 train shape: (66403, 48, 48, 1)\n",
            "iteration: 31063 train shape: (66404, 48, 48, 1)\n",
            "iteration: 31064 train shape: (66405, 48, 48, 1)\n",
            "iteration: 31065 train shape: (66406, 48, 48, 1)\n",
            "iteration: 31066 train shape: (66407, 48, 48, 1)\n",
            "iteration: 31067 train shape: (66408, 48, 48, 1)\n",
            "iteration: 31068 train shape: (66409, 48, 48, 1)\n",
            "iteration: 31069 train shape: (66410, 48, 48, 1)\n",
            "iteration: 31070 train shape: (66411, 48, 48, 1)\n",
            "iteration: 31071 train shape: (66412, 48, 48, 1)\n",
            "iteration: 31072 train shape: (66413, 48, 48, 1)\n",
            "iteration: 31073 train shape: (66414, 48, 48, 1)\n",
            "iteration: 31074 train shape: (66415, 48, 48, 1)\n",
            "iteration: 31075 train shape: (66416, 48, 48, 1)\n",
            "iteration: 31076 train shape: (66417, 48, 48, 1)\n",
            "iteration: 31077 train shape: (66418, 48, 48, 1)\n",
            "iteration: 31078 train shape: (66419, 48, 48, 1)\n",
            "iteration: 31079 train shape: (66420, 48, 48, 1)\n",
            "iteration: 31080 train shape: (66421, 48, 48, 1)\n",
            "iteration: 31081 train shape: (66422, 48, 48, 1)\n",
            "iteration: 31082 train shape: (66423, 48, 48, 1)\n",
            "iteration: 31083 train shape: (66424, 48, 48, 1)\n",
            "iteration: 31084 train shape: (66425, 48, 48, 1)\n",
            "iteration: 31085 train shape: (66426, 48, 48, 1)\n",
            "iteration: 31086 train shape: (66427, 48, 48, 1)\n",
            "iteration: 31087 train shape: (66428, 48, 48, 1)\n",
            "iteration: 31088 train shape: (66429, 48, 48, 1)\n",
            "iteration: 31089 train shape: (66430, 48, 48, 1)\n",
            "iteration: 31090 train shape: (66431, 48, 48, 1)\n",
            "iteration: 31091 train shape: (66432, 48, 48, 1)\n",
            "iteration: 31092 train shape: (66433, 48, 48, 1)\n",
            "iteration: 31093 train shape: (66434, 48, 48, 1)\n",
            "iteration: 31094 train shape: (66435, 48, 48, 1)\n",
            "iteration: 31095 train shape: (66436, 48, 48, 1)\n",
            "iteration: 31096 train shape: (66437, 48, 48, 1)\n",
            "iteration: 31097 train shape: (66438, 48, 48, 1)\n",
            "iteration: 31098 train shape: (66439, 48, 48, 1)\n",
            "iteration: 31099 train shape: (66440, 48, 48, 1)\n",
            "iteration: 31100 train shape: (66441, 48, 48, 1)\n",
            "iteration: 31101 train shape: (66442, 48, 48, 1)\n",
            "iteration: 31102 train shape: (66443, 48, 48, 1)\n",
            "iteration: 31103 train shape: (66444, 48, 48, 1)\n",
            "iteration: 31104 train shape: (66445, 48, 48, 1)\n",
            "iteration: 31105 train shape: (66446, 48, 48, 1)\n",
            "iteration: 31106 train shape: (66447, 48, 48, 1)\n",
            "iteration: 31107 train shape: (66448, 48, 48, 1)\n",
            "iteration: 31108 train shape: (66449, 48, 48, 1)\n",
            "iteration: 31109 train shape: (66450, 48, 48, 1)\n",
            "iteration: 31110 train shape: (66451, 48, 48, 1)\n",
            "iteration: 31111 train shape: (66452, 48, 48, 1)\n",
            "iteration: 31112 train shape: (66453, 48, 48, 1)\n",
            "iteration: 31113 train shape: (66454, 48, 48, 1)\n",
            "iteration: 31114 train shape: (66455, 48, 48, 1)\n",
            "iteration: 31115 train shape: (66456, 48, 48, 1)\n",
            "iteration: 31116 train shape: (66457, 48, 48, 1)\n",
            "iteration: 31117 train shape: (66458, 48, 48, 1)\n",
            "iteration: 31118 train shape: (66459, 48, 48, 1)\n",
            "iteration: 31119 train shape: (66460, 48, 48, 1)\n",
            "iteration: 31120 train shape: (66461, 48, 48, 1)\n",
            "iteration: 31121 train shape: (66462, 48, 48, 1)\n",
            "iteration: 31122 train shape: (66463, 48, 48, 1)\n",
            "iteration: 31123 train shape: (66464, 48, 48, 1)\n",
            "iteration: 31124 train shape: (66465, 48, 48, 1)\n",
            "iteration: 31125 train shape: (66466, 48, 48, 1)\n",
            "iteration: 31126 train shape: (66467, 48, 48, 1)\n",
            "iteration: 31127 train shape: (66468, 48, 48, 1)\n",
            "iteration: 31128 train shape: (66469, 48, 48, 1)\n",
            "iteration: 31129 train shape: (66470, 48, 48, 1)\n",
            "iteration: 31130 train shape: (66471, 48, 48, 1)\n",
            "iteration: 31131 train shape: (66472, 48, 48, 1)\n",
            "iteration: 31132 train shape: (66473, 48, 48, 1)\n",
            "iteration: 31133 train shape: (66474, 48, 48, 1)\n",
            "iteration: 31134 train shape: (66475, 48, 48, 1)\n",
            "iteration: 31135 train shape: (66476, 48, 48, 1)\n",
            "iteration: 31136 train shape: (66477, 48, 48, 1)\n",
            "iteration: 31137 train shape: (66478, 48, 48, 1)\n",
            "iteration: 31138 train shape: (66479, 48, 48, 1)\n",
            "iteration: 31139 train shape: (66480, 48, 48, 1)\n",
            "iteration: 31140 train shape: (66481, 48, 48, 1)\n",
            "iteration: 31141 train shape: (66482, 48, 48, 1)\n",
            "iteration: 31142 train shape: (66483, 48, 48, 1)\n",
            "iteration: 31143 train shape: (66484, 48, 48, 1)\n",
            "iteration: 31144 train shape: (66485, 48, 48, 1)\n",
            "iteration: 31145 train shape: (66486, 48, 48, 1)\n",
            "iteration: 31146 train shape: (66487, 48, 48, 1)\n",
            "iteration: 31147 train shape: (66488, 48, 48, 1)\n",
            "iteration: 31148 train shape: (66489, 48, 48, 1)\n",
            "iteration: 31149 train shape: (66490, 48, 48, 1)\n",
            "iteration: 31150 train shape: (66491, 48, 48, 1)\n",
            "iteration: 31151 train shape: (66492, 48, 48, 1)\n",
            "iteration: 31152 train shape: (66493, 48, 48, 1)\n",
            "iteration: 31153 train shape: (66494, 48, 48, 1)\n",
            "iteration: 31154 train shape: (66495, 48, 48, 1)\n",
            "iteration: 31155 train shape: (66496, 48, 48, 1)\n",
            "iteration: 31156 train shape: (66497, 48, 48, 1)\n",
            "iteration: 31157 train shape: (66498, 48, 48, 1)\n",
            "iteration: 31158 train shape: (66499, 48, 48, 1)\n",
            "iteration: 31159 train shape: (66500, 48, 48, 1)\n",
            "iteration: 31160 train shape: (66501, 48, 48, 1)\n",
            "iteration: 31161 train shape: (66502, 48, 48, 1)\n",
            "iteration: 31162 train shape: (66503, 48, 48, 1)\n",
            "iteration: 31163 train shape: (66504, 48, 48, 1)\n",
            "iteration: 31164 train shape: (66505, 48, 48, 1)\n",
            "iteration: 31165 train shape: (66506, 48, 48, 1)\n",
            "iteration: 31166 train shape: (66507, 48, 48, 1)\n",
            "iteration: 31167 train shape: (66508, 48, 48, 1)\n",
            "iteration: 31168 train shape: (66509, 48, 48, 1)\n",
            "iteration: 31169 train shape: (66510, 48, 48, 1)\n",
            "iteration: 31170 train shape: (66511, 48, 48, 1)\n",
            "iteration: 31171 train shape: (66512, 48, 48, 1)\n",
            "iteration: 31172 train shape: (66513, 48, 48, 1)\n",
            "iteration: 31173 train shape: (66514, 48, 48, 1)\n",
            "iteration: 31174 train shape: (66515, 48, 48, 1)\n",
            "iteration: 31175 train shape: (66516, 48, 48, 1)\n",
            "iteration: 31176 train shape: (66517, 48, 48, 1)\n",
            "iteration: 31177 train shape: (66518, 48, 48, 1)\n",
            "iteration: 31178 train shape: (66519, 48, 48, 1)\n",
            "iteration: 31179 train shape: (66520, 48, 48, 1)\n",
            "iteration: 31180 train shape: (66521, 48, 48, 1)\n",
            "iteration: 31181 train shape: (66522, 48, 48, 1)\n",
            "iteration: 31182 train shape: (66523, 48, 48, 1)\n",
            "iteration: 31183 train shape: (66524, 48, 48, 1)\n",
            "iteration: 31184 train shape: (66525, 48, 48, 1)\n",
            "iteration: 31185 train shape: (66526, 48, 48, 1)\n",
            "iteration: 31186 train shape: (66527, 48, 48, 1)\n",
            "iteration: 31187 train shape: (66528, 48, 48, 1)\n",
            "iteration: 31188 train shape: (66529, 48, 48, 1)\n",
            "iteration: 31189 train shape: (66530, 48, 48, 1)\n",
            "iteration: 31190 train shape: (66531, 48, 48, 1)\n",
            "iteration: 31191 train shape: (66532, 48, 48, 1)\n",
            "iteration: 31192 train shape: (66533, 48, 48, 1)\n",
            "iteration: 31193 train shape: (66534, 48, 48, 1)\n",
            "iteration: 31194 train shape: (66535, 48, 48, 1)\n",
            "iteration: 31195 train shape: (66536, 48, 48, 1)\n",
            "iteration: 31196 train shape: (66537, 48, 48, 1)\n",
            "iteration: 31197 train shape: (66538, 48, 48, 1)\n",
            "iteration: 31198 train shape: (66539, 48, 48, 1)\n",
            "iteration: 31199 train shape: (66540, 48, 48, 1)\n",
            "iteration: 31200 train shape: (66541, 48, 48, 1)\n",
            "iteration: 31201 train shape: (66542, 48, 48, 1)\n",
            "iteration: 31202 train shape: (66543, 48, 48, 1)\n",
            "iteration: 31203 train shape: (66544, 48, 48, 1)\n",
            "iteration: 31204 train shape: (66545, 48, 48, 1)\n",
            "iteration: 31205 train shape: (66546, 48, 48, 1)\n",
            "iteration: 31206 train shape: (66547, 48, 48, 1)\n",
            "iteration: 31207 train shape: (66548, 48, 48, 1)\n",
            "iteration: 31208 train shape: (66549, 48, 48, 1)\n",
            "iteration: 31209 train shape: (66550, 48, 48, 1)\n",
            "iteration: 31210 train shape: (66551, 48, 48, 1)\n",
            "iteration: 31211 train shape: (66552, 48, 48, 1)\n",
            "iteration: 31212 train shape: (66553, 48, 48, 1)\n",
            "iteration: 31213 train shape: (66554, 48, 48, 1)\n",
            "iteration: 31214 train shape: (66555, 48, 48, 1)\n",
            "iteration: 31215 train shape: (66556, 48, 48, 1)\n",
            "iteration: 31216 train shape: (66557, 48, 48, 1)\n",
            "iteration: 31217 train shape: (66558, 48, 48, 1)\n",
            "iteration: 31218 train shape: (66559, 48, 48, 1)\n",
            "iteration: 31219 train shape: (66560, 48, 48, 1)\n",
            "iteration: 31220 train shape: (66561, 48, 48, 1)\n",
            "iteration: 31221 train shape: (66562, 48, 48, 1)\n",
            "iteration: 31222 train shape: (66563, 48, 48, 1)\n",
            "iteration: 31223 train shape: (66564, 48, 48, 1)\n",
            "iteration: 31224 train shape: (66565, 48, 48, 1)\n",
            "iteration: 31225 train shape: (66566, 48, 48, 1)\n",
            "iteration: 31226 train shape: (66567, 48, 48, 1)\n",
            "iteration: 31227 train shape: (66568, 48, 48, 1)\n",
            "iteration: 31228 train shape: (66569, 48, 48, 1)\n",
            "iteration: 31229 train shape: (66570, 48, 48, 1)\n",
            "iteration: 31230 train shape: (66571, 48, 48, 1)\n",
            "iteration: 31231 train shape: (66572, 48, 48, 1)\n",
            "iteration: 31232 train shape: (66573, 48, 48, 1)\n",
            "iteration: 31233 train shape: (66574, 48, 48, 1)\n",
            "iteration: 31234 train shape: (66575, 48, 48, 1)\n",
            "iteration: 31235 train shape: (66576, 48, 48, 1)\n",
            "iteration: 31236 train shape: (66577, 48, 48, 1)\n",
            "iteration: 31237 train shape: (66578, 48, 48, 1)\n",
            "iteration: 31238 train shape: (66579, 48, 48, 1)\n",
            "iteration: 31239 train shape: (66580, 48, 48, 1)\n",
            "iteration: 31240 train shape: (66581, 48, 48, 1)\n",
            "iteration: 31241 train shape: (66582, 48, 48, 1)\n",
            "iteration: 31242 train shape: (66583, 48, 48, 1)\n",
            "iteration: 31243 train shape: (66584, 48, 48, 1)\n",
            "iteration: 31244 train shape: (66585, 48, 48, 1)\n",
            "iteration: 31245 train shape: (66586, 48, 48, 1)\n",
            "iteration: 31246 train shape: (66587, 48, 48, 1)\n",
            "iteration: 31247 train shape: (66588, 48, 48, 1)\n",
            "iteration: 31248 train shape: (66589, 48, 48, 1)\n",
            "iteration: 31249 train shape: (66590, 48, 48, 1)\n",
            "iteration: 31250 train shape: (66591, 48, 48, 1)\n",
            "iteration: 31251 train shape: (66592, 48, 48, 1)\n",
            "iteration: 31252 train shape: (66593, 48, 48, 1)\n",
            "iteration: 31253 train shape: (66594, 48, 48, 1)\n",
            "iteration: 31254 train shape: (66595, 48, 48, 1)\n",
            "iteration: 31255 train shape: (66596, 48, 48, 1)\n",
            "iteration: 31256 train shape: (66597, 48, 48, 1)\n",
            "iteration: 31257 train shape: (66598, 48, 48, 1)\n",
            "iteration: 31258 train shape: (66599, 48, 48, 1)\n",
            "iteration: 31259 train shape: (66600, 48, 48, 1)\n",
            "iteration: 31260 train shape: (66601, 48, 48, 1)\n",
            "iteration: 31261 train shape: (66602, 48, 48, 1)\n",
            "iteration: 31262 train shape: (66603, 48, 48, 1)\n",
            "iteration: 31263 train shape: (66604, 48, 48, 1)\n",
            "iteration: 31264 train shape: (66605, 48, 48, 1)\n",
            "iteration: 31265 train shape: (66606, 48, 48, 1)\n",
            "iteration: 31266 train shape: (66607, 48, 48, 1)\n",
            "iteration: 31267 train shape: (66608, 48, 48, 1)\n",
            "iteration: 31268 train shape: (66609, 48, 48, 1)\n",
            "iteration: 31269 train shape: (66610, 48, 48, 1)\n",
            "iteration: 31270 train shape: (66611, 48, 48, 1)\n",
            "iteration: 31271 train shape: (66612, 48, 48, 1)\n",
            "iteration: 31272 train shape: (66613, 48, 48, 1)\n",
            "iteration: 31273 train shape: (66614, 48, 48, 1)\n",
            "iteration: 31274 train shape: (66615, 48, 48, 1)\n",
            "iteration: 31275 train shape: (66616, 48, 48, 1)\n",
            "iteration: 31276 train shape: (66617, 48, 48, 1)\n",
            "iteration: 31277 train shape: (66618, 48, 48, 1)\n",
            "iteration: 31278 train shape: (66619, 48, 48, 1)\n",
            "iteration: 31279 train shape: (66620, 48, 48, 1)\n",
            "iteration: 31280 train shape: (66621, 48, 48, 1)\n",
            "iteration: 31281 train shape: (66622, 48, 48, 1)\n",
            "iteration: 31282 train shape: (66623, 48, 48, 1)\n",
            "iteration: 31283 train shape: (66624, 48, 48, 1)\n",
            "iteration: 31284 train shape: (66625, 48, 48, 1)\n",
            "iteration: 31285 train shape: (66626, 48, 48, 1)\n",
            "iteration: 31286 train shape: (66627, 48, 48, 1)\n",
            "iteration: 31287 train shape: (66628, 48, 48, 1)\n",
            "iteration: 31288 train shape: (66629, 48, 48, 1)\n",
            "iteration: 31289 train shape: (66630, 48, 48, 1)\n",
            "iteration: 31290 train shape: (66631, 48, 48, 1)\n",
            "iteration: 31291 train shape: (66632, 48, 48, 1)\n",
            "iteration: 31292 train shape: (66633, 48, 48, 1)\n",
            "iteration: 31293 train shape: (66634, 48, 48, 1)\n",
            "iteration: 31294 train shape: (66635, 48, 48, 1)\n",
            "iteration: 31295 train shape: (66636, 48, 48, 1)\n",
            "iteration: 31296 train shape: (66637, 48, 48, 1)\n",
            "iteration: 31297 train shape: (66638, 48, 48, 1)\n",
            "iteration: 31298 train shape: (66639, 48, 48, 1)\n",
            "iteration: 31299 train shape: (66640, 48, 48, 1)\n",
            "iteration: 31300 train shape: (66641, 48, 48, 1)\n",
            "iteration: 31301 train shape: (66642, 48, 48, 1)\n",
            "iteration: 31302 train shape: (66643, 48, 48, 1)\n",
            "iteration: 31303 train shape: (66644, 48, 48, 1)\n",
            "iteration: 31304 train shape: (66645, 48, 48, 1)\n",
            "iteration: 31305 train shape: (66646, 48, 48, 1)\n",
            "iteration: 31306 train shape: (66647, 48, 48, 1)\n",
            "iteration: 31307 train shape: (66648, 48, 48, 1)\n",
            "iteration: 31308 train shape: (66649, 48, 48, 1)\n",
            "iteration: 31309 train shape: (66650, 48, 48, 1)\n",
            "iteration: 31310 train shape: (66651, 48, 48, 1)\n",
            "iteration: 31311 train shape: (66652, 48, 48, 1)\n",
            "iteration: 31312 train shape: (66653, 48, 48, 1)\n",
            "iteration: 31313 train shape: (66654, 48, 48, 1)\n",
            "iteration: 31314 train shape: (66655, 48, 48, 1)\n",
            "iteration: 31315 train shape: (66656, 48, 48, 1)\n",
            "iteration: 31316 train shape: (66657, 48, 48, 1)\n",
            "iteration: 31317 train shape: (66658, 48, 48, 1)\n",
            "iteration: 31318 train shape: (66659, 48, 48, 1)\n",
            "iteration: 31319 train shape: (66660, 48, 48, 1)\n",
            "iteration: 31320 train shape: (66661, 48, 48, 1)\n",
            "iteration: 31321 train shape: (66662, 48, 48, 1)\n",
            "iteration: 31322 train shape: (66663, 48, 48, 1)\n",
            "iteration: 31323 train shape: (66664, 48, 48, 1)\n",
            "iteration: 31324 train shape: (66665, 48, 48, 1)\n",
            "iteration: 31325 train shape: (66666, 48, 48, 1)\n",
            "iteration: 31326 train shape: (66667, 48, 48, 1)\n",
            "iteration: 31327 train shape: (66668, 48, 48, 1)\n",
            "iteration: 31328 train shape: (66669, 48, 48, 1)\n",
            "iteration: 31329 train shape: (66670, 48, 48, 1)\n",
            "iteration: 31330 train shape: (66671, 48, 48, 1)\n",
            "iteration: 31331 train shape: (66672, 48, 48, 1)\n",
            "iteration: 31332 train shape: (66673, 48, 48, 1)\n",
            "iteration: 31333 train shape: (66674, 48, 48, 1)\n",
            "iteration: 31334 train shape: (66675, 48, 48, 1)\n",
            "iteration: 31335 train shape: (66676, 48, 48, 1)\n",
            "iteration: 31336 train shape: (66677, 48, 48, 1)\n",
            "iteration: 31337 train shape: (66678, 48, 48, 1)\n",
            "iteration: 31338 train shape: (66679, 48, 48, 1)\n",
            "iteration: 31339 train shape: (66680, 48, 48, 1)\n",
            "iteration: 31340 train shape: (66681, 48, 48, 1)\n",
            "iteration: 31341 train shape: (66682, 48, 48, 1)\n",
            "iteration: 31342 train shape: (66683, 48, 48, 1)\n",
            "iteration: 31343 train shape: (66684, 48, 48, 1)\n",
            "iteration: 31344 train shape: (66685, 48, 48, 1)\n",
            "iteration: 31345 train shape: (66686, 48, 48, 1)\n",
            "iteration: 31346 train shape: (66687, 48, 48, 1)\n",
            "iteration: 31347 train shape: (66688, 48, 48, 1)\n",
            "iteration: 31348 train shape: (66689, 48, 48, 1)\n",
            "iteration: 31349 train shape: (66690, 48, 48, 1)\n",
            "iteration: 31350 train shape: (66691, 48, 48, 1)\n",
            "iteration: 31351 train shape: (66692, 48, 48, 1)\n",
            "iteration: 31352 train shape: (66693, 48, 48, 1)\n",
            "iteration: 31353 train shape: (66694, 48, 48, 1)\n",
            "iteration: 31354 train shape: (66695, 48, 48, 1)\n",
            "iteration: 31355 train shape: (66696, 48, 48, 1)\n",
            "iteration: 31356 train shape: (66697, 48, 48, 1)\n",
            "iteration: 31357 train shape: (66698, 48, 48, 1)\n",
            "iteration: 31358 train shape: (66699, 48, 48, 1)\n",
            "iteration: 31359 train shape: (66700, 48, 48, 1)\n",
            "iteration: 31360 train shape: (66701, 48, 48, 1)\n",
            "iteration: 31361 train shape: (66702, 48, 48, 1)\n",
            "iteration: 31362 train shape: (66703, 48, 48, 1)\n",
            "iteration: 31363 train shape: (66704, 48, 48, 1)\n",
            "iteration: 31364 train shape: (66705, 48, 48, 1)\n",
            "iteration: 31365 train shape: (66706, 48, 48, 1)\n",
            "iteration: 31366 train shape: (66707, 48, 48, 1)\n",
            "iteration: 31367 train shape: (66708, 48, 48, 1)\n",
            "iteration: 31368 train shape: (66709, 48, 48, 1)\n",
            "iteration: 31369 train shape: (66710, 48, 48, 1)\n",
            "iteration: 31370 train shape: (66711, 48, 48, 1)\n",
            "iteration: 31371 train shape: (66712, 48, 48, 1)\n",
            "iteration: 31372 train shape: (66713, 48, 48, 1)\n",
            "iteration: 31373 train shape: (66714, 48, 48, 1)\n",
            "iteration: 31374 train shape: (66715, 48, 48, 1)\n",
            "iteration: 31375 train shape: (66716, 48, 48, 1)\n",
            "iteration: 31376 train shape: (66717, 48, 48, 1)\n",
            "iteration: 31377 train shape: (66718, 48, 48, 1)\n",
            "iteration: 31378 train shape: (66719, 48, 48, 1)\n",
            "iteration: 31379 train shape: (66720, 48, 48, 1)\n",
            "iteration: 31380 train shape: (66721, 48, 48, 1)\n",
            "iteration: 31381 train shape: (66722, 48, 48, 1)\n",
            "iteration: 31382 train shape: (66723, 48, 48, 1)\n",
            "iteration: 31383 train shape: (66724, 48, 48, 1)\n",
            "iteration: 31384 train shape: (66725, 48, 48, 1)\n",
            "iteration: 31385 train shape: (66726, 48, 48, 1)\n",
            "iteration: 31386 train shape: (66727, 48, 48, 1)\n",
            "iteration: 31387 train shape: (66728, 48, 48, 1)\n",
            "iteration: 31388 train shape: (66729, 48, 48, 1)\n",
            "iteration: 31389 train shape: (66730, 48, 48, 1)\n",
            "iteration: 31390 train shape: (66731, 48, 48, 1)\n",
            "iteration: 31391 train shape: (66732, 48, 48, 1)\n",
            "iteration: 31392 train shape: (66733, 48, 48, 1)\n",
            "iteration: 31393 train shape: (66734, 48, 48, 1)\n",
            "iteration: 31394 train shape: (66735, 48, 48, 1)\n",
            "iteration: 31395 train shape: (66736, 48, 48, 1)\n",
            "iteration: 31396 train shape: (66737, 48, 48, 1)\n",
            "iteration: 31397 train shape: (66738, 48, 48, 1)\n",
            "iteration: 31398 train shape: (66739, 48, 48, 1)\n",
            "iteration: 31399 train shape: (66740, 48, 48, 1)\n",
            "iteration: 31400 train shape: (66741, 48, 48, 1)\n",
            "iteration: 31401 train shape: (66742, 48, 48, 1)\n",
            "iteration: 31402 train shape: (66743, 48, 48, 1)\n",
            "iteration: 31403 train shape: (66744, 48, 48, 1)\n",
            "iteration: 31404 train shape: (66745, 48, 48, 1)\n",
            "iteration: 31405 train shape: (66746, 48, 48, 1)\n",
            "iteration: 31406 train shape: (66747, 48, 48, 1)\n",
            "iteration: 31407 train shape: (66748, 48, 48, 1)\n",
            "iteration: 31408 train shape: (66749, 48, 48, 1)\n",
            "iteration: 31409 train shape: (66750, 48, 48, 1)\n",
            "iteration: 31410 train shape: (66751, 48, 48, 1)\n",
            "iteration: 31411 train shape: (66752, 48, 48, 1)\n",
            "iteration: 31412 train shape: (66753, 48, 48, 1)\n",
            "iteration: 31413 train shape: (66754, 48, 48, 1)\n",
            "iteration: 31414 train shape: (66755, 48, 48, 1)\n",
            "iteration: 31415 train shape: (66756, 48, 48, 1)\n",
            "iteration: 31416 train shape: (66757, 48, 48, 1)\n",
            "iteration: 31417 train shape: (66758, 48, 48, 1)\n",
            "iteration: 31418 train shape: (66759, 48, 48, 1)\n",
            "iteration: 31419 train shape: (66760, 48, 48, 1)\n",
            "iteration: 31420 train shape: (66761, 48, 48, 1)\n",
            "iteration: 31421 train shape: (66762, 48, 48, 1)\n",
            "iteration: 31422 train shape: (66763, 48, 48, 1)\n",
            "iteration: 31423 train shape: (66764, 48, 48, 1)\n",
            "iteration: 31424 train shape: (66765, 48, 48, 1)\n",
            "iteration: 31425 train shape: (66766, 48, 48, 1)\n",
            "iteration: 31426 train shape: (66767, 48, 48, 1)\n",
            "iteration: 31427 train shape: (66768, 48, 48, 1)\n",
            "iteration: 31428 train shape: (66769, 48, 48, 1)\n",
            "iteration: 31429 train shape: (66770, 48, 48, 1)\n",
            "iteration: 31430 train shape: (66771, 48, 48, 1)\n",
            "iteration: 31431 train shape: (66772, 48, 48, 1)\n",
            "iteration: 31432 train shape: (66773, 48, 48, 1)\n",
            "iteration: 31433 train shape: (66774, 48, 48, 1)\n",
            "iteration: 31434 train shape: (66775, 48, 48, 1)\n",
            "iteration: 31435 train shape: (66776, 48, 48, 1)\n",
            "iteration: 31436 train shape: (66777, 48, 48, 1)\n",
            "iteration: 31437 train shape: (66778, 48, 48, 1)\n",
            "iteration: 31438 train shape: (66779, 48, 48, 1)\n",
            "iteration: 31439 train shape: (66780, 48, 48, 1)\n",
            "iteration: 31440 train shape: (66781, 48, 48, 1)\n",
            "iteration: 31441 train shape: (66782, 48, 48, 1)\n",
            "iteration: 31442 train shape: (66783, 48, 48, 1)\n",
            "iteration: 31443 train shape: (66784, 48, 48, 1)\n",
            "iteration: 31444 train shape: (66785, 48, 48, 1)\n",
            "iteration: 31445 train shape: (66786, 48, 48, 1)\n",
            "iteration: 31446 train shape: (66787, 48, 48, 1)\n",
            "iteration: 31447 train shape: (66788, 48, 48, 1)\n",
            "iteration: 31448 train shape: (66789, 48, 48, 1)\n",
            "iteration: 31449 train shape: (66790, 48, 48, 1)\n",
            "iteration: 31450 train shape: (66791, 48, 48, 1)\n",
            "iteration: 31451 train shape: (66792, 48, 48, 1)\n",
            "iteration: 31452 train shape: (66793, 48, 48, 1)\n",
            "iteration: 31453 train shape: (66794, 48, 48, 1)\n",
            "iteration: 31454 train shape: (66795, 48, 48, 1)\n",
            "iteration: 31455 train shape: (66796, 48, 48, 1)\n",
            "iteration: 31456 train shape: (66797, 48, 48, 1)\n",
            "iteration: 31457 train shape: (66798, 48, 48, 1)\n",
            "iteration: 31458 train shape: (66799, 48, 48, 1)\n",
            "iteration: 31459 train shape: (66800, 48, 48, 1)\n",
            "iteration: 31460 train shape: (66801, 48, 48, 1)\n",
            "iteration: 31461 train shape: (66802, 48, 48, 1)\n",
            "iteration: 31462 train shape: (66803, 48, 48, 1)\n",
            "iteration: 31463 train shape: (66804, 48, 48, 1)\n",
            "iteration: 31464 train shape: (66805, 48, 48, 1)\n",
            "iteration: 31465 train shape: (66806, 48, 48, 1)\n",
            "iteration: 31466 train shape: (66807, 48, 48, 1)\n",
            "iteration: 31467 train shape: (66808, 48, 48, 1)\n",
            "iteration: 31468 train shape: (66809, 48, 48, 1)\n",
            "iteration: 31469 train shape: (66810, 48, 48, 1)\n",
            "iteration: 31470 train shape: (66811, 48, 48, 1)\n",
            "iteration: 31471 train shape: (66812, 48, 48, 1)\n",
            "iteration: 31472 train shape: (66813, 48, 48, 1)\n",
            "iteration: 31473 train shape: (66814, 48, 48, 1)\n",
            "iteration: 31474 train shape: (66815, 48, 48, 1)\n",
            "iteration: 31475 train shape: (66816, 48, 48, 1)\n",
            "iteration: 31476 train shape: (66817, 48, 48, 1)\n",
            "iteration: 31477 train shape: (66818, 48, 48, 1)\n",
            "iteration: 31478 train shape: (66819, 48, 48, 1)\n",
            "iteration: 31479 train shape: (66820, 48, 48, 1)\n",
            "iteration: 31480 train shape: (66821, 48, 48, 1)\n",
            "iteration: 31481 train shape: (66822, 48, 48, 1)\n",
            "iteration: 31482 train shape: (66823, 48, 48, 1)\n",
            "iteration: 31483 train shape: (66824, 48, 48, 1)\n",
            "iteration: 31484 train shape: (66825, 48, 48, 1)\n",
            "iteration: 31485 train shape: (66826, 48, 48, 1)\n",
            "iteration: 31486 train shape: (66827, 48, 48, 1)\n",
            "iteration: 31487 train shape: (66828, 48, 48, 1)\n",
            "iteration: 31488 train shape: (66829, 48, 48, 1)\n",
            "iteration: 31489 train shape: (66830, 48, 48, 1)\n",
            "iteration: 31490 train shape: (66831, 48, 48, 1)\n",
            "iteration: 31491 train shape: (66832, 48, 48, 1)\n",
            "iteration: 31492 train shape: (66833, 48, 48, 1)\n",
            "iteration: 31493 train shape: (66834, 48, 48, 1)\n",
            "iteration: 31494 train shape: (66835, 48, 48, 1)\n",
            "iteration: 31495 train shape: (66836, 48, 48, 1)\n",
            "iteration: 31496 train shape: (66837, 48, 48, 1)\n",
            "iteration: 31497 train shape: (66838, 48, 48, 1)\n",
            "iteration: 31498 train shape: (66839, 48, 48, 1)\n",
            "iteration: 31499 train shape: (66840, 48, 48, 1)\n",
            "iteration: 31500 train shape: (66841, 48, 48, 1)\n",
            "iteration: 31501 train shape: (66842, 48, 48, 1)\n",
            "iteration: 31502 train shape: (66843, 48, 48, 1)\n",
            "iteration: 31503 train shape: (66844, 48, 48, 1)\n",
            "iteration: 31504 train shape: (66845, 48, 48, 1)\n",
            "iteration: 31505 train shape: (66846, 48, 48, 1)\n",
            "iteration: 31506 train shape: (66847, 48, 48, 1)\n",
            "iteration: 31507 train shape: (66848, 48, 48, 1)\n",
            "iteration: 31508 train shape: (66849, 48, 48, 1)\n",
            "iteration: 31509 train shape: (66850, 48, 48, 1)\n",
            "iteration: 31510 train shape: (66851, 48, 48, 1)\n",
            "iteration: 31511 train shape: (66852, 48, 48, 1)\n",
            "iteration: 31512 train shape: (66853, 48, 48, 1)\n",
            "iteration: 31513 train shape: (66854, 48, 48, 1)\n",
            "iteration: 31514 train shape: (66855, 48, 48, 1)\n",
            "iteration: 31515 train shape: (66856, 48, 48, 1)\n",
            "iteration: 31516 train shape: (66857, 48, 48, 1)\n",
            "iteration: 31517 train shape: (66858, 48, 48, 1)\n",
            "iteration: 31518 train shape: (66859, 48, 48, 1)\n",
            "iteration: 31519 train shape: (66860, 48, 48, 1)\n",
            "iteration: 31520 train shape: (66861, 48, 48, 1)\n",
            "iteration: 31521 train shape: (66862, 48, 48, 1)\n",
            "iteration: 31522 train shape: (66863, 48, 48, 1)\n",
            "iteration: 31523 train shape: (66864, 48, 48, 1)\n",
            "iteration: 31524 train shape: (66865, 48, 48, 1)\n",
            "iteration: 31525 train shape: (66866, 48, 48, 1)\n",
            "iteration: 31526 train shape: (66867, 48, 48, 1)\n",
            "iteration: 31527 train shape: (66868, 48, 48, 1)\n",
            "iteration: 31528 train shape: (66869, 48, 48, 1)\n",
            "iteration: 31529 train shape: (66870, 48, 48, 1)\n",
            "iteration: 31530 train shape: (66871, 48, 48, 1)\n",
            "iteration: 31531 train shape: (66872, 48, 48, 1)\n",
            "iteration: 31532 train shape: (66873, 48, 48, 1)\n",
            "iteration: 31533 train shape: (66874, 48, 48, 1)\n",
            "iteration: 31534 train shape: (66875, 48, 48, 1)\n",
            "iteration: 31535 train shape: (66876, 48, 48, 1)\n",
            "iteration: 31536 train shape: (66877, 48, 48, 1)\n",
            "iteration: 31537 train shape: (66878, 48, 48, 1)\n",
            "iteration: 31538 train shape: (66879, 48, 48, 1)\n",
            "iteration: 31539 train shape: (66880, 48, 48, 1)\n",
            "iteration: 31540 train shape: (66881, 48, 48, 1)\n",
            "iteration: 31541 train shape: (66882, 48, 48, 1)\n",
            "iteration: 31542 train shape: (66883, 48, 48, 1)\n",
            "iteration: 31543 train shape: (66884, 48, 48, 1)\n",
            "iteration: 31544 train shape: (66885, 48, 48, 1)\n",
            "iteration: 31545 train shape: (66886, 48, 48, 1)\n",
            "iteration: 31546 train shape: (66887, 48, 48, 1)\n",
            "iteration: 31547 train shape: (66888, 48, 48, 1)\n",
            "iteration: 31548 train shape: (66889, 48, 48, 1)\n",
            "iteration: 31549 train shape: (66890, 48, 48, 1)\n",
            "iteration: 31550 train shape: (66891, 48, 48, 1)\n",
            "iteration: 31551 train shape: (66892, 48, 48, 1)\n",
            "iteration: 31552 train shape: (66893, 48, 48, 1)\n",
            "iteration: 31553 train shape: (66894, 48, 48, 1)\n",
            "iteration: 31554 train shape: (66895, 48, 48, 1)\n",
            "iteration: 31555 train shape: (66896, 48, 48, 1)\n",
            "iteration: 31556 train shape: (66897, 48, 48, 1)\n",
            "iteration: 31557 train shape: (66898, 48, 48, 1)\n",
            "iteration: 31558 train shape: (66899, 48, 48, 1)\n",
            "iteration: 31559 train shape: (66900, 48, 48, 1)\n",
            "iteration: 31560 train shape: (66901, 48, 48, 1)\n",
            "iteration: 31561 train shape: (66902, 48, 48, 1)\n",
            "iteration: 31562 train shape: (66903, 48, 48, 1)\n",
            "iteration: 31563 train shape: (66904, 48, 48, 1)\n",
            "iteration: 31564 train shape: (66905, 48, 48, 1)\n",
            "iteration: 31565 train shape: (66906, 48, 48, 1)\n",
            "iteration: 31566 train shape: (66907, 48, 48, 1)\n",
            "iteration: 31567 train shape: (66908, 48, 48, 1)\n",
            "iteration: 31568 train shape: (66909, 48, 48, 1)\n",
            "iteration: 31569 train shape: (66910, 48, 48, 1)\n",
            "iteration: 31570 train shape: (66911, 48, 48, 1)\n",
            "iteration: 31571 train shape: (66912, 48, 48, 1)\n",
            "iteration: 31572 train shape: (66913, 48, 48, 1)\n",
            "iteration: 31573 train shape: (66914, 48, 48, 1)\n",
            "iteration: 31574 train shape: (66915, 48, 48, 1)\n",
            "iteration: 31575 train shape: (66916, 48, 48, 1)\n",
            "iteration: 31576 train shape: (66917, 48, 48, 1)\n",
            "iteration: 31577 train shape: (66918, 48, 48, 1)\n",
            "iteration: 31578 train shape: (66919, 48, 48, 1)\n",
            "iteration: 31579 train shape: (66920, 48, 48, 1)\n",
            "iteration: 31580 train shape: (66921, 48, 48, 1)\n",
            "iteration: 31581 train shape: (66922, 48, 48, 1)\n",
            "iteration: 31582 train shape: (66923, 48, 48, 1)\n",
            "iteration: 31583 train shape: (66924, 48, 48, 1)\n",
            "iteration: 31584 train shape: (66925, 48, 48, 1)\n",
            "iteration: 31585 train shape: (66926, 48, 48, 1)\n",
            "iteration: 31586 train shape: (66927, 48, 48, 1)\n",
            "iteration: 31587 train shape: (66928, 48, 48, 1)\n",
            "iteration: 31588 train shape: (66929, 48, 48, 1)\n",
            "iteration: 31589 train shape: (66930, 48, 48, 1)\n",
            "iteration: 31590 train shape: (66931, 48, 48, 1)\n",
            "iteration: 31591 train shape: (66932, 48, 48, 1)\n",
            "iteration: 31592 train shape: (66933, 48, 48, 1)\n",
            "iteration: 31593 train shape: (66934, 48, 48, 1)\n",
            "iteration: 31594 train shape: (66935, 48, 48, 1)\n",
            "iteration: 31595 train shape: (66936, 48, 48, 1)\n",
            "iteration: 31596 train shape: (66937, 48, 48, 1)\n",
            "iteration: 31597 train shape: (66938, 48, 48, 1)\n",
            "iteration: 31598 train shape: (66939, 48, 48, 1)\n",
            "iteration: 31599 train shape: (66940, 48, 48, 1)\n",
            "iteration: 31600 train shape: (66941, 48, 48, 1)\n",
            "iteration: 31601 train shape: (66942, 48, 48, 1)\n",
            "iteration: 31602 train shape: (66943, 48, 48, 1)\n",
            "iteration: 31603 train shape: (66944, 48, 48, 1)\n",
            "iteration: 31604 train shape: (66945, 48, 48, 1)\n",
            "iteration: 31605 train shape: (66946, 48, 48, 1)\n",
            "iteration: 31606 train shape: (66947, 48, 48, 1)\n",
            "iteration: 31607 train shape: (66948, 48, 48, 1)\n",
            "iteration: 31608 train shape: (66949, 48, 48, 1)\n",
            "iteration: 31609 train shape: (66950, 48, 48, 1)\n",
            "iteration: 31610 train shape: (66951, 48, 48, 1)\n",
            "iteration: 31611 train shape: (66952, 48, 48, 1)\n",
            "iteration: 31612 train shape: (66953, 48, 48, 1)\n",
            "iteration: 31613 train shape: (66954, 48, 48, 1)\n",
            "iteration: 31614 train shape: (66955, 48, 48, 1)\n",
            "iteration: 31615 train shape: (66956, 48, 48, 1)\n",
            "iteration: 31616 train shape: (66957, 48, 48, 1)\n",
            "iteration: 31617 train shape: (66958, 48, 48, 1)\n",
            "iteration: 31618 train shape: (66959, 48, 48, 1)\n",
            "iteration: 31619 train shape: (66960, 48, 48, 1)\n",
            "iteration: 31620 train shape: (66961, 48, 48, 1)\n",
            "iteration: 31621 train shape: (66962, 48, 48, 1)\n",
            "iteration: 31622 train shape: (66963, 48, 48, 1)\n",
            "iteration: 31623 train shape: (66964, 48, 48, 1)\n",
            "iteration: 31624 train shape: (66965, 48, 48, 1)\n",
            "iteration: 31625 train shape: (66966, 48, 48, 1)\n",
            "iteration: 31626 train shape: (66967, 48, 48, 1)\n",
            "iteration: 31627 train shape: (66968, 48, 48, 1)\n",
            "iteration: 31628 train shape: (66969, 48, 48, 1)\n",
            "iteration: 31629 train shape: (66970, 48, 48, 1)\n",
            "iteration: 31630 train shape: (66971, 48, 48, 1)\n",
            "iteration: 31631 train shape: (66972, 48, 48, 1)\n",
            "iteration: 31632 train shape: (66973, 48, 48, 1)\n",
            "iteration: 31633 train shape: (66974, 48, 48, 1)\n",
            "iteration: 31634 train shape: (66975, 48, 48, 1)\n",
            "iteration: 31635 train shape: (66976, 48, 48, 1)\n",
            "iteration: 31636 train shape: (66977, 48, 48, 1)\n",
            "iteration: 31637 train shape: (66978, 48, 48, 1)\n",
            "iteration: 31638 train shape: (66979, 48, 48, 1)\n",
            "iteration: 31639 train shape: (66980, 48, 48, 1)\n",
            "iteration: 31640 train shape: (66981, 48, 48, 1)\n",
            "iteration: 31641 train shape: (66982, 48, 48, 1)\n",
            "iteration: 31642 train shape: (66983, 48, 48, 1)\n",
            "iteration: 31643 train shape: (66984, 48, 48, 1)\n",
            "iteration: 31644 train shape: (66985, 48, 48, 1)\n",
            "iteration: 31645 train shape: (66986, 48, 48, 1)\n",
            "iteration: 31646 train shape: (66987, 48, 48, 1)\n",
            "iteration: 31647 train shape: (66988, 48, 48, 1)\n",
            "iteration: 31648 train shape: (66989, 48, 48, 1)\n",
            "iteration: 31649 train shape: (66990, 48, 48, 1)\n",
            "iteration: 31650 train shape: (66991, 48, 48, 1)\n",
            "iteration: 31651 train shape: (66992, 48, 48, 1)\n",
            "iteration: 31652 train shape: (66993, 48, 48, 1)\n",
            "iteration: 31653 train shape: (66994, 48, 48, 1)\n",
            "iteration: 31654 train shape: (66995, 48, 48, 1)\n",
            "iteration: 31655 train shape: (66996, 48, 48, 1)\n",
            "iteration: 31656 train shape: (66997, 48, 48, 1)\n",
            "iteration: 31657 train shape: (66998, 48, 48, 1)\n",
            "iteration: 31658 train shape: (66999, 48, 48, 1)\n",
            "iteration: 31659 train shape: (67000, 48, 48, 1)\n",
            "iteration: 31660 train shape: (67001, 48, 48, 1)\n",
            "iteration: 31661 train shape: (67002, 48, 48, 1)\n",
            "iteration: 31662 train shape: (67003, 48, 48, 1)\n",
            "iteration: 31663 train shape: (67004, 48, 48, 1)\n",
            "iteration: 31664 train shape: (67005, 48, 48, 1)\n",
            "iteration: 31665 train shape: (67006, 48, 48, 1)\n",
            "iteration: 31666 train shape: (67007, 48, 48, 1)\n",
            "iteration: 31667 train shape: (67008, 48, 48, 1)\n",
            "iteration: 31668 train shape: (67009, 48, 48, 1)\n",
            "iteration: 31669 train shape: (67010, 48, 48, 1)\n",
            "iteration: 31670 train shape: (67011, 48, 48, 1)\n",
            "iteration: 31671 train shape: (67012, 48, 48, 1)\n",
            "iteration: 31672 train shape: (67013, 48, 48, 1)\n",
            "iteration: 31673 train shape: (67014, 48, 48, 1)\n",
            "iteration: 31674 train shape: (67015, 48, 48, 1)\n",
            "iteration: 31675 train shape: (67016, 48, 48, 1)\n",
            "iteration: 31676 train shape: (67017, 48, 48, 1)\n",
            "iteration: 31677 train shape: (67018, 48, 48, 1)\n",
            "iteration: 31678 train shape: (67019, 48, 48, 1)\n",
            "iteration: 31679 train shape: (67020, 48, 48, 1)\n",
            "iteration: 31680 train shape: (67021, 48, 48, 1)\n",
            "iteration: 31681 train shape: (67022, 48, 48, 1)\n",
            "iteration: 31682 train shape: (67023, 48, 48, 1)\n",
            "iteration: 31683 train shape: (67024, 48, 48, 1)\n",
            "iteration: 31684 train shape: (67025, 48, 48, 1)\n",
            "iteration: 31685 train shape: (67026, 48, 48, 1)\n",
            "iteration: 31686 train shape: (67027, 48, 48, 1)\n",
            "iteration: 31687 train shape: (67028, 48, 48, 1)\n",
            "iteration: 31688 train shape: (67029, 48, 48, 1)\n",
            "iteration: 31689 train shape: (67030, 48, 48, 1)\n",
            "iteration: 31690 train shape: (67031, 48, 48, 1)\n",
            "iteration: 31691 train shape: (67032, 48, 48, 1)\n",
            "iteration: 31692 train shape: (67033, 48, 48, 1)\n",
            "iteration: 31693 train shape: (67034, 48, 48, 1)\n",
            "iteration: 31694 train shape: (67035, 48, 48, 1)\n",
            "iteration: 31695 train shape: (67036, 48, 48, 1)\n",
            "iteration: 31696 train shape: (67037, 48, 48, 1)\n",
            "iteration: 31697 train shape: (67038, 48, 48, 1)\n",
            "iteration: 31698 train shape: (67039, 48, 48, 1)\n",
            "iteration: 31699 train shape: (67040, 48, 48, 1)\n",
            "iteration: 31700 train shape: (67041, 48, 48, 1)\n",
            "iteration: 31701 train shape: (67042, 48, 48, 1)\n",
            "iteration: 31702 train shape: (67043, 48, 48, 1)\n",
            "iteration: 31703 train shape: (67044, 48, 48, 1)\n",
            "iteration: 31704 train shape: (67045, 48, 48, 1)\n",
            "iteration: 31705 train shape: (67046, 48, 48, 1)\n",
            "iteration: 31706 train shape: (67047, 48, 48, 1)\n",
            "iteration: 31707 train shape: (67048, 48, 48, 1)\n",
            "iteration: 31708 train shape: (67049, 48, 48, 1)\n",
            "iteration: 31709 train shape: (67050, 48, 48, 1)\n",
            "iteration: 31710 train shape: (67051, 48, 48, 1)\n",
            "iteration: 31711 train shape: (67052, 48, 48, 1)\n",
            "iteration: 31712 train shape: (67053, 48, 48, 1)\n",
            "iteration: 31713 train shape: (67054, 48, 48, 1)\n",
            "iteration: 31714 train shape: (67055, 48, 48, 1)\n",
            "iteration: 31715 train shape: (67056, 48, 48, 1)\n",
            "iteration: 31716 train shape: (67057, 48, 48, 1)\n",
            "iteration: 31717 train shape: (67058, 48, 48, 1)\n",
            "iteration: 31718 train shape: (67059, 48, 48, 1)\n",
            "iteration: 31719 train shape: (67060, 48, 48, 1)\n",
            "iteration: 31720 train shape: (67061, 48, 48, 1)\n",
            "iteration: 31721 train shape: (67062, 48, 48, 1)\n",
            "iteration: 31722 train shape: (67063, 48, 48, 1)\n",
            "iteration: 31723 train shape: (67064, 48, 48, 1)\n",
            "iteration: 31724 train shape: (67065, 48, 48, 1)\n",
            "iteration: 31725 train shape: (67066, 48, 48, 1)\n",
            "iteration: 31726 train shape: (67067, 48, 48, 1)\n",
            "iteration: 31727 train shape: (67068, 48, 48, 1)\n",
            "iteration: 31728 train shape: (67069, 48, 48, 1)\n",
            "iteration: 31729 train shape: (67070, 48, 48, 1)\n",
            "iteration: 31730 train shape: (67071, 48, 48, 1)\n",
            "iteration: 31731 train shape: (67072, 48, 48, 1)\n",
            "iteration: 31732 train shape: (67073, 48, 48, 1)\n",
            "iteration: 31733 train shape: (67074, 48, 48, 1)\n",
            "iteration: 31734 train shape: (67075, 48, 48, 1)\n",
            "iteration: 31735 train shape: (67076, 48, 48, 1)\n",
            "iteration: 31736 train shape: (67077, 48, 48, 1)\n",
            "iteration: 31737 train shape: (67078, 48, 48, 1)\n",
            "iteration: 31738 train shape: (67079, 48, 48, 1)\n",
            "iteration: 31739 train shape: (67080, 48, 48, 1)\n",
            "iteration: 31740 train shape: (67081, 48, 48, 1)\n",
            "iteration: 31741 train shape: (67082, 48, 48, 1)\n",
            "iteration: 31742 train shape: (67083, 48, 48, 1)\n",
            "iteration: 31743 train shape: (67084, 48, 48, 1)\n",
            "iteration: 31744 train shape: (67085, 48, 48, 1)\n",
            "iteration: 31745 train shape: (67086, 48, 48, 1)\n",
            "iteration: 31746 train shape: (67087, 48, 48, 1)\n",
            "iteration: 31747 train shape: (67088, 48, 48, 1)\n",
            "iteration: 31748 train shape: (67089, 48, 48, 1)\n",
            "iteration: 31749 train shape: (67090, 48, 48, 1)\n",
            "iteration: 31750 train shape: (67091, 48, 48, 1)\n",
            "iteration: 31751 train shape: (67092, 48, 48, 1)\n",
            "iteration: 31752 train shape: (67093, 48, 48, 1)\n",
            "iteration: 31753 train shape: (67094, 48, 48, 1)\n",
            "iteration: 31754 train shape: (67095, 48, 48, 1)\n",
            "iteration: 31755 train shape: (67096, 48, 48, 1)\n",
            "iteration: 31756 train shape: (67097, 48, 48, 1)\n",
            "iteration: 31757 train shape: (67098, 48, 48, 1)\n",
            "iteration: 31758 train shape: (67099, 48, 48, 1)\n",
            "iteration: 31759 train shape: (67100, 48, 48, 1)\n",
            "iteration: 31760 train shape: (67101, 48, 48, 1)\n",
            "iteration: 31761 train shape: (67102, 48, 48, 1)\n",
            "iteration: 31762 train shape: (67103, 48, 48, 1)\n",
            "iteration: 31763 train shape: (67104, 48, 48, 1)\n",
            "iteration: 31764 train shape: (67105, 48, 48, 1)\n",
            "iteration: 31765 train shape: (67106, 48, 48, 1)\n",
            "iteration: 31766 train shape: (67107, 48, 48, 1)\n",
            "iteration: 31767 train shape: (67108, 48, 48, 1)\n",
            "iteration: 31768 train shape: (67109, 48, 48, 1)\n",
            "iteration: 31769 train shape: (67110, 48, 48, 1)\n",
            "iteration: 31770 train shape: (67111, 48, 48, 1)\n",
            "iteration: 31771 train shape: (67112, 48, 48, 1)\n",
            "iteration: 31772 train shape: (67113, 48, 48, 1)\n",
            "iteration: 31773 train shape: (67114, 48, 48, 1)\n",
            "iteration: 31774 train shape: (67115, 48, 48, 1)\n",
            "iteration: 31775 train shape: (67116, 48, 48, 1)\n",
            "iteration: 31776 train shape: (67117, 48, 48, 1)\n",
            "iteration: 31777 train shape: (67118, 48, 48, 1)\n",
            "iteration: 31778 train shape: (67119, 48, 48, 1)\n",
            "iteration: 31779 train shape: (67120, 48, 48, 1)\n",
            "iteration: 31780 train shape: (67121, 48, 48, 1)\n",
            "iteration: 31781 train shape: (67122, 48, 48, 1)\n",
            "iteration: 31782 train shape: (67123, 48, 48, 1)\n",
            "iteration: 31783 train shape: (67124, 48, 48, 1)\n",
            "iteration: 31784 train shape: (67125, 48, 48, 1)\n",
            "iteration: 31785 train shape: (67126, 48, 48, 1)\n",
            "iteration: 31786 train shape: (67127, 48, 48, 1)\n",
            "iteration: 31787 train shape: (67128, 48, 48, 1)\n",
            "iteration: 31788 train shape: (67129, 48, 48, 1)\n",
            "iteration: 31789 train shape: (67130, 48, 48, 1)\n",
            "iteration: 31790 train shape: (67131, 48, 48, 1)\n",
            "iteration: 31791 train shape: (67132, 48, 48, 1)\n",
            "iteration: 31792 train shape: (67133, 48, 48, 1)\n",
            "iteration: 31793 train shape: (67134, 48, 48, 1)\n",
            "iteration: 31794 train shape: (67135, 48, 48, 1)\n",
            "iteration: 31795 train shape: (67136, 48, 48, 1)\n",
            "iteration: 31796 train shape: (67137, 48, 48, 1)\n",
            "iteration: 31797 train shape: (67138, 48, 48, 1)\n",
            "iteration: 31798 train shape: (67139, 48, 48, 1)\n",
            "iteration: 31799 train shape: (67140, 48, 48, 1)\n",
            "iteration: 31800 train shape: (67141, 48, 48, 1)\n",
            "iteration: 31801 train shape: (67142, 48, 48, 1)\n",
            "iteration: 31802 train shape: (67143, 48, 48, 1)\n",
            "iteration: 31803 train shape: (67144, 48, 48, 1)\n",
            "iteration: 31804 train shape: (67145, 48, 48, 1)\n",
            "iteration: 31805 train shape: (67146, 48, 48, 1)\n",
            "iteration: 31806 train shape: (67147, 48, 48, 1)\n",
            "iteration: 31807 train shape: (67148, 48, 48, 1)\n",
            "iteration: 31808 train shape: (67149, 48, 48, 1)\n",
            "iteration: 31809 train shape: (67150, 48, 48, 1)\n",
            "iteration: 31810 train shape: (67151, 48, 48, 1)\n",
            "iteration: 31811 train shape: (67152, 48, 48, 1)\n",
            "iteration: 31812 train shape: (67153, 48, 48, 1)\n",
            "iteration: 31813 train shape: (67154, 48, 48, 1)\n",
            "iteration: 31814 train shape: (67155, 48, 48, 1)\n",
            "iteration: 31815 train shape: (67156, 48, 48, 1)\n",
            "iteration: 31816 train shape: (67157, 48, 48, 1)\n",
            "iteration: 31817 train shape: (67158, 48, 48, 1)\n",
            "iteration: 31818 train shape: (67159, 48, 48, 1)\n",
            "iteration: 31819 train shape: (67160, 48, 48, 1)\n",
            "iteration: 31820 train shape: (67161, 48, 48, 1)\n",
            "iteration: 31821 train shape: (67162, 48, 48, 1)\n",
            "iteration: 31822 train shape: (67163, 48, 48, 1)\n",
            "iteration: 31823 train shape: (67164, 48, 48, 1)\n",
            "iteration: 31824 train shape: (67165, 48, 48, 1)\n",
            "iteration: 31825 train shape: (67166, 48, 48, 1)\n",
            "iteration: 31826 train shape: (67167, 48, 48, 1)\n",
            "iteration: 31827 train shape: (67168, 48, 48, 1)\n",
            "iteration: 31828 train shape: (67169, 48, 48, 1)\n",
            "iteration: 31829 train shape: (67170, 48, 48, 1)\n",
            "iteration: 31830 train shape: (67171, 48, 48, 1)\n",
            "iteration: 31831 train shape: (67172, 48, 48, 1)\n",
            "iteration: 31832 train shape: (67173, 48, 48, 1)\n",
            "iteration: 31833 train shape: (67174, 48, 48, 1)\n",
            "iteration: 31834 train shape: (67175, 48, 48, 1)\n",
            "iteration: 31835 train shape: (67176, 48, 48, 1)\n",
            "iteration: 31836 train shape: (67177, 48, 48, 1)\n",
            "iteration: 31837 train shape: (67178, 48, 48, 1)\n",
            "iteration: 31838 train shape: (67179, 48, 48, 1)\n",
            "iteration: 31839 train shape: (67180, 48, 48, 1)\n",
            "iteration: 31840 train shape: (67181, 48, 48, 1)\n",
            "iteration: 31841 train shape: (67182, 48, 48, 1)\n",
            "iteration: 31842 train shape: (67183, 48, 48, 1)\n",
            "iteration: 31843 train shape: (67184, 48, 48, 1)\n",
            "iteration: 31844 train shape: (67185, 48, 48, 1)\n",
            "iteration: 31845 train shape: (67186, 48, 48, 1)\n",
            "iteration: 31846 train shape: (67187, 48, 48, 1)\n",
            "iteration: 31847 train shape: (67188, 48, 48, 1)\n",
            "iteration: 31848 train shape: (67189, 48, 48, 1)\n",
            "iteration: 31849 train shape: (67190, 48, 48, 1)\n",
            "iteration: 31850 train shape: (67191, 48, 48, 1)\n",
            "iteration: 31851 train shape: (67192, 48, 48, 1)\n",
            "iteration: 31852 train shape: (67193, 48, 48, 1)\n",
            "iteration: 31853 train shape: (67194, 48, 48, 1)\n",
            "iteration: 31854 train shape: (67195, 48, 48, 1)\n",
            "iteration: 31855 train shape: (67196, 48, 48, 1)\n",
            "iteration: 31856 train shape: (67197, 48, 48, 1)\n",
            "iteration: 31857 train shape: (67198, 48, 48, 1)\n",
            "iteration: 31858 train shape: (67199, 48, 48, 1)\n",
            "iteration: 31859 train shape: (67200, 48, 48, 1)\n",
            "iteration: 31860 train shape: (67201, 48, 48, 1)\n",
            "iteration: 31861 train shape: (67202, 48, 48, 1)\n",
            "iteration: 31862 train shape: (67203, 48, 48, 1)\n",
            "iteration: 31863 train shape: (67204, 48, 48, 1)\n",
            "iteration: 31864 train shape: (67205, 48, 48, 1)\n",
            "iteration: 31865 train shape: (67206, 48, 48, 1)\n",
            "iteration: 31866 train shape: (67207, 48, 48, 1)\n",
            "iteration: 31867 train shape: (67208, 48, 48, 1)\n",
            "iteration: 31868 train shape: (67209, 48, 48, 1)\n",
            "iteration: 31869 train shape: (67210, 48, 48, 1)\n",
            "iteration: 31870 train shape: (67211, 48, 48, 1)\n",
            "iteration: 31871 train shape: (67212, 48, 48, 1)\n",
            "iteration: 31872 train shape: (67213, 48, 48, 1)\n",
            "iteration: 31873 train shape: (67214, 48, 48, 1)\n",
            "iteration: 31874 train shape: (67215, 48, 48, 1)\n",
            "iteration: 31875 train shape: (67216, 48, 48, 1)\n",
            "iteration: 31876 train shape: (67217, 48, 48, 1)\n",
            "iteration: 31877 train shape: (67218, 48, 48, 1)\n",
            "iteration: 31878 train shape: (67219, 48, 48, 1)\n",
            "iteration: 31879 train shape: (67220, 48, 48, 1)\n",
            "iteration: 31880 train shape: (67221, 48, 48, 1)\n",
            "iteration: 31881 train shape: (67222, 48, 48, 1)\n",
            "iteration: 31882 train shape: (67223, 48, 48, 1)\n",
            "iteration: 31883 train shape: (67224, 48, 48, 1)\n",
            "iteration: 31884 train shape: (67225, 48, 48, 1)\n",
            "iteration: 31885 train shape: (67226, 48, 48, 1)\n",
            "iteration: 31886 train shape: (67227, 48, 48, 1)\n",
            "iteration: 31887 train shape: (67228, 48, 48, 1)\n",
            "iteration: 31888 train shape: (67229, 48, 48, 1)\n",
            "iteration: 31889 train shape: (67230, 48, 48, 1)\n",
            "iteration: 31890 train shape: (67231, 48, 48, 1)\n",
            "iteration: 31891 train shape: (67232, 48, 48, 1)\n",
            "iteration: 31892 train shape: (67233, 48, 48, 1)\n",
            "iteration: 31893 train shape: (67234, 48, 48, 1)\n",
            "iteration: 31894 train shape: (67235, 48, 48, 1)\n",
            "iteration: 31895 train shape: (67236, 48, 48, 1)\n",
            "iteration: 31896 train shape: (67237, 48, 48, 1)\n",
            "iteration: 31897 train shape: (67238, 48, 48, 1)\n",
            "iteration: 31898 train shape: (67239, 48, 48, 1)\n",
            "iteration: 31899 train shape: (67240, 48, 48, 1)\n",
            "iteration: 31900 train shape: (67241, 48, 48, 1)\n",
            "iteration: 31901 train shape: (67242, 48, 48, 1)\n",
            "iteration: 31902 train shape: (67243, 48, 48, 1)\n",
            "iteration: 31903 train shape: (67244, 48, 48, 1)\n",
            "iteration: 31904 train shape: (67245, 48, 48, 1)\n",
            "iteration: 31905 train shape: (67246, 48, 48, 1)\n",
            "iteration: 31906 train shape: (67247, 48, 48, 1)\n",
            "iteration: 31907 train shape: (67248, 48, 48, 1)\n",
            "iteration: 31908 train shape: (67249, 48, 48, 1)\n",
            "iteration: 31909 train shape: (67250, 48, 48, 1)\n",
            "iteration: 31910 train shape: (67251, 48, 48, 1)\n",
            "iteration: 31911 train shape: (67252, 48, 48, 1)\n",
            "iteration: 31912 train shape: (67253, 48, 48, 1)\n",
            "iteration: 31913 train shape: (67254, 48, 48, 1)\n",
            "iteration: 31914 train shape: (67255, 48, 48, 1)\n",
            "iteration: 31915 train shape: (67256, 48, 48, 1)\n",
            "iteration: 31916 train shape: (67257, 48, 48, 1)\n",
            "iteration: 31917 train shape: (67258, 48, 48, 1)\n",
            "iteration: 31918 train shape: (67259, 48, 48, 1)\n",
            "iteration: 31919 train shape: (67260, 48, 48, 1)\n",
            "iteration: 31920 train shape: (67261, 48, 48, 1)\n",
            "iteration: 31921 train shape: (67262, 48, 48, 1)\n",
            "iteration: 31922 train shape: (67263, 48, 48, 1)\n",
            "iteration: 31923 train shape: (67264, 48, 48, 1)\n",
            "iteration: 31924 train shape: (67265, 48, 48, 1)\n",
            "iteration: 31925 train shape: (67266, 48, 48, 1)\n",
            "iteration: 31926 train shape: (67267, 48, 48, 1)\n",
            "iteration: 31927 train shape: (67268, 48, 48, 1)\n",
            "iteration: 31928 train shape: (67269, 48, 48, 1)\n",
            "iteration: 31929 train shape: (67270, 48, 48, 1)\n",
            "iteration: 31930 train shape: (67271, 48, 48, 1)\n",
            "iteration: 31931 train shape: (67272, 48, 48, 1)\n",
            "iteration: 31932 train shape: (67273, 48, 48, 1)\n",
            "iteration: 31933 train shape: (67274, 48, 48, 1)\n",
            "iteration: 31934 train shape: (67275, 48, 48, 1)\n",
            "iteration: 31935 train shape: (67276, 48, 48, 1)\n",
            "iteration: 31936 train shape: (67277, 48, 48, 1)\n",
            "iteration: 31937 train shape: (67278, 48, 48, 1)\n",
            "iteration: 31938 train shape: (67279, 48, 48, 1)\n",
            "iteration: 31939 train shape: (67280, 48, 48, 1)\n",
            "iteration: 31940 train shape: (67281, 48, 48, 1)\n",
            "iteration: 31941 train shape: (67282, 48, 48, 1)\n",
            "iteration: 31942 train shape: (67283, 48, 48, 1)\n",
            "iteration: 31943 train shape: (67284, 48, 48, 1)\n",
            "iteration: 31944 train shape: (67285, 48, 48, 1)\n",
            "iteration: 31945 train shape: (67286, 48, 48, 1)\n",
            "iteration: 31946 train shape: (67287, 48, 48, 1)\n",
            "iteration: 31947 train shape: (67288, 48, 48, 1)\n",
            "iteration: 31948 train shape: (67289, 48, 48, 1)\n",
            "iteration: 31949 train shape: (67290, 48, 48, 1)\n",
            "iteration: 31950 train shape: (67291, 48, 48, 1)\n",
            "iteration: 31951 train shape: (67292, 48, 48, 1)\n",
            "iteration: 31952 train shape: (67293, 48, 48, 1)\n",
            "iteration: 31953 train shape: (67294, 48, 48, 1)\n",
            "iteration: 31954 train shape: (67295, 48, 48, 1)\n",
            "iteration: 31955 train shape: (67296, 48, 48, 1)\n",
            "iteration: 31956 train shape: (67297, 48, 48, 1)\n",
            "iteration: 31957 train shape: (67298, 48, 48, 1)\n",
            "iteration: 31958 train shape: (67299, 48, 48, 1)\n",
            "iteration: 31959 train shape: (67300, 48, 48, 1)\n",
            "iteration: 31960 train shape: (67301, 48, 48, 1)\n",
            "iteration: 31961 train shape: (67302, 48, 48, 1)\n",
            "iteration: 31962 train shape: (67303, 48, 48, 1)\n",
            "iteration: 31963 train shape: (67304, 48, 48, 1)\n",
            "iteration: 31964 train shape: (67305, 48, 48, 1)\n",
            "iteration: 31965 train shape: (67306, 48, 48, 1)\n",
            "iteration: 31966 train shape: (67307, 48, 48, 1)\n",
            "iteration: 31967 train shape: (67308, 48, 48, 1)\n",
            "iteration: 31968 train shape: (67309, 48, 48, 1)\n",
            "iteration: 31969 train shape: (67310, 48, 48, 1)\n",
            "iteration: 31970 train shape: (67311, 48, 48, 1)\n",
            "iteration: 31971 train shape: (67312, 48, 48, 1)\n",
            "iteration: 31972 train shape: (67313, 48, 48, 1)\n",
            "iteration: 31973 train shape: (67314, 48, 48, 1)\n",
            "iteration: 31974 train shape: (67315, 48, 48, 1)\n",
            "iteration: 31975 train shape: (67316, 48, 48, 1)\n",
            "iteration: 31976 train shape: (67317, 48, 48, 1)\n",
            "iteration: 31977 train shape: (67318, 48, 48, 1)\n",
            "iteration: 31978 train shape: (67319, 48, 48, 1)\n",
            "iteration: 31979 train shape: (67320, 48, 48, 1)\n",
            "iteration: 31980 train shape: (67321, 48, 48, 1)\n",
            "iteration: 31981 train shape: (67322, 48, 48, 1)\n",
            "iteration: 31982 train shape: (67323, 48, 48, 1)\n",
            "iteration: 31983 train shape: (67324, 48, 48, 1)\n",
            "iteration: 31984 train shape: (67325, 48, 48, 1)\n",
            "iteration: 31985 train shape: (67326, 48, 48, 1)\n",
            "iteration: 31986 train shape: (67327, 48, 48, 1)\n",
            "iteration: 31987 train shape: (67328, 48, 48, 1)\n",
            "iteration: 31988 train shape: (67329, 48, 48, 1)\n",
            "iteration: 31989 train shape: (67330, 48, 48, 1)\n",
            "iteration: 31990 train shape: (67331, 48, 48, 1)\n",
            "iteration: 31991 train shape: (67332, 48, 48, 1)\n",
            "iteration: 31992 train shape: (67333, 48, 48, 1)\n",
            "iteration: 31993 train shape: (67334, 48, 48, 1)\n",
            "iteration: 31994 train shape: (67335, 48, 48, 1)\n",
            "iteration: 31995 train shape: (67336, 48, 48, 1)\n",
            "iteration: 31996 train shape: (67337, 48, 48, 1)\n",
            "iteration: 31997 train shape: (67338, 48, 48, 1)\n",
            "iteration: 31998 train shape: (67339, 48, 48, 1)\n",
            "iteration: 31999 train shape: (67340, 48, 48, 1)\n",
            "iteration: 32000 train shape: (67341, 48, 48, 1)\n",
            "iteration: 32001 train shape: (67342, 48, 48, 1)\n",
            "iteration: 32002 train shape: (67343, 48, 48, 1)\n",
            "iteration: 32003 train shape: (67344, 48, 48, 1)\n",
            "iteration: 32004 train shape: (67345, 48, 48, 1)\n",
            "iteration: 32005 train shape: (67346, 48, 48, 1)\n",
            "iteration: 32006 train shape: (67347, 48, 48, 1)\n",
            "iteration: 32007 train shape: (67348, 48, 48, 1)\n",
            "iteration: 32008 train shape: (67349, 48, 48, 1)\n",
            "iteration: 32009 train shape: (67350, 48, 48, 1)\n",
            "iteration: 32010 train shape: (67351, 48, 48, 1)\n",
            "iteration: 32011 train shape: (67352, 48, 48, 1)\n",
            "iteration: 32012 train shape: (67353, 48, 48, 1)\n",
            "iteration: 32013 train shape: (67354, 48, 48, 1)\n",
            "iteration: 32014 train shape: (67355, 48, 48, 1)\n",
            "iteration: 32015 train shape: (67356, 48, 48, 1)\n",
            "iteration: 32016 train shape: (67357, 48, 48, 1)\n",
            "iteration: 32017 train shape: (67358, 48, 48, 1)\n",
            "iteration: 32018 train shape: (67359, 48, 48, 1)\n",
            "iteration: 32019 train shape: (67360, 48, 48, 1)\n",
            "iteration: 32020 train shape: (67361, 48, 48, 1)\n",
            "iteration: 32021 train shape: (67362, 48, 48, 1)\n",
            "iteration: 32022 train shape: (67363, 48, 48, 1)\n",
            "iteration: 32023 train shape: (67364, 48, 48, 1)\n",
            "iteration: 32024 train shape: (67365, 48, 48, 1)\n",
            "iteration: 32025 train shape: (67366, 48, 48, 1)\n",
            "iteration: 32026 train shape: (67367, 48, 48, 1)\n",
            "iteration: 32027 train shape: (67368, 48, 48, 1)\n",
            "iteration: 32028 train shape: (67369, 48, 48, 1)\n",
            "iteration: 32029 train shape: (67370, 48, 48, 1)\n",
            "iteration: 32030 train shape: (67371, 48, 48, 1)\n",
            "iteration: 32031 train shape: (67372, 48, 48, 1)\n",
            "iteration: 32032 train shape: (67373, 48, 48, 1)\n",
            "iteration: 32033 train shape: (67374, 48, 48, 1)\n",
            "iteration: 32034 train shape: (67375, 48, 48, 1)\n",
            "iteration: 32035 train shape: (67376, 48, 48, 1)\n",
            "iteration: 32036 train shape: (67377, 48, 48, 1)\n",
            "iteration: 32037 train shape: (67378, 48, 48, 1)\n",
            "iteration: 32038 train shape: (67379, 48, 48, 1)\n",
            "iteration: 32039 train shape: (67380, 48, 48, 1)\n",
            "iteration: 32040 train shape: (67381, 48, 48, 1)\n",
            "iteration: 32041 train shape: (67382, 48, 48, 1)\n",
            "iteration: 32042 train shape: (67383, 48, 48, 1)\n",
            "iteration: 32043 train shape: (67384, 48, 48, 1)\n",
            "iteration: 32044 train shape: (67385, 48, 48, 1)\n",
            "iteration: 32045 train shape: (67386, 48, 48, 1)\n",
            "iteration: 32046 train shape: (67387, 48, 48, 1)\n",
            "iteration: 32047 train shape: (67388, 48, 48, 1)\n",
            "iteration: 32048 train shape: (67389, 48, 48, 1)\n",
            "iteration: 32049 train shape: (67390, 48, 48, 1)\n",
            "iteration: 32050 train shape: (67391, 48, 48, 1)\n",
            "iteration: 32051 train shape: (67392, 48, 48, 1)\n",
            "iteration: 32052 train shape: (67393, 48, 48, 1)\n",
            "iteration: 32053 train shape: (67394, 48, 48, 1)\n",
            "iteration: 32054 train shape: (67395, 48, 48, 1)\n",
            "iteration: 32055 train shape: (67396, 48, 48, 1)\n",
            "iteration: 32056 train shape: (67397, 48, 48, 1)\n",
            "iteration: 32057 train shape: (67398, 48, 48, 1)\n",
            "iteration: 32058 train shape: (67399, 48, 48, 1)\n",
            "iteration: 32059 train shape: (67400, 48, 48, 1)\n",
            "iteration: 32060 train shape: (67401, 48, 48, 1)\n",
            "iteration: 32061 train shape: (67402, 48, 48, 1)\n",
            "iteration: 32062 train shape: (67403, 48, 48, 1)\n",
            "iteration: 32063 train shape: (67404, 48, 48, 1)\n",
            "iteration: 32064 train shape: (67405, 48, 48, 1)\n",
            "iteration: 32065 train shape: (67406, 48, 48, 1)\n",
            "iteration: 32066 train shape: (67407, 48, 48, 1)\n",
            "iteration: 32067 train shape: (67408, 48, 48, 1)\n",
            "iteration: 32068 train shape: (67409, 48, 48, 1)\n",
            "iteration: 32069 train shape: (67410, 48, 48, 1)\n",
            "iteration: 32070 train shape: (67411, 48, 48, 1)\n",
            "iteration: 32071 train shape: (67412, 48, 48, 1)\n",
            "iteration: 32072 train shape: (67413, 48, 48, 1)\n",
            "iteration: 32073 train shape: (67414, 48, 48, 1)\n",
            "iteration: 32074 train shape: (67415, 48, 48, 1)\n",
            "iteration: 32075 train shape: (67416, 48, 48, 1)\n",
            "iteration: 32076 train shape: (67417, 48, 48, 1)\n",
            "iteration: 32077 train shape: (67418, 48, 48, 1)\n",
            "iteration: 32078 train shape: (67419, 48, 48, 1)\n",
            "iteration: 32079 train shape: (67420, 48, 48, 1)\n",
            "iteration: 32080 train shape: (67421, 48, 48, 1)\n",
            "iteration: 32081 train shape: (67422, 48, 48, 1)\n",
            "iteration: 32082 train shape: (67423, 48, 48, 1)\n",
            "iteration: 32083 train shape: (67424, 48, 48, 1)\n",
            "iteration: 32084 train shape: (67425, 48, 48, 1)\n",
            "iteration: 32085 train shape: (67426, 48, 48, 1)\n",
            "iteration: 32086 train shape: (67427, 48, 48, 1)\n",
            "iteration: 32087 train shape: (67428, 48, 48, 1)\n",
            "iteration: 32088 train shape: (67429, 48, 48, 1)\n",
            "iteration: 32089 train shape: (67430, 48, 48, 1)\n",
            "iteration: 32090 train shape: (67431, 48, 48, 1)\n",
            "iteration: 32091 train shape: (67432, 48, 48, 1)\n",
            "iteration: 32092 train shape: (67433, 48, 48, 1)\n",
            "iteration: 32093 train shape: (67434, 48, 48, 1)\n",
            "iteration: 32094 train shape: (67435, 48, 48, 1)\n",
            "iteration: 32095 train shape: (67436, 48, 48, 1)\n",
            "iteration: 32096 train shape: (67437, 48, 48, 1)\n",
            "iteration: 32097 train shape: (67438, 48, 48, 1)\n",
            "iteration: 32098 train shape: (67439, 48, 48, 1)\n",
            "iteration: 32099 train shape: (67440, 48, 48, 1)\n",
            "iteration: 32100 train shape: (67441, 48, 48, 1)\n",
            "iteration: 32101 train shape: (67442, 48, 48, 1)\n",
            "iteration: 32102 train shape: (67443, 48, 48, 1)\n",
            "iteration: 32103 train shape: (67444, 48, 48, 1)\n",
            "iteration: 32104 train shape: (67445, 48, 48, 1)\n",
            "iteration: 32105 train shape: (67446, 48, 48, 1)\n",
            "iteration: 32106 train shape: (67447, 48, 48, 1)\n",
            "iteration: 32107 train shape: (67448, 48, 48, 1)\n",
            "iteration: 32108 train shape: (67449, 48, 48, 1)\n",
            "iteration: 32109 train shape: (67450, 48, 48, 1)\n",
            "iteration: 32110 train shape: (67451, 48, 48, 1)\n",
            "iteration: 32111 train shape: (67452, 48, 48, 1)\n",
            "iteration: 32112 train shape: (67453, 48, 48, 1)\n",
            "iteration: 32113 train shape: (67454, 48, 48, 1)\n",
            "iteration: 32114 train shape: (67455, 48, 48, 1)\n",
            "iteration: 32115 train shape: (67456, 48, 48, 1)\n",
            "iteration: 32116 train shape: (67457, 48, 48, 1)\n",
            "iteration: 32117 train shape: (67458, 48, 48, 1)\n",
            "iteration: 32118 train shape: (67459, 48, 48, 1)\n",
            "iteration: 32119 train shape: (67460, 48, 48, 1)\n",
            "iteration: 32120 train shape: (67461, 48, 48, 1)\n",
            "iteration: 32121 train shape: (67462, 48, 48, 1)\n",
            "iteration: 32122 train shape: (67463, 48, 48, 1)\n",
            "iteration: 32123 train shape: (67464, 48, 48, 1)\n",
            "iteration: 32124 train shape: (67465, 48, 48, 1)\n",
            "iteration: 32125 train shape: (67466, 48, 48, 1)\n",
            "iteration: 32126 train shape: (67467, 48, 48, 1)\n",
            "iteration: 32127 train shape: (67468, 48, 48, 1)\n",
            "iteration: 32128 train shape: (67469, 48, 48, 1)\n",
            "iteration: 32129 train shape: (67470, 48, 48, 1)\n",
            "iteration: 32130 train shape: (67471, 48, 48, 1)\n",
            "iteration: 32131 train shape: (67472, 48, 48, 1)\n",
            "iteration: 32132 train shape: (67473, 48, 48, 1)\n",
            "iteration: 32133 train shape: (67474, 48, 48, 1)\n",
            "iteration: 32134 train shape: (67475, 48, 48, 1)\n",
            "iteration: 32135 train shape: (67476, 48, 48, 1)\n",
            "iteration: 32136 train shape: (67477, 48, 48, 1)\n",
            "iteration: 32137 train shape: (67478, 48, 48, 1)\n",
            "iteration: 32138 train shape: (67479, 48, 48, 1)\n",
            "iteration: 32139 train shape: (67480, 48, 48, 1)\n",
            "iteration: 32140 train shape: (67481, 48, 48, 1)\n",
            "iteration: 32141 train shape: (67482, 48, 48, 1)\n",
            "iteration: 32142 train shape: (67483, 48, 48, 1)\n",
            "iteration: 32143 train shape: (67484, 48, 48, 1)\n",
            "iteration: 32144 train shape: (67485, 48, 48, 1)\n",
            "iteration: 32145 train shape: (67486, 48, 48, 1)\n",
            "iteration: 32146 train shape: (67487, 48, 48, 1)\n",
            "iteration: 32147 train shape: (67488, 48, 48, 1)\n",
            "iteration: 32148 train shape: (67489, 48, 48, 1)\n",
            "iteration: 32149 train shape: (67490, 48, 48, 1)\n",
            "iteration: 32150 train shape: (67491, 48, 48, 1)\n",
            "iteration: 32151 train shape: (67492, 48, 48, 1)\n",
            "iteration: 32152 train shape: (67493, 48, 48, 1)\n",
            "iteration: 32153 train shape: (67494, 48, 48, 1)\n",
            "iteration: 32154 train shape: (67495, 48, 48, 1)\n",
            "iteration: 32155 train shape: (67496, 48, 48, 1)\n",
            "iteration: 32156 train shape: (67497, 48, 48, 1)\n",
            "iteration: 32157 train shape: (67498, 48, 48, 1)\n",
            "iteration: 32158 train shape: (67499, 48, 48, 1)\n",
            "iteration: 32159 train shape: (67500, 48, 48, 1)\n",
            "iteration: 32160 train shape: (67501, 48, 48, 1)\n",
            "iteration: 32161 train shape: (67502, 48, 48, 1)\n",
            "iteration: 32162 train shape: (67503, 48, 48, 1)\n",
            "iteration: 32163 train shape: (67504, 48, 48, 1)\n",
            "iteration: 32164 train shape: (67505, 48, 48, 1)\n",
            "iteration: 32165 train shape: (67506, 48, 48, 1)\n",
            "iteration: 32166 train shape: (67507, 48, 48, 1)\n",
            "iteration: 32167 train shape: (67508, 48, 48, 1)\n",
            "iteration: 32168 train shape: (67509, 48, 48, 1)\n",
            "iteration: 32169 train shape: (67510, 48, 48, 1)\n",
            "iteration: 32170 train shape: (67511, 48, 48, 1)\n",
            "iteration: 32171 train shape: (67512, 48, 48, 1)\n",
            "iteration: 32172 train shape: (67513, 48, 48, 1)\n",
            "iteration: 32173 train shape: (67514, 48, 48, 1)\n",
            "iteration: 32174 train shape: (67515, 48, 48, 1)\n",
            "iteration: 32175 train shape: (67516, 48, 48, 1)\n",
            "iteration: 32176 train shape: (67517, 48, 48, 1)\n",
            "iteration: 32177 train shape: (67518, 48, 48, 1)\n",
            "iteration: 32178 train shape: (67519, 48, 48, 1)\n",
            "iteration: 32179 train shape: (67520, 48, 48, 1)\n",
            "iteration: 32180 train shape: (67521, 48, 48, 1)\n",
            "iteration: 32181 train shape: (67522, 48, 48, 1)\n",
            "iteration: 32182 train shape: (67523, 48, 48, 1)\n",
            "iteration: 32183 train shape: (67524, 48, 48, 1)\n",
            "iteration: 32184 train shape: (67525, 48, 48, 1)\n",
            "iteration: 32185 train shape: (67526, 48, 48, 1)\n",
            "iteration: 32186 train shape: (67527, 48, 48, 1)\n",
            "iteration: 32187 train shape: (67528, 48, 48, 1)\n",
            "iteration: 32188 train shape: (67529, 48, 48, 1)\n",
            "iteration: 32189 train shape: (67530, 48, 48, 1)\n",
            "iteration: 32190 train shape: (67531, 48, 48, 1)\n",
            "iteration: 32191 train shape: (67532, 48, 48, 1)\n",
            "iteration: 32192 train shape: (67533, 48, 48, 1)\n",
            "iteration: 32193 train shape: (67534, 48, 48, 1)\n",
            "iteration: 32194 train shape: (67535, 48, 48, 1)\n",
            "iteration: 32195 train shape: (67536, 48, 48, 1)\n",
            "iteration: 32196 train shape: (67537, 48, 48, 1)\n",
            "iteration: 32197 train shape: (67538, 48, 48, 1)\n",
            "iteration: 32198 train shape: (67539, 48, 48, 1)\n",
            "iteration: 32199 train shape: (67540, 48, 48, 1)\n",
            "iteration: 32200 train shape: (67541, 48, 48, 1)\n",
            "iteration: 32201 train shape: (67542, 48, 48, 1)\n",
            "iteration: 32202 train shape: (67543, 48, 48, 1)\n",
            "iteration: 32203 train shape: (67544, 48, 48, 1)\n",
            "iteration: 32204 train shape: (67545, 48, 48, 1)\n",
            "iteration: 32205 train shape: (67546, 48, 48, 1)\n",
            "iteration: 32206 train shape: (67547, 48, 48, 1)\n",
            "iteration: 32207 train shape: (67548, 48, 48, 1)\n",
            "iteration: 32208 train shape: (67549, 48, 48, 1)\n",
            "iteration: 32209 train shape: (67550, 48, 48, 1)\n",
            "iteration: 32210 train shape: (67551, 48, 48, 1)\n",
            "iteration: 32211 train shape: (67552, 48, 48, 1)\n",
            "iteration: 32212 train shape: (67553, 48, 48, 1)\n",
            "iteration: 32213 train shape: (67554, 48, 48, 1)\n",
            "iteration: 32214 train shape: (67555, 48, 48, 1)\n",
            "iteration: 32215 train shape: (67556, 48, 48, 1)\n",
            "iteration: 32216 train shape: (67557, 48, 48, 1)\n",
            "iteration: 32217 train shape: (67558, 48, 48, 1)\n",
            "iteration: 32218 train shape: (67559, 48, 48, 1)\n",
            "iteration: 32219 train shape: (67560, 48, 48, 1)\n",
            "iteration: 32220 train shape: (67561, 48, 48, 1)\n",
            "iteration: 32221 train shape: (67562, 48, 48, 1)\n",
            "iteration: 32222 train shape: (67563, 48, 48, 1)\n",
            "iteration: 32223 train shape: (67564, 48, 48, 1)\n",
            "iteration: 32224 train shape: (67565, 48, 48, 1)\n",
            "iteration: 32225 train shape: (67566, 48, 48, 1)\n",
            "iteration: 32226 train shape: (67567, 48, 48, 1)\n",
            "iteration: 32227 train shape: (67568, 48, 48, 1)\n",
            "iteration: 32228 train shape: (67569, 48, 48, 1)\n",
            "iteration: 32229 train shape: (67570, 48, 48, 1)\n",
            "iteration: 32230 train shape: (67571, 48, 48, 1)\n",
            "iteration: 32231 train shape: (67572, 48, 48, 1)\n",
            "iteration: 32232 train shape: (67573, 48, 48, 1)\n",
            "iteration: 32233 train shape: (67574, 48, 48, 1)\n",
            "iteration: 32234 train shape: (67575, 48, 48, 1)\n",
            "iteration: 32235 train shape: (67576, 48, 48, 1)\n",
            "iteration: 32236 train shape: (67577, 48, 48, 1)\n",
            "iteration: 32237 train shape: (67578, 48, 48, 1)\n",
            "iteration: 32238 train shape: (67579, 48, 48, 1)\n",
            "iteration: 32239 train shape: (67580, 48, 48, 1)\n",
            "iteration: 32240 train shape: (67581, 48, 48, 1)\n",
            "iteration: 32241 train shape: (67582, 48, 48, 1)\n",
            "iteration: 32242 train shape: (67583, 48, 48, 1)\n",
            "iteration: 32243 train shape: (67584, 48, 48, 1)\n",
            "iteration: 32244 train shape: (67585, 48, 48, 1)\n",
            "iteration: 32245 train shape: (67586, 48, 48, 1)\n",
            "iteration: 32246 train shape: (67587, 48, 48, 1)\n",
            "iteration: 32247 train shape: (67588, 48, 48, 1)\n",
            "iteration: 32248 train shape: (67589, 48, 48, 1)\n",
            "iteration: 32249 train shape: (67590, 48, 48, 1)\n",
            "iteration: 32250 train shape: (67591, 48, 48, 1)\n",
            "iteration: 32251 train shape: (67592, 48, 48, 1)\n",
            "iteration: 32252 train shape: (67593, 48, 48, 1)\n",
            "iteration: 32253 train shape: (67594, 48, 48, 1)\n",
            "iteration: 32254 train shape: (67595, 48, 48, 1)\n",
            "iteration: 32255 train shape: (67596, 48, 48, 1)\n",
            "iteration: 32256 train shape: (67597, 48, 48, 1)\n",
            "iteration: 32257 train shape: (67598, 48, 48, 1)\n",
            "iteration: 32258 train shape: (67599, 48, 48, 1)\n",
            "iteration: 32259 train shape: (67600, 48, 48, 1)\n",
            "iteration: 32260 train shape: (67601, 48, 48, 1)\n",
            "iteration: 32261 train shape: (67602, 48, 48, 1)\n",
            "iteration: 32262 train shape: (67603, 48, 48, 1)\n",
            "iteration: 32263 train shape: (67604, 48, 48, 1)\n",
            "iteration: 32264 train shape: (67605, 48, 48, 1)\n",
            "iteration: 32265 train shape: (67606, 48, 48, 1)\n",
            "iteration: 32266 train shape: (67607, 48, 48, 1)\n",
            "iteration: 32267 train shape: (67608, 48, 48, 1)\n",
            "iteration: 32268 train shape: (67609, 48, 48, 1)\n",
            "iteration: 32269 train shape: (67610, 48, 48, 1)\n",
            "iteration: 32270 train shape: (67611, 48, 48, 1)\n",
            "iteration: 32271 train shape: (67612, 48, 48, 1)\n",
            "iteration: 32272 train shape: (67613, 48, 48, 1)\n",
            "iteration: 32273 train shape: (67614, 48, 48, 1)\n",
            "iteration: 32274 train shape: (67615, 48, 48, 1)\n",
            "iteration: 32275 train shape: (67616, 48, 48, 1)\n",
            "iteration: 32276 train shape: (67617, 48, 48, 1)\n",
            "iteration: 32277 train shape: (67618, 48, 48, 1)\n",
            "iteration: 32278 train shape: (67619, 48, 48, 1)\n",
            "iteration: 32279 train shape: (67620, 48, 48, 1)\n",
            "iteration: 32280 train shape: (67621, 48, 48, 1)\n",
            "iteration: 32281 train shape: (67622, 48, 48, 1)\n",
            "iteration: 32282 train shape: (67623, 48, 48, 1)\n",
            "iteration: 32283 train shape: (67624, 48, 48, 1)\n",
            "iteration: 32284 train shape: (67625, 48, 48, 1)\n",
            "iteration: 32285 train shape: (67626, 48, 48, 1)\n",
            "iteration: 32286 train shape: (67627, 48, 48, 1)\n",
            "iteration: 32287 train shape: (67628, 48, 48, 1)\n",
            "iteration: 32288 train shape: (67629, 48, 48, 1)\n",
            "iteration: 32289 train shape: (67630, 48, 48, 1)\n",
            "iteration: 32290 train shape: (67631, 48, 48, 1)\n",
            "iteration: 32291 train shape: (67632, 48, 48, 1)\n",
            "iteration: 32292 train shape: (67633, 48, 48, 1)\n",
            "iteration: 32293 train shape: (67634, 48, 48, 1)\n",
            "iteration: 32294 train shape: (67635, 48, 48, 1)\n",
            "iteration: 32295 train shape: (67636, 48, 48, 1)\n",
            "iteration: 32296 train shape: (67637, 48, 48, 1)\n",
            "iteration: 32297 train shape: (67638, 48, 48, 1)\n",
            "iteration: 32298 train shape: (67639, 48, 48, 1)\n",
            "iteration: 32299 train shape: (67640, 48, 48, 1)\n",
            "iteration: 32300 train shape: (67641, 48, 48, 1)\n",
            "iteration: 32301 train shape: (67642, 48, 48, 1)\n",
            "iteration: 32302 train shape: (67643, 48, 48, 1)\n",
            "iteration: 32303 train shape: (67644, 48, 48, 1)\n",
            "iteration: 32304 train shape: (67645, 48, 48, 1)\n",
            "iteration: 32305 train shape: (67646, 48, 48, 1)\n",
            "iteration: 32306 train shape: (67647, 48, 48, 1)\n",
            "iteration: 32307 train shape: (67648, 48, 48, 1)\n",
            "iteration: 32308 train shape: (67649, 48, 48, 1)\n",
            "iteration: 32309 train shape: (67650, 48, 48, 1)\n",
            "iteration: 32310 train shape: (67651, 48, 48, 1)\n",
            "iteration: 32311 train shape: (67652, 48, 48, 1)\n",
            "iteration: 32312 train shape: (67653, 48, 48, 1)\n",
            "iteration: 32313 train shape: (67654, 48, 48, 1)\n",
            "iteration: 32314 train shape: (67655, 48, 48, 1)\n",
            "iteration: 32315 train shape: (67656, 48, 48, 1)\n",
            "iteration: 32316 train shape: (67657, 48, 48, 1)\n",
            "iteration: 32317 train shape: (67658, 48, 48, 1)\n",
            "iteration: 32318 train shape: (67659, 48, 48, 1)\n",
            "iteration: 32319 train shape: (67660, 48, 48, 1)\n",
            "iteration: 32320 train shape: (67661, 48, 48, 1)\n",
            "iteration: 32321 train shape: (67662, 48, 48, 1)\n",
            "iteration: 32322 train shape: (67663, 48, 48, 1)\n",
            "iteration: 32323 train shape: (67664, 48, 48, 1)\n",
            "iteration: 32324 train shape: (67665, 48, 48, 1)\n",
            "iteration: 32325 train shape: (67666, 48, 48, 1)\n",
            "iteration: 32326 train shape: (67667, 48, 48, 1)\n",
            "iteration: 32327 train shape: (67668, 48, 48, 1)\n",
            "iteration: 32328 train shape: (67669, 48, 48, 1)\n",
            "iteration: 32329 train shape: (67670, 48, 48, 1)\n",
            "iteration: 32330 train shape: (67671, 48, 48, 1)\n",
            "iteration: 32331 train shape: (67672, 48, 48, 1)\n",
            "iteration: 32332 train shape: (67673, 48, 48, 1)\n",
            "iteration: 32333 train shape: (67674, 48, 48, 1)\n",
            "iteration: 32334 train shape: (67675, 48, 48, 1)\n",
            "iteration: 32335 train shape: (67676, 48, 48, 1)\n",
            "iteration: 32336 train shape: (67677, 48, 48, 1)\n",
            "iteration: 32337 train shape: (67678, 48, 48, 1)\n",
            "iteration: 32338 train shape: (67679, 48, 48, 1)\n",
            "iteration: 32339 train shape: (67680, 48, 48, 1)\n",
            "iteration: 32340 train shape: (67681, 48, 48, 1)\n",
            "iteration: 32341 train shape: (67682, 48, 48, 1)\n",
            "iteration: 32342 train shape: (67683, 48, 48, 1)\n",
            "iteration: 32343 train shape: (67684, 48, 48, 1)\n",
            "iteration: 32344 train shape: (67685, 48, 48, 1)\n",
            "iteration: 32345 train shape: (67686, 48, 48, 1)\n",
            "iteration: 32346 train shape: (67687, 48, 48, 1)\n",
            "iteration: 32347 train shape: (67688, 48, 48, 1)\n",
            "iteration: 32348 train shape: (67689, 48, 48, 1)\n",
            "iteration: 32349 train shape: (67690, 48, 48, 1)\n",
            "iteration: 32350 train shape: (67691, 48, 48, 1)\n",
            "iteration: 32351 train shape: (67692, 48, 48, 1)\n",
            "iteration: 32352 train shape: (67693, 48, 48, 1)\n",
            "iteration: 32353 train shape: (67694, 48, 48, 1)\n",
            "iteration: 32354 train shape: (67695, 48, 48, 1)\n",
            "iteration: 32355 train shape: (67696, 48, 48, 1)\n",
            "iteration: 32356 train shape: (67697, 48, 48, 1)\n",
            "iteration: 32357 train shape: (67698, 48, 48, 1)\n",
            "iteration: 32358 train shape: (67699, 48, 48, 1)\n",
            "iteration: 32359 train shape: (67700, 48, 48, 1)\n",
            "iteration: 32360 train shape: (67701, 48, 48, 1)\n",
            "iteration: 32361 train shape: (67702, 48, 48, 1)\n",
            "iteration: 32362 train shape: (67703, 48, 48, 1)\n",
            "iteration: 32363 train shape: (67704, 48, 48, 1)\n",
            "iteration: 32364 train shape: (67705, 48, 48, 1)\n",
            "iteration: 32365 train shape: (67706, 48, 48, 1)\n",
            "iteration: 32366 train shape: (67707, 48, 48, 1)\n",
            "iteration: 32367 train shape: (67708, 48, 48, 1)\n",
            "iteration: 32368 train shape: (67709, 48, 48, 1)\n",
            "iteration: 32369 train shape: (67710, 48, 48, 1)\n",
            "iteration: 32370 train shape: (67711, 48, 48, 1)\n",
            "iteration: 32371 train shape: (67712, 48, 48, 1)\n",
            "iteration: 32372 train shape: (67713, 48, 48, 1)\n",
            "iteration: 32373 train shape: (67714, 48, 48, 1)\n",
            "iteration: 32374 train shape: (67715, 48, 48, 1)\n",
            "iteration: 32375 train shape: (67716, 48, 48, 1)\n",
            "iteration: 32376 train shape: (67717, 48, 48, 1)\n",
            "iteration: 32377 train shape: (67718, 48, 48, 1)\n",
            "iteration: 32378 train shape: (67719, 48, 48, 1)\n",
            "iteration: 32379 train shape: (67720, 48, 48, 1)\n",
            "iteration: 32380 train shape: (67721, 48, 48, 1)\n",
            "iteration: 32381 train shape: (67722, 48, 48, 1)\n",
            "iteration: 32382 train shape: (67723, 48, 48, 1)\n",
            "iteration: 32383 train shape: (67724, 48, 48, 1)\n",
            "iteration: 32384 train shape: (67725, 48, 48, 1)\n",
            "iteration: 32385 train shape: (67726, 48, 48, 1)\n",
            "iteration: 32386 train shape: (67727, 48, 48, 1)\n",
            "iteration: 32387 train shape: (67728, 48, 48, 1)\n",
            "iteration: 32388 train shape: (67729, 48, 48, 1)\n",
            "iteration: 32389 train shape: (67730, 48, 48, 1)\n",
            "iteration: 32390 train shape: (67731, 48, 48, 1)\n",
            "iteration: 32391 train shape: (67732, 48, 48, 1)\n",
            "iteration: 32392 train shape: (67733, 48, 48, 1)\n",
            "iteration: 32393 train shape: (67734, 48, 48, 1)\n",
            "iteration: 32394 train shape: (67735, 48, 48, 1)\n",
            "iteration: 32395 train shape: (67736, 48, 48, 1)\n",
            "iteration: 32396 train shape: (67737, 48, 48, 1)\n",
            "iteration: 32397 train shape: (67738, 48, 48, 1)\n",
            "iteration: 32398 train shape: (67739, 48, 48, 1)\n",
            "iteration: 32399 train shape: (67740, 48, 48, 1)\n",
            "iteration: 32400 train shape: (67741, 48, 48, 1)\n",
            "iteration: 32401 train shape: (67742, 48, 48, 1)\n",
            "iteration: 32402 train shape: (67743, 48, 48, 1)\n",
            "iteration: 32403 train shape: (67744, 48, 48, 1)\n",
            "iteration: 32404 train shape: (67745, 48, 48, 1)\n",
            "iteration: 32405 train shape: (67746, 48, 48, 1)\n",
            "iteration: 32406 train shape: (67747, 48, 48, 1)\n",
            "iteration: 32407 train shape: (67748, 48, 48, 1)\n",
            "iteration: 32408 train shape: (67749, 48, 48, 1)\n",
            "iteration: 32409 train shape: (67750, 48, 48, 1)\n",
            "iteration: 32410 train shape: (67751, 48, 48, 1)\n",
            "iteration: 32411 train shape: (67752, 48, 48, 1)\n",
            "iteration: 32412 train shape: (67753, 48, 48, 1)\n",
            "iteration: 32413 train shape: (67754, 48, 48, 1)\n",
            "iteration: 32414 train shape: (67755, 48, 48, 1)\n",
            "iteration: 32415 train shape: (67756, 48, 48, 1)\n",
            "iteration: 32416 train shape: (67757, 48, 48, 1)\n",
            "iteration: 32417 train shape: (67758, 48, 48, 1)\n",
            "iteration: 32418 train shape: (67759, 48, 48, 1)\n",
            "iteration: 32419 train shape: (67760, 48, 48, 1)\n",
            "iteration: 32420 train shape: (67761, 48, 48, 1)\n",
            "iteration: 32421 train shape: (67762, 48, 48, 1)\n",
            "iteration: 32422 train shape: (67763, 48, 48, 1)\n",
            "iteration: 32423 train shape: (67764, 48, 48, 1)\n",
            "iteration: 32424 train shape: (67765, 48, 48, 1)\n",
            "iteration: 32425 train shape: (67766, 48, 48, 1)\n",
            "iteration: 32426 train shape: (67767, 48, 48, 1)\n",
            "iteration: 32427 train shape: (67768, 48, 48, 1)\n",
            "iteration: 32428 train shape: (67769, 48, 48, 1)\n",
            "iteration: 32429 train shape: (67770, 48, 48, 1)\n",
            "iteration: 32430 train shape: (67771, 48, 48, 1)\n",
            "iteration: 32431 train shape: (67772, 48, 48, 1)\n",
            "iteration: 32432 train shape: (67773, 48, 48, 1)\n",
            "iteration: 32433 train shape: (67774, 48, 48, 1)\n",
            "iteration: 32434 train shape: (67775, 48, 48, 1)\n",
            "iteration: 32435 train shape: (67776, 48, 48, 1)\n",
            "iteration: 32436 train shape: (67777, 48, 48, 1)\n",
            "iteration: 32437 train shape: (67778, 48, 48, 1)\n",
            "iteration: 32438 train shape: (67779, 48, 48, 1)\n",
            "iteration: 32439 train shape: (67780, 48, 48, 1)\n",
            "iteration: 32440 train shape: (67781, 48, 48, 1)\n",
            "iteration: 32441 train shape: (67782, 48, 48, 1)\n",
            "iteration: 32442 train shape: (67783, 48, 48, 1)\n",
            "iteration: 32443 train shape: (67784, 48, 48, 1)\n",
            "iteration: 32444 train shape: (67785, 48, 48, 1)\n",
            "iteration: 32445 train shape: (67786, 48, 48, 1)\n",
            "iteration: 32446 train shape: (67787, 48, 48, 1)\n",
            "iteration: 32447 train shape: (67788, 48, 48, 1)\n",
            "iteration: 32448 train shape: (67789, 48, 48, 1)\n",
            "iteration: 32449 train shape: (67790, 48, 48, 1)\n",
            "iteration: 32450 train shape: (67791, 48, 48, 1)\n",
            "iteration: 32451 train shape: (67792, 48, 48, 1)\n",
            "iteration: 32452 train shape: (67793, 48, 48, 1)\n",
            "iteration: 32453 train shape: (67794, 48, 48, 1)\n",
            "iteration: 32454 train shape: (67795, 48, 48, 1)\n",
            "iteration: 32455 train shape: (67796, 48, 48, 1)\n",
            "iteration: 32456 train shape: (67797, 48, 48, 1)\n",
            "iteration: 32457 train shape: (67798, 48, 48, 1)\n",
            "iteration: 32458 train shape: (67799, 48, 48, 1)\n",
            "iteration: 32459 train shape: (67800, 48, 48, 1)\n",
            "iteration: 32460 train shape: (67801, 48, 48, 1)\n",
            "iteration: 32461 train shape: (67802, 48, 48, 1)\n",
            "iteration: 32462 train shape: (67803, 48, 48, 1)\n",
            "iteration: 32463 train shape: (67804, 48, 48, 1)\n",
            "iteration: 32464 train shape: (67805, 48, 48, 1)\n",
            "iteration: 32465 train shape: (67806, 48, 48, 1)\n",
            "iteration: 32466 train shape: (67807, 48, 48, 1)\n",
            "iteration: 32467 train shape: (67808, 48, 48, 1)\n",
            "iteration: 32468 train shape: (67809, 48, 48, 1)\n",
            "iteration: 32469 train shape: (67810, 48, 48, 1)\n",
            "iteration: 32470 train shape: (67811, 48, 48, 1)\n",
            "iteration: 32471 train shape: (67812, 48, 48, 1)\n",
            "iteration: 32472 train shape: (67813, 48, 48, 1)\n",
            "iteration: 32473 train shape: (67814, 48, 48, 1)\n",
            "iteration: 32474 train shape: (67815, 48, 48, 1)\n",
            "iteration: 32475 train shape: (67816, 48, 48, 1)\n",
            "iteration: 32476 train shape: (67817, 48, 48, 1)\n",
            "iteration: 32477 train shape: (67818, 48, 48, 1)\n",
            "iteration: 32478 train shape: (67819, 48, 48, 1)\n",
            "iteration: 32479 train shape: (67820, 48, 48, 1)\n",
            "iteration: 32480 train shape: (67821, 48, 48, 1)\n",
            "iteration: 32481 train shape: (67822, 48, 48, 1)\n",
            "iteration: 32482 train shape: (67823, 48, 48, 1)\n",
            "iteration: 32483 train shape: (67824, 48, 48, 1)\n",
            "iteration: 32484 train shape: (67825, 48, 48, 1)\n",
            "iteration: 32485 train shape: (67826, 48, 48, 1)\n",
            "iteration: 32486 train shape: (67827, 48, 48, 1)\n",
            "iteration: 32487 train shape: (67828, 48, 48, 1)\n",
            "iteration: 32488 train shape: (67829, 48, 48, 1)\n",
            "iteration: 32489 train shape: (67830, 48, 48, 1)\n",
            "iteration: 32490 train shape: (67831, 48, 48, 1)\n",
            "iteration: 32491 train shape: (67832, 48, 48, 1)\n",
            "iteration: 32492 train shape: (67833, 48, 48, 1)\n",
            "iteration: 32493 train shape: (67834, 48, 48, 1)\n",
            "iteration: 32494 train shape: (67835, 48, 48, 1)\n",
            "iteration: 32495 train shape: (67836, 48, 48, 1)\n",
            "iteration: 32496 train shape: (67837, 48, 48, 1)\n",
            "iteration: 32497 train shape: (67838, 48, 48, 1)\n",
            "iteration: 32498 train shape: (67839, 48, 48, 1)\n",
            "iteration: 32499 train shape: (67840, 48, 48, 1)\n",
            "iteration: 32500 train shape: (67841, 48, 48, 1)\n",
            "iteration: 32501 train shape: (67842, 48, 48, 1)\n",
            "iteration: 32502 train shape: (67843, 48, 48, 1)\n",
            "iteration: 32503 train shape: (67844, 48, 48, 1)\n",
            "iteration: 32504 train shape: (67845, 48, 48, 1)\n",
            "iteration: 32505 train shape: (67846, 48, 48, 1)\n",
            "iteration: 32506 train shape: (67847, 48, 48, 1)\n",
            "iteration: 32507 train shape: (67848, 48, 48, 1)\n",
            "iteration: 32508 train shape: (67849, 48, 48, 1)\n",
            "iteration: 32509 train shape: (67850, 48, 48, 1)\n",
            "iteration: 32510 train shape: (67851, 48, 48, 1)\n",
            "iteration: 32511 train shape: (67852, 48, 48, 1)\n",
            "iteration: 32512 train shape: (67853, 48, 48, 1)\n",
            "iteration: 32513 train shape: (67854, 48, 48, 1)\n",
            "iteration: 32514 train shape: (67855, 48, 48, 1)\n",
            "iteration: 32515 train shape: (67856, 48, 48, 1)\n",
            "iteration: 32516 train shape: (67857, 48, 48, 1)\n",
            "iteration: 32517 train shape: (67858, 48, 48, 1)\n",
            "iteration: 32518 train shape: (67859, 48, 48, 1)\n",
            "iteration: 32519 train shape: (67860, 48, 48, 1)\n",
            "iteration: 32520 train shape: (67861, 48, 48, 1)\n",
            "iteration: 32521 train shape: (67862, 48, 48, 1)\n",
            "iteration: 32522 train shape: (67863, 48, 48, 1)\n",
            "iteration: 32523 train shape: (67864, 48, 48, 1)\n",
            "iteration: 32524 train shape: (67865, 48, 48, 1)\n",
            "iteration: 32525 train shape: (67866, 48, 48, 1)\n",
            "iteration: 32526 train shape: (67867, 48, 48, 1)\n",
            "iteration: 32527 train shape: (67868, 48, 48, 1)\n",
            "iteration: 32528 train shape: (67869, 48, 48, 1)\n",
            "iteration: 32529 train shape: (67870, 48, 48, 1)\n",
            "iteration: 32530 train shape: (67871, 48, 48, 1)\n",
            "iteration: 32531 train shape: (67872, 48, 48, 1)\n",
            "iteration: 32532 train shape: (67873, 48, 48, 1)\n",
            "iteration: 32533 train shape: (67874, 48, 48, 1)\n",
            "iteration: 32534 train shape: (67875, 48, 48, 1)\n",
            "iteration: 32535 train shape: (67876, 48, 48, 1)\n",
            "iteration: 32536 train shape: (67877, 48, 48, 1)\n",
            "iteration: 32537 train shape: (67878, 48, 48, 1)\n",
            "iteration: 32538 train shape: (67879, 48, 48, 1)\n",
            "iteration: 32539 train shape: (67880, 48, 48, 1)\n",
            "iteration: 32540 train shape: (67881, 48, 48, 1)\n",
            "iteration: 32541 train shape: (67882, 48, 48, 1)\n",
            "iteration: 32542 train shape: (67883, 48, 48, 1)\n",
            "iteration: 32543 train shape: (67884, 48, 48, 1)\n",
            "iteration: 32544 train shape: (67885, 48, 48, 1)\n",
            "iteration: 32545 train shape: (67886, 48, 48, 1)\n",
            "iteration: 32546 train shape: (67887, 48, 48, 1)\n",
            "iteration: 32547 train shape: (67888, 48, 48, 1)\n",
            "iteration: 32548 train shape: (67889, 48, 48, 1)\n",
            "iteration: 32549 train shape: (67890, 48, 48, 1)\n",
            "iteration: 32550 train shape: (67891, 48, 48, 1)\n",
            "iteration: 32551 train shape: (67892, 48, 48, 1)\n",
            "iteration: 32552 train shape: (67893, 48, 48, 1)\n",
            "iteration: 32553 train shape: (67894, 48, 48, 1)\n",
            "iteration: 32554 train shape: (67895, 48, 48, 1)\n",
            "iteration: 32555 train shape: (67896, 48, 48, 1)\n",
            "iteration: 32556 train shape: (67897, 48, 48, 1)\n",
            "iteration: 32557 train shape: (67898, 48, 48, 1)\n",
            "iteration: 32558 train shape: (67899, 48, 48, 1)\n",
            "iteration: 32559 train shape: (67900, 48, 48, 1)\n",
            "iteration: 32560 train shape: (67901, 48, 48, 1)\n",
            "iteration: 32561 train shape: (67902, 48, 48, 1)\n",
            "iteration: 32562 train shape: (67903, 48, 48, 1)\n",
            "iteration: 32563 train shape: (67904, 48, 48, 1)\n",
            "iteration: 32564 train shape: (67905, 48, 48, 1)\n",
            "iteration: 32565 train shape: (67906, 48, 48, 1)\n",
            "iteration: 32566 train shape: (67907, 48, 48, 1)\n",
            "iteration: 32567 train shape: (67908, 48, 48, 1)\n",
            "iteration: 32568 train shape: (67909, 48, 48, 1)\n",
            "iteration: 32569 train shape: (67910, 48, 48, 1)\n",
            "iteration: 32570 train shape: (67911, 48, 48, 1)\n",
            "iteration: 32571 train shape: (67912, 48, 48, 1)\n",
            "iteration: 32572 train shape: (67913, 48, 48, 1)\n",
            "iteration: 32573 train shape: (67914, 48, 48, 1)\n",
            "iteration: 32574 train shape: (67915, 48, 48, 1)\n",
            "iteration: 32575 train shape: (67916, 48, 48, 1)\n",
            "iteration: 32576 train shape: (67917, 48, 48, 1)\n",
            "iteration: 32577 train shape: (67918, 48, 48, 1)\n",
            "iteration: 32578 train shape: (67919, 48, 48, 1)\n",
            "iteration: 32579 train shape: (67920, 48, 48, 1)\n",
            "iteration: 32580 train shape: (67921, 48, 48, 1)\n",
            "iteration: 32581 train shape: (67922, 48, 48, 1)\n",
            "iteration: 32582 train shape: (67923, 48, 48, 1)\n",
            "iteration: 32583 train shape: (67924, 48, 48, 1)\n",
            "iteration: 32584 train shape: (67925, 48, 48, 1)\n",
            "iteration: 32585 train shape: (67926, 48, 48, 1)\n",
            "iteration: 32586 train shape: (67927, 48, 48, 1)\n",
            "iteration: 32587 train shape: (67928, 48, 48, 1)\n",
            "iteration: 32588 train shape: (67929, 48, 48, 1)\n",
            "iteration: 32589 train shape: (67930, 48, 48, 1)\n",
            "iteration: 32590 train shape: (67931, 48, 48, 1)\n",
            "iteration: 32591 train shape: (67932, 48, 48, 1)\n",
            "iteration: 32592 train shape: (67933, 48, 48, 1)\n",
            "iteration: 32593 train shape: (67934, 48, 48, 1)\n",
            "iteration: 32594 train shape: (67935, 48, 48, 1)\n",
            "iteration: 32595 train shape: (67936, 48, 48, 1)\n",
            "iteration: 32596 train shape: (67937, 48, 48, 1)\n",
            "iteration: 32597 train shape: (67938, 48, 48, 1)\n",
            "iteration: 32598 train shape: (67939, 48, 48, 1)\n",
            "iteration: 32599 train shape: (67940, 48, 48, 1)\n",
            "iteration: 32600 train shape: (67941, 48, 48, 1)\n",
            "iteration: 32601 train shape: (67942, 48, 48, 1)\n",
            "iteration: 32602 train shape: (67943, 48, 48, 1)\n",
            "iteration: 32603 train shape: (67944, 48, 48, 1)\n",
            "iteration: 32604 train shape: (67945, 48, 48, 1)\n",
            "iteration: 32605 train shape: (67946, 48, 48, 1)\n",
            "iteration: 32606 train shape: (67947, 48, 48, 1)\n",
            "iteration: 32607 train shape: (67948, 48, 48, 1)\n",
            "iteration: 32608 train shape: (67949, 48, 48, 1)\n",
            "iteration: 32609 train shape: (67950, 48, 48, 1)\n",
            "iteration: 32610 train shape: (67951, 48, 48, 1)\n",
            "iteration: 32611 train shape: (67952, 48, 48, 1)\n",
            "iteration: 32612 train shape: (67953, 48, 48, 1)\n",
            "iteration: 32613 train shape: (67954, 48, 48, 1)\n",
            "iteration: 32614 train shape: (67955, 48, 48, 1)\n",
            "iteration: 32615 train shape: (67956, 48, 48, 1)\n",
            "iteration: 32616 train shape: (67957, 48, 48, 1)\n",
            "iteration: 32617 train shape: (67958, 48, 48, 1)\n",
            "iteration: 32618 train shape: (67959, 48, 48, 1)\n",
            "iteration: 32619 train shape: (67960, 48, 48, 1)\n",
            "iteration: 32620 train shape: (67961, 48, 48, 1)\n",
            "iteration: 32621 train shape: (67962, 48, 48, 1)\n",
            "iteration: 32622 train shape: (67963, 48, 48, 1)\n",
            "iteration: 32623 train shape: (67964, 48, 48, 1)\n",
            "iteration: 32624 train shape: (67965, 48, 48, 1)\n",
            "iteration: 32625 train shape: (67966, 48, 48, 1)\n",
            "iteration: 32626 train shape: (67967, 48, 48, 1)\n",
            "iteration: 32627 train shape: (67968, 48, 48, 1)\n",
            "iteration: 32628 train shape: (67969, 48, 48, 1)\n",
            "iteration: 32629 train shape: (67970, 48, 48, 1)\n",
            "iteration: 32630 train shape: (67971, 48, 48, 1)\n",
            "iteration: 32631 train shape: (67972, 48, 48, 1)\n",
            "iteration: 32632 train shape: (67973, 48, 48, 1)\n",
            "iteration: 32633 train shape: (67974, 48, 48, 1)\n",
            "iteration: 32634 train shape: (67975, 48, 48, 1)\n",
            "iteration: 32635 train shape: (67976, 48, 48, 1)\n",
            "iteration: 32636 train shape: (67977, 48, 48, 1)\n",
            "iteration: 32637 train shape: (67978, 48, 48, 1)\n",
            "iteration: 32638 train shape: (67979, 48, 48, 1)\n",
            "iteration: 32639 train shape: (67980, 48, 48, 1)\n",
            "iteration: 32640 train shape: (67981, 48, 48, 1)\n",
            "iteration: 32641 train shape: (67982, 48, 48, 1)\n",
            "iteration: 32642 train shape: (67983, 48, 48, 1)\n",
            "iteration: 32643 train shape: (67984, 48, 48, 1)\n",
            "iteration: 32644 train shape: (67985, 48, 48, 1)\n",
            "iteration: 32645 train shape: (67986, 48, 48, 1)\n",
            "iteration: 32646 train shape: (67987, 48, 48, 1)\n",
            "iteration: 32647 train shape: (67988, 48, 48, 1)\n",
            "iteration: 32648 train shape: (67989, 48, 48, 1)\n",
            "iteration: 32649 train shape: (67990, 48, 48, 1)\n",
            "iteration: 32650 train shape: (67991, 48, 48, 1)\n",
            "iteration: 32651 train shape: (67992, 48, 48, 1)\n",
            "iteration: 32652 train shape: (67993, 48, 48, 1)\n",
            "iteration: 32653 train shape: (67994, 48, 48, 1)\n",
            "iteration: 32654 train shape: (67995, 48, 48, 1)\n",
            "iteration: 32655 train shape: (67996, 48, 48, 1)\n",
            "iteration: 32656 train shape: (67997, 48, 48, 1)\n",
            "iteration: 32657 train shape: (67998, 48, 48, 1)\n",
            "iteration: 32658 train shape: (67999, 48, 48, 1)\n",
            "iteration: 32659 train shape: (68000, 48, 48, 1)\n",
            "iteration: 32660 train shape: (68001, 48, 48, 1)\n",
            "iteration: 32661 train shape: (68002, 48, 48, 1)\n",
            "iteration: 32662 train shape: (68003, 48, 48, 1)\n",
            "iteration: 32663 train shape: (68004, 48, 48, 1)\n",
            "iteration: 32664 train shape: (68005, 48, 48, 1)\n",
            "iteration: 32665 train shape: (68006, 48, 48, 1)\n",
            "iteration: 32666 train shape: (68007, 48, 48, 1)\n",
            "iteration: 32667 train shape: (68008, 48, 48, 1)\n",
            "iteration: 32668 train shape: (68009, 48, 48, 1)\n",
            "iteration: 32669 train shape: (68010, 48, 48, 1)\n",
            "iteration: 32670 train shape: (68011, 48, 48, 1)\n",
            "iteration: 32671 train shape: (68012, 48, 48, 1)\n",
            "iteration: 32672 train shape: (68013, 48, 48, 1)\n",
            "iteration: 32673 train shape: (68014, 48, 48, 1)\n",
            "iteration: 32674 train shape: (68015, 48, 48, 1)\n",
            "iteration: 32675 train shape: (68016, 48, 48, 1)\n",
            "iteration: 32676 train shape: (68017, 48, 48, 1)\n",
            "iteration: 32677 train shape: (68018, 48, 48, 1)\n",
            "iteration: 32678 train shape: (68019, 48, 48, 1)\n",
            "iteration: 32679 train shape: (68020, 48, 48, 1)\n",
            "iteration: 32680 train shape: (68021, 48, 48, 1)\n",
            "iteration: 32681 train shape: (68022, 48, 48, 1)\n",
            "iteration: 32682 train shape: (68023, 48, 48, 1)\n",
            "iteration: 32683 train shape: (68024, 48, 48, 1)\n",
            "iteration: 32684 train shape: (68025, 48, 48, 1)\n",
            "iteration: 32685 train shape: (68026, 48, 48, 1)\n",
            "iteration: 32686 train shape: (68027, 48, 48, 1)\n",
            "iteration: 32687 train shape: (68028, 48, 48, 1)\n",
            "iteration: 32688 train shape: (68029, 48, 48, 1)\n",
            "iteration: 32689 train shape: (68030, 48, 48, 1)\n",
            "iteration: 32690 train shape: (68031, 48, 48, 1)\n",
            "iteration: 32691 train shape: (68032, 48, 48, 1)\n",
            "iteration: 32692 train shape: (68033, 48, 48, 1)\n",
            "iteration: 32693 train shape: (68034, 48, 48, 1)\n",
            "iteration: 32694 train shape: (68035, 48, 48, 1)\n",
            "iteration: 32695 train shape: (68036, 48, 48, 1)\n",
            "iteration: 32696 train shape: (68037, 48, 48, 1)\n",
            "iteration: 32697 train shape: (68038, 48, 48, 1)\n",
            "iteration: 32698 train shape: (68039, 48, 48, 1)\n",
            "iteration: 32699 train shape: (68040, 48, 48, 1)\n",
            "iteration: 32700 train shape: (68041, 48, 48, 1)\n",
            "iteration: 32701 train shape: (68042, 48, 48, 1)\n",
            "iteration: 32702 train shape: (68043, 48, 48, 1)\n",
            "iteration: 32703 train shape: (68044, 48, 48, 1)\n",
            "iteration: 32704 train shape: (68045, 48, 48, 1)\n",
            "iteration: 32705 train shape: (68046, 48, 48, 1)\n",
            "iteration: 32706 train shape: (68047, 48, 48, 1)\n",
            "iteration: 32707 train shape: (68048, 48, 48, 1)\n",
            "iteration: 32708 train shape: (68049, 48, 48, 1)\n",
            "iteration: 32709 train shape: (68050, 48, 48, 1)\n",
            "iteration: 32710 train shape: (68051, 48, 48, 1)\n",
            "iteration: 32711 train shape: (68052, 48, 48, 1)\n",
            "iteration: 32712 train shape: (68053, 48, 48, 1)\n",
            "iteration: 32713 train shape: (68054, 48, 48, 1)\n",
            "iteration: 32714 train shape: (68055, 48, 48, 1)\n",
            "iteration: 32715 train shape: (68056, 48, 48, 1)\n",
            "iteration: 32716 train shape: (68057, 48, 48, 1)\n",
            "iteration: 32717 train shape: (68058, 48, 48, 1)\n",
            "iteration: 32718 train shape: (68059, 48, 48, 1)\n",
            "iteration: 32719 train shape: (68060, 48, 48, 1)\n",
            "iteration: 32720 train shape: (68061, 48, 48, 1)\n",
            "iteration: 32721 train shape: (68062, 48, 48, 1)\n",
            "iteration: 32722 train shape: (68063, 48, 48, 1)\n",
            "iteration: 32723 train shape: (68064, 48, 48, 1)\n",
            "iteration: 32724 train shape: (68065, 48, 48, 1)\n",
            "iteration: 32725 train shape: (68066, 48, 48, 1)\n",
            "iteration: 32726 train shape: (68067, 48, 48, 1)\n",
            "iteration: 32727 train shape: (68068, 48, 48, 1)\n",
            "iteration: 32728 train shape: (68069, 48, 48, 1)\n",
            "iteration: 32729 train shape: (68070, 48, 48, 1)\n",
            "iteration: 32730 train shape: (68071, 48, 48, 1)\n",
            "iteration: 32731 train shape: (68072, 48, 48, 1)\n",
            "iteration: 32732 train shape: (68073, 48, 48, 1)\n",
            "iteration: 32733 train shape: (68074, 48, 48, 1)\n",
            "iteration: 32734 train shape: (68075, 48, 48, 1)\n",
            "iteration: 32735 train shape: (68076, 48, 48, 1)\n",
            "iteration: 32736 train shape: (68077, 48, 48, 1)\n",
            "iteration: 32737 train shape: (68078, 48, 48, 1)\n",
            "iteration: 32738 train shape: (68079, 48, 48, 1)\n",
            "iteration: 32739 train shape: (68080, 48, 48, 1)\n",
            "iteration: 32740 train shape: (68081, 48, 48, 1)\n",
            "iteration: 32741 train shape: (68082, 48, 48, 1)\n",
            "iteration: 32742 train shape: (68083, 48, 48, 1)\n",
            "iteration: 32743 train shape: (68084, 48, 48, 1)\n",
            "iteration: 32744 train shape: (68085, 48, 48, 1)\n",
            "iteration: 32745 train shape: (68086, 48, 48, 1)\n",
            "iteration: 32746 train shape: (68087, 48, 48, 1)\n",
            "iteration: 32747 train shape: (68088, 48, 48, 1)\n",
            "iteration: 32748 train shape: (68089, 48, 48, 1)\n",
            "iteration: 32749 train shape: (68090, 48, 48, 1)\n",
            "iteration: 32750 train shape: (68091, 48, 48, 1)\n",
            "iteration: 32751 train shape: (68092, 48, 48, 1)\n",
            "iteration: 32752 train shape: (68093, 48, 48, 1)\n",
            "iteration: 32753 train shape: (68094, 48, 48, 1)\n",
            "iteration: 32754 train shape: (68095, 48, 48, 1)\n",
            "iteration: 32755 train shape: (68096, 48, 48, 1)\n",
            "iteration: 32756 train shape: (68097, 48, 48, 1)\n",
            "iteration: 32757 train shape: (68098, 48, 48, 1)\n",
            "iteration: 32758 train shape: (68099, 48, 48, 1)\n",
            "iteration: 32759 train shape: (68100, 48, 48, 1)\n",
            "iteration: 32760 train shape: (68101, 48, 48, 1)\n",
            "iteration: 32761 train shape: (68102, 48, 48, 1)\n",
            "iteration: 32762 train shape: (68103, 48, 48, 1)\n",
            "iteration: 32763 train shape: (68104, 48, 48, 1)\n",
            "iteration: 32764 train shape: (68105, 48, 48, 1)\n",
            "iteration: 32765 train shape: (68106, 48, 48, 1)\n",
            "iteration: 32766 train shape: (68107, 48, 48, 1)\n",
            "iteration: 32767 train shape: (68108, 48, 48, 1)\n",
            "iteration: 32768 train shape: (68109, 48, 48, 1)\n",
            "iteration: 32769 train shape: (68110, 48, 48, 1)\n",
            "iteration: 32770 train shape: (68111, 48, 48, 1)\n",
            "iteration: 32771 train shape: (68112, 48, 48, 1)\n",
            "iteration: 32772 train shape: (68113, 48, 48, 1)\n",
            "iteration: 32773 train shape: (68114, 48, 48, 1)\n",
            "iteration: 32774 train shape: (68115, 48, 48, 1)\n",
            "iteration: 32775 train shape: (68116, 48, 48, 1)\n",
            "iteration: 32776 train shape: (68117, 48, 48, 1)\n",
            "iteration: 32777 train shape: (68118, 48, 48, 1)\n",
            "iteration: 32778 train shape: (68119, 48, 48, 1)\n",
            "iteration: 32779 train shape: (68120, 48, 48, 1)\n",
            "iteration: 32780 train shape: (68121, 48, 48, 1)\n",
            "iteration: 32781 train shape: (68122, 48, 48, 1)\n",
            "iteration: 32782 train shape: (68123, 48, 48, 1)\n",
            "iteration: 32783 train shape: (68124, 48, 48, 1)\n",
            "iteration: 32784 train shape: (68125, 48, 48, 1)\n",
            "iteration: 32785 train shape: (68126, 48, 48, 1)\n",
            "iteration: 32786 train shape: (68127, 48, 48, 1)\n",
            "iteration: 32787 train shape: (68128, 48, 48, 1)\n",
            "iteration: 32788 train shape: (68129, 48, 48, 1)\n",
            "iteration: 32789 train shape: (68130, 48, 48, 1)\n",
            "iteration: 32790 train shape: (68131, 48, 48, 1)\n",
            "iteration: 32791 train shape: (68132, 48, 48, 1)\n",
            "iteration: 32792 train shape: (68133, 48, 48, 1)\n",
            "iteration: 32793 train shape: (68134, 48, 48, 1)\n",
            "iteration: 32794 train shape: (68135, 48, 48, 1)\n",
            "iteration: 32795 train shape: (68136, 48, 48, 1)\n",
            "iteration: 32796 train shape: (68137, 48, 48, 1)\n",
            "iteration: 32797 train shape: (68138, 48, 48, 1)\n",
            "iteration: 32798 train shape: (68139, 48, 48, 1)\n",
            "iteration: 32799 train shape: (68140, 48, 48, 1)\n",
            "iteration: 32800 train shape: (68141, 48, 48, 1)\n",
            "iteration: 32801 train shape: (68142, 48, 48, 1)\n",
            "iteration: 32802 train shape: (68143, 48, 48, 1)\n",
            "iteration: 32803 train shape: (68144, 48, 48, 1)\n",
            "iteration: 32804 train shape: (68145, 48, 48, 1)\n",
            "iteration: 32805 train shape: (68146, 48, 48, 1)\n",
            "iteration: 32806 train shape: (68147, 48, 48, 1)\n",
            "iteration: 32807 train shape: (68148, 48, 48, 1)\n",
            "iteration: 32808 train shape: (68149, 48, 48, 1)\n",
            "iteration: 32809 train shape: (68150, 48, 48, 1)\n",
            "iteration: 32810 train shape: (68151, 48, 48, 1)\n",
            "iteration: 32811 train shape: (68152, 48, 48, 1)\n",
            "iteration: 32812 train shape: (68153, 48, 48, 1)\n",
            "iteration: 32813 train shape: (68154, 48, 48, 1)\n",
            "iteration: 32814 train shape: (68155, 48, 48, 1)\n",
            "iteration: 32815 train shape: (68156, 48, 48, 1)\n",
            "iteration: 32816 train shape: (68157, 48, 48, 1)\n",
            "iteration: 32817 train shape: (68158, 48, 48, 1)\n",
            "iteration: 32818 train shape: (68159, 48, 48, 1)\n",
            "iteration: 32819 train shape: (68160, 48, 48, 1)\n",
            "iteration: 32820 train shape: (68161, 48, 48, 1)\n",
            "iteration: 32821 train shape: (68162, 48, 48, 1)\n",
            "iteration: 32822 train shape: (68163, 48, 48, 1)\n",
            "iteration: 32823 train shape: (68164, 48, 48, 1)\n",
            "iteration: 32824 train shape: (68165, 48, 48, 1)\n",
            "iteration: 32825 train shape: (68166, 48, 48, 1)\n",
            "iteration: 32826 train shape: (68167, 48, 48, 1)\n",
            "iteration: 32827 train shape: (68168, 48, 48, 1)\n",
            "iteration: 32828 train shape: (68169, 48, 48, 1)\n",
            "iteration: 32829 train shape: (68170, 48, 48, 1)\n",
            "iteration: 32830 train shape: (68171, 48, 48, 1)\n",
            "iteration: 32831 train shape: (68172, 48, 48, 1)\n",
            "iteration: 32832 train shape: (68173, 48, 48, 1)\n",
            "iteration: 32833 train shape: (68174, 48, 48, 1)\n",
            "iteration: 32834 train shape: (68175, 48, 48, 1)\n",
            "iteration: 32835 train shape: (68176, 48, 48, 1)\n",
            "iteration: 32836 train shape: (68177, 48, 48, 1)\n",
            "iteration: 32837 train shape: (68178, 48, 48, 1)\n",
            "iteration: 32838 train shape: (68179, 48, 48, 1)\n",
            "iteration: 32839 train shape: (68180, 48, 48, 1)\n",
            "iteration: 32840 train shape: (68181, 48, 48, 1)\n",
            "iteration: 32841 train shape: (68182, 48, 48, 1)\n",
            "iteration: 32842 train shape: (68183, 48, 48, 1)\n",
            "iteration: 32843 train shape: (68184, 48, 48, 1)\n",
            "iteration: 32844 train shape: (68185, 48, 48, 1)\n",
            "iteration: 32845 train shape: (68186, 48, 48, 1)\n",
            "iteration: 32846 train shape: (68187, 48, 48, 1)\n",
            "iteration: 32847 train shape: (68188, 48, 48, 1)\n",
            "iteration: 32848 train shape: (68189, 48, 48, 1)\n",
            "iteration: 32849 train shape: (68190, 48, 48, 1)\n",
            "iteration: 32850 train shape: (68191, 48, 48, 1)\n",
            "iteration: 32851 train shape: (68192, 48, 48, 1)\n",
            "iteration: 32852 train shape: (68193, 48, 48, 1)\n",
            "iteration: 32853 train shape: (68194, 48, 48, 1)\n",
            "iteration: 32854 train shape: (68195, 48, 48, 1)\n",
            "iteration: 32855 train shape: (68196, 48, 48, 1)\n",
            "iteration: 32856 train shape: (68197, 48, 48, 1)\n",
            "iteration: 32857 train shape: (68198, 48, 48, 1)\n",
            "iteration: 32858 train shape: (68199, 48, 48, 1)\n",
            "iteration: 32859 train shape: (68200, 48, 48, 1)\n",
            "iteration: 32860 train shape: (68201, 48, 48, 1)\n",
            "iteration: 32861 train shape: (68202, 48, 48, 1)\n",
            "iteration: 32862 train shape: (68203, 48, 48, 1)\n",
            "iteration: 32863 train shape: (68204, 48, 48, 1)\n",
            "iteration: 32864 train shape: (68205, 48, 48, 1)\n",
            "iteration: 32865 train shape: (68206, 48, 48, 1)\n",
            "iteration: 32866 train shape: (68207, 48, 48, 1)\n",
            "iteration: 32867 train shape: (68208, 48, 48, 1)\n",
            "iteration: 32868 train shape: (68209, 48, 48, 1)\n",
            "iteration: 32869 train shape: (68210, 48, 48, 1)\n",
            "iteration: 32870 train shape: (68211, 48, 48, 1)\n",
            "iteration: 32871 train shape: (68212, 48, 48, 1)\n",
            "iteration: 32872 train shape: (68213, 48, 48, 1)\n",
            "iteration: 32873 train shape: (68214, 48, 48, 1)\n",
            "iteration: 32874 train shape: (68215, 48, 48, 1)\n",
            "iteration: 32875 train shape: (68216, 48, 48, 1)\n",
            "iteration: 32876 train shape: (68217, 48, 48, 1)\n",
            "iteration: 32877 train shape: (68218, 48, 48, 1)\n",
            "iteration: 32878 train shape: (68219, 48, 48, 1)\n",
            "iteration: 32879 train shape: (68220, 48, 48, 1)\n",
            "iteration: 32880 train shape: (68221, 48, 48, 1)\n",
            "iteration: 32881 train shape: (68222, 48, 48, 1)\n",
            "iteration: 32882 train shape: (68223, 48, 48, 1)\n",
            "iteration: 32883 train shape: (68224, 48, 48, 1)\n",
            "iteration: 32884 train shape: (68225, 48, 48, 1)\n",
            "iteration: 32885 train shape: (68226, 48, 48, 1)\n",
            "iteration: 32886 train shape: (68227, 48, 48, 1)\n",
            "iteration: 32887 train shape: (68228, 48, 48, 1)\n",
            "iteration: 32888 train shape: (68229, 48, 48, 1)\n",
            "iteration: 32889 train shape: (68230, 48, 48, 1)\n",
            "iteration: 32890 train shape: (68231, 48, 48, 1)\n",
            "iteration: 32891 train shape: (68232, 48, 48, 1)\n",
            "iteration: 32892 train shape: (68233, 48, 48, 1)\n",
            "iteration: 32893 train shape: (68234, 48, 48, 1)\n",
            "iteration: 32894 train shape: (68235, 48, 48, 1)\n",
            "iteration: 32895 train shape: (68236, 48, 48, 1)\n",
            "iteration: 32896 train shape: (68237, 48, 48, 1)\n",
            "iteration: 32897 train shape: (68238, 48, 48, 1)\n",
            "iteration: 32898 train shape: (68239, 48, 48, 1)\n",
            "iteration: 32899 train shape: (68240, 48, 48, 1)\n",
            "iteration: 32900 train shape: (68241, 48, 48, 1)\n",
            "iteration: 32901 train shape: (68242, 48, 48, 1)\n",
            "iteration: 32902 train shape: (68243, 48, 48, 1)\n",
            "iteration: 32903 train shape: (68244, 48, 48, 1)\n",
            "iteration: 32904 train shape: (68245, 48, 48, 1)\n",
            "iteration: 32905 train shape: (68246, 48, 48, 1)\n",
            "iteration: 32906 train shape: (68247, 48, 48, 1)\n",
            "iteration: 32907 train shape: (68248, 48, 48, 1)\n",
            "iteration: 32908 train shape: (68249, 48, 48, 1)\n",
            "iteration: 32909 train shape: (68250, 48, 48, 1)\n",
            "iteration: 32910 train shape: (68251, 48, 48, 1)\n",
            "iteration: 32911 train shape: (68252, 48, 48, 1)\n",
            "iteration: 32912 train shape: (68253, 48, 48, 1)\n",
            "iteration: 32913 train shape: (68254, 48, 48, 1)\n",
            "iteration: 32914 train shape: (68255, 48, 48, 1)\n",
            "iteration: 32915 train shape: (68256, 48, 48, 1)\n",
            "iteration: 32916 train shape: (68257, 48, 48, 1)\n",
            "iteration: 32917 train shape: (68258, 48, 48, 1)\n",
            "iteration: 32918 train shape: (68259, 48, 48, 1)\n",
            "iteration: 32919 train shape: (68260, 48, 48, 1)\n",
            "iteration: 32920 train shape: (68261, 48, 48, 1)\n",
            "iteration: 32921 train shape: (68262, 48, 48, 1)\n",
            "iteration: 32922 train shape: (68263, 48, 48, 1)\n",
            "iteration: 32923 train shape: (68264, 48, 48, 1)\n",
            "iteration: 32924 train shape: (68265, 48, 48, 1)\n",
            "iteration: 32925 train shape: (68266, 48, 48, 1)\n",
            "iteration: 32926 train shape: (68267, 48, 48, 1)\n",
            "iteration: 32927 train shape: (68268, 48, 48, 1)\n",
            "iteration: 32928 train shape: (68269, 48, 48, 1)\n",
            "iteration: 32929 train shape: (68270, 48, 48, 1)\n",
            "iteration: 32930 train shape: (68271, 48, 48, 1)\n",
            "iteration: 32931 train shape: (68272, 48, 48, 1)\n",
            "iteration: 32932 train shape: (68273, 48, 48, 1)\n",
            "iteration: 32933 train shape: (68274, 48, 48, 1)\n",
            "iteration: 32934 train shape: (68275, 48, 48, 1)\n",
            "iteration: 32935 train shape: (68276, 48, 48, 1)\n",
            "iteration: 32936 train shape: (68277, 48, 48, 1)\n",
            "iteration: 32937 train shape: (68278, 48, 48, 1)\n",
            "iteration: 32938 train shape: (68279, 48, 48, 1)\n",
            "iteration: 32939 train shape: (68280, 48, 48, 1)\n",
            "iteration: 32940 train shape: (68281, 48, 48, 1)\n",
            "iteration: 32941 train shape: (68282, 48, 48, 1)\n",
            "iteration: 32942 train shape: (68283, 48, 48, 1)\n",
            "iteration: 32943 train shape: (68284, 48, 48, 1)\n",
            "iteration: 32944 train shape: (68285, 48, 48, 1)\n",
            "iteration: 32945 train shape: (68286, 48, 48, 1)\n",
            "iteration: 32946 train shape: (68287, 48, 48, 1)\n",
            "iteration: 32947 train shape: (68288, 48, 48, 1)\n",
            "iteration: 32948 train shape: (68289, 48, 48, 1)\n",
            "iteration: 32949 train shape: (68290, 48, 48, 1)\n",
            "iteration: 32950 train shape: (68291, 48, 48, 1)\n",
            "iteration: 32951 train shape: (68292, 48, 48, 1)\n",
            "iteration: 32952 train shape: (68293, 48, 48, 1)\n",
            "iteration: 32953 train shape: (68294, 48, 48, 1)\n",
            "iteration: 32954 train shape: (68295, 48, 48, 1)\n",
            "iteration: 32955 train shape: (68296, 48, 48, 1)\n",
            "iteration: 32956 train shape: (68297, 48, 48, 1)\n",
            "iteration: 32957 train shape: (68298, 48, 48, 1)\n",
            "iteration: 32958 train shape: (68299, 48, 48, 1)\n",
            "iteration: 32959 train shape: (68300, 48, 48, 1)\n",
            "iteration: 32960 train shape: (68301, 48, 48, 1)\n",
            "iteration: 32961 train shape: (68302, 48, 48, 1)\n",
            "iteration: 32962 train shape: (68303, 48, 48, 1)\n",
            "iteration: 32963 train shape: (68304, 48, 48, 1)\n",
            "iteration: 32964 train shape: (68305, 48, 48, 1)\n",
            "iteration: 32965 train shape: (68306, 48, 48, 1)\n",
            "iteration: 32966 train shape: (68307, 48, 48, 1)\n",
            "iteration: 32967 train shape: (68308, 48, 48, 1)\n",
            "iteration: 32968 train shape: (68309, 48, 48, 1)\n",
            "iteration: 32969 train shape: (68310, 48, 48, 1)\n",
            "iteration: 32970 train shape: (68311, 48, 48, 1)\n",
            "iteration: 32971 train shape: (68312, 48, 48, 1)\n",
            "iteration: 32972 train shape: (68313, 48, 48, 1)\n",
            "iteration: 32973 train shape: (68314, 48, 48, 1)\n",
            "iteration: 32974 train shape: (68315, 48, 48, 1)\n",
            "iteration: 32975 train shape: (68316, 48, 48, 1)\n",
            "iteration: 32976 train shape: (68317, 48, 48, 1)\n",
            "iteration: 32977 train shape: (68318, 48, 48, 1)\n",
            "iteration: 32978 train shape: (68319, 48, 48, 1)\n",
            "iteration: 32979 train shape: (68320, 48, 48, 1)\n",
            "iteration: 32980 train shape: (68321, 48, 48, 1)\n",
            "iteration: 32981 train shape: (68322, 48, 48, 1)\n",
            "iteration: 32982 train shape: (68323, 48, 48, 1)\n",
            "iteration: 32983 train shape: (68324, 48, 48, 1)\n",
            "iteration: 32984 train shape: (68325, 48, 48, 1)\n",
            "iteration: 32985 train shape: (68326, 48, 48, 1)\n",
            "iteration: 32986 train shape: (68327, 48, 48, 1)\n",
            "iteration: 32987 train shape: (68328, 48, 48, 1)\n",
            "iteration: 32988 train shape: (68329, 48, 48, 1)\n",
            "iteration: 32989 train shape: (68330, 48, 48, 1)\n",
            "iteration: 32990 train shape: (68331, 48, 48, 1)\n",
            "iteration: 32991 train shape: (68332, 48, 48, 1)\n",
            "iteration: 32992 train shape: (68333, 48, 48, 1)\n",
            "iteration: 32993 train shape: (68334, 48, 48, 1)\n",
            "iteration: 32994 train shape: (68335, 48, 48, 1)\n",
            "iteration: 32995 train shape: (68336, 48, 48, 1)\n",
            "iteration: 32996 train shape: (68337, 48, 48, 1)\n",
            "iteration: 32997 train shape: (68338, 48, 48, 1)\n",
            "iteration: 32998 train shape: (68339, 48, 48, 1)\n",
            "iteration: 32999 train shape: (68340, 48, 48, 1)\n",
            "iteration: 33000 train shape: (68341, 48, 48, 1)\n",
            "iteration: 33001 train shape: (68342, 48, 48, 1)\n",
            "iteration: 33002 train shape: (68343, 48, 48, 1)\n",
            "iteration: 33003 train shape: (68344, 48, 48, 1)\n",
            "iteration: 33004 train shape: (68345, 48, 48, 1)\n",
            "iteration: 33005 train shape: (68346, 48, 48, 1)\n",
            "iteration: 33006 train shape: (68347, 48, 48, 1)\n",
            "iteration: 33007 train shape: (68348, 48, 48, 1)\n",
            "iteration: 33008 train shape: (68349, 48, 48, 1)\n",
            "iteration: 33009 train shape: (68350, 48, 48, 1)\n",
            "iteration: 33010 train shape: (68351, 48, 48, 1)\n",
            "iteration: 33011 train shape: (68352, 48, 48, 1)\n",
            "iteration: 33012 train shape: (68353, 48, 48, 1)\n",
            "iteration: 33013 train shape: (68354, 48, 48, 1)\n",
            "iteration: 33014 train shape: (68355, 48, 48, 1)\n",
            "iteration: 33015 train shape: (68356, 48, 48, 1)\n",
            "iteration: 33016 train shape: (68357, 48, 48, 1)\n",
            "iteration: 33017 train shape: (68358, 48, 48, 1)\n",
            "iteration: 33018 train shape: (68359, 48, 48, 1)\n",
            "iteration: 33019 train shape: (68360, 48, 48, 1)\n",
            "iteration: 33020 train shape: (68361, 48, 48, 1)\n",
            "iteration: 33021 train shape: (68362, 48, 48, 1)\n",
            "iteration: 33022 train shape: (68363, 48, 48, 1)\n",
            "iteration: 33023 train shape: (68364, 48, 48, 1)\n",
            "iteration: 33024 train shape: (68365, 48, 48, 1)\n",
            "iteration: 33025 train shape: (68366, 48, 48, 1)\n",
            "iteration: 33026 train shape: (68367, 48, 48, 1)\n",
            "iteration: 33027 train shape: (68368, 48, 48, 1)\n",
            "iteration: 33028 train shape: (68369, 48, 48, 1)\n",
            "iteration: 33029 train shape: (68370, 48, 48, 1)\n",
            "iteration: 33030 train shape: (68371, 48, 48, 1)\n",
            "iteration: 33031 train shape: (68372, 48, 48, 1)\n",
            "iteration: 33032 train shape: (68373, 48, 48, 1)\n",
            "iteration: 33033 train shape: (68374, 48, 48, 1)\n",
            "iteration: 33034 train shape: (68375, 48, 48, 1)\n",
            "iteration: 33035 train shape: (68376, 48, 48, 1)\n",
            "iteration: 33036 train shape: (68377, 48, 48, 1)\n",
            "iteration: 33037 train shape: (68378, 48, 48, 1)\n",
            "iteration: 33038 train shape: (68379, 48, 48, 1)\n",
            "iteration: 33039 train shape: (68380, 48, 48, 1)\n",
            "iteration: 33040 train shape: (68381, 48, 48, 1)\n",
            "iteration: 33041 train shape: (68382, 48, 48, 1)\n",
            "iteration: 33042 train shape: (68383, 48, 48, 1)\n",
            "iteration: 33043 train shape: (68384, 48, 48, 1)\n",
            "iteration: 33044 train shape: (68385, 48, 48, 1)\n",
            "iteration: 33045 train shape: (68386, 48, 48, 1)\n",
            "iteration: 33046 train shape: (68387, 48, 48, 1)\n",
            "iteration: 33047 train shape: (68388, 48, 48, 1)\n",
            "iteration: 33048 train shape: (68389, 48, 48, 1)\n",
            "iteration: 33049 train shape: (68390, 48, 48, 1)\n",
            "iteration: 33050 train shape: (68391, 48, 48, 1)\n",
            "iteration: 33051 train shape: (68392, 48, 48, 1)\n",
            "iteration: 33052 train shape: (68393, 48, 48, 1)\n",
            "iteration: 33053 train shape: (68394, 48, 48, 1)\n",
            "iteration: 33054 train shape: (68395, 48, 48, 1)\n",
            "iteration: 33055 train shape: (68396, 48, 48, 1)\n",
            "iteration: 33056 train shape: (68397, 48, 48, 1)\n",
            "iteration: 33057 train shape: (68398, 48, 48, 1)\n",
            "iteration: 33058 train shape: (68399, 48, 48, 1)\n",
            "iteration: 33059 train shape: (68400, 48, 48, 1)\n",
            "iteration: 33060 train shape: (68401, 48, 48, 1)\n",
            "iteration: 33061 train shape: (68402, 48, 48, 1)\n",
            "iteration: 33062 train shape: (68403, 48, 48, 1)\n",
            "iteration: 33063 train shape: (68404, 48, 48, 1)\n",
            "iteration: 33064 train shape: (68405, 48, 48, 1)\n",
            "iteration: 33065 train shape: (68406, 48, 48, 1)\n",
            "iteration: 33066 train shape: (68407, 48, 48, 1)\n",
            "iteration: 33067 train shape: (68408, 48, 48, 1)\n",
            "iteration: 33068 train shape: (68409, 48, 48, 1)\n",
            "iteration: 33069 train shape: (68410, 48, 48, 1)\n",
            "iteration: 33070 train shape: (68411, 48, 48, 1)\n",
            "iteration: 33071 train shape: (68412, 48, 48, 1)\n",
            "iteration: 33072 train shape: (68413, 48, 48, 1)\n",
            "iteration: 33073 train shape: (68414, 48, 48, 1)\n",
            "iteration: 33074 train shape: (68415, 48, 48, 1)\n",
            "iteration: 33075 train shape: (68416, 48, 48, 1)\n",
            "iteration: 33076 train shape: (68417, 48, 48, 1)\n",
            "iteration: 33077 train shape: (68418, 48, 48, 1)\n",
            "iteration: 33078 train shape: (68419, 48, 48, 1)\n",
            "iteration: 33079 train shape: (68420, 48, 48, 1)\n",
            "iteration: 33080 train shape: (68421, 48, 48, 1)\n",
            "iteration: 33081 train shape: (68422, 48, 48, 1)\n",
            "iteration: 33082 train shape: (68423, 48, 48, 1)\n",
            "iteration: 33083 train shape: (68424, 48, 48, 1)\n",
            "iteration: 33084 train shape: (68425, 48, 48, 1)\n",
            "iteration: 33085 train shape: (68426, 48, 48, 1)\n",
            "iteration: 33086 train shape: (68427, 48, 48, 1)\n",
            "iteration: 33087 train shape: (68428, 48, 48, 1)\n",
            "iteration: 33088 train shape: (68429, 48, 48, 1)\n",
            "iteration: 33089 train shape: (68430, 48, 48, 1)\n",
            "iteration: 33090 train shape: (68431, 48, 48, 1)\n",
            "iteration: 33091 train shape: (68432, 48, 48, 1)\n",
            "iteration: 33092 train shape: (68433, 48, 48, 1)\n",
            "iteration: 33093 train shape: (68434, 48, 48, 1)\n",
            "iteration: 33094 train shape: (68435, 48, 48, 1)\n",
            "iteration: 33095 train shape: (68436, 48, 48, 1)\n",
            "iteration: 33096 train shape: (68437, 48, 48, 1)\n",
            "iteration: 33097 train shape: (68438, 48, 48, 1)\n",
            "iteration: 33098 train shape: (68439, 48, 48, 1)\n",
            "iteration: 33099 train shape: (68440, 48, 48, 1)\n",
            "iteration: 33100 train shape: (68441, 48, 48, 1)\n",
            "iteration: 33101 train shape: (68442, 48, 48, 1)\n",
            "iteration: 33102 train shape: (68443, 48, 48, 1)\n",
            "iteration: 33103 train shape: (68444, 48, 48, 1)\n",
            "iteration: 33104 train shape: (68445, 48, 48, 1)\n",
            "iteration: 33105 train shape: (68446, 48, 48, 1)\n",
            "iteration: 33106 train shape: (68447, 48, 48, 1)\n",
            "iteration: 33107 train shape: (68448, 48, 48, 1)\n",
            "iteration: 33108 train shape: (68449, 48, 48, 1)\n",
            "iteration: 33109 train shape: (68450, 48, 48, 1)\n",
            "iteration: 33110 train shape: (68451, 48, 48, 1)\n",
            "iteration: 33111 train shape: (68452, 48, 48, 1)\n",
            "iteration: 33112 train shape: (68453, 48, 48, 1)\n",
            "iteration: 33113 train shape: (68454, 48, 48, 1)\n",
            "iteration: 33114 train shape: (68455, 48, 48, 1)\n",
            "iteration: 33115 train shape: (68456, 48, 48, 1)\n",
            "iteration: 33116 train shape: (68457, 48, 48, 1)\n",
            "iteration: 33117 train shape: (68458, 48, 48, 1)\n",
            "iteration: 33118 train shape: (68459, 48, 48, 1)\n",
            "iteration: 33119 train shape: (68460, 48, 48, 1)\n",
            "iteration: 33120 train shape: (68461, 48, 48, 1)\n",
            "iteration: 33121 train shape: (68462, 48, 48, 1)\n",
            "iteration: 33122 train shape: (68463, 48, 48, 1)\n",
            "iteration: 33123 train shape: (68464, 48, 48, 1)\n",
            "iteration: 33124 train shape: (68465, 48, 48, 1)\n",
            "iteration: 33125 train shape: (68466, 48, 48, 1)\n",
            "iteration: 33126 train shape: (68467, 48, 48, 1)\n",
            "iteration: 33127 train shape: (68468, 48, 48, 1)\n",
            "iteration: 33128 train shape: (68469, 48, 48, 1)\n",
            "iteration: 33129 train shape: (68470, 48, 48, 1)\n",
            "iteration: 33130 train shape: (68471, 48, 48, 1)\n",
            "iteration: 33131 train shape: (68472, 48, 48, 1)\n",
            "iteration: 33132 train shape: (68473, 48, 48, 1)\n",
            "iteration: 33133 train shape: (68474, 48, 48, 1)\n",
            "iteration: 33134 train shape: (68475, 48, 48, 1)\n",
            "iteration: 33135 train shape: (68476, 48, 48, 1)\n",
            "iteration: 33136 train shape: (68477, 48, 48, 1)\n",
            "iteration: 33137 train shape: (68478, 48, 48, 1)\n",
            "iteration: 33138 train shape: (68479, 48, 48, 1)\n",
            "iteration: 33139 train shape: (68480, 48, 48, 1)\n",
            "iteration: 33140 train shape: (68481, 48, 48, 1)\n",
            "iteration: 33141 train shape: (68482, 48, 48, 1)\n",
            "iteration: 33142 train shape: (68483, 48, 48, 1)\n",
            "iteration: 33143 train shape: (68484, 48, 48, 1)\n",
            "iteration: 33144 train shape: (68485, 48, 48, 1)\n",
            "iteration: 33145 train shape: (68486, 48, 48, 1)\n",
            "iteration: 33146 train shape: (68487, 48, 48, 1)\n",
            "iteration: 33147 train shape: (68488, 48, 48, 1)\n",
            "iteration: 33148 train shape: (68489, 48, 48, 1)\n",
            "iteration: 33149 train shape: (68490, 48, 48, 1)\n",
            "iteration: 33150 train shape: (68491, 48, 48, 1)\n",
            "iteration: 33151 train shape: (68492, 48, 48, 1)\n",
            "iteration: 33152 train shape: (68493, 48, 48, 1)\n",
            "iteration: 33153 train shape: (68494, 48, 48, 1)\n",
            "iteration: 33154 train shape: (68495, 48, 48, 1)\n",
            "iteration: 33155 train shape: (68496, 48, 48, 1)\n",
            "iteration: 33156 train shape: (68497, 48, 48, 1)\n",
            "iteration: 33157 train shape: (68498, 48, 48, 1)\n",
            "iteration: 33158 train shape: (68499, 48, 48, 1)\n",
            "iteration: 33159 train shape: (68500, 48, 48, 1)\n",
            "iteration: 33160 train shape: (68501, 48, 48, 1)\n",
            "iteration: 33161 train shape: (68502, 48, 48, 1)\n",
            "iteration: 33162 train shape: (68503, 48, 48, 1)\n",
            "iteration: 33163 train shape: (68504, 48, 48, 1)\n",
            "iteration: 33164 train shape: (68505, 48, 48, 1)\n",
            "iteration: 33165 train shape: (68506, 48, 48, 1)\n",
            "iteration: 33166 train shape: (68507, 48, 48, 1)\n",
            "iteration: 33167 train shape: (68508, 48, 48, 1)\n",
            "iteration: 33168 train shape: (68509, 48, 48, 1)\n",
            "iteration: 33169 train shape: (68510, 48, 48, 1)\n",
            "iteration: 33170 train shape: (68511, 48, 48, 1)\n",
            "iteration: 33171 train shape: (68512, 48, 48, 1)\n",
            "iteration: 33172 train shape: (68513, 48, 48, 1)\n",
            "iteration: 33173 train shape: (68514, 48, 48, 1)\n",
            "iteration: 33174 train shape: (68515, 48, 48, 1)\n",
            "iteration: 33175 train shape: (68516, 48, 48, 1)\n",
            "iteration: 33176 train shape: (68517, 48, 48, 1)\n",
            "iteration: 33177 train shape: (68518, 48, 48, 1)\n",
            "iteration: 33178 train shape: (68519, 48, 48, 1)\n",
            "iteration: 33179 train shape: (68520, 48, 48, 1)\n",
            "iteration: 33180 train shape: (68521, 48, 48, 1)\n",
            "iteration: 33181 train shape: (68522, 48, 48, 1)\n",
            "iteration: 33182 train shape: (68523, 48, 48, 1)\n",
            "iteration: 33183 train shape: (68524, 48, 48, 1)\n",
            "iteration: 33184 train shape: (68525, 48, 48, 1)\n",
            "iteration: 33185 train shape: (68526, 48, 48, 1)\n",
            "iteration: 33186 train shape: (68527, 48, 48, 1)\n",
            "iteration: 33187 train shape: (68528, 48, 48, 1)\n",
            "iteration: 33188 train shape: (68529, 48, 48, 1)\n",
            "iteration: 33189 train shape: (68530, 48, 48, 1)\n",
            "iteration: 33190 train shape: (68531, 48, 48, 1)\n",
            "iteration: 33191 train shape: (68532, 48, 48, 1)\n",
            "iteration: 33192 train shape: (68533, 48, 48, 1)\n",
            "iteration: 33193 train shape: (68534, 48, 48, 1)\n",
            "iteration: 33194 train shape: (68535, 48, 48, 1)\n",
            "iteration: 33195 train shape: (68536, 48, 48, 1)\n",
            "iteration: 33196 train shape: (68537, 48, 48, 1)\n",
            "iteration: 33197 train shape: (68538, 48, 48, 1)\n",
            "iteration: 33198 train shape: (68539, 48, 48, 1)\n",
            "iteration: 33199 train shape: (68540, 48, 48, 1)\n",
            "iteration: 33200 train shape: (68541, 48, 48, 1)\n",
            "iteration: 33201 train shape: (68542, 48, 48, 1)\n",
            "iteration: 33202 train shape: (68543, 48, 48, 1)\n",
            "iteration: 33203 train shape: (68544, 48, 48, 1)\n",
            "iteration: 33204 train shape: (68545, 48, 48, 1)\n",
            "iteration: 33205 train shape: (68546, 48, 48, 1)\n",
            "iteration: 33206 train shape: (68547, 48, 48, 1)\n",
            "iteration: 33207 train shape: (68548, 48, 48, 1)\n",
            "iteration: 33208 train shape: (68549, 48, 48, 1)\n",
            "iteration: 33209 train shape: (68550, 48, 48, 1)\n",
            "iteration: 33210 train shape: (68551, 48, 48, 1)\n",
            "iteration: 33211 train shape: (68552, 48, 48, 1)\n",
            "iteration: 33212 train shape: (68553, 48, 48, 1)\n",
            "iteration: 33213 train shape: (68554, 48, 48, 1)\n",
            "iteration: 33214 train shape: (68555, 48, 48, 1)\n",
            "iteration: 33215 train shape: (68556, 48, 48, 1)\n",
            "iteration: 33216 train shape: (68557, 48, 48, 1)\n",
            "iteration: 33217 train shape: (68558, 48, 48, 1)\n",
            "iteration: 33218 train shape: (68559, 48, 48, 1)\n",
            "iteration: 33219 train shape: (68560, 48, 48, 1)\n",
            "iteration: 33220 train shape: (68561, 48, 48, 1)\n",
            "iteration: 33221 train shape: (68562, 48, 48, 1)\n",
            "iteration: 33222 train shape: (68563, 48, 48, 1)\n",
            "iteration: 33223 train shape: (68564, 48, 48, 1)\n",
            "iteration: 33224 train shape: (68565, 48, 48, 1)\n",
            "iteration: 33225 train shape: (68566, 48, 48, 1)\n",
            "iteration: 33226 train shape: (68567, 48, 48, 1)\n",
            "iteration: 33227 train shape: (68568, 48, 48, 1)\n",
            "iteration: 33228 train shape: (68569, 48, 48, 1)\n",
            "iteration: 33229 train shape: (68570, 48, 48, 1)\n",
            "iteration: 33230 train shape: (68571, 48, 48, 1)\n",
            "iteration: 33231 train shape: (68572, 48, 48, 1)\n",
            "iteration: 33232 train shape: (68573, 48, 48, 1)\n",
            "iteration: 33233 train shape: (68574, 48, 48, 1)\n",
            "iteration: 33234 train shape: (68575, 48, 48, 1)\n",
            "iteration: 33235 train shape: (68576, 48, 48, 1)\n",
            "iteration: 33236 train shape: (68577, 48, 48, 1)\n",
            "iteration: 33237 train shape: (68578, 48, 48, 1)\n",
            "iteration: 33238 train shape: (68579, 48, 48, 1)\n",
            "iteration: 33239 train shape: (68580, 48, 48, 1)\n",
            "iteration: 33240 train shape: (68581, 48, 48, 1)\n",
            "iteration: 33241 train shape: (68582, 48, 48, 1)\n",
            "iteration: 33242 train shape: (68583, 48, 48, 1)\n",
            "iteration: 33243 train shape: (68584, 48, 48, 1)\n",
            "iteration: 33244 train shape: (68585, 48, 48, 1)\n",
            "iteration: 33245 train shape: (68586, 48, 48, 1)\n",
            "iteration: 33246 train shape: (68587, 48, 48, 1)\n",
            "iteration: 33247 train shape: (68588, 48, 48, 1)\n",
            "iteration: 33248 train shape: (68589, 48, 48, 1)\n",
            "iteration: 33249 train shape: (68590, 48, 48, 1)\n",
            "iteration: 33250 train shape: (68591, 48, 48, 1)\n",
            "iteration: 33251 train shape: (68592, 48, 48, 1)\n",
            "iteration: 33252 train shape: (68593, 48, 48, 1)\n",
            "iteration: 33253 train shape: (68594, 48, 48, 1)\n",
            "iteration: 33254 train shape: (68595, 48, 48, 1)\n",
            "iteration: 33255 train shape: (68596, 48, 48, 1)\n",
            "iteration: 33256 train shape: (68597, 48, 48, 1)\n",
            "iteration: 33257 train shape: (68598, 48, 48, 1)\n",
            "iteration: 33258 train shape: (68599, 48, 48, 1)\n",
            "iteration: 33259 train shape: (68600, 48, 48, 1)\n",
            "iteration: 33260 train shape: (68601, 48, 48, 1)\n",
            "iteration: 33261 train shape: (68602, 48, 48, 1)\n",
            "iteration: 33262 train shape: (68603, 48, 48, 1)\n",
            "iteration: 33263 train shape: (68604, 48, 48, 1)\n",
            "iteration: 33264 train shape: (68605, 48, 48, 1)\n",
            "iteration: 33265 train shape: (68606, 48, 48, 1)\n",
            "iteration: 33266 train shape: (68607, 48, 48, 1)\n",
            "iteration: 33267 train shape: (68608, 48, 48, 1)\n",
            "iteration: 33268 train shape: (68609, 48, 48, 1)\n",
            "iteration: 33269 train shape: (68610, 48, 48, 1)\n",
            "iteration: 33270 train shape: (68611, 48, 48, 1)\n",
            "iteration: 33271 train shape: (68612, 48, 48, 1)\n",
            "iteration: 33272 train shape: (68613, 48, 48, 1)\n",
            "iteration: 33273 train shape: (68614, 48, 48, 1)\n",
            "iteration: 33274 train shape: (68615, 48, 48, 1)\n",
            "iteration: 33275 train shape: (68616, 48, 48, 1)\n",
            "iteration: 33276 train shape: (68617, 48, 48, 1)\n",
            "iteration: 33277 train shape: (68618, 48, 48, 1)\n",
            "iteration: 33278 train shape: (68619, 48, 48, 1)\n",
            "iteration: 33279 train shape: (68620, 48, 48, 1)\n",
            "iteration: 33280 train shape: (68621, 48, 48, 1)\n",
            "iteration: 33281 train shape: (68622, 48, 48, 1)\n",
            "iteration: 33282 train shape: (68623, 48, 48, 1)\n",
            "iteration: 33283 train shape: (68624, 48, 48, 1)\n",
            "iteration: 33284 train shape: (68625, 48, 48, 1)\n",
            "iteration: 33285 train shape: (68626, 48, 48, 1)\n",
            "iteration: 33286 train shape: (68627, 48, 48, 1)\n",
            "iteration: 33287 train shape: (68628, 48, 48, 1)\n",
            "iteration: 33288 train shape: (68629, 48, 48, 1)\n",
            "iteration: 33289 train shape: (68630, 48, 48, 1)\n",
            "iteration: 33290 train shape: (68631, 48, 48, 1)\n",
            "iteration: 33291 train shape: (68632, 48, 48, 1)\n",
            "iteration: 33292 train shape: (68633, 48, 48, 1)\n",
            "iteration: 33293 train shape: (68634, 48, 48, 1)\n",
            "iteration: 33294 train shape: (68635, 48, 48, 1)\n",
            "iteration: 33295 train shape: (68636, 48, 48, 1)\n",
            "iteration: 33296 train shape: (68637, 48, 48, 1)\n",
            "iteration: 33297 train shape: (68638, 48, 48, 1)\n",
            "iteration: 33298 train shape: (68639, 48, 48, 1)\n",
            "iteration: 33299 train shape: (68640, 48, 48, 1)\n",
            "iteration: 33300 train shape: (68641, 48, 48, 1)\n",
            "iteration: 33301 train shape: (68642, 48, 48, 1)\n",
            "iteration: 33302 train shape: (68643, 48, 48, 1)\n",
            "iteration: 33303 train shape: (68644, 48, 48, 1)\n",
            "iteration: 33304 train shape: (68645, 48, 48, 1)\n",
            "iteration: 33305 train shape: (68646, 48, 48, 1)\n",
            "iteration: 33306 train shape: (68647, 48, 48, 1)\n",
            "iteration: 33307 train shape: (68648, 48, 48, 1)\n",
            "iteration: 33308 train shape: (68649, 48, 48, 1)\n",
            "iteration: 33309 train shape: (68650, 48, 48, 1)\n",
            "iteration: 33310 train shape: (68651, 48, 48, 1)\n",
            "iteration: 33311 train shape: (68652, 48, 48, 1)\n",
            "iteration: 33312 train shape: (68653, 48, 48, 1)\n",
            "iteration: 33313 train shape: (68654, 48, 48, 1)\n",
            "iteration: 33314 train shape: (68655, 48, 48, 1)\n",
            "iteration: 33315 train shape: (68656, 48, 48, 1)\n",
            "iteration: 33316 train shape: (68657, 48, 48, 1)\n",
            "iteration: 33317 train shape: (68658, 48, 48, 1)\n",
            "iteration: 33318 train shape: (68659, 48, 48, 1)\n",
            "iteration: 33319 train shape: (68660, 48, 48, 1)\n",
            "iteration: 33320 train shape: (68661, 48, 48, 1)\n",
            "iteration: 33321 train shape: (68662, 48, 48, 1)\n",
            "iteration: 33322 train shape: (68663, 48, 48, 1)\n",
            "iteration: 33323 train shape: (68664, 48, 48, 1)\n",
            "iteration: 33324 train shape: (68665, 48, 48, 1)\n",
            "iteration: 33325 train shape: (68666, 48, 48, 1)\n",
            "iteration: 33326 train shape: (68667, 48, 48, 1)\n",
            "iteration: 33327 train shape: (68668, 48, 48, 1)\n",
            "iteration: 33328 train shape: (68669, 48, 48, 1)\n",
            "iteration: 33329 train shape: (68670, 48, 48, 1)\n",
            "iteration: 33330 train shape: (68671, 48, 48, 1)\n",
            "iteration: 33331 train shape: (68672, 48, 48, 1)\n",
            "iteration: 33332 train shape: (68673, 48, 48, 1)\n",
            "iteration: 33333 train shape: (68674, 48, 48, 1)\n",
            "iteration: 33334 train shape: (68675, 48, 48, 1)\n",
            "iteration: 33335 train shape: (68676, 48, 48, 1)\n",
            "iteration: 33336 train shape: (68677, 48, 48, 1)\n",
            "iteration: 33337 train shape: (68678, 48, 48, 1)\n",
            "iteration: 33338 train shape: (68679, 48, 48, 1)\n",
            "iteration: 33339 train shape: (68680, 48, 48, 1)\n",
            "iteration: 33340 train shape: (68681, 48, 48, 1)\n",
            "iteration: 33341 train shape: (68682, 48, 48, 1)\n",
            "iteration: 33342 train shape: (68683, 48, 48, 1)\n",
            "iteration: 33343 train shape: (68684, 48, 48, 1)\n",
            "iteration: 33344 train shape: (68685, 48, 48, 1)\n",
            "iteration: 33345 train shape: (68686, 48, 48, 1)\n",
            "iteration: 33346 train shape: (68687, 48, 48, 1)\n",
            "iteration: 33347 train shape: (68688, 48, 48, 1)\n",
            "iteration: 33348 train shape: (68689, 48, 48, 1)\n",
            "iteration: 33349 train shape: (68690, 48, 48, 1)\n",
            "iteration: 33350 train shape: (68691, 48, 48, 1)\n",
            "iteration: 33351 train shape: (68692, 48, 48, 1)\n",
            "iteration: 33352 train shape: (68693, 48, 48, 1)\n",
            "iteration: 33353 train shape: (68694, 48, 48, 1)\n",
            "iteration: 33354 train shape: (68695, 48, 48, 1)\n",
            "iteration: 33355 train shape: (68696, 48, 48, 1)\n",
            "iteration: 33356 train shape: (68697, 48, 48, 1)\n",
            "iteration: 33357 train shape: (68698, 48, 48, 1)\n",
            "iteration: 33358 train shape: (68699, 48, 48, 1)\n",
            "iteration: 33359 train shape: (68700, 48, 48, 1)\n",
            "iteration: 33360 train shape: (68701, 48, 48, 1)\n",
            "iteration: 33361 train shape: (68702, 48, 48, 1)\n",
            "iteration: 33362 train shape: (68703, 48, 48, 1)\n",
            "iteration: 33363 train shape: (68704, 48, 48, 1)\n",
            "iteration: 33364 train shape: (68705, 48, 48, 1)\n",
            "iteration: 33365 train shape: (68706, 48, 48, 1)\n",
            "iteration: 33366 train shape: (68707, 48, 48, 1)\n",
            "iteration: 33367 train shape: (68708, 48, 48, 1)\n",
            "iteration: 33368 train shape: (68709, 48, 48, 1)\n",
            "iteration: 33369 train shape: (68710, 48, 48, 1)\n",
            "iteration: 33370 train shape: (68711, 48, 48, 1)\n",
            "iteration: 33371 train shape: (68712, 48, 48, 1)\n",
            "iteration: 33372 train shape: (68713, 48, 48, 1)\n",
            "iteration: 33373 train shape: (68714, 48, 48, 1)\n",
            "iteration: 33374 train shape: (68715, 48, 48, 1)\n",
            "iteration: 33375 train shape: (68716, 48, 48, 1)\n",
            "iteration: 33376 train shape: (68717, 48, 48, 1)\n",
            "iteration: 33377 train shape: (68718, 48, 48, 1)\n",
            "iteration: 33378 train shape: (68719, 48, 48, 1)\n",
            "iteration: 33379 train shape: (68720, 48, 48, 1)\n",
            "iteration: 33380 train shape: (68721, 48, 48, 1)\n",
            "iteration: 33381 train shape: (68722, 48, 48, 1)\n",
            "iteration: 33382 train shape: (68723, 48, 48, 1)\n",
            "iteration: 33383 train shape: (68724, 48, 48, 1)\n",
            "iteration: 33384 train shape: (68725, 48, 48, 1)\n",
            "iteration: 33385 train shape: (68726, 48, 48, 1)\n",
            "iteration: 33386 train shape: (68727, 48, 48, 1)\n",
            "iteration: 33387 train shape: (68728, 48, 48, 1)\n",
            "iteration: 33388 train shape: (68729, 48, 48, 1)\n",
            "iteration: 33389 train shape: (68730, 48, 48, 1)\n",
            "iteration: 33390 train shape: (68731, 48, 48, 1)\n",
            "iteration: 33391 train shape: (68732, 48, 48, 1)\n",
            "iteration: 33392 train shape: (68733, 48, 48, 1)\n",
            "iteration: 33393 train shape: (68734, 48, 48, 1)\n",
            "iteration: 33394 train shape: (68735, 48, 48, 1)\n",
            "iteration: 33395 train shape: (68736, 48, 48, 1)\n",
            "iteration: 33396 train shape: (68737, 48, 48, 1)\n",
            "iteration: 33397 train shape: (68738, 48, 48, 1)\n",
            "iteration: 33398 train shape: (68739, 48, 48, 1)\n",
            "iteration: 33399 train shape: (68740, 48, 48, 1)\n",
            "iteration: 33400 train shape: (68741, 48, 48, 1)\n",
            "iteration: 33401 train shape: (68742, 48, 48, 1)\n",
            "iteration: 33402 train shape: (68743, 48, 48, 1)\n",
            "iteration: 33403 train shape: (68744, 48, 48, 1)\n",
            "iteration: 33404 train shape: (68745, 48, 48, 1)\n",
            "iteration: 33405 train shape: (68746, 48, 48, 1)\n",
            "iteration: 33406 train shape: (68747, 48, 48, 1)\n",
            "iteration: 33407 train shape: (68748, 48, 48, 1)\n",
            "iteration: 33408 train shape: (68749, 48, 48, 1)\n",
            "iteration: 33409 train shape: (68750, 48, 48, 1)\n",
            "iteration: 33410 train shape: (68751, 48, 48, 1)\n",
            "iteration: 33411 train shape: (68752, 48, 48, 1)\n",
            "iteration: 33412 train shape: (68753, 48, 48, 1)\n",
            "iteration: 33413 train shape: (68754, 48, 48, 1)\n",
            "iteration: 33414 train shape: (68755, 48, 48, 1)\n",
            "iteration: 33415 train shape: (68756, 48, 48, 1)\n",
            "iteration: 33416 train shape: (68757, 48, 48, 1)\n",
            "iteration: 33417 train shape: (68758, 48, 48, 1)\n",
            "iteration: 33418 train shape: (68759, 48, 48, 1)\n",
            "iteration: 33419 train shape: (68760, 48, 48, 1)\n",
            "iteration: 33420 train shape: (68761, 48, 48, 1)\n",
            "iteration: 33421 train shape: (68762, 48, 48, 1)\n",
            "iteration: 33422 train shape: (68763, 48, 48, 1)\n",
            "iteration: 33423 train shape: (68764, 48, 48, 1)\n",
            "iteration: 33424 train shape: (68765, 48, 48, 1)\n",
            "iteration: 33425 train shape: (68766, 48, 48, 1)\n",
            "iteration: 33426 train shape: (68767, 48, 48, 1)\n",
            "iteration: 33427 train shape: (68768, 48, 48, 1)\n",
            "iteration: 33428 train shape: (68769, 48, 48, 1)\n",
            "iteration: 33429 train shape: (68770, 48, 48, 1)\n",
            "iteration: 33430 train shape: (68771, 48, 48, 1)\n",
            "iteration: 33431 train shape: (68772, 48, 48, 1)\n",
            "iteration: 33432 train shape: (68773, 48, 48, 1)\n",
            "iteration: 33433 train shape: (68774, 48, 48, 1)\n",
            "iteration: 33434 train shape: (68775, 48, 48, 1)\n",
            "iteration: 33435 train shape: (68776, 48, 48, 1)\n",
            "iteration: 33436 train shape: (68777, 48, 48, 1)\n",
            "iteration: 33437 train shape: (68778, 48, 48, 1)\n",
            "iteration: 33438 train shape: (68779, 48, 48, 1)\n",
            "iteration: 33439 train shape: (68780, 48, 48, 1)\n",
            "iteration: 33440 train shape: (68781, 48, 48, 1)\n",
            "iteration: 33441 train shape: (68782, 48, 48, 1)\n",
            "iteration: 33442 train shape: (68783, 48, 48, 1)\n",
            "iteration: 33443 train shape: (68784, 48, 48, 1)\n",
            "iteration: 33444 train shape: (68785, 48, 48, 1)\n",
            "iteration: 33445 train shape: (68786, 48, 48, 1)\n",
            "iteration: 33446 train shape: (68787, 48, 48, 1)\n",
            "iteration: 33447 train shape: (68788, 48, 48, 1)\n",
            "iteration: 33448 train shape: (68789, 48, 48, 1)\n",
            "iteration: 33449 train shape: (68790, 48, 48, 1)\n",
            "iteration: 33450 train shape: (68791, 48, 48, 1)\n",
            "iteration: 33451 train shape: (68792, 48, 48, 1)\n",
            "iteration: 33452 train shape: (68793, 48, 48, 1)\n",
            "iteration: 33453 train shape: (68794, 48, 48, 1)\n",
            "iteration: 33454 train shape: (68795, 48, 48, 1)\n",
            "iteration: 33455 train shape: (68796, 48, 48, 1)\n",
            "iteration: 33456 train shape: (68797, 48, 48, 1)\n",
            "iteration: 33457 train shape: (68798, 48, 48, 1)\n",
            "iteration: 33458 train shape: (68799, 48, 48, 1)\n",
            "iteration: 33459 train shape: (68800, 48, 48, 1)\n",
            "iteration: 33460 train shape: (68801, 48, 48, 1)\n",
            "iteration: 33461 train shape: (68802, 48, 48, 1)\n",
            "iteration: 33462 train shape: (68803, 48, 48, 1)\n",
            "iteration: 33463 train shape: (68804, 48, 48, 1)\n",
            "iteration: 33464 train shape: (68805, 48, 48, 1)\n",
            "iteration: 33465 train shape: (68806, 48, 48, 1)\n",
            "iteration: 33466 train shape: (68807, 48, 48, 1)\n",
            "iteration: 33467 train shape: (68808, 48, 48, 1)\n",
            "iteration: 33468 train shape: (68809, 48, 48, 1)\n",
            "iteration: 33469 train shape: (68810, 48, 48, 1)\n",
            "iteration: 33470 train shape: (68811, 48, 48, 1)\n",
            "iteration: 33471 train shape: (68812, 48, 48, 1)\n",
            "iteration: 33472 train shape: (68813, 48, 48, 1)\n",
            "iteration: 33473 train shape: (68814, 48, 48, 1)\n",
            "iteration: 33474 train shape: (68815, 48, 48, 1)\n",
            "iteration: 33475 train shape: (68816, 48, 48, 1)\n",
            "iteration: 33476 train shape: (68817, 48, 48, 1)\n",
            "iteration: 33477 train shape: (68818, 48, 48, 1)\n",
            "iteration: 33478 train shape: (68819, 48, 48, 1)\n",
            "iteration: 33479 train shape: (68820, 48, 48, 1)\n",
            "iteration: 33480 train shape: (68821, 48, 48, 1)\n",
            "iteration: 33481 train shape: (68822, 48, 48, 1)\n",
            "iteration: 33482 train shape: (68823, 48, 48, 1)\n",
            "iteration: 33483 train shape: (68824, 48, 48, 1)\n",
            "iteration: 33484 train shape: (68825, 48, 48, 1)\n",
            "iteration: 33485 train shape: (68826, 48, 48, 1)\n",
            "iteration: 33486 train shape: (68827, 48, 48, 1)\n",
            "iteration: 33487 train shape: (68828, 48, 48, 1)\n",
            "iteration: 33488 train shape: (68829, 48, 48, 1)\n",
            "iteration: 33489 train shape: (68830, 48, 48, 1)\n",
            "iteration: 33490 train shape: (68831, 48, 48, 1)\n",
            "iteration: 33491 train shape: (68832, 48, 48, 1)\n",
            "iteration: 33492 train shape: (68833, 48, 48, 1)\n",
            "iteration: 33493 train shape: (68834, 48, 48, 1)\n",
            "iteration: 33494 train shape: (68835, 48, 48, 1)\n",
            "iteration: 33495 train shape: (68836, 48, 48, 1)\n",
            "iteration: 33496 train shape: (68837, 48, 48, 1)\n",
            "iteration: 33497 train shape: (68838, 48, 48, 1)\n",
            "iteration: 33498 train shape: (68839, 48, 48, 1)\n",
            "iteration: 33499 train shape: (68840, 48, 48, 1)\n",
            "iteration: 33500 train shape: (68841, 48, 48, 1)\n",
            "iteration: 33501 train shape: (68842, 48, 48, 1)\n",
            "iteration: 33502 train shape: (68843, 48, 48, 1)\n",
            "iteration: 33503 train shape: (68844, 48, 48, 1)\n",
            "iteration: 33504 train shape: (68845, 48, 48, 1)\n",
            "iteration: 33505 train shape: (68846, 48, 48, 1)\n",
            "iteration: 33506 train shape: (68847, 48, 48, 1)\n",
            "iteration: 33507 train shape: (68848, 48, 48, 1)\n",
            "iteration: 33508 train shape: (68849, 48, 48, 1)\n",
            "iteration: 33509 train shape: (68850, 48, 48, 1)\n",
            "iteration: 33510 train shape: (68851, 48, 48, 1)\n",
            "iteration: 33511 train shape: (68852, 48, 48, 1)\n",
            "iteration: 33512 train shape: (68853, 48, 48, 1)\n",
            "iteration: 33513 train shape: (68854, 48, 48, 1)\n",
            "iteration: 33514 train shape: (68855, 48, 48, 1)\n",
            "iteration: 33515 train shape: (68856, 48, 48, 1)\n",
            "iteration: 33516 train shape: (68857, 48, 48, 1)\n",
            "iteration: 33517 train shape: (68858, 48, 48, 1)\n",
            "iteration: 33518 train shape: (68859, 48, 48, 1)\n",
            "iteration: 33519 train shape: (68860, 48, 48, 1)\n",
            "iteration: 33520 train shape: (68861, 48, 48, 1)\n",
            "iteration: 33521 train shape: (68862, 48, 48, 1)\n",
            "iteration: 33522 train shape: (68863, 48, 48, 1)\n",
            "iteration: 33523 train shape: (68864, 48, 48, 1)\n",
            "iteration: 33524 train shape: (68865, 48, 48, 1)\n",
            "iteration: 33525 train shape: (68866, 48, 48, 1)\n",
            "iteration: 33526 train shape: (68867, 48, 48, 1)\n",
            "iteration: 33527 train shape: (68868, 48, 48, 1)\n",
            "iteration: 33528 train shape: (68869, 48, 48, 1)\n",
            "iteration: 33529 train shape: (68870, 48, 48, 1)\n",
            "iteration: 33530 train shape: (68871, 48, 48, 1)\n",
            "iteration: 33531 train shape: (68872, 48, 48, 1)\n",
            "iteration: 33532 train shape: (68873, 48, 48, 1)\n",
            "iteration: 33533 train shape: (68874, 48, 48, 1)\n",
            "iteration: 33534 train shape: (68875, 48, 48, 1)\n",
            "iteration: 33535 train shape: (68876, 48, 48, 1)\n",
            "iteration: 33536 train shape: (68877, 48, 48, 1)\n",
            "iteration: 33537 train shape: (68878, 48, 48, 1)\n",
            "iteration: 33538 train shape: (68879, 48, 48, 1)\n",
            "iteration: 33539 train shape: (68880, 48, 48, 1)\n",
            "iteration: 33540 train shape: (68881, 48, 48, 1)\n",
            "iteration: 33541 train shape: (68882, 48, 48, 1)\n",
            "iteration: 33542 train shape: (68883, 48, 48, 1)\n",
            "iteration: 33543 train shape: (68884, 48, 48, 1)\n",
            "iteration: 33544 train shape: (68885, 48, 48, 1)\n",
            "iteration: 33545 train shape: (68886, 48, 48, 1)\n",
            "iteration: 33546 train shape: (68887, 48, 48, 1)\n",
            "iteration: 33547 train shape: (68888, 48, 48, 1)\n",
            "iteration: 33548 train shape: (68889, 48, 48, 1)\n",
            "iteration: 33549 train shape: (68890, 48, 48, 1)\n",
            "iteration: 33550 train shape: (68891, 48, 48, 1)\n",
            "iteration: 33551 train shape: (68892, 48, 48, 1)\n",
            "iteration: 33552 train shape: (68893, 48, 48, 1)\n",
            "iteration: 33553 train shape: (68894, 48, 48, 1)\n",
            "iteration: 33554 train shape: (68895, 48, 48, 1)\n",
            "iteration: 33555 train shape: (68896, 48, 48, 1)\n",
            "iteration: 33556 train shape: (68897, 48, 48, 1)\n",
            "iteration: 33557 train shape: (68898, 48, 48, 1)\n",
            "iteration: 33558 train shape: (68899, 48, 48, 1)\n",
            "iteration: 33559 train shape: (68900, 48, 48, 1)\n",
            "iteration: 33560 train shape: (68901, 48, 48, 1)\n",
            "iteration: 33561 train shape: (68902, 48, 48, 1)\n",
            "iteration: 33562 train shape: (68903, 48, 48, 1)\n",
            "iteration: 33563 train shape: (68904, 48, 48, 1)\n",
            "iteration: 33564 train shape: (68905, 48, 48, 1)\n",
            "iteration: 33565 train shape: (68906, 48, 48, 1)\n",
            "iteration: 33566 train shape: (68907, 48, 48, 1)\n",
            "iteration: 33567 train shape: (68908, 48, 48, 1)\n",
            "iteration: 33568 train shape: (68909, 48, 48, 1)\n",
            "iteration: 33569 train shape: (68910, 48, 48, 1)\n",
            "iteration: 33570 train shape: (68911, 48, 48, 1)\n",
            "iteration: 33571 train shape: (68912, 48, 48, 1)\n",
            "iteration: 33572 train shape: (68913, 48, 48, 1)\n",
            "iteration: 33573 train shape: (68914, 48, 48, 1)\n",
            "iteration: 33574 train shape: (68915, 48, 48, 1)\n",
            "iteration: 33575 train shape: (68916, 48, 48, 1)\n",
            "iteration: 33576 train shape: (68917, 48, 48, 1)\n",
            "iteration: 33577 train shape: (68918, 48, 48, 1)\n",
            "iteration: 33578 train shape: (68919, 48, 48, 1)\n",
            "iteration: 33579 train shape: (68920, 48, 48, 1)\n",
            "iteration: 33580 train shape: (68921, 48, 48, 1)\n",
            "iteration: 33581 train shape: (68922, 48, 48, 1)\n",
            "iteration: 33582 train shape: (68923, 48, 48, 1)\n",
            "iteration: 33583 train shape: (68924, 48, 48, 1)\n",
            "iteration: 33584 train shape: (68925, 48, 48, 1)\n",
            "iteration: 33585 train shape: (68926, 48, 48, 1)\n",
            "iteration: 33586 train shape: (68927, 48, 48, 1)\n",
            "iteration: 33587 train shape: (68928, 48, 48, 1)\n",
            "iteration: 33588 train shape: (68929, 48, 48, 1)\n",
            "iteration: 33589 train shape: (68930, 48, 48, 1)\n",
            "iteration: 33590 train shape: (68931, 48, 48, 1)\n",
            "iteration: 33591 train shape: (68932, 48, 48, 1)\n",
            "iteration: 33592 train shape: (68933, 48, 48, 1)\n",
            "iteration: 33593 train shape: (68934, 48, 48, 1)\n",
            "iteration: 33594 train shape: (68935, 48, 48, 1)\n",
            "iteration: 33595 train shape: (68936, 48, 48, 1)\n",
            "iteration: 33596 train shape: (68937, 48, 48, 1)\n",
            "iteration: 33597 train shape: (68938, 48, 48, 1)\n",
            "iteration: 33598 train shape: (68939, 48, 48, 1)\n",
            "iteration: 33599 train shape: (68940, 48, 48, 1)\n",
            "iteration: 33600 train shape: (68941, 48, 48, 1)\n",
            "iteration: 33601 train shape: (68942, 48, 48, 1)\n",
            "iteration: 33602 train shape: (68943, 48, 48, 1)\n",
            "iteration: 33603 train shape: (68944, 48, 48, 1)\n",
            "iteration: 33604 train shape: (68945, 48, 48, 1)\n",
            "iteration: 33605 train shape: (68946, 48, 48, 1)\n",
            "iteration: 33606 train shape: (68947, 48, 48, 1)\n",
            "iteration: 33607 train shape: (68948, 48, 48, 1)\n",
            "iteration: 33608 train shape: (68949, 48, 48, 1)\n",
            "iteration: 33609 train shape: (68950, 48, 48, 1)\n",
            "iteration: 33610 train shape: (68951, 48, 48, 1)\n",
            "iteration: 33611 train shape: (68952, 48, 48, 1)\n",
            "iteration: 33612 train shape: (68953, 48, 48, 1)\n",
            "iteration: 33613 train shape: (68954, 48, 48, 1)\n",
            "iteration: 33614 train shape: (68955, 48, 48, 1)\n",
            "iteration: 33615 train shape: (68956, 48, 48, 1)\n",
            "iteration: 33616 train shape: (68957, 48, 48, 1)\n",
            "iteration: 33617 train shape: (68958, 48, 48, 1)\n",
            "iteration: 33618 train shape: (68959, 48, 48, 1)\n",
            "iteration: 33619 train shape: (68960, 48, 48, 1)\n",
            "iteration: 33620 train shape: (68961, 48, 48, 1)\n",
            "iteration: 33621 train shape: (68962, 48, 48, 1)\n",
            "iteration: 33622 train shape: (68963, 48, 48, 1)\n",
            "iteration: 33623 train shape: (68964, 48, 48, 1)\n",
            "iteration: 33624 train shape: (68965, 48, 48, 1)\n",
            "iteration: 33625 train shape: (68966, 48, 48, 1)\n",
            "iteration: 33626 train shape: (68967, 48, 48, 1)\n",
            "iteration: 33627 train shape: (68968, 48, 48, 1)\n",
            "iteration: 33628 train shape: (68969, 48, 48, 1)\n",
            "iteration: 33629 train shape: (68970, 48, 48, 1)\n",
            "iteration: 33630 train shape: (68971, 48, 48, 1)\n",
            "iteration: 33631 train shape: (68972, 48, 48, 1)\n",
            "iteration: 33632 train shape: (68973, 48, 48, 1)\n",
            "iteration: 33633 train shape: (68974, 48, 48, 1)\n",
            "iteration: 33634 train shape: (68975, 48, 48, 1)\n",
            "iteration: 33635 train shape: (68976, 48, 48, 1)\n",
            "iteration: 33636 train shape: (68977, 48, 48, 1)\n",
            "iteration: 33637 train shape: (68978, 48, 48, 1)\n",
            "iteration: 33638 train shape: (68979, 48, 48, 1)\n",
            "iteration: 33639 train shape: (68980, 48, 48, 1)\n",
            "iteration: 33640 train shape: (68981, 48, 48, 1)\n",
            "iteration: 33641 train shape: (68982, 48, 48, 1)\n",
            "iteration: 33642 train shape: (68983, 48, 48, 1)\n",
            "iteration: 33643 train shape: (68984, 48, 48, 1)\n",
            "iteration: 33644 train shape: (68985, 48, 48, 1)\n",
            "iteration: 33645 train shape: (68986, 48, 48, 1)\n",
            "iteration: 33646 train shape: (68987, 48, 48, 1)\n",
            "iteration: 33647 train shape: (68988, 48, 48, 1)\n",
            "iteration: 33648 train shape: (68989, 48, 48, 1)\n",
            "iteration: 33649 train shape: (68990, 48, 48, 1)\n",
            "iteration: 33650 train shape: (68991, 48, 48, 1)\n",
            "iteration: 33651 train shape: (68992, 48, 48, 1)\n",
            "iteration: 33652 train shape: (68993, 48, 48, 1)\n",
            "iteration: 33653 train shape: (68994, 48, 48, 1)\n",
            "iteration: 33654 train shape: (68995, 48, 48, 1)\n",
            "iteration: 33655 train shape: (68996, 48, 48, 1)\n",
            "iteration: 33656 train shape: (68997, 48, 48, 1)\n",
            "iteration: 33657 train shape: (68998, 48, 48, 1)\n",
            "iteration: 33658 train shape: (68999, 48, 48, 1)\n",
            "iteration: 33659 train shape: (69000, 48, 48, 1)\n",
            "iteration: 33660 train shape: (69001, 48, 48, 1)\n",
            "iteration: 33661 train shape: (69002, 48, 48, 1)\n",
            "iteration: 33662 train shape: (69003, 48, 48, 1)\n",
            "iteration: 33663 train shape: (69004, 48, 48, 1)\n",
            "iteration: 33664 train shape: (69005, 48, 48, 1)\n",
            "iteration: 33665 train shape: (69006, 48, 48, 1)\n",
            "iteration: 33666 train shape: (69007, 48, 48, 1)\n",
            "iteration: 33667 train shape: (69008, 48, 48, 1)\n",
            "iteration: 33668 train shape: (69009, 48, 48, 1)\n",
            "iteration: 33669 train shape: (69010, 48, 48, 1)\n",
            "iteration: 33670 train shape: (69011, 48, 48, 1)\n",
            "iteration: 33671 train shape: (69012, 48, 48, 1)\n",
            "iteration: 33672 train shape: (69013, 48, 48, 1)\n",
            "iteration: 33673 train shape: (69014, 48, 48, 1)\n",
            "iteration: 33674 train shape: (69015, 48, 48, 1)\n",
            "iteration: 33675 train shape: (69016, 48, 48, 1)\n",
            "iteration: 33676 train shape: (69017, 48, 48, 1)\n",
            "iteration: 33677 train shape: (69018, 48, 48, 1)\n",
            "iteration: 33678 train shape: (69019, 48, 48, 1)\n",
            "iteration: 33679 train shape: (69020, 48, 48, 1)\n",
            "iteration: 33680 train shape: (69021, 48, 48, 1)\n",
            "iteration: 33681 train shape: (69022, 48, 48, 1)\n",
            "iteration: 33682 train shape: (69023, 48, 48, 1)\n",
            "iteration: 33683 train shape: (69024, 48, 48, 1)\n",
            "iteration: 33684 train shape: (69025, 48, 48, 1)\n",
            "iteration: 33685 train shape: (69026, 48, 48, 1)\n",
            "iteration: 33686 train shape: (69027, 48, 48, 1)\n",
            "iteration: 33687 train shape: (69028, 48, 48, 1)\n",
            "iteration: 33688 train shape: (69029, 48, 48, 1)\n",
            "iteration: 33689 train shape: (69030, 48, 48, 1)\n",
            "iteration: 33690 train shape: (69031, 48, 48, 1)\n",
            "iteration: 33691 train shape: (69032, 48, 48, 1)\n",
            "iteration: 33692 train shape: (69033, 48, 48, 1)\n",
            "iteration: 33693 train shape: (69034, 48, 48, 1)\n",
            "iteration: 33694 train shape: (69035, 48, 48, 1)\n",
            "iteration: 33695 train shape: (69036, 48, 48, 1)\n",
            "iteration: 33696 train shape: (69037, 48, 48, 1)\n",
            "iteration: 33697 train shape: (69038, 48, 48, 1)\n",
            "iteration: 33698 train shape: (69039, 48, 48, 1)\n",
            "iteration: 33699 train shape: (69040, 48, 48, 1)\n",
            "iteration: 33700 train shape: (69041, 48, 48, 1)\n",
            "iteration: 33701 train shape: (69042, 48, 48, 1)\n",
            "iteration: 33702 train shape: (69043, 48, 48, 1)\n",
            "iteration: 33703 train shape: (69044, 48, 48, 1)\n",
            "iteration: 33704 train shape: (69045, 48, 48, 1)\n",
            "iteration: 33705 train shape: (69046, 48, 48, 1)\n",
            "iteration: 33706 train shape: (69047, 48, 48, 1)\n",
            "iteration: 33707 train shape: (69048, 48, 48, 1)\n",
            "iteration: 33708 train shape: (69049, 48, 48, 1)\n",
            "iteration: 33709 train shape: (69050, 48, 48, 1)\n",
            "iteration: 33710 train shape: (69051, 48, 48, 1)\n",
            "iteration: 33711 train shape: (69052, 48, 48, 1)\n",
            "iteration: 33712 train shape: (69053, 48, 48, 1)\n",
            "iteration: 33713 train shape: (69054, 48, 48, 1)\n",
            "iteration: 33714 train shape: (69055, 48, 48, 1)\n",
            "iteration: 33715 train shape: (69056, 48, 48, 1)\n",
            "iteration: 33716 train shape: (69057, 48, 48, 1)\n",
            "iteration: 33717 train shape: (69058, 48, 48, 1)\n",
            "iteration: 33718 train shape: (69059, 48, 48, 1)\n",
            "iteration: 33719 train shape: (69060, 48, 48, 1)\n",
            "iteration: 33720 train shape: (69061, 48, 48, 1)\n",
            "iteration: 33721 train shape: (69062, 48, 48, 1)\n",
            "iteration: 33722 train shape: (69063, 48, 48, 1)\n",
            "iteration: 33723 train shape: (69064, 48, 48, 1)\n",
            "iteration: 33724 train shape: (69065, 48, 48, 1)\n",
            "iteration: 33725 train shape: (69066, 48, 48, 1)\n",
            "iteration: 33726 train shape: (69067, 48, 48, 1)\n",
            "iteration: 33727 train shape: (69068, 48, 48, 1)\n",
            "iteration: 33728 train shape: (69069, 48, 48, 1)\n",
            "iteration: 33729 train shape: (69070, 48, 48, 1)\n",
            "iteration: 33730 train shape: (69071, 48, 48, 1)\n",
            "iteration: 33731 train shape: (69072, 48, 48, 1)\n",
            "iteration: 33732 train shape: (69073, 48, 48, 1)\n",
            "iteration: 33733 train shape: (69074, 48, 48, 1)\n",
            "iteration: 33734 train shape: (69075, 48, 48, 1)\n",
            "iteration: 33735 train shape: (69076, 48, 48, 1)\n",
            "iteration: 33736 train shape: (69077, 48, 48, 1)\n",
            "iteration: 33737 train shape: (69078, 48, 48, 1)\n",
            "iteration: 33738 train shape: (69079, 48, 48, 1)\n",
            "iteration: 33739 train shape: (69080, 48, 48, 1)\n",
            "iteration: 33740 train shape: (69081, 48, 48, 1)\n",
            "iteration: 33741 train shape: (69082, 48, 48, 1)\n",
            "iteration: 33742 train shape: (69083, 48, 48, 1)\n",
            "iteration: 33743 train shape: (69084, 48, 48, 1)\n",
            "iteration: 33744 train shape: (69085, 48, 48, 1)\n",
            "iteration: 33745 train shape: (69086, 48, 48, 1)\n",
            "iteration: 33746 train shape: (69087, 48, 48, 1)\n",
            "iteration: 33747 train shape: (69088, 48, 48, 1)\n",
            "iteration: 33748 train shape: (69089, 48, 48, 1)\n",
            "iteration: 33749 train shape: (69090, 48, 48, 1)\n",
            "iteration: 33750 train shape: (69091, 48, 48, 1)\n",
            "iteration: 33751 train shape: (69092, 48, 48, 1)\n",
            "iteration: 33752 train shape: (69093, 48, 48, 1)\n",
            "iteration: 33753 train shape: (69094, 48, 48, 1)\n",
            "iteration: 33754 train shape: (69095, 48, 48, 1)\n",
            "iteration: 33755 train shape: (69096, 48, 48, 1)\n",
            "iteration: 33756 train shape: (69097, 48, 48, 1)\n",
            "iteration: 33757 train shape: (69098, 48, 48, 1)\n",
            "iteration: 33758 train shape: (69099, 48, 48, 1)\n",
            "iteration: 33759 train shape: (69100, 48, 48, 1)\n",
            "iteration: 33760 train shape: (69101, 48, 48, 1)\n",
            "iteration: 33761 train shape: (69102, 48, 48, 1)\n",
            "iteration: 33762 train shape: (69103, 48, 48, 1)\n",
            "iteration: 33763 train shape: (69104, 48, 48, 1)\n",
            "iteration: 33764 train shape: (69105, 48, 48, 1)\n",
            "iteration: 33765 train shape: (69106, 48, 48, 1)\n",
            "iteration: 33766 train shape: (69107, 48, 48, 1)\n",
            "iteration: 33767 train shape: (69108, 48, 48, 1)\n",
            "iteration: 33768 train shape: (69109, 48, 48, 1)\n",
            "iteration: 33769 train shape: (69110, 48, 48, 1)\n",
            "iteration: 33770 train shape: (69111, 48, 48, 1)\n",
            "iteration: 33771 train shape: (69112, 48, 48, 1)\n",
            "iteration: 33772 train shape: (69113, 48, 48, 1)\n",
            "iteration: 33773 train shape: (69114, 48, 48, 1)\n",
            "iteration: 33774 train shape: (69115, 48, 48, 1)\n",
            "iteration: 33775 train shape: (69116, 48, 48, 1)\n",
            "iteration: 33776 train shape: (69117, 48, 48, 1)\n",
            "iteration: 33777 train shape: (69118, 48, 48, 1)\n",
            "iteration: 33778 train shape: (69119, 48, 48, 1)\n",
            "iteration: 33779 train shape: (69120, 48, 48, 1)\n",
            "iteration: 33780 train shape: (69121, 48, 48, 1)\n",
            "iteration: 33781 train shape: (69122, 48, 48, 1)\n",
            "iteration: 33782 train shape: (69123, 48, 48, 1)\n",
            "iteration: 33783 train shape: (69124, 48, 48, 1)\n",
            "iteration: 33784 train shape: (69125, 48, 48, 1)\n",
            "iteration: 33785 train shape: (69126, 48, 48, 1)\n",
            "iteration: 33786 train shape: (69127, 48, 48, 1)\n",
            "iteration: 33787 train shape: (69128, 48, 48, 1)\n",
            "iteration: 33788 train shape: (69129, 48, 48, 1)\n",
            "iteration: 33789 train shape: (69130, 48, 48, 1)\n",
            "iteration: 33790 train shape: (69131, 48, 48, 1)\n",
            "iteration: 33791 train shape: (69132, 48, 48, 1)\n",
            "iteration: 33792 train shape: (69133, 48, 48, 1)\n",
            "iteration: 33793 train shape: (69134, 48, 48, 1)\n",
            "iteration: 33794 train shape: (69135, 48, 48, 1)\n",
            "iteration: 33795 train shape: (69136, 48, 48, 1)\n",
            "iteration: 33796 train shape: (69137, 48, 48, 1)\n",
            "iteration: 33797 train shape: (69138, 48, 48, 1)\n",
            "iteration: 33798 train shape: (69139, 48, 48, 1)\n",
            "iteration: 33799 train shape: (69140, 48, 48, 1)\n",
            "iteration: 33800 train shape: (69141, 48, 48, 1)\n",
            "iteration: 33801 train shape: (69142, 48, 48, 1)\n",
            "iteration: 33802 train shape: (69143, 48, 48, 1)\n",
            "iteration: 33803 train shape: (69144, 48, 48, 1)\n",
            "iteration: 33804 train shape: (69145, 48, 48, 1)\n",
            "iteration: 33805 train shape: (69146, 48, 48, 1)\n",
            "iteration: 33806 train shape: (69147, 48, 48, 1)\n",
            "iteration: 33807 train shape: (69148, 48, 48, 1)\n",
            "iteration: 33808 train shape: (69149, 48, 48, 1)\n",
            "iteration: 33809 train shape: (69150, 48, 48, 1)\n",
            "iteration: 33810 train shape: (69151, 48, 48, 1)\n",
            "iteration: 33811 train shape: (69152, 48, 48, 1)\n",
            "iteration: 33812 train shape: (69153, 48, 48, 1)\n",
            "iteration: 33813 train shape: (69154, 48, 48, 1)\n",
            "iteration: 33814 train shape: (69155, 48, 48, 1)\n",
            "iteration: 33815 train shape: (69156, 48, 48, 1)\n",
            "iteration: 33816 train shape: (69157, 48, 48, 1)\n",
            "iteration: 33817 train shape: (69158, 48, 48, 1)\n",
            "iteration: 33818 train shape: (69159, 48, 48, 1)\n",
            "iteration: 33819 train shape: (69160, 48, 48, 1)\n",
            "iteration: 33820 train shape: (69161, 48, 48, 1)\n",
            "iteration: 33821 train shape: (69162, 48, 48, 1)\n",
            "iteration: 33822 train shape: (69163, 48, 48, 1)\n",
            "iteration: 33823 train shape: (69164, 48, 48, 1)\n",
            "iteration: 33824 train shape: (69165, 48, 48, 1)\n",
            "iteration: 33825 train shape: (69166, 48, 48, 1)\n",
            "iteration: 33826 train shape: (69167, 48, 48, 1)\n",
            "iteration: 33827 train shape: (69168, 48, 48, 1)\n",
            "iteration: 33828 train shape: (69169, 48, 48, 1)\n",
            "iteration: 33829 train shape: (69170, 48, 48, 1)\n",
            "iteration: 33830 train shape: (69171, 48, 48, 1)\n",
            "iteration: 33831 train shape: (69172, 48, 48, 1)\n",
            "iteration: 33832 train shape: (69173, 48, 48, 1)\n",
            "iteration: 33833 train shape: (69174, 48, 48, 1)\n",
            "iteration: 33834 train shape: (69175, 48, 48, 1)\n",
            "iteration: 33835 train shape: (69176, 48, 48, 1)\n",
            "iteration: 33836 train shape: (69177, 48, 48, 1)\n",
            "iteration: 33837 train shape: (69178, 48, 48, 1)\n",
            "iteration: 33838 train shape: (69179, 48, 48, 1)\n",
            "iteration: 33839 train shape: (69180, 48, 48, 1)\n",
            "iteration: 33840 train shape: (69181, 48, 48, 1)\n",
            "iteration: 33841 train shape: (69182, 48, 48, 1)\n",
            "iteration: 33842 train shape: (69183, 48, 48, 1)\n",
            "iteration: 33843 train shape: (69184, 48, 48, 1)\n",
            "iteration: 33844 train shape: (69185, 48, 48, 1)\n",
            "iteration: 33845 train shape: (69186, 48, 48, 1)\n",
            "iteration: 33846 train shape: (69187, 48, 48, 1)\n",
            "iteration: 33847 train shape: (69188, 48, 48, 1)\n",
            "iteration: 33848 train shape: (69189, 48, 48, 1)\n",
            "iteration: 33849 train shape: (69190, 48, 48, 1)\n",
            "iteration: 33850 train shape: (69191, 48, 48, 1)\n",
            "iteration: 33851 train shape: (69192, 48, 48, 1)\n",
            "iteration: 33852 train shape: (69193, 48, 48, 1)\n",
            "iteration: 33853 train shape: (69194, 48, 48, 1)\n",
            "iteration: 33854 train shape: (69195, 48, 48, 1)\n",
            "iteration: 33855 train shape: (69196, 48, 48, 1)\n",
            "iteration: 33856 train shape: (69197, 48, 48, 1)\n",
            "iteration: 33857 train shape: (69198, 48, 48, 1)\n",
            "iteration: 33858 train shape: (69199, 48, 48, 1)\n",
            "iteration: 33859 train shape: (69200, 48, 48, 1)\n",
            "iteration: 33860 train shape: (69201, 48, 48, 1)\n",
            "iteration: 33861 train shape: (69202, 48, 48, 1)\n",
            "iteration: 33862 train shape: (69203, 48, 48, 1)\n",
            "iteration: 33863 train shape: (69204, 48, 48, 1)\n",
            "iteration: 33864 train shape: (69205, 48, 48, 1)\n",
            "iteration: 33865 train shape: (69206, 48, 48, 1)\n",
            "iteration: 33866 train shape: (69207, 48, 48, 1)\n",
            "iteration: 33867 train shape: (69208, 48, 48, 1)\n",
            "iteration: 33868 train shape: (69209, 48, 48, 1)\n",
            "iteration: 33869 train shape: (69210, 48, 48, 1)\n",
            "iteration: 33870 train shape: (69211, 48, 48, 1)\n",
            "iteration: 33871 train shape: (69212, 48, 48, 1)\n",
            "iteration: 33872 train shape: (69213, 48, 48, 1)\n",
            "iteration: 33873 train shape: (69214, 48, 48, 1)\n",
            "iteration: 33874 train shape: (69215, 48, 48, 1)\n",
            "iteration: 33875 train shape: (69216, 48, 48, 1)\n",
            "iteration: 33876 train shape: (69217, 48, 48, 1)\n",
            "iteration: 33877 train shape: (69218, 48, 48, 1)\n",
            "iteration: 33878 train shape: (69219, 48, 48, 1)\n",
            "iteration: 33879 train shape: (69220, 48, 48, 1)\n",
            "iteration: 33880 train shape: (69221, 48, 48, 1)\n",
            "iteration: 33881 train shape: (69222, 48, 48, 1)\n",
            "iteration: 33882 train shape: (69223, 48, 48, 1)\n",
            "iteration: 33883 train shape: (69224, 48, 48, 1)\n",
            "iteration: 33884 train shape: (69225, 48, 48, 1)\n",
            "iteration: 33885 train shape: (69226, 48, 48, 1)\n",
            "iteration: 33886 train shape: (69227, 48, 48, 1)\n",
            "iteration: 33887 train shape: (69228, 48, 48, 1)\n",
            "iteration: 33888 train shape: (69229, 48, 48, 1)\n",
            "iteration: 33889 train shape: (69230, 48, 48, 1)\n",
            "iteration: 33890 train shape: (69231, 48, 48, 1)\n",
            "iteration: 33891 train shape: (69232, 48, 48, 1)\n",
            "iteration: 33892 train shape: (69233, 48, 48, 1)\n",
            "iteration: 33893 train shape: (69234, 48, 48, 1)\n",
            "iteration: 33894 train shape: (69235, 48, 48, 1)\n",
            "iteration: 33895 train shape: (69236, 48, 48, 1)\n",
            "iteration: 33896 train shape: (69237, 48, 48, 1)\n",
            "iteration: 33897 train shape: (69238, 48, 48, 1)\n",
            "iteration: 33898 train shape: (69239, 48, 48, 1)\n",
            "iteration: 33899 train shape: (69240, 48, 48, 1)\n",
            "iteration: 33900 train shape: (69241, 48, 48, 1)\n",
            "iteration: 33901 train shape: (69242, 48, 48, 1)\n",
            "iteration: 33902 train shape: (69243, 48, 48, 1)\n",
            "iteration: 33903 train shape: (69244, 48, 48, 1)\n",
            "iteration: 33904 train shape: (69245, 48, 48, 1)\n",
            "iteration: 33905 train shape: (69246, 48, 48, 1)\n",
            "iteration: 33906 train shape: (69247, 48, 48, 1)\n",
            "iteration: 33907 train shape: (69248, 48, 48, 1)\n",
            "iteration: 33908 train shape: (69249, 48, 48, 1)\n",
            "iteration: 33909 train shape: (69250, 48, 48, 1)\n",
            "iteration: 33910 train shape: (69251, 48, 48, 1)\n",
            "iteration: 33911 train shape: (69252, 48, 48, 1)\n",
            "iteration: 33912 train shape: (69253, 48, 48, 1)\n",
            "iteration: 33913 train shape: (69254, 48, 48, 1)\n",
            "iteration: 33914 train shape: (69255, 48, 48, 1)\n",
            "iteration: 33915 train shape: (69256, 48, 48, 1)\n",
            "iteration: 33916 train shape: (69257, 48, 48, 1)\n",
            "iteration: 33917 train shape: (69258, 48, 48, 1)\n",
            "iteration: 33918 train shape: (69259, 48, 48, 1)\n",
            "iteration: 33919 train shape: (69260, 48, 48, 1)\n",
            "iteration: 33920 train shape: (69261, 48, 48, 1)\n",
            "iteration: 33921 train shape: (69262, 48, 48, 1)\n",
            "iteration: 33922 train shape: (69263, 48, 48, 1)\n",
            "iteration: 33923 train shape: (69264, 48, 48, 1)\n",
            "iteration: 33924 train shape: (69265, 48, 48, 1)\n",
            "iteration: 33925 train shape: (69266, 48, 48, 1)\n",
            "iteration: 33926 train shape: (69267, 48, 48, 1)\n",
            "iteration: 33927 train shape: (69268, 48, 48, 1)\n",
            "iteration: 33928 train shape: (69269, 48, 48, 1)\n",
            "iteration: 33929 train shape: (69270, 48, 48, 1)\n",
            "iteration: 33930 train shape: (69271, 48, 48, 1)\n",
            "iteration: 33931 train shape: (69272, 48, 48, 1)\n",
            "iteration: 33932 train shape: (69273, 48, 48, 1)\n",
            "iteration: 33933 train shape: (69274, 48, 48, 1)\n",
            "iteration: 33934 train shape: (69275, 48, 48, 1)\n",
            "iteration: 33935 train shape: (69276, 48, 48, 1)\n",
            "iteration: 33936 train shape: (69277, 48, 48, 1)\n",
            "iteration: 33937 train shape: (69278, 48, 48, 1)\n",
            "iteration: 33938 train shape: (69279, 48, 48, 1)\n",
            "iteration: 33939 train shape: (69280, 48, 48, 1)\n",
            "iteration: 33940 train shape: (69281, 48, 48, 1)\n",
            "iteration: 33941 train shape: (69282, 48, 48, 1)\n",
            "iteration: 33942 train shape: (69283, 48, 48, 1)\n",
            "iteration: 33943 train shape: (69284, 48, 48, 1)\n",
            "iteration: 33944 train shape: (69285, 48, 48, 1)\n",
            "iteration: 33945 train shape: (69286, 48, 48, 1)\n",
            "iteration: 33946 train shape: (69287, 48, 48, 1)\n",
            "iteration: 33947 train shape: (69288, 48, 48, 1)\n",
            "iteration: 33948 train shape: (69289, 48, 48, 1)\n",
            "iteration: 33949 train shape: (69290, 48, 48, 1)\n",
            "iteration: 33950 train shape: (69291, 48, 48, 1)\n",
            "iteration: 33951 train shape: (69292, 48, 48, 1)\n",
            "iteration: 33952 train shape: (69293, 48, 48, 1)\n",
            "iteration: 33953 train shape: (69294, 48, 48, 1)\n",
            "iteration: 33954 train shape: (69295, 48, 48, 1)\n",
            "iteration: 33955 train shape: (69296, 48, 48, 1)\n",
            "iteration: 33956 train shape: (69297, 48, 48, 1)\n",
            "iteration: 33957 train shape: (69298, 48, 48, 1)\n",
            "iteration: 33958 train shape: (69299, 48, 48, 1)\n",
            "iteration: 33959 train shape: (69300, 48, 48, 1)\n",
            "iteration: 33960 train shape: (69301, 48, 48, 1)\n",
            "iteration: 33961 train shape: (69302, 48, 48, 1)\n",
            "iteration: 33962 train shape: (69303, 48, 48, 1)\n",
            "iteration: 33963 train shape: (69304, 48, 48, 1)\n",
            "iteration: 33964 train shape: (69305, 48, 48, 1)\n",
            "iteration: 33965 train shape: (69306, 48, 48, 1)\n",
            "iteration: 33966 train shape: (69307, 48, 48, 1)\n",
            "iteration: 33967 train shape: (69308, 48, 48, 1)\n",
            "iteration: 33968 train shape: (69309, 48, 48, 1)\n",
            "iteration: 33969 train shape: (69310, 48, 48, 1)\n",
            "iteration: 33970 train shape: (69311, 48, 48, 1)\n",
            "iteration: 33971 train shape: (69312, 48, 48, 1)\n",
            "iteration: 33972 train shape: (69313, 48, 48, 1)\n",
            "iteration: 33973 train shape: (69314, 48, 48, 1)\n",
            "iteration: 33974 train shape: (69315, 48, 48, 1)\n",
            "iteration: 33975 train shape: (69316, 48, 48, 1)\n",
            "iteration: 33976 train shape: (69317, 48, 48, 1)\n",
            "iteration: 33977 train shape: (69318, 48, 48, 1)\n",
            "iteration: 33978 train shape: (69319, 48, 48, 1)\n",
            "iteration: 33979 train shape: (69320, 48, 48, 1)\n",
            "iteration: 33980 train shape: (69321, 48, 48, 1)\n",
            "iteration: 33981 train shape: (69322, 48, 48, 1)\n",
            "iteration: 33982 train shape: (69323, 48, 48, 1)\n",
            "iteration: 33983 train shape: (69324, 48, 48, 1)\n",
            "iteration: 33984 train shape: (69325, 48, 48, 1)\n",
            "iteration: 33985 train shape: (69326, 48, 48, 1)\n",
            "iteration: 33986 train shape: (69327, 48, 48, 1)\n",
            "iteration: 33987 train shape: (69328, 48, 48, 1)\n",
            "iteration: 33988 train shape: (69329, 48, 48, 1)\n",
            "iteration: 33989 train shape: (69330, 48, 48, 1)\n",
            "iteration: 33990 train shape: (69331, 48, 48, 1)\n",
            "iteration: 33991 train shape: (69332, 48, 48, 1)\n",
            "iteration: 33992 train shape: (69333, 48, 48, 1)\n",
            "iteration: 33993 train shape: (69334, 48, 48, 1)\n",
            "iteration: 33994 train shape: (69335, 48, 48, 1)\n",
            "iteration: 33995 train shape: (69336, 48, 48, 1)\n",
            "iteration: 33996 train shape: (69337, 48, 48, 1)\n",
            "iteration: 33997 train shape: (69338, 48, 48, 1)\n",
            "iteration: 33998 train shape: (69339, 48, 48, 1)\n",
            "iteration: 33999 train shape: (69340, 48, 48, 1)\n",
            "iteration: 34000 train shape: (69341, 48, 48, 1)\n",
            "iteration: 34001 train shape: (69342, 48, 48, 1)\n",
            "iteration: 34002 train shape: (69343, 48, 48, 1)\n",
            "iteration: 34003 train shape: (69344, 48, 48, 1)\n",
            "iteration: 34004 train shape: (69345, 48, 48, 1)\n",
            "iteration: 34005 train shape: (69346, 48, 48, 1)\n",
            "iteration: 34006 train shape: (69347, 48, 48, 1)\n",
            "iteration: 34007 train shape: (69348, 48, 48, 1)\n",
            "iteration: 34008 train shape: (69349, 48, 48, 1)\n",
            "iteration: 34009 train shape: (69350, 48, 48, 1)\n",
            "iteration: 34010 train shape: (69351, 48, 48, 1)\n",
            "iteration: 34011 train shape: (69352, 48, 48, 1)\n",
            "iteration: 34012 train shape: (69353, 48, 48, 1)\n",
            "iteration: 34013 train shape: (69354, 48, 48, 1)\n",
            "iteration: 34014 train shape: (69355, 48, 48, 1)\n",
            "iteration: 34015 train shape: (69356, 48, 48, 1)\n",
            "iteration: 34016 train shape: (69357, 48, 48, 1)\n",
            "iteration: 34017 train shape: (69358, 48, 48, 1)\n",
            "iteration: 34018 train shape: (69359, 48, 48, 1)\n",
            "iteration: 34019 train shape: (69360, 48, 48, 1)\n",
            "iteration: 34020 train shape: (69361, 48, 48, 1)\n",
            "iteration: 34021 train shape: (69362, 48, 48, 1)\n",
            "iteration: 34022 train shape: (69363, 48, 48, 1)\n",
            "iteration: 34023 train shape: (69364, 48, 48, 1)\n",
            "iteration: 34024 train shape: (69365, 48, 48, 1)\n",
            "iteration: 34025 train shape: (69366, 48, 48, 1)\n",
            "iteration: 34026 train shape: (69367, 48, 48, 1)\n",
            "iteration: 34027 train shape: (69368, 48, 48, 1)\n",
            "iteration: 34028 train shape: (69369, 48, 48, 1)\n",
            "iteration: 34029 train shape: (69370, 48, 48, 1)\n",
            "iteration: 34030 train shape: (69371, 48, 48, 1)\n",
            "iteration: 34031 train shape: (69372, 48, 48, 1)\n",
            "iteration: 34032 train shape: (69373, 48, 48, 1)\n",
            "iteration: 34033 train shape: (69374, 48, 48, 1)\n",
            "iteration: 34034 train shape: (69375, 48, 48, 1)\n",
            "iteration: 34035 train shape: (69376, 48, 48, 1)\n",
            "iteration: 34036 train shape: (69377, 48, 48, 1)\n",
            "iteration: 34037 train shape: (69378, 48, 48, 1)\n",
            "iteration: 34038 train shape: (69379, 48, 48, 1)\n",
            "iteration: 34039 train shape: (69380, 48, 48, 1)\n",
            "iteration: 34040 train shape: (69381, 48, 48, 1)\n",
            "iteration: 34041 train shape: (69382, 48, 48, 1)\n",
            "iteration: 34042 train shape: (69383, 48, 48, 1)\n",
            "iteration: 34043 train shape: (69384, 48, 48, 1)\n",
            "iteration: 34044 train shape: (69385, 48, 48, 1)\n",
            "iteration: 34045 train shape: (69386, 48, 48, 1)\n",
            "iteration: 34046 train shape: (69387, 48, 48, 1)\n",
            "iteration: 34047 train shape: (69388, 48, 48, 1)\n",
            "iteration: 34048 train shape: (69389, 48, 48, 1)\n",
            "iteration: 34049 train shape: (69390, 48, 48, 1)\n",
            "iteration: 34050 train shape: (69391, 48, 48, 1)\n",
            "iteration: 34051 train shape: (69392, 48, 48, 1)\n",
            "iteration: 34052 train shape: (69393, 48, 48, 1)\n",
            "iteration: 34053 train shape: (69394, 48, 48, 1)\n",
            "iteration: 34054 train shape: (69395, 48, 48, 1)\n",
            "iteration: 34055 train shape: (69396, 48, 48, 1)\n",
            "iteration: 34056 train shape: (69397, 48, 48, 1)\n",
            "iteration: 34057 train shape: (69398, 48, 48, 1)\n",
            "iteration: 34058 train shape: (69399, 48, 48, 1)\n",
            "iteration: 34059 train shape: (69400, 48, 48, 1)\n",
            "iteration: 34060 train shape: (69401, 48, 48, 1)\n",
            "iteration: 34061 train shape: (69402, 48, 48, 1)\n",
            "iteration: 34062 train shape: (69403, 48, 48, 1)\n",
            "iteration: 34063 train shape: (69404, 48, 48, 1)\n",
            "iteration: 34064 train shape: (69405, 48, 48, 1)\n",
            "iteration: 34065 train shape: (69406, 48, 48, 1)\n",
            "iteration: 34066 train shape: (69407, 48, 48, 1)\n",
            "iteration: 34067 train shape: (69408, 48, 48, 1)\n",
            "iteration: 34068 train shape: (69409, 48, 48, 1)\n",
            "iteration: 34069 train shape: (69410, 48, 48, 1)\n",
            "iteration: 34070 train shape: (69411, 48, 48, 1)\n",
            "iteration: 34071 train shape: (69412, 48, 48, 1)\n",
            "iteration: 34072 train shape: (69413, 48, 48, 1)\n",
            "iteration: 34073 train shape: (69414, 48, 48, 1)\n",
            "iteration: 34074 train shape: (69415, 48, 48, 1)\n",
            "iteration: 34075 train shape: (69416, 48, 48, 1)\n",
            "iteration: 34076 train shape: (69417, 48, 48, 1)\n",
            "iteration: 34077 train shape: (69418, 48, 48, 1)\n",
            "iteration: 34078 train shape: (69419, 48, 48, 1)\n",
            "iteration: 34079 train shape: (69420, 48, 48, 1)\n",
            "iteration: 34080 train shape: (69421, 48, 48, 1)\n",
            "iteration: 34081 train shape: (69422, 48, 48, 1)\n",
            "iteration: 34082 train shape: (69423, 48, 48, 1)\n",
            "iteration: 34083 train shape: (69424, 48, 48, 1)\n",
            "iteration: 34084 train shape: (69425, 48, 48, 1)\n",
            "iteration: 34085 train shape: (69426, 48, 48, 1)\n",
            "iteration: 34086 train shape: (69427, 48, 48, 1)\n",
            "iteration: 34087 train shape: (69428, 48, 48, 1)\n",
            "iteration: 34088 train shape: (69429, 48, 48, 1)\n",
            "iteration: 34089 train shape: (69430, 48, 48, 1)\n",
            "iteration: 34090 train shape: (69431, 48, 48, 1)\n",
            "iteration: 34091 train shape: (69432, 48, 48, 1)\n",
            "iteration: 34092 train shape: (69433, 48, 48, 1)\n",
            "iteration: 34093 train shape: (69434, 48, 48, 1)\n",
            "iteration: 34094 train shape: (69435, 48, 48, 1)\n",
            "iteration: 34095 train shape: (69436, 48, 48, 1)\n",
            "iteration: 34096 train shape: (69437, 48, 48, 1)\n",
            "iteration: 34097 train shape: (69438, 48, 48, 1)\n",
            "iteration: 34098 train shape: (69439, 48, 48, 1)\n",
            "iteration: 34099 train shape: (69440, 48, 48, 1)\n",
            "iteration: 34100 train shape: (69441, 48, 48, 1)\n",
            "iteration: 34101 train shape: (69442, 48, 48, 1)\n",
            "iteration: 34102 train shape: (69443, 48, 48, 1)\n",
            "iteration: 34103 train shape: (69444, 48, 48, 1)\n",
            "iteration: 34104 train shape: (69445, 48, 48, 1)\n",
            "iteration: 34105 train shape: (69446, 48, 48, 1)\n",
            "iteration: 34106 train shape: (69447, 48, 48, 1)\n",
            "iteration: 34107 train shape: (69448, 48, 48, 1)\n",
            "iteration: 34108 train shape: (69449, 48, 48, 1)\n",
            "iteration: 34109 train shape: (69450, 48, 48, 1)\n",
            "iteration: 34110 train shape: (69451, 48, 48, 1)\n",
            "iteration: 34111 train shape: (69452, 48, 48, 1)\n",
            "iteration: 34112 train shape: (69453, 48, 48, 1)\n",
            "iteration: 34113 train shape: (69454, 48, 48, 1)\n",
            "iteration: 34114 train shape: (69455, 48, 48, 1)\n",
            "iteration: 34115 train shape: (69456, 48, 48, 1)\n",
            "iteration: 34116 train shape: (69457, 48, 48, 1)\n",
            "iteration: 34117 train shape: (69458, 48, 48, 1)\n",
            "iteration: 34118 train shape: (69459, 48, 48, 1)\n",
            "iteration: 34119 train shape: (69460, 48, 48, 1)\n",
            "iteration: 34120 train shape: (69461, 48, 48, 1)\n",
            "iteration: 34121 train shape: (69462, 48, 48, 1)\n",
            "iteration: 34122 train shape: (69463, 48, 48, 1)\n",
            "iteration: 34123 train shape: (69464, 48, 48, 1)\n",
            "iteration: 34124 train shape: (69465, 48, 48, 1)\n",
            "iteration: 34125 train shape: (69466, 48, 48, 1)\n",
            "iteration: 34126 train shape: (69467, 48, 48, 1)\n",
            "iteration: 34127 train shape: (69468, 48, 48, 1)\n",
            "iteration: 34128 train shape: (69469, 48, 48, 1)\n",
            "iteration: 34129 train shape: (69470, 48, 48, 1)\n",
            "iteration: 34130 train shape: (69471, 48, 48, 1)\n",
            "iteration: 34131 train shape: (69472, 48, 48, 1)\n",
            "iteration: 34132 train shape: (69473, 48, 48, 1)\n",
            "iteration: 34133 train shape: (69474, 48, 48, 1)\n",
            "iteration: 34134 train shape: (69475, 48, 48, 1)\n",
            "iteration: 34135 train shape: (69476, 48, 48, 1)\n",
            "iteration: 34136 train shape: (69477, 48, 48, 1)\n",
            "iteration: 34137 train shape: (69478, 48, 48, 1)\n",
            "iteration: 34138 train shape: (69479, 48, 48, 1)\n",
            "iteration: 34139 train shape: (69480, 48, 48, 1)\n",
            "iteration: 34140 train shape: (69481, 48, 48, 1)\n",
            "iteration: 34141 train shape: (69482, 48, 48, 1)\n",
            "iteration: 34142 train shape: (69483, 48, 48, 1)\n",
            "iteration: 34143 train shape: (69484, 48, 48, 1)\n",
            "iteration: 34144 train shape: (69485, 48, 48, 1)\n",
            "iteration: 34145 train shape: (69486, 48, 48, 1)\n",
            "iteration: 34146 train shape: (69487, 48, 48, 1)\n",
            "iteration: 34147 train shape: (69488, 48, 48, 1)\n",
            "iteration: 34148 train shape: (69489, 48, 48, 1)\n",
            "iteration: 34149 train shape: (69490, 48, 48, 1)\n",
            "iteration: 34150 train shape: (69491, 48, 48, 1)\n",
            "iteration: 34151 train shape: (69492, 48, 48, 1)\n",
            "iteration: 34152 train shape: (69493, 48, 48, 1)\n",
            "iteration: 34153 train shape: (69494, 48, 48, 1)\n",
            "iteration: 34154 train shape: (69495, 48, 48, 1)\n",
            "iteration: 34155 train shape: (69496, 48, 48, 1)\n",
            "iteration: 34156 train shape: (69497, 48, 48, 1)\n",
            "iteration: 34157 train shape: (69498, 48, 48, 1)\n",
            "iteration: 34158 train shape: (69499, 48, 48, 1)\n",
            "iteration: 34159 train shape: (69500, 48, 48, 1)\n",
            "iteration: 34160 train shape: (69501, 48, 48, 1)\n",
            "iteration: 34161 train shape: (69502, 48, 48, 1)\n",
            "iteration: 34162 train shape: (69503, 48, 48, 1)\n",
            "iteration: 34163 train shape: (69504, 48, 48, 1)\n",
            "iteration: 34164 train shape: (69505, 48, 48, 1)\n",
            "iteration: 34165 train shape: (69506, 48, 48, 1)\n",
            "iteration: 34166 train shape: (69507, 48, 48, 1)\n",
            "iteration: 34167 train shape: (69508, 48, 48, 1)\n",
            "iteration: 34168 train shape: (69509, 48, 48, 1)\n",
            "iteration: 34169 train shape: (69510, 48, 48, 1)\n",
            "iteration: 34170 train shape: (69511, 48, 48, 1)\n",
            "iteration: 34171 train shape: (69512, 48, 48, 1)\n",
            "iteration: 34172 train shape: (69513, 48, 48, 1)\n",
            "iteration: 34173 train shape: (69514, 48, 48, 1)\n",
            "iteration: 34174 train shape: (69515, 48, 48, 1)\n",
            "iteration: 34175 train shape: (69516, 48, 48, 1)\n",
            "iteration: 34176 train shape: (69517, 48, 48, 1)\n",
            "iteration: 34177 train shape: (69518, 48, 48, 1)\n",
            "iteration: 34178 train shape: (69519, 48, 48, 1)\n",
            "iteration: 34179 train shape: (69520, 48, 48, 1)\n",
            "iteration: 34180 train shape: (69521, 48, 48, 1)\n",
            "iteration: 34181 train shape: (69522, 48, 48, 1)\n",
            "iteration: 34182 train shape: (69523, 48, 48, 1)\n",
            "iteration: 34183 train shape: (69524, 48, 48, 1)\n",
            "iteration: 34184 train shape: (69525, 48, 48, 1)\n",
            "iteration: 34185 train shape: (69526, 48, 48, 1)\n",
            "iteration: 34186 train shape: (69527, 48, 48, 1)\n",
            "iteration: 34187 train shape: (69528, 48, 48, 1)\n",
            "iteration: 34188 train shape: (69529, 48, 48, 1)\n",
            "iteration: 34189 train shape: (69530, 48, 48, 1)\n",
            "iteration: 34190 train shape: (69531, 48, 48, 1)\n",
            "iteration: 34191 train shape: (69532, 48, 48, 1)\n",
            "iteration: 34192 train shape: (69533, 48, 48, 1)\n",
            "iteration: 34193 train shape: (69534, 48, 48, 1)\n",
            "iteration: 34194 train shape: (69535, 48, 48, 1)\n",
            "iteration: 34195 train shape: (69536, 48, 48, 1)\n",
            "iteration: 34196 train shape: (69537, 48, 48, 1)\n",
            "iteration: 34197 train shape: (69538, 48, 48, 1)\n",
            "iteration: 34198 train shape: (69539, 48, 48, 1)\n",
            "iteration: 34199 train shape: (69540, 48, 48, 1)\n",
            "iteration: 34200 train shape: (69541, 48, 48, 1)\n",
            "iteration: 34201 train shape: (69542, 48, 48, 1)\n",
            "iteration: 34202 train shape: (69543, 48, 48, 1)\n",
            "iteration: 34203 train shape: (69544, 48, 48, 1)\n",
            "iteration: 34204 train shape: (69545, 48, 48, 1)\n",
            "iteration: 34205 train shape: (69546, 48, 48, 1)\n",
            "iteration: 34206 train shape: (69547, 48, 48, 1)\n",
            "iteration: 34207 train shape: (69548, 48, 48, 1)\n",
            "iteration: 34208 train shape: (69549, 48, 48, 1)\n",
            "iteration: 34209 train shape: (69550, 48, 48, 1)\n",
            "iteration: 34210 train shape: (69551, 48, 48, 1)\n",
            "iteration: 34211 train shape: (69552, 48, 48, 1)\n",
            "iteration: 34212 train shape: (69553, 48, 48, 1)\n",
            "iteration: 34213 train shape: (69554, 48, 48, 1)\n",
            "iteration: 34214 train shape: (69555, 48, 48, 1)\n",
            "iteration: 34215 train shape: (69556, 48, 48, 1)\n",
            "iteration: 34216 train shape: (69557, 48, 48, 1)\n",
            "iteration: 34217 train shape: (69558, 48, 48, 1)\n",
            "iteration: 34218 train shape: (69559, 48, 48, 1)\n",
            "iteration: 34219 train shape: (69560, 48, 48, 1)\n",
            "iteration: 34220 train shape: (69561, 48, 48, 1)\n",
            "iteration: 34221 train shape: (69562, 48, 48, 1)\n",
            "iteration: 34222 train shape: (69563, 48, 48, 1)\n",
            "iteration: 34223 train shape: (69564, 48, 48, 1)\n",
            "iteration: 34224 train shape: (69565, 48, 48, 1)\n",
            "iteration: 34225 train shape: (69566, 48, 48, 1)\n",
            "iteration: 34226 train shape: (69567, 48, 48, 1)\n",
            "iteration: 34227 train shape: (69568, 48, 48, 1)\n",
            "iteration: 34228 train shape: (69569, 48, 48, 1)\n",
            "iteration: 34229 train shape: (69570, 48, 48, 1)\n",
            "iteration: 34230 train shape: (69571, 48, 48, 1)\n",
            "iteration: 34231 train shape: (69572, 48, 48, 1)\n",
            "iteration: 34232 train shape: (69573, 48, 48, 1)\n",
            "iteration: 34233 train shape: (69574, 48, 48, 1)\n",
            "iteration: 34234 train shape: (69575, 48, 48, 1)\n",
            "iteration: 34235 train shape: (69576, 48, 48, 1)\n",
            "iteration: 34236 train shape: (69577, 48, 48, 1)\n",
            "iteration: 34237 train shape: (69578, 48, 48, 1)\n",
            "iteration: 34238 train shape: (69579, 48, 48, 1)\n",
            "iteration: 34239 train shape: (69580, 48, 48, 1)\n",
            "iteration: 34240 train shape: (69581, 48, 48, 1)\n",
            "iteration: 34241 train shape: (69582, 48, 48, 1)\n",
            "iteration: 34242 train shape: (69583, 48, 48, 1)\n",
            "iteration: 34243 train shape: (69584, 48, 48, 1)\n",
            "iteration: 34244 train shape: (69585, 48, 48, 1)\n",
            "iteration: 34245 train shape: (69586, 48, 48, 1)\n",
            "iteration: 34246 train shape: (69587, 48, 48, 1)\n",
            "iteration: 34247 train shape: (69588, 48, 48, 1)\n",
            "iteration: 34248 train shape: (69589, 48, 48, 1)\n",
            "iteration: 34249 train shape: (69590, 48, 48, 1)\n",
            "iteration: 34250 train shape: (69591, 48, 48, 1)\n",
            "iteration: 34251 train shape: (69592, 48, 48, 1)\n",
            "iteration: 34252 train shape: (69593, 48, 48, 1)\n",
            "iteration: 34253 train shape: (69594, 48, 48, 1)\n",
            "iteration: 34254 train shape: (69595, 48, 48, 1)\n",
            "iteration: 34255 train shape: (69596, 48, 48, 1)\n",
            "iteration: 34256 train shape: (69597, 48, 48, 1)\n",
            "iteration: 34257 train shape: (69598, 48, 48, 1)\n",
            "iteration: 34258 train shape: (69599, 48, 48, 1)\n",
            "iteration: 34259 train shape: (69600, 48, 48, 1)\n",
            "iteration: 34260 train shape: (69601, 48, 48, 1)\n",
            "iteration: 34261 train shape: (69602, 48, 48, 1)\n",
            "iteration: 34262 train shape: (69603, 48, 48, 1)\n",
            "iteration: 34263 train shape: (69604, 48, 48, 1)\n",
            "iteration: 34264 train shape: (69605, 48, 48, 1)\n",
            "iteration: 34265 train shape: (69606, 48, 48, 1)\n",
            "iteration: 34266 train shape: (69607, 48, 48, 1)\n",
            "iteration: 34267 train shape: (69608, 48, 48, 1)\n",
            "iteration: 34268 train shape: (69609, 48, 48, 1)\n",
            "iteration: 34269 train shape: (69610, 48, 48, 1)\n",
            "iteration: 34270 train shape: (69611, 48, 48, 1)\n",
            "iteration: 34271 train shape: (69612, 48, 48, 1)\n",
            "iteration: 34272 train shape: (69613, 48, 48, 1)\n",
            "iteration: 34273 train shape: (69614, 48, 48, 1)\n",
            "iteration: 34274 train shape: (69615, 48, 48, 1)\n",
            "iteration: 34275 train shape: (69616, 48, 48, 1)\n",
            "iteration: 34276 train shape: (69617, 48, 48, 1)\n",
            "iteration: 34277 train shape: (69618, 48, 48, 1)\n",
            "iteration: 34278 train shape: (69619, 48, 48, 1)\n",
            "iteration: 34279 train shape: (69620, 48, 48, 1)\n",
            "iteration: 34280 train shape: (69621, 48, 48, 1)\n",
            "iteration: 34281 train shape: (69622, 48, 48, 1)\n",
            "iteration: 34282 train shape: (69623, 48, 48, 1)\n",
            "iteration: 34283 train shape: (69624, 48, 48, 1)\n",
            "iteration: 34284 train shape: (69625, 48, 48, 1)\n",
            "iteration: 34285 train shape: (69626, 48, 48, 1)\n",
            "iteration: 34286 train shape: (69627, 48, 48, 1)\n",
            "iteration: 34287 train shape: (69628, 48, 48, 1)\n",
            "iteration: 34288 train shape: (69629, 48, 48, 1)\n",
            "iteration: 34289 train shape: (69630, 48, 48, 1)\n",
            "iteration: 34290 train shape: (69631, 48, 48, 1)\n",
            "iteration: 34291 train shape: (69632, 48, 48, 1)\n",
            "iteration: 34292 train shape: (69633, 48, 48, 1)\n",
            "iteration: 34293 train shape: (69634, 48, 48, 1)\n",
            "iteration: 34294 train shape: (69635, 48, 48, 1)\n",
            "iteration: 34295 train shape: (69636, 48, 48, 1)\n",
            "iteration: 34296 train shape: (69637, 48, 48, 1)\n",
            "iteration: 34297 train shape: (69638, 48, 48, 1)\n",
            "iteration: 34298 train shape: (69639, 48, 48, 1)\n",
            "iteration: 34299 train shape: (69640, 48, 48, 1)\n",
            "iteration: 34300 train shape: (69641, 48, 48, 1)\n",
            "iteration: 34301 train shape: (69642, 48, 48, 1)\n",
            "iteration: 34302 train shape: (69643, 48, 48, 1)\n",
            "iteration: 34303 train shape: (69644, 48, 48, 1)\n",
            "iteration: 34304 train shape: (69645, 48, 48, 1)\n",
            "iteration: 34305 train shape: (69646, 48, 48, 1)\n",
            "iteration: 34306 train shape: (69647, 48, 48, 1)\n",
            "iteration: 34307 train shape: (69648, 48, 48, 1)\n",
            "iteration: 34308 train shape: (69649, 48, 48, 1)\n",
            "iteration: 34309 train shape: (69650, 48, 48, 1)\n",
            "iteration: 34310 train shape: (69651, 48, 48, 1)\n",
            "iteration: 34311 train shape: (69652, 48, 48, 1)\n",
            "iteration: 34312 train shape: (69653, 48, 48, 1)\n",
            "iteration: 34313 train shape: (69654, 48, 48, 1)\n",
            "iteration: 34314 train shape: (69655, 48, 48, 1)\n",
            "iteration: 34315 train shape: (69656, 48, 48, 1)\n",
            "iteration: 34316 train shape: (69657, 48, 48, 1)\n",
            "iteration: 34317 train shape: (69658, 48, 48, 1)\n",
            "iteration: 34318 train shape: (69659, 48, 48, 1)\n",
            "iteration: 34319 train shape: (69660, 48, 48, 1)\n",
            "iteration: 34320 train shape: (69661, 48, 48, 1)\n",
            "iteration: 34321 train shape: (69662, 48, 48, 1)\n",
            "iteration: 34322 train shape: (69663, 48, 48, 1)\n",
            "iteration: 34323 train shape: (69664, 48, 48, 1)\n",
            "iteration: 34324 train shape: (69665, 48, 48, 1)\n",
            "iteration: 34325 train shape: (69666, 48, 48, 1)\n",
            "iteration: 34326 train shape: (69667, 48, 48, 1)\n",
            "iteration: 34327 train shape: (69668, 48, 48, 1)\n",
            "iteration: 34328 train shape: (69669, 48, 48, 1)\n",
            "iteration: 34329 train shape: (69670, 48, 48, 1)\n",
            "iteration: 34330 train shape: (69671, 48, 48, 1)\n",
            "iteration: 34331 train shape: (69672, 48, 48, 1)\n",
            "iteration: 34332 train shape: (69673, 48, 48, 1)\n",
            "iteration: 34333 train shape: (69674, 48, 48, 1)\n",
            "iteration: 34334 train shape: (69675, 48, 48, 1)\n",
            "iteration: 34335 train shape: (69676, 48, 48, 1)\n",
            "iteration: 34336 train shape: (69677, 48, 48, 1)\n",
            "iteration: 34337 train shape: (69678, 48, 48, 1)\n",
            "iteration: 34338 train shape: (69679, 48, 48, 1)\n",
            "iteration: 34339 train shape: (69680, 48, 48, 1)\n",
            "iteration: 34340 train shape: (69681, 48, 48, 1)\n",
            "iteration: 34341 train shape: (69682, 48, 48, 1)\n",
            "iteration: 34342 train shape: (69683, 48, 48, 1)\n",
            "iteration: 34343 train shape: (69684, 48, 48, 1)\n",
            "iteration: 34344 train shape: (69685, 48, 48, 1)\n",
            "iteration: 34345 train shape: (69686, 48, 48, 1)\n",
            "iteration: 34346 train shape: (69687, 48, 48, 1)\n",
            "iteration: 34347 train shape: (69688, 48, 48, 1)\n",
            "iteration: 34348 train shape: (69689, 48, 48, 1)\n",
            "iteration: 34349 train shape: (69690, 48, 48, 1)\n",
            "iteration: 34350 train shape: (69691, 48, 48, 1)\n",
            "iteration: 34351 train shape: (69692, 48, 48, 1)\n",
            "iteration: 34352 train shape: (69693, 48, 48, 1)\n",
            "iteration: 34353 train shape: (69694, 48, 48, 1)\n",
            "iteration: 34354 train shape: (69695, 48, 48, 1)\n",
            "iteration: 34355 train shape: (69696, 48, 48, 1)\n",
            "iteration: 34356 train shape: (69697, 48, 48, 1)\n",
            "iteration: 34357 train shape: (69698, 48, 48, 1)\n",
            "iteration: 34358 train shape: (69699, 48, 48, 1)\n",
            "iteration: 34359 train shape: (69700, 48, 48, 1)\n",
            "iteration: 34360 train shape: (69701, 48, 48, 1)\n",
            "iteration: 34361 train shape: (69702, 48, 48, 1)\n",
            "iteration: 34362 train shape: (69703, 48, 48, 1)\n",
            "iteration: 34363 train shape: (69704, 48, 48, 1)\n",
            "iteration: 34364 train shape: (69705, 48, 48, 1)\n",
            "iteration: 34365 train shape: (69706, 48, 48, 1)\n",
            "iteration: 34366 train shape: (69707, 48, 48, 1)\n",
            "iteration: 34367 train shape: (69708, 48, 48, 1)\n",
            "iteration: 34368 train shape: (69709, 48, 48, 1)\n",
            "iteration: 34369 train shape: (69710, 48, 48, 1)\n",
            "iteration: 34370 train shape: (69711, 48, 48, 1)\n",
            "iteration: 34371 train shape: (69712, 48, 48, 1)\n",
            "iteration: 34372 train shape: (69713, 48, 48, 1)\n",
            "iteration: 34373 train shape: (69714, 48, 48, 1)\n",
            "iteration: 34374 train shape: (69715, 48, 48, 1)\n",
            "iteration: 34375 train shape: (69716, 48, 48, 1)\n",
            "iteration: 34376 train shape: (69717, 48, 48, 1)\n",
            "iteration: 34377 train shape: (69718, 48, 48, 1)\n",
            "iteration: 34378 train shape: (69719, 48, 48, 1)\n",
            "iteration: 34379 train shape: (69720, 48, 48, 1)\n",
            "iteration: 34380 train shape: (69721, 48, 48, 1)\n",
            "iteration: 34381 train shape: (69722, 48, 48, 1)\n",
            "iteration: 34382 train shape: (69723, 48, 48, 1)\n",
            "iteration: 34383 train shape: (69724, 48, 48, 1)\n",
            "iteration: 34384 train shape: (69725, 48, 48, 1)\n",
            "iteration: 34385 train shape: (69726, 48, 48, 1)\n",
            "iteration: 34386 train shape: (69727, 48, 48, 1)\n",
            "iteration: 34387 train shape: (69728, 48, 48, 1)\n",
            "iteration: 34388 train shape: (69729, 48, 48, 1)\n",
            "iteration: 34389 train shape: (69730, 48, 48, 1)\n",
            "iteration: 34390 train shape: (69731, 48, 48, 1)\n",
            "iteration: 34391 train shape: (69732, 48, 48, 1)\n",
            "iteration: 34392 train shape: (69733, 48, 48, 1)\n",
            "iteration: 34393 train shape: (69734, 48, 48, 1)\n",
            "iteration: 34394 train shape: (69735, 48, 48, 1)\n",
            "iteration: 34395 train shape: (69736, 48, 48, 1)\n",
            "iteration: 34396 train shape: (69737, 48, 48, 1)\n",
            "iteration: 34397 train shape: (69738, 48, 48, 1)\n",
            "iteration: 34398 train shape: (69739, 48, 48, 1)\n",
            "iteration: 34399 train shape: (69740, 48, 48, 1)\n",
            "iteration: 34400 train shape: (69741, 48, 48, 1)\n",
            "iteration: 34401 train shape: (69742, 48, 48, 1)\n",
            "iteration: 34402 train shape: (69743, 48, 48, 1)\n",
            "iteration: 34403 train shape: (69744, 48, 48, 1)\n",
            "iteration: 34404 train shape: (69745, 48, 48, 1)\n",
            "iteration: 34405 train shape: (69746, 48, 48, 1)\n",
            "iteration: 34406 train shape: (69747, 48, 48, 1)\n",
            "iteration: 34407 train shape: (69748, 48, 48, 1)\n",
            "iteration: 34408 train shape: (69749, 48, 48, 1)\n",
            "iteration: 34409 train shape: (69750, 48, 48, 1)\n",
            "iteration: 34410 train shape: (69751, 48, 48, 1)\n",
            "iteration: 34411 train shape: (69752, 48, 48, 1)\n",
            "iteration: 34412 train shape: (69753, 48, 48, 1)\n",
            "iteration: 34413 train shape: (69754, 48, 48, 1)\n",
            "iteration: 34414 train shape: (69755, 48, 48, 1)\n",
            "iteration: 34415 train shape: (69756, 48, 48, 1)\n",
            "iteration: 34416 train shape: (69757, 48, 48, 1)\n",
            "iteration: 34417 train shape: (69758, 48, 48, 1)\n",
            "iteration: 34418 train shape: (69759, 48, 48, 1)\n",
            "iteration: 34419 train shape: (69760, 48, 48, 1)\n",
            "iteration: 34420 train shape: (69761, 48, 48, 1)\n",
            "iteration: 34421 train shape: (69762, 48, 48, 1)\n",
            "iteration: 34422 train shape: (69763, 48, 48, 1)\n",
            "iteration: 34423 train shape: (69764, 48, 48, 1)\n",
            "iteration: 34424 train shape: (69765, 48, 48, 1)\n",
            "iteration: 34425 train shape: (69766, 48, 48, 1)\n",
            "iteration: 34426 train shape: (69767, 48, 48, 1)\n",
            "iteration: 34427 train shape: (69768, 48, 48, 1)\n",
            "iteration: 34428 train shape: (69769, 48, 48, 1)\n",
            "iteration: 34429 train shape: (69770, 48, 48, 1)\n",
            "iteration: 34430 train shape: (69771, 48, 48, 1)\n",
            "iteration: 34431 train shape: (69772, 48, 48, 1)\n",
            "iteration: 34432 train shape: (69773, 48, 48, 1)\n",
            "iteration: 34433 train shape: (69774, 48, 48, 1)\n",
            "iteration: 34434 train shape: (69775, 48, 48, 1)\n",
            "iteration: 34435 train shape: (69776, 48, 48, 1)\n",
            "iteration: 34436 train shape: (69777, 48, 48, 1)\n",
            "iteration: 34437 train shape: (69778, 48, 48, 1)\n",
            "iteration: 34438 train shape: (69779, 48, 48, 1)\n",
            "iteration: 34439 train shape: (69780, 48, 48, 1)\n",
            "iteration: 34440 train shape: (69781, 48, 48, 1)\n",
            "iteration: 34441 train shape: (69782, 48, 48, 1)\n",
            "iteration: 34442 train shape: (69783, 48, 48, 1)\n",
            "iteration: 34443 train shape: (69784, 48, 48, 1)\n",
            "iteration: 34444 train shape: (69785, 48, 48, 1)\n",
            "iteration: 34445 train shape: (69786, 48, 48, 1)\n",
            "iteration: 34446 train shape: (69787, 48, 48, 1)\n",
            "iteration: 34447 train shape: (69788, 48, 48, 1)\n",
            "iteration: 34448 train shape: (69789, 48, 48, 1)\n",
            "iteration: 34449 train shape: (69790, 48, 48, 1)\n",
            "iteration: 34450 train shape: (69791, 48, 48, 1)\n",
            "iteration: 34451 train shape: (69792, 48, 48, 1)\n",
            "iteration: 34452 train shape: (69793, 48, 48, 1)\n",
            "iteration: 34453 train shape: (69794, 48, 48, 1)\n",
            "iteration: 34454 train shape: (69795, 48, 48, 1)\n",
            "iteration: 34455 train shape: (69796, 48, 48, 1)\n",
            "iteration: 34456 train shape: (69797, 48, 48, 1)\n",
            "iteration: 34457 train shape: (69798, 48, 48, 1)\n",
            "iteration: 34458 train shape: (69799, 48, 48, 1)\n",
            "iteration: 34459 train shape: (69800, 48, 48, 1)\n",
            "iteration: 34460 train shape: (69801, 48, 48, 1)\n",
            "iteration: 34461 train shape: (69802, 48, 48, 1)\n",
            "iteration: 34462 train shape: (69803, 48, 48, 1)\n",
            "iteration: 34463 train shape: (69804, 48, 48, 1)\n",
            "iteration: 34464 train shape: (69805, 48, 48, 1)\n",
            "iteration: 34465 train shape: (69806, 48, 48, 1)\n",
            "iteration: 34466 train shape: (69807, 48, 48, 1)\n",
            "iteration: 34467 train shape: (69808, 48, 48, 1)\n",
            "iteration: 34468 train shape: (69809, 48, 48, 1)\n",
            "iteration: 34469 train shape: (69810, 48, 48, 1)\n",
            "iteration: 34470 train shape: (69811, 48, 48, 1)\n",
            "iteration: 34471 train shape: (69812, 48, 48, 1)\n",
            "iteration: 34472 train shape: (69813, 48, 48, 1)\n",
            "iteration: 34473 train shape: (69814, 48, 48, 1)\n",
            "iteration: 34474 train shape: (69815, 48, 48, 1)\n",
            "iteration: 34475 train shape: (69816, 48, 48, 1)\n",
            "iteration: 34476 train shape: (69817, 48, 48, 1)\n",
            "iteration: 34477 train shape: (69818, 48, 48, 1)\n",
            "iteration: 34478 train shape: (69819, 48, 48, 1)\n",
            "iteration: 34479 train shape: (69820, 48, 48, 1)\n",
            "iteration: 34480 train shape: (69821, 48, 48, 1)\n",
            "iteration: 34481 train shape: (69822, 48, 48, 1)\n",
            "iteration: 34482 train shape: (69823, 48, 48, 1)\n",
            "iteration: 34483 train shape: (69824, 48, 48, 1)\n",
            "iteration: 34484 train shape: (69825, 48, 48, 1)\n",
            "iteration: 34485 train shape: (69826, 48, 48, 1)\n",
            "iteration: 34486 train shape: (69827, 48, 48, 1)\n",
            "iteration: 34487 train shape: (69828, 48, 48, 1)\n",
            "iteration: 34488 train shape: (69829, 48, 48, 1)\n",
            "iteration: 34489 train shape: (69830, 48, 48, 1)\n",
            "iteration: 34490 train shape: (69831, 48, 48, 1)\n",
            "iteration: 34491 train shape: (69832, 48, 48, 1)\n",
            "iteration: 34492 train shape: (69833, 48, 48, 1)\n",
            "iteration: 34493 train shape: (69834, 48, 48, 1)\n",
            "iteration: 34494 train shape: (69835, 48, 48, 1)\n",
            "iteration: 34495 train shape: (69836, 48, 48, 1)\n",
            "iteration: 34496 train shape: (69837, 48, 48, 1)\n",
            "iteration: 34497 train shape: (69838, 48, 48, 1)\n",
            "iteration: 34498 train shape: (69839, 48, 48, 1)\n",
            "iteration: 34499 train shape: (69840, 48, 48, 1)\n",
            "iteration: 34500 train shape: (69841, 48, 48, 1)\n",
            "iteration: 34501 train shape: (69842, 48, 48, 1)\n",
            "iteration: 34502 train shape: (69843, 48, 48, 1)\n",
            "iteration: 34503 train shape: (69844, 48, 48, 1)\n",
            "iteration: 34504 train shape: (69845, 48, 48, 1)\n",
            "iteration: 34505 train shape: (69846, 48, 48, 1)\n",
            "iteration: 34506 train shape: (69847, 48, 48, 1)\n",
            "iteration: 34507 train shape: (69848, 48, 48, 1)\n",
            "iteration: 34508 train shape: (69849, 48, 48, 1)\n",
            "iteration: 34509 train shape: (69850, 48, 48, 1)\n",
            "iteration: 34510 train shape: (69851, 48, 48, 1)\n",
            "iteration: 34511 train shape: (69852, 48, 48, 1)\n",
            "iteration: 34512 train shape: (69853, 48, 48, 1)\n",
            "iteration: 34513 train shape: (69854, 48, 48, 1)\n",
            "iteration: 34514 train shape: (69855, 48, 48, 1)\n",
            "iteration: 34515 train shape: (69856, 48, 48, 1)\n",
            "iteration: 34516 train shape: (69857, 48, 48, 1)\n",
            "iteration: 34517 train shape: (69858, 48, 48, 1)\n",
            "iteration: 34518 train shape: (69859, 48, 48, 1)\n",
            "iteration: 34519 train shape: (69860, 48, 48, 1)\n",
            "iteration: 34520 train shape: (69861, 48, 48, 1)\n",
            "iteration: 34521 train shape: (69862, 48, 48, 1)\n",
            "iteration: 34522 train shape: (69863, 48, 48, 1)\n",
            "iteration: 34523 train shape: (69864, 48, 48, 1)\n",
            "iteration: 34524 train shape: (69865, 48, 48, 1)\n",
            "iteration: 34525 train shape: (69866, 48, 48, 1)\n",
            "iteration: 34526 train shape: (69867, 48, 48, 1)\n",
            "iteration: 34527 train shape: (69868, 48, 48, 1)\n",
            "iteration: 34528 train shape: (69869, 48, 48, 1)\n",
            "iteration: 34529 train shape: (69870, 48, 48, 1)\n",
            "iteration: 34530 train shape: (69871, 48, 48, 1)\n",
            "iteration: 34531 train shape: (69872, 48, 48, 1)\n",
            "iteration: 34532 train shape: (69873, 48, 48, 1)\n",
            "iteration: 34533 train shape: (69874, 48, 48, 1)\n",
            "iteration: 34534 train shape: (69875, 48, 48, 1)\n",
            "iteration: 34535 train shape: (69876, 48, 48, 1)\n",
            "iteration: 34536 train shape: (69877, 48, 48, 1)\n",
            "iteration: 34537 train shape: (69878, 48, 48, 1)\n",
            "iteration: 34538 train shape: (69879, 48, 48, 1)\n",
            "iteration: 34539 train shape: (69880, 48, 48, 1)\n",
            "iteration: 34540 train shape: (69881, 48, 48, 1)\n",
            "iteration: 34541 train shape: (69882, 48, 48, 1)\n",
            "iteration: 34542 train shape: (69883, 48, 48, 1)\n",
            "iteration: 34543 train shape: (69884, 48, 48, 1)\n",
            "iteration: 34544 train shape: (69885, 48, 48, 1)\n",
            "iteration: 34545 train shape: (69886, 48, 48, 1)\n",
            "iteration: 34546 train shape: (69887, 48, 48, 1)\n",
            "iteration: 34547 train shape: (69888, 48, 48, 1)\n",
            "iteration: 34548 train shape: (69889, 48, 48, 1)\n",
            "iteration: 34549 train shape: (69890, 48, 48, 1)\n",
            "iteration: 34550 train shape: (69891, 48, 48, 1)\n",
            "iteration: 34551 train shape: (69892, 48, 48, 1)\n",
            "iteration: 34552 train shape: (69893, 48, 48, 1)\n",
            "iteration: 34553 train shape: (69894, 48, 48, 1)\n",
            "iteration: 34554 train shape: (69895, 48, 48, 1)\n",
            "iteration: 34555 train shape: (69896, 48, 48, 1)\n",
            "iteration: 34556 train shape: (69897, 48, 48, 1)\n",
            "iteration: 34557 train shape: (69898, 48, 48, 1)\n",
            "iteration: 34558 train shape: (69899, 48, 48, 1)\n",
            "iteration: 34559 train shape: (69900, 48, 48, 1)\n",
            "iteration: 34560 train shape: (69901, 48, 48, 1)\n",
            "iteration: 34561 train shape: (69902, 48, 48, 1)\n",
            "iteration: 34562 train shape: (69903, 48, 48, 1)\n",
            "iteration: 34563 train shape: (69904, 48, 48, 1)\n",
            "iteration: 34564 train shape: (69905, 48, 48, 1)\n",
            "iteration: 34565 train shape: (69906, 48, 48, 1)\n",
            "iteration: 34566 train shape: (69907, 48, 48, 1)\n",
            "iteration: 34567 train shape: (69908, 48, 48, 1)\n",
            "iteration: 34568 train shape: (69909, 48, 48, 1)\n",
            "iteration: 34569 train shape: (69910, 48, 48, 1)\n",
            "iteration: 34570 train shape: (69911, 48, 48, 1)\n",
            "iteration: 34571 train shape: (69912, 48, 48, 1)\n",
            "iteration: 34572 train shape: (69913, 48, 48, 1)\n",
            "iteration: 34573 train shape: (69914, 48, 48, 1)\n",
            "iteration: 34574 train shape: (69915, 48, 48, 1)\n",
            "iteration: 34575 train shape: (69916, 48, 48, 1)\n",
            "iteration: 34576 train shape: (69917, 48, 48, 1)\n",
            "iteration: 34577 train shape: (69918, 48, 48, 1)\n",
            "iteration: 34578 train shape: (69919, 48, 48, 1)\n",
            "iteration: 34579 train shape: (69920, 48, 48, 1)\n",
            "iteration: 34580 train shape: (69921, 48, 48, 1)\n",
            "iteration: 34581 train shape: (69922, 48, 48, 1)\n",
            "iteration: 34582 train shape: (69923, 48, 48, 1)\n",
            "iteration: 34583 train shape: (69924, 48, 48, 1)\n",
            "iteration: 34584 train shape: (69925, 48, 48, 1)\n",
            "iteration: 34585 train shape: (69926, 48, 48, 1)\n",
            "iteration: 34586 train shape: (69927, 48, 48, 1)\n",
            "iteration: 34587 train shape: (69928, 48, 48, 1)\n",
            "iteration: 34588 train shape: (69929, 48, 48, 1)\n",
            "iteration: 34589 train shape: (69930, 48, 48, 1)\n",
            "iteration: 34590 train shape: (69931, 48, 48, 1)\n",
            "iteration: 34591 train shape: (69932, 48, 48, 1)\n",
            "iteration: 34592 train shape: (69933, 48, 48, 1)\n",
            "iteration: 34593 train shape: (69934, 48, 48, 1)\n",
            "iteration: 34594 train shape: (69935, 48, 48, 1)\n",
            "iteration: 34595 train shape: (69936, 48, 48, 1)\n",
            "iteration: 34596 train shape: (69937, 48, 48, 1)\n",
            "iteration: 34597 train shape: (69938, 48, 48, 1)\n",
            "iteration: 34598 train shape: (69939, 48, 48, 1)\n",
            "iteration: 34599 train shape: (69940, 48, 48, 1)\n",
            "iteration: 34600 train shape: (69941, 48, 48, 1)\n",
            "iteration: 34601 train shape: (69942, 48, 48, 1)\n",
            "iteration: 34602 train shape: (69943, 48, 48, 1)\n",
            "iteration: 34603 train shape: (69944, 48, 48, 1)\n",
            "iteration: 34604 train shape: (69945, 48, 48, 1)\n",
            "iteration: 34605 train shape: (69946, 48, 48, 1)\n",
            "iteration: 34606 train shape: (69947, 48, 48, 1)\n",
            "iteration: 34607 train shape: (69948, 48, 48, 1)\n",
            "iteration: 34608 train shape: (69949, 48, 48, 1)\n",
            "iteration: 34609 train shape: (69950, 48, 48, 1)\n",
            "iteration: 34610 train shape: (69951, 48, 48, 1)\n",
            "iteration: 34611 train shape: (69952, 48, 48, 1)\n",
            "iteration: 34612 train shape: (69953, 48, 48, 1)\n",
            "iteration: 34613 train shape: (69954, 48, 48, 1)\n",
            "iteration: 34614 train shape: (69955, 48, 48, 1)\n",
            "iteration: 34615 train shape: (69956, 48, 48, 1)\n",
            "iteration: 34616 train shape: (69957, 48, 48, 1)\n",
            "iteration: 34617 train shape: (69958, 48, 48, 1)\n",
            "iteration: 34618 train shape: (69959, 48, 48, 1)\n",
            "iteration: 34619 train shape: (69960, 48, 48, 1)\n",
            "iteration: 34620 train shape: (69961, 48, 48, 1)\n",
            "iteration: 34621 train shape: (69962, 48, 48, 1)\n",
            "iteration: 34622 train shape: (69963, 48, 48, 1)\n",
            "iteration: 34623 train shape: (69964, 48, 48, 1)\n",
            "iteration: 34624 train shape: (69965, 48, 48, 1)\n",
            "iteration: 34625 train shape: (69966, 48, 48, 1)\n",
            "iteration: 34626 train shape: (69967, 48, 48, 1)\n",
            "iteration: 34627 train shape: (69968, 48, 48, 1)\n",
            "iteration: 34628 train shape: (69969, 48, 48, 1)\n",
            "iteration: 34629 train shape: (69970, 48, 48, 1)\n",
            "iteration: 34630 train shape: (69971, 48, 48, 1)\n",
            "iteration: 34631 train shape: (69972, 48, 48, 1)\n",
            "iteration: 34632 train shape: (69973, 48, 48, 1)\n",
            "iteration: 34633 train shape: (69974, 48, 48, 1)\n",
            "iteration: 34634 train shape: (69975, 48, 48, 1)\n",
            "iteration: 34635 train shape: (69976, 48, 48, 1)\n",
            "iteration: 34636 train shape: (69977, 48, 48, 1)\n",
            "iteration: 34637 train shape: (69978, 48, 48, 1)\n",
            "iteration: 34638 train shape: (69979, 48, 48, 1)\n",
            "iteration: 34639 train shape: (69980, 48, 48, 1)\n",
            "iteration: 34640 train shape: (69981, 48, 48, 1)\n",
            "iteration: 34641 train shape: (69982, 48, 48, 1)\n",
            "iteration: 34642 train shape: (69983, 48, 48, 1)\n",
            "iteration: 34643 train shape: (69984, 48, 48, 1)\n",
            "iteration: 34644 train shape: (69985, 48, 48, 1)\n",
            "iteration: 34645 train shape: (69986, 48, 48, 1)\n",
            "iteration: 34646 train shape: (69987, 48, 48, 1)\n",
            "iteration: 34647 train shape: (69988, 48, 48, 1)\n",
            "iteration: 34648 train shape: (69989, 48, 48, 1)\n",
            "iteration: 34649 train shape: (69990, 48, 48, 1)\n",
            "iteration: 34650 train shape: (69991, 48, 48, 1)\n",
            "iteration: 34651 train shape: (69992, 48, 48, 1)\n",
            "iteration: 34652 train shape: (69993, 48, 48, 1)\n",
            "iteration: 34653 train shape: (69994, 48, 48, 1)\n",
            "iteration: 34654 train shape: (69995, 48, 48, 1)\n",
            "iteration: 34655 train shape: (69996, 48, 48, 1)\n",
            "iteration: 34656 train shape: (69997, 48, 48, 1)\n",
            "iteration: 34657 train shape: (69998, 48, 48, 1)\n",
            "iteration: 34658 train shape: (69999, 48, 48, 1)\n",
            "iteration: 34659 train shape: (70000, 48, 48, 1)\n",
            "iteration: 34660 train shape: (70001, 48, 48, 1)\n",
            "iteration: 34661 train shape: (70002, 48, 48, 1)\n",
            "iteration: 34662 train shape: (70003, 48, 48, 1)\n",
            "iteration: 34663 train shape: (70004, 48, 48, 1)\n",
            "iteration: 34664 train shape: (70005, 48, 48, 1)\n",
            "iteration: 34665 train shape: (70006, 48, 48, 1)\n",
            "iteration: 34666 train shape: (70007, 48, 48, 1)\n",
            "iteration: 34667 train shape: (70008, 48, 48, 1)\n",
            "iteration: 34668 train shape: (70009, 48, 48, 1)\n",
            "iteration: 34669 train shape: (70010, 48, 48, 1)\n",
            "iteration: 34670 train shape: (70011, 48, 48, 1)\n",
            "iteration: 34671 train shape: (70012, 48, 48, 1)\n",
            "iteration: 34672 train shape: (70013, 48, 48, 1)\n",
            "iteration: 34673 train shape: (70014, 48, 48, 1)\n",
            "iteration: 34674 train shape: (70015, 48, 48, 1)\n",
            "iteration: 34675 train shape: (70016, 48, 48, 1)\n",
            "iteration: 34676 train shape: (70017, 48, 48, 1)\n",
            "iteration: 34677 train shape: (70018, 48, 48, 1)\n",
            "iteration: 34678 train shape: (70019, 48, 48, 1)\n",
            "iteration: 34679 train shape: (70020, 48, 48, 1)\n",
            "iteration: 34680 train shape: (70021, 48, 48, 1)\n",
            "iteration: 34681 train shape: (70022, 48, 48, 1)\n",
            "iteration: 34682 train shape: (70023, 48, 48, 1)\n",
            "iteration: 34683 train shape: (70024, 48, 48, 1)\n",
            "iteration: 34684 train shape: (70025, 48, 48, 1)\n",
            "iteration: 34685 train shape: (70026, 48, 48, 1)\n",
            "iteration: 34686 train shape: (70027, 48, 48, 1)\n",
            "iteration: 34687 train shape: (70028, 48, 48, 1)\n",
            "iteration: 34688 train shape: (70029, 48, 48, 1)\n",
            "iteration: 34689 train shape: (70030, 48, 48, 1)\n",
            "iteration: 34690 train shape: (70031, 48, 48, 1)\n",
            "iteration: 34691 train shape: (70032, 48, 48, 1)\n",
            "iteration: 34692 train shape: (70033, 48, 48, 1)\n",
            "iteration: 34693 train shape: (70034, 48, 48, 1)\n",
            "iteration: 34694 train shape: (70035, 48, 48, 1)\n",
            "iteration: 34695 train shape: (70036, 48, 48, 1)\n",
            "iteration: 34696 train shape: (70037, 48, 48, 1)\n",
            "iteration: 34697 train shape: (70038, 48, 48, 1)\n",
            "iteration: 34698 train shape: (70039, 48, 48, 1)\n",
            "iteration: 34699 train shape: (70040, 48, 48, 1)\n",
            "iteration: 34700 train shape: (70041, 48, 48, 1)\n",
            "iteration: 34701 train shape: (70042, 48, 48, 1)\n",
            "iteration: 34702 train shape: (70043, 48, 48, 1)\n",
            "iteration: 34703 train shape: (70044, 48, 48, 1)\n",
            "iteration: 34704 train shape: (70045, 48, 48, 1)\n",
            "iteration: 34705 train shape: (70046, 48, 48, 1)\n",
            "iteration: 34706 train shape: (70047, 48, 48, 1)\n",
            "iteration: 34707 train shape: (70048, 48, 48, 1)\n",
            "iteration: 34708 train shape: (70049, 48, 48, 1)\n",
            "iteration: 34709 train shape: (70050, 48, 48, 1)\n",
            "iteration: 34710 train shape: (70051, 48, 48, 1)\n",
            "iteration: 34711 train shape: (70052, 48, 48, 1)\n",
            "iteration: 34712 train shape: (70053, 48, 48, 1)\n",
            "iteration: 34713 train shape: (70054, 48, 48, 1)\n",
            "iteration: 34714 train shape: (70055, 48, 48, 1)\n",
            "iteration: 34715 train shape: (70056, 48, 48, 1)\n",
            "iteration: 34716 train shape: (70057, 48, 48, 1)\n",
            "iteration: 34717 train shape: (70058, 48, 48, 1)\n",
            "iteration: 34718 train shape: (70059, 48, 48, 1)\n",
            "iteration: 34719 train shape: (70060, 48, 48, 1)\n",
            "iteration: 34720 train shape: (70061, 48, 48, 1)\n",
            "iteration: 34721 train shape: (70062, 48, 48, 1)\n",
            "iteration: 34722 train shape: (70063, 48, 48, 1)\n",
            "iteration: 34723 train shape: (70064, 48, 48, 1)\n",
            "iteration: 34724 train shape: (70065, 48, 48, 1)\n",
            "iteration: 34725 train shape: (70066, 48, 48, 1)\n",
            "iteration: 34726 train shape: (70067, 48, 48, 1)\n",
            "iteration: 34727 train shape: (70068, 48, 48, 1)\n",
            "iteration: 34728 train shape: (70069, 48, 48, 1)\n",
            "iteration: 34729 train shape: (70070, 48, 48, 1)\n",
            "iteration: 34730 train shape: (70071, 48, 48, 1)\n",
            "iteration: 34731 train shape: (70072, 48, 48, 1)\n",
            "iteration: 34732 train shape: (70073, 48, 48, 1)\n",
            "iteration: 34733 train shape: (70074, 48, 48, 1)\n",
            "iteration: 34734 train shape: (70075, 48, 48, 1)\n",
            "iteration: 34735 train shape: (70076, 48, 48, 1)\n",
            "iteration: 34736 train shape: (70077, 48, 48, 1)\n",
            "iteration: 34737 train shape: (70078, 48, 48, 1)\n",
            "iteration: 34738 train shape: (70079, 48, 48, 1)\n",
            "iteration: 34739 train shape: (70080, 48, 48, 1)\n",
            "iteration: 34740 train shape: (70081, 48, 48, 1)\n",
            "iteration: 34741 train shape: (70082, 48, 48, 1)\n",
            "iteration: 34742 train shape: (70083, 48, 48, 1)\n",
            "iteration: 34743 train shape: (70084, 48, 48, 1)\n",
            "iteration: 34744 train shape: (70085, 48, 48, 1)\n",
            "iteration: 34745 train shape: (70086, 48, 48, 1)\n",
            "iteration: 34746 train shape: (70087, 48, 48, 1)\n",
            "iteration: 34747 train shape: (70088, 48, 48, 1)\n",
            "iteration: 34748 train shape: (70089, 48, 48, 1)\n",
            "iteration: 34749 train shape: (70090, 48, 48, 1)\n",
            "iteration: 34750 train shape: (70091, 48, 48, 1)\n",
            "iteration: 34751 train shape: (70092, 48, 48, 1)\n",
            "iteration: 34752 train shape: (70093, 48, 48, 1)\n",
            "iteration: 34753 train shape: (70094, 48, 48, 1)\n",
            "iteration: 34754 train shape: (70095, 48, 48, 1)\n",
            "iteration: 34755 train shape: (70096, 48, 48, 1)\n",
            "iteration: 34756 train shape: (70097, 48, 48, 1)\n",
            "iteration: 34757 train shape: (70098, 48, 48, 1)\n",
            "iteration: 34758 train shape: (70099, 48, 48, 1)\n",
            "iteration: 34759 train shape: (70100, 48, 48, 1)\n",
            "iteration: 34760 train shape: (70101, 48, 48, 1)\n",
            "iteration: 34761 train shape: (70102, 48, 48, 1)\n",
            "iteration: 34762 train shape: (70103, 48, 48, 1)\n",
            "iteration: 34763 train shape: (70104, 48, 48, 1)\n",
            "iteration: 34764 train shape: (70105, 48, 48, 1)\n",
            "iteration: 34765 train shape: (70106, 48, 48, 1)\n",
            "iteration: 34766 train shape: (70107, 48, 48, 1)\n",
            "iteration: 34767 train shape: (70108, 48, 48, 1)\n",
            "iteration: 34768 train shape: (70109, 48, 48, 1)\n",
            "iteration: 34769 train shape: (70110, 48, 48, 1)\n",
            "iteration: 34770 train shape: (70111, 48, 48, 1)\n",
            "iteration: 34771 train shape: (70112, 48, 48, 1)\n",
            "iteration: 34772 train shape: (70113, 48, 48, 1)\n",
            "iteration: 34773 train shape: (70114, 48, 48, 1)\n",
            "iteration: 34774 train shape: (70115, 48, 48, 1)\n",
            "iteration: 34775 train shape: (70116, 48, 48, 1)\n",
            "iteration: 34776 train shape: (70117, 48, 48, 1)\n",
            "iteration: 34777 train shape: (70118, 48, 48, 1)\n",
            "iteration: 34778 train shape: (70119, 48, 48, 1)\n",
            "iteration: 34779 train shape: (70120, 48, 48, 1)\n",
            "iteration: 34780 train shape: (70121, 48, 48, 1)\n",
            "iteration: 34781 train shape: (70122, 48, 48, 1)\n",
            "iteration: 34782 train shape: (70123, 48, 48, 1)\n",
            "iteration: 34783 train shape: (70124, 48, 48, 1)\n",
            "iteration: 34784 train shape: (70125, 48, 48, 1)\n",
            "iteration: 34785 train shape: (70126, 48, 48, 1)\n",
            "iteration: 34786 train shape: (70127, 48, 48, 1)\n",
            "iteration: 34787 train shape: (70128, 48, 48, 1)\n",
            "iteration: 34788 train shape: (70129, 48, 48, 1)\n",
            "iteration: 34789 train shape: (70130, 48, 48, 1)\n",
            "iteration: 34790 train shape: (70131, 48, 48, 1)\n",
            "iteration: 34791 train shape: (70132, 48, 48, 1)\n",
            "iteration: 34792 train shape: (70133, 48, 48, 1)\n",
            "iteration: 34793 train shape: (70134, 48, 48, 1)\n",
            "iteration: 34794 train shape: (70135, 48, 48, 1)\n",
            "iteration: 34795 train shape: (70136, 48, 48, 1)\n",
            "iteration: 34796 train shape: (70137, 48, 48, 1)\n",
            "iteration: 34797 train shape: (70138, 48, 48, 1)\n",
            "iteration: 34798 train shape: (70139, 48, 48, 1)\n",
            "iteration: 34799 train shape: (70140, 48, 48, 1)\n",
            "iteration: 34800 train shape: (70141, 48, 48, 1)\n",
            "iteration: 34801 train shape: (70142, 48, 48, 1)\n",
            "iteration: 34802 train shape: (70143, 48, 48, 1)\n",
            "iteration: 34803 train shape: (70144, 48, 48, 1)\n",
            "iteration: 34804 train shape: (70145, 48, 48, 1)\n",
            "iteration: 34805 train shape: (70146, 48, 48, 1)\n",
            "iteration: 34806 train shape: (70147, 48, 48, 1)\n",
            "iteration: 34807 train shape: (70148, 48, 48, 1)\n",
            "iteration: 34808 train shape: (70149, 48, 48, 1)\n",
            "iteration: 34809 train shape: (70150, 48, 48, 1)\n",
            "iteration: 34810 train shape: (70151, 48, 48, 1)\n",
            "iteration: 34811 train shape: (70152, 48, 48, 1)\n",
            "iteration: 34812 train shape: (70153, 48, 48, 1)\n",
            "iteration: 34813 train shape: (70154, 48, 48, 1)\n",
            "iteration: 34814 train shape: (70155, 48, 48, 1)\n",
            "iteration: 34815 train shape: (70156, 48, 48, 1)\n",
            "iteration: 34816 train shape: (70157, 48, 48, 1)\n",
            "iteration: 34817 train shape: (70158, 48, 48, 1)\n",
            "iteration: 34818 train shape: (70159, 48, 48, 1)\n",
            "iteration: 34819 train shape: (70160, 48, 48, 1)\n",
            "iteration: 34820 train shape: (70161, 48, 48, 1)\n",
            "iteration: 34821 train shape: (70162, 48, 48, 1)\n",
            "iteration: 34822 train shape: (70163, 48, 48, 1)\n",
            "iteration: 34823 train shape: (70164, 48, 48, 1)\n",
            "iteration: 34824 train shape: (70165, 48, 48, 1)\n",
            "iteration: 34825 train shape: (70166, 48, 48, 1)\n",
            "iteration: 34826 train shape: (70167, 48, 48, 1)\n",
            "iteration: 34827 train shape: (70168, 48, 48, 1)\n",
            "iteration: 34828 train shape: (70169, 48, 48, 1)\n",
            "iteration: 34829 train shape: (70170, 48, 48, 1)\n",
            "iteration: 34830 train shape: (70171, 48, 48, 1)\n",
            "iteration: 34831 train shape: (70172, 48, 48, 1)\n",
            "iteration: 34832 train shape: (70173, 48, 48, 1)\n",
            "iteration: 34833 train shape: (70174, 48, 48, 1)\n",
            "iteration: 34834 train shape: (70175, 48, 48, 1)\n",
            "iteration: 34835 train shape: (70176, 48, 48, 1)\n",
            "iteration: 34836 train shape: (70177, 48, 48, 1)\n",
            "iteration: 34837 train shape: (70178, 48, 48, 1)\n",
            "iteration: 34838 train shape: (70179, 48, 48, 1)\n",
            "iteration: 34839 train shape: (70180, 48, 48, 1)\n",
            "iteration: 34840 train shape: (70181, 48, 48, 1)\n",
            "iteration: 34841 train shape: (70182, 48, 48, 1)\n",
            "iteration: 34842 train shape: (70183, 48, 48, 1)\n",
            "iteration: 34843 train shape: (70184, 48, 48, 1)\n",
            "iteration: 34844 train shape: (70185, 48, 48, 1)\n",
            "iteration: 34845 train shape: (70186, 48, 48, 1)\n",
            "iteration: 34846 train shape: (70187, 48, 48, 1)\n",
            "iteration: 34847 train shape: (70188, 48, 48, 1)\n",
            "iteration: 34848 train shape: (70189, 48, 48, 1)\n",
            "iteration: 34849 train shape: (70190, 48, 48, 1)\n",
            "iteration: 34850 train shape: (70191, 48, 48, 1)\n",
            "iteration: 34851 train shape: (70192, 48, 48, 1)\n",
            "iteration: 34852 train shape: (70193, 48, 48, 1)\n",
            "iteration: 34853 train shape: (70194, 48, 48, 1)\n",
            "iteration: 34854 train shape: (70195, 48, 48, 1)\n",
            "iteration: 34855 train shape: (70196, 48, 48, 1)\n",
            "iteration: 34856 train shape: (70197, 48, 48, 1)\n",
            "iteration: 34857 train shape: (70198, 48, 48, 1)\n",
            "iteration: 34858 train shape: (70199, 48, 48, 1)\n",
            "iteration: 34859 train shape: (70200, 48, 48, 1)\n",
            "iteration: 34860 train shape: (70201, 48, 48, 1)\n",
            "iteration: 34861 train shape: (70202, 48, 48, 1)\n",
            "iteration: 34862 train shape: (70203, 48, 48, 1)\n",
            "iteration: 34863 train shape: (70204, 48, 48, 1)\n",
            "iteration: 34864 train shape: (70205, 48, 48, 1)\n",
            "iteration: 34865 train shape: (70206, 48, 48, 1)\n",
            "iteration: 34866 train shape: (70207, 48, 48, 1)\n",
            "iteration: 34867 train shape: (70208, 48, 48, 1)\n",
            "iteration: 34868 train shape: (70209, 48, 48, 1)\n",
            "iteration: 34869 train shape: (70210, 48, 48, 1)\n",
            "iteration: 34870 train shape: (70211, 48, 48, 1)\n",
            "iteration: 34871 train shape: (70212, 48, 48, 1)\n",
            "iteration: 34872 train shape: (70213, 48, 48, 1)\n",
            "iteration: 34873 train shape: (70214, 48, 48, 1)\n",
            "iteration: 34874 train shape: (70215, 48, 48, 1)\n",
            "iteration: 34875 train shape: (70216, 48, 48, 1)\n",
            "iteration: 34876 train shape: (70217, 48, 48, 1)\n",
            "iteration: 34877 train shape: (70218, 48, 48, 1)\n",
            "iteration: 34878 train shape: (70219, 48, 48, 1)\n",
            "iteration: 34879 train shape: (70220, 48, 48, 1)\n",
            "iteration: 34880 train shape: (70221, 48, 48, 1)\n",
            "iteration: 34881 train shape: (70222, 48, 48, 1)\n",
            "iteration: 34882 train shape: (70223, 48, 48, 1)\n",
            "iteration: 34883 train shape: (70224, 48, 48, 1)\n",
            "iteration: 34884 train shape: (70225, 48, 48, 1)\n",
            "iteration: 34885 train shape: (70226, 48, 48, 1)\n",
            "iteration: 34886 train shape: (70227, 48, 48, 1)\n",
            "iteration: 34887 train shape: (70228, 48, 48, 1)\n",
            "iteration: 34888 train shape: (70229, 48, 48, 1)\n",
            "iteration: 34889 train shape: (70230, 48, 48, 1)\n",
            "iteration: 34890 train shape: (70231, 48, 48, 1)\n",
            "iteration: 34891 train shape: (70232, 48, 48, 1)\n",
            "iteration: 34892 train shape: (70233, 48, 48, 1)\n",
            "iteration: 34893 train shape: (70234, 48, 48, 1)\n",
            "iteration: 34894 train shape: (70235, 48, 48, 1)\n",
            "iteration: 34895 train shape: (70236, 48, 48, 1)\n",
            "iteration: 34896 train shape: (70237, 48, 48, 1)\n",
            "iteration: 34897 train shape: (70238, 48, 48, 1)\n",
            "iteration: 34898 train shape: (70239, 48, 48, 1)\n",
            "iteration: 34899 train shape: (70240, 48, 48, 1)\n",
            "iteration: 34900 train shape: (70241, 48, 48, 1)\n",
            "iteration: 34901 train shape: (70242, 48, 48, 1)\n",
            "iteration: 34902 train shape: (70243, 48, 48, 1)\n",
            "iteration: 34903 train shape: (70244, 48, 48, 1)\n",
            "iteration: 34904 train shape: (70245, 48, 48, 1)\n",
            "iteration: 34905 train shape: (70246, 48, 48, 1)\n",
            "iteration: 34906 train shape: (70247, 48, 48, 1)\n",
            "iteration: 34907 train shape: (70248, 48, 48, 1)\n",
            "iteration: 34908 train shape: (70249, 48, 48, 1)\n",
            "iteration: 34909 train shape: (70250, 48, 48, 1)\n",
            "iteration: 34910 train shape: (70251, 48, 48, 1)\n",
            "iteration: 34911 train shape: (70252, 48, 48, 1)\n",
            "iteration: 34912 train shape: (70253, 48, 48, 1)\n",
            "iteration: 34913 train shape: (70254, 48, 48, 1)\n",
            "iteration: 34914 train shape: (70255, 48, 48, 1)\n",
            "iteration: 34915 train shape: (70256, 48, 48, 1)\n",
            "iteration: 34916 train shape: (70257, 48, 48, 1)\n",
            "iteration: 34917 train shape: (70258, 48, 48, 1)\n",
            "iteration: 34918 train shape: (70259, 48, 48, 1)\n",
            "iteration: 34919 train shape: (70260, 48, 48, 1)\n",
            "iteration: 34920 train shape: (70261, 48, 48, 1)\n",
            "iteration: 34921 train shape: (70262, 48, 48, 1)\n",
            "iteration: 34922 train shape: (70263, 48, 48, 1)\n",
            "iteration: 34923 train shape: (70264, 48, 48, 1)\n",
            "iteration: 34924 train shape: (70265, 48, 48, 1)\n",
            "iteration: 34925 train shape: (70266, 48, 48, 1)\n",
            "iteration: 34926 train shape: (70267, 48, 48, 1)\n",
            "iteration: 34927 train shape: (70268, 48, 48, 1)\n",
            "iteration: 34928 train shape: (70269, 48, 48, 1)\n",
            "iteration: 34929 train shape: (70270, 48, 48, 1)\n",
            "iteration: 34930 train shape: (70271, 48, 48, 1)\n",
            "iteration: 34931 train shape: (70272, 48, 48, 1)\n",
            "iteration: 34932 train shape: (70273, 48, 48, 1)\n",
            "iteration: 34933 train shape: (70274, 48, 48, 1)\n",
            "iteration: 34934 train shape: (70275, 48, 48, 1)\n",
            "iteration: 34935 train shape: (70276, 48, 48, 1)\n",
            "iteration: 34936 train shape: (70277, 48, 48, 1)\n",
            "iteration: 34937 train shape: (70278, 48, 48, 1)\n",
            "iteration: 34938 train shape: (70279, 48, 48, 1)\n",
            "iteration: 34939 train shape: (70280, 48, 48, 1)\n",
            "iteration: 34940 train shape: (70281, 48, 48, 1)\n",
            "iteration: 34941 train shape: (70282, 48, 48, 1)\n",
            "iteration: 34942 train shape: (70283, 48, 48, 1)\n",
            "iteration: 34943 train shape: (70284, 48, 48, 1)\n",
            "iteration: 34944 train shape: (70285, 48, 48, 1)\n",
            "iteration: 34945 train shape: (70286, 48, 48, 1)\n",
            "iteration: 34946 train shape: (70287, 48, 48, 1)\n",
            "iteration: 34947 train shape: (70288, 48, 48, 1)\n",
            "iteration: 34948 train shape: (70289, 48, 48, 1)\n",
            "iteration: 34949 train shape: (70290, 48, 48, 1)\n",
            "iteration: 34950 train shape: (70291, 48, 48, 1)\n",
            "iteration: 34951 train shape: (70292, 48, 48, 1)\n",
            "iteration: 34952 train shape: (70293, 48, 48, 1)\n",
            "iteration: 34953 train shape: (70294, 48, 48, 1)\n",
            "iteration: 34954 train shape: (70295, 48, 48, 1)\n",
            "iteration: 34955 train shape: (70296, 48, 48, 1)\n",
            "iteration: 34956 train shape: (70297, 48, 48, 1)\n",
            "iteration: 34957 train shape: (70298, 48, 48, 1)\n",
            "iteration: 34958 train shape: (70299, 48, 48, 1)\n",
            "iteration: 34959 train shape: (70300, 48, 48, 1)\n",
            "iteration: 34960 train shape: (70301, 48, 48, 1)\n",
            "iteration: 34961 train shape: (70302, 48, 48, 1)\n",
            "iteration: 34962 train shape: (70303, 48, 48, 1)\n",
            "iteration: 34963 train shape: (70304, 48, 48, 1)\n",
            "iteration: 34964 train shape: (70305, 48, 48, 1)\n",
            "iteration: 34965 train shape: (70306, 48, 48, 1)\n",
            "iteration: 34966 train shape: (70307, 48, 48, 1)\n",
            "iteration: 34967 train shape: (70308, 48, 48, 1)\n",
            "iteration: 34968 train shape: (70309, 48, 48, 1)\n",
            "iteration: 34969 train shape: (70310, 48, 48, 1)\n",
            "iteration: 34970 train shape: (70311, 48, 48, 1)\n",
            "iteration: 34971 train shape: (70312, 48, 48, 1)\n",
            "iteration: 34972 train shape: (70313, 48, 48, 1)\n",
            "iteration: 34973 train shape: (70314, 48, 48, 1)\n",
            "iteration: 34974 train shape: (70315, 48, 48, 1)\n",
            "iteration: 34975 train shape: (70316, 48, 48, 1)\n",
            "iteration: 34976 train shape: (70317, 48, 48, 1)\n",
            "iteration: 34977 train shape: (70318, 48, 48, 1)\n",
            "iteration: 34978 train shape: (70319, 48, 48, 1)\n",
            "iteration: 34979 train shape: (70320, 48, 48, 1)\n",
            "iteration: 34980 train shape: (70321, 48, 48, 1)\n",
            "iteration: 34981 train shape: (70322, 48, 48, 1)\n",
            "iteration: 34982 train shape: (70323, 48, 48, 1)\n",
            "iteration: 34983 train shape: (70324, 48, 48, 1)\n",
            "iteration: 34984 train shape: (70325, 48, 48, 1)\n",
            "iteration: 34985 train shape: (70326, 48, 48, 1)\n",
            "iteration: 34986 train shape: (70327, 48, 48, 1)\n",
            "iteration: 34987 train shape: (70328, 48, 48, 1)\n",
            "iteration: 34988 train shape: (70329, 48, 48, 1)\n",
            "iteration: 34989 train shape: (70330, 48, 48, 1)\n",
            "iteration: 34990 train shape: (70331, 48, 48, 1)\n",
            "iteration: 34991 train shape: (70332, 48, 48, 1)\n",
            "iteration: 34992 train shape: (70333, 48, 48, 1)\n",
            "iteration: 34993 train shape: (70334, 48, 48, 1)\n",
            "iteration: 34994 train shape: (70335, 48, 48, 1)\n",
            "iteration: 34995 train shape: (70336, 48, 48, 1)\n",
            "iteration: 34996 train shape: (70337, 48, 48, 1)\n",
            "iteration: 34997 train shape: (70338, 48, 48, 1)\n",
            "iteration: 34998 train shape: (70339, 48, 48, 1)\n",
            "iteration: 34999 train shape: (70340, 48, 48, 1)\n",
            "iteration: 35000 train shape: (70341, 48, 48, 1)\n",
            "iteration: 35001 train shape: (70342, 48, 48, 1)\n",
            "iteration: 35002 train shape: (70343, 48, 48, 1)\n",
            "iteration: 35003 train shape: (70344, 48, 48, 1)\n",
            "iteration: 35004 train shape: (70345, 48, 48, 1)\n",
            "iteration: 35005 train shape: (70346, 48, 48, 1)\n",
            "iteration: 35006 train shape: (70347, 48, 48, 1)\n",
            "iteration: 35007 train shape: (70348, 48, 48, 1)\n",
            "iteration: 35008 train shape: (70349, 48, 48, 1)\n",
            "iteration: 35009 train shape: (70350, 48, 48, 1)\n",
            "iteration: 35010 train shape: (70351, 48, 48, 1)\n",
            "iteration: 35011 train shape: (70352, 48, 48, 1)\n",
            "iteration: 35012 train shape: (70353, 48, 48, 1)\n",
            "iteration: 35013 train shape: (70354, 48, 48, 1)\n",
            "iteration: 35014 train shape: (70355, 48, 48, 1)\n",
            "iteration: 35015 train shape: (70356, 48, 48, 1)\n",
            "iteration: 35016 train shape: (70357, 48, 48, 1)\n",
            "iteration: 35017 train shape: (70358, 48, 48, 1)\n",
            "iteration: 35018 train shape: (70359, 48, 48, 1)\n",
            "iteration: 35019 train shape: (70360, 48, 48, 1)\n",
            "iteration: 35020 train shape: (70361, 48, 48, 1)\n",
            "iteration: 35021 train shape: (70362, 48, 48, 1)\n",
            "iteration: 35022 train shape: (70363, 48, 48, 1)\n",
            "iteration: 35023 train shape: (70364, 48, 48, 1)\n",
            "iteration: 35024 train shape: (70365, 48, 48, 1)\n",
            "iteration: 35025 train shape: (70366, 48, 48, 1)\n",
            "iteration: 35026 train shape: (70367, 48, 48, 1)\n",
            "iteration: 35027 train shape: (70368, 48, 48, 1)\n",
            "iteration: 35028 train shape: (70369, 48, 48, 1)\n",
            "iteration: 35029 train shape: (70370, 48, 48, 1)\n",
            "iteration: 35030 train shape: (70371, 48, 48, 1)\n",
            "iteration: 35031 train shape: (70372, 48, 48, 1)\n",
            "iteration: 35032 train shape: (70373, 48, 48, 1)\n",
            "iteration: 35033 train shape: (70374, 48, 48, 1)\n",
            "iteration: 35034 train shape: (70375, 48, 48, 1)\n",
            "iteration: 35035 train shape: (70376, 48, 48, 1)\n",
            "iteration: 35036 train shape: (70377, 48, 48, 1)\n",
            "iteration: 35037 train shape: (70378, 48, 48, 1)\n",
            "iteration: 35038 train shape: (70379, 48, 48, 1)\n",
            "iteration: 35039 train shape: (70380, 48, 48, 1)\n",
            "iteration: 35040 train shape: (70381, 48, 48, 1)\n",
            "iteration: 35041 train shape: (70382, 48, 48, 1)\n",
            "iteration: 35042 train shape: (70383, 48, 48, 1)\n",
            "iteration: 35043 train shape: (70384, 48, 48, 1)\n",
            "iteration: 35044 train shape: (70385, 48, 48, 1)\n",
            "iteration: 35045 train shape: (70386, 48, 48, 1)\n",
            "iteration: 35046 train shape: (70387, 48, 48, 1)\n",
            "iteration: 35047 train shape: (70388, 48, 48, 1)\n",
            "iteration: 35048 train shape: (70389, 48, 48, 1)\n",
            "iteration: 35049 train shape: (70390, 48, 48, 1)\n",
            "iteration: 35050 train shape: (70391, 48, 48, 1)\n",
            "iteration: 35051 train shape: (70392, 48, 48, 1)\n",
            "iteration: 35052 train shape: (70393, 48, 48, 1)\n",
            "iteration: 35053 train shape: (70394, 48, 48, 1)\n",
            "iteration: 35054 train shape: (70395, 48, 48, 1)\n",
            "iteration: 35055 train shape: (70396, 48, 48, 1)\n",
            "iteration: 35056 train shape: (70397, 48, 48, 1)\n",
            "iteration: 35057 train shape: (70398, 48, 48, 1)\n",
            "iteration: 35058 train shape: (70399, 48, 48, 1)\n",
            "iteration: 35059 train shape: (70400, 48, 48, 1)\n",
            "iteration: 35060 train shape: (70401, 48, 48, 1)\n",
            "iteration: 35061 train shape: (70402, 48, 48, 1)\n",
            "iteration: 35062 train shape: (70403, 48, 48, 1)\n",
            "iteration: 35063 train shape: (70404, 48, 48, 1)\n",
            "iteration: 35064 train shape: (70405, 48, 48, 1)\n",
            "iteration: 35067 train shape: (70408, 48, 48, 1)\n",
            "iteration: 35068 train shape: (70409, 48, 48, 1)\n",
            "iteration: 35069 train shape: (70410, 48, 48, 1)\n",
            "iteration: 35070 train shape: (70411, 48, 48, 1)\n",
            "iteration: 35071 train shape: (70412, 48, 48, 1)\n",
            "iteration: 35072 train shape: (70413, 48, 48, 1)\n",
            "iteration: 35073 train shape: (70414, 48, 48, 1)\n",
            "iteration: 35074 train shape: (70415, 48, 48, 1)\n",
            "iteration: 35075 train shape: (70416, 48, 48, 1)\n",
            "iteration: 35076 train shape: (70417, 48, 48, 1)\n",
            "iteration: 35077 train shape: (70418, 48, 48, 1)\n",
            "iteration: 35078 train shape: (70419, 48, 48, 1)\n",
            "iteration: 35079 train shape: (70420, 48, 48, 1)\n",
            "iteration: 35080 train shape: (70421, 48, 48, 1)\n",
            "iteration: 35081 train shape: (70422, 48, 48, 1)\n",
            "iteration: 35082 train shape: (70423, 48, 48, 1)\n",
            "iteration: 35083 train shape: (70424, 48, 48, 1)\n",
            "iteration: 35084 train shape: (70425, 48, 48, 1)\n",
            "iteration: 35085 train shape: (70426, 48, 48, 1)\n",
            "iteration: 35086 train shape: (70427, 48, 48, 1)\n",
            "iteration: 35087 train shape: (70428, 48, 48, 1)\n",
            "iteration: 35088 train shape: (70429, 48, 48, 1)\n",
            "iteration: 35089 train shape: (70430, 48, 48, 1)\n",
            "iteration: 35090 train shape: (70431, 48, 48, 1)\n",
            "iteration: 35091 train shape: (70432, 48, 48, 1)\n",
            "iteration: 35092 train shape: (70433, 48, 48, 1)\n",
            "iteration: 35093 train shape: (70434, 48, 48, 1)\n",
            "iteration: 35094 train shape: (70435, 48, 48, 1)\n",
            "iteration: 35095 train shape: (70436, 48, 48, 1)\n",
            "iteration: 35096 train shape: (70437, 48, 48, 1)\n",
            "iteration: 35097 train shape: (70438, 48, 48, 1)\n",
            "iteration: 35098 train shape: (70439, 48, 48, 1)\n",
            "iteration: 35099 train shape: (70440, 48, 48, 1)\n",
            "iteration: 35100 train shape: (70441, 48, 48, 1)\n",
            "iteration: 35101 train shape: (70442, 48, 48, 1)\n",
            "iteration: 35102 train shape: (70443, 48, 48, 1)\n",
            "iteration: 35103 train shape: (70444, 48, 48, 1)\n",
            "iteration: 35104 train shape: (70445, 48, 48, 1)\n",
            "iteration: 35105 train shape: (70446, 48, 48, 1)\n",
            "iteration: 35106 train shape: (70447, 48, 48, 1)\n",
            "iteration: 35107 train shape: (70448, 48, 48, 1)\n",
            "iteration: 35108 train shape: (70449, 48, 48, 1)\n",
            "iteration: 35109 train shape: (70450, 48, 48, 1)\n",
            "iteration: 35110 train shape: (70451, 48, 48, 1)\n",
            "iteration: 35111 train shape: (70452, 48, 48, 1)\n",
            "iteration: 35112 train shape: (70453, 48, 48, 1)\n",
            "iteration: 35113 train shape: (70454, 48, 48, 1)\n",
            "iteration: 35114 train shape: (70455, 48, 48, 1)\n",
            "iteration: 35115 train shape: (70456, 48, 48, 1)\n",
            "iteration: 35116 train shape: (70457, 48, 48, 1)\n",
            "iteration: 35117 train shape: (70458, 48, 48, 1)\n",
            "iteration: 35118 train shape: (70459, 48, 48, 1)\n",
            "iteration: 35119 train shape: (70460, 48, 48, 1)\n",
            "iteration: 35120 train shape: (70461, 48, 48, 1)\n",
            "iteration: 35121 train shape: (70462, 48, 48, 1)\n",
            "iteration: 35122 train shape: (70463, 48, 48, 1)\n",
            "iteration: 35123 train shape: (70464, 48, 48, 1)\n",
            "iteration: 35124 train shape: (70465, 48, 48, 1)\n",
            "iteration: 35125 train shape: (70466, 48, 48, 1)\n",
            "iteration: 35126 train shape: (70467, 48, 48, 1)\n",
            "iteration: 35127 train shape: (70468, 48, 48, 1)\n",
            "iteration: 35128 train shape: (70469, 48, 48, 1)\n",
            "iteration: 35129 train shape: (70470, 48, 48, 1)\n",
            "iteration: 35130 train shape: (70471, 48, 48, 1)\n",
            "iteration: 35131 train shape: (70472, 48, 48, 1)\n",
            "iteration: 35132 train shape: (70473, 48, 48, 1)\n",
            "iteration: 35133 train shape: (70474, 48, 48, 1)\n",
            "iteration: 35134 train shape: (70475, 48, 48, 1)\n",
            "iteration: 35135 train shape: (70476, 48, 48, 1)\n",
            "iteration: 35136 train shape: (70477, 48, 48, 1)\n",
            "iteration: 35137 train shape: (70478, 48, 48, 1)\n",
            "iteration: 35138 train shape: (70479, 48, 48, 1)\n",
            "iteration: 35139 train shape: (70480, 48, 48, 1)\n",
            "iteration: 35140 train shape: (70481, 48, 48, 1)\n",
            "iteration: 35141 train shape: (70482, 48, 48, 1)\n",
            "iteration: 35142 train shape: (70483, 48, 48, 1)\n",
            "iteration: 35143 train shape: (70484, 48, 48, 1)\n",
            "iteration: 35144 train shape: (70485, 48, 48, 1)\n",
            "iteration: 35145 train shape: (70486, 48, 48, 1)\n",
            "iteration: 35146 train shape: (70487, 48, 48, 1)\n",
            "iteration: 35147 train shape: (70488, 48, 48, 1)\n",
            "iteration: 35148 train shape: (70489, 48, 48, 1)\n",
            "iteration: 35149 train shape: (70490, 48, 48, 1)\n",
            "iteration: 35150 train shape: (70491, 48, 48, 1)\n",
            "iteration: 35151 train shape: (70492, 48, 48, 1)\n",
            "iteration: 35152 train shape: (70493, 48, 48, 1)\n",
            "iteration: 35153 train shape: (70494, 48, 48, 1)\n",
            "iteration: 35154 train shape: (70495, 48, 48, 1)\n",
            "iteration: 35155 train shape: (70496, 48, 48, 1)\n",
            "iteration: 35156 train shape: (70497, 48, 48, 1)\n",
            "iteration: 35157 train shape: (70498, 48, 48, 1)\n",
            "iteration: 35158 train shape: (70499, 48, 48, 1)\n",
            "iteration: 35159 train shape: (70500, 48, 48, 1)\n",
            "iteration: 35160 train shape: (70501, 48, 48, 1)\n",
            "iteration: 35161 train shape: (70502, 48, 48, 1)\n",
            "iteration: 35162 train shape: (70503, 48, 48, 1)\n",
            "iteration: 35163 train shape: (70504, 48, 48, 1)\n",
            "iteration: 35164 train shape: (70505, 48, 48, 1)\n",
            "iteration: 35165 train shape: (70506, 48, 48, 1)\n",
            "iteration: 35166 train shape: (70507, 48, 48, 1)\n",
            "iteration: 35167 train shape: (70508, 48, 48, 1)\n",
            "iteration: 35168 train shape: (70509, 48, 48, 1)\n",
            "iteration: 35169 train shape: (70510, 48, 48, 1)\n",
            "iteration: 35170 train shape: (70511, 48, 48, 1)\n",
            "iteration: 35171 train shape: (70512, 48, 48, 1)\n",
            "iteration: 35172 train shape: (70513, 48, 48, 1)\n",
            "iteration: 35173 train shape: (70514, 48, 48, 1)\n",
            "iteration: 35174 train shape: (70515, 48, 48, 1)\n",
            "iteration: 35175 train shape: (70516, 48, 48, 1)\n",
            "iteration: 35176 train shape: (70517, 48, 48, 1)\n",
            "iteration: 35177 train shape: (70518, 48, 48, 1)\n",
            "iteration: 35178 train shape: (70519, 48, 48, 1)\n",
            "iteration: 35179 train shape: (70520, 48, 48, 1)\n",
            "iteration: 35180 train shape: (70521, 48, 48, 1)\n",
            "iteration: 35181 train shape: (70522, 48, 48, 1)\n",
            "iteration: 35182 train shape: (70523, 48, 48, 1)\n",
            "iteration: 35183 train shape: (70524, 48, 48, 1)\n",
            "iteration: 35184 train shape: (70525, 48, 48, 1)\n",
            "iteration: 35185 train shape: (70526, 48, 48, 1)\n",
            "iteration: 35186 train shape: (70527, 48, 48, 1)\n",
            "iteration: 35187 train shape: (70528, 48, 48, 1)\n",
            "iteration: 35188 train shape: (70529, 48, 48, 1)\n",
            "iteration: 35189 train shape: (70530, 48, 48, 1)\n",
            "iteration: 35190 train shape: (70531, 48, 48, 1)\n",
            "iteration: 35191 train shape: (70532, 48, 48, 1)\n",
            "iteration: 35192 train shape: (70533, 48, 48, 1)\n",
            "iteration: 35193 train shape: (70534, 48, 48, 1)\n",
            "iteration: 35194 train shape: (70535, 48, 48, 1)\n",
            "iteration: 35195 train shape: (70536, 48, 48, 1)\n",
            "iteration: 35196 train shape: (70537, 48, 48, 1)\n",
            "iteration: 35197 train shape: (70538, 48, 48, 1)\n",
            "iteration: 35198 train shape: (70539, 48, 48, 1)\n",
            "iteration: 35199 train shape: (70540, 48, 48, 1)\n",
            "iteration: 35200 train shape: (70541, 48, 48, 1)\n",
            "iteration: 35201 train shape: (70542, 48, 48, 1)\n",
            "iteration: 35202 train shape: (70543, 48, 48, 1)\n",
            "iteration: 35203 train shape: (70544, 48, 48, 1)\n",
            "iteration: 35204 train shape: (70545, 48, 48, 1)\n",
            "iteration: 35205 train shape: (70546, 48, 48, 1)\n",
            "iteration: 35206 train shape: (70547, 48, 48, 1)\n",
            "iteration: 35207 train shape: (70548, 48, 48, 1)\n",
            "iteration: 35208 train shape: (70549, 48, 48, 1)\n",
            "iteration: 35209 train shape: (70550, 48, 48, 1)\n",
            "iteration: 35210 train shape: (70551, 48, 48, 1)\n",
            "iteration: 35211 train shape: (70552, 48, 48, 1)\n",
            "iteration: 35212 train shape: (70553, 48, 48, 1)\n",
            "iteration: 35213 train shape: (70554, 48, 48, 1)\n",
            "iteration: 35214 train shape: (70555, 48, 48, 1)\n",
            "iteration: 35215 train shape: (70556, 48, 48, 1)\n",
            "iteration: 35216 train shape: (70557, 48, 48, 1)\n",
            "iteration: 35217 train shape: (70558, 48, 48, 1)\n",
            "iteration: 35218 train shape: (70559, 48, 48, 1)\n",
            "iteration: 35219 train shape: (70560, 48, 48, 1)\n",
            "iteration: 35220 train shape: (70561, 48, 48, 1)\n",
            "iteration: 35221 train shape: (70562, 48, 48, 1)\n",
            "iteration: 35222 train shape: (70563, 48, 48, 1)\n",
            "iteration: 35223 train shape: (70564, 48, 48, 1)\n",
            "iteration: 35224 train shape: (70565, 48, 48, 1)\n",
            "iteration: 35225 train shape: (70566, 48, 48, 1)\n",
            "iteration: 35226 train shape: (70567, 48, 48, 1)\n",
            "iteration: 35227 train shape: (70568, 48, 48, 1)\n",
            "iteration: 35228 train shape: (70569, 48, 48, 1)\n",
            "iteration: 35229 train shape: (70570, 48, 48, 1)\n",
            "iteration: 35230 train shape: (70571, 48, 48, 1)\n",
            "iteration: 35231 train shape: (70572, 48, 48, 1)\n",
            "iteration: 35232 train shape: (70573, 48, 48, 1)\n",
            "iteration: 35233 train shape: (70574, 48, 48, 1)\n",
            "iteration: 35234 train shape: (70575, 48, 48, 1)\n",
            "iteration: 35235 train shape: (70576, 48, 48, 1)\n",
            "iteration: 35236 train shape: (70577, 48, 48, 1)\n",
            "iteration: 35237 train shape: (70578, 48, 48, 1)\n",
            "iteration: 35238 train shape: (70579, 48, 48, 1)\n",
            "iteration: 35239 train shape: (70580, 48, 48, 1)\n",
            "iteration: 35240 train shape: (70581, 48, 48, 1)\n",
            "iteration: 35241 train shape: (70582, 48, 48, 1)\n",
            "iteration: 35242 train shape: (70583, 48, 48, 1)\n",
            "iteration: 35243 train shape: (70584, 48, 48, 1)\n",
            "iteration: 35244 train shape: (70585, 48, 48, 1)\n",
            "iteration: 35245 train shape: (70586, 48, 48, 1)\n",
            "iteration: 35246 train shape: (70587, 48, 48, 1)\n",
            "iteration: 35247 train shape: (70588, 48, 48, 1)\n",
            "iteration: 35248 train shape: (70589, 48, 48, 1)\n",
            "iteration: 35249 train shape: (70590, 48, 48, 1)\n",
            "iteration: 35250 train shape: (70591, 48, 48, 1)\n",
            "iteration: 35251 train shape: (70592, 48, 48, 1)\n",
            "iteration: 35252 train shape: (70593, 48, 48, 1)\n",
            "iteration: 35253 train shape: (70594, 48, 48, 1)\n",
            "iteration: 35254 train shape: (70595, 48, 48, 1)\n",
            "iteration: 35255 train shape: (70596, 48, 48, 1)\n",
            "iteration: 35256 train shape: (70597, 48, 48, 1)\n",
            "iteration: 35257 train shape: (70598, 48, 48, 1)\n",
            "iteration: 35258 train shape: (70599, 48, 48, 1)\n",
            "iteration: 35259 train shape: (70600, 48, 48, 1)\n",
            "iteration: 35260 train shape: (70601, 48, 48, 1)\n",
            "iteration: 35261 train shape: (70602, 48, 48, 1)\n",
            "iteration: 35262 train shape: (70603, 48, 48, 1)\n",
            "iteration: 35263 train shape: (70604, 48, 48, 1)\n",
            "iteration: 35264 train shape: (70605, 48, 48, 1)\n",
            "iteration: 35265 train shape: (70606, 48, 48, 1)\n",
            "iteration: 35266 train shape: (70607, 48, 48, 1)\n",
            "iteration: 35267 train shape: (70608, 48, 48, 1)\n",
            "iteration: 35268 train shape: (70609, 48, 48, 1)\n",
            "iteration: 35269 train shape: (70610, 48, 48, 1)\n",
            "iteration: 35270 train shape: (70611, 48, 48, 1)\n",
            "iteration: 35271 train shape: (70612, 48, 48, 1)\n",
            "iteration: 35272 train shape: (70613, 48, 48, 1)\n",
            "iteration: 35273 train shape: (70614, 48, 48, 1)\n",
            "iteration: 35274 train shape: (70615, 48, 48, 1)\n",
            "iteration: 35275 train shape: (70616, 48, 48, 1)\n",
            "iteration: 35276 train shape: (70617, 48, 48, 1)\n",
            "iteration: 35277 train shape: (70618, 48, 48, 1)\n",
            "iteration: 35278 train shape: (70619, 48, 48, 1)\n",
            "iteration: 35279 train shape: (70620, 48, 48, 1)\n",
            "iteration: 35280 train shape: (70621, 48, 48, 1)\n",
            "iteration: 35281 train shape: (70622, 48, 48, 1)\n",
            "iteration: 35282 train shape: (70623, 48, 48, 1)\n",
            "iteration: 35283 train shape: (70624, 48, 48, 1)\n",
            "iteration: 35284 train shape: (70625, 48, 48, 1)\n",
            "iteration: 35285 train shape: (70626, 48, 48, 1)\n",
            "iteration: 35286 train shape: (70627, 48, 48, 1)\n",
            "iteration: 35287 train shape: (70628, 48, 48, 1)\n",
            "iteration: 35288 train shape: (70629, 48, 48, 1)\n",
            "iteration: 35289 train shape: (70630, 48, 48, 1)\n",
            "iteration: 35290 train shape: (70631, 48, 48, 1)\n",
            "iteration: 35291 train shape: (70632, 48, 48, 1)\n",
            "iteration: 35292 train shape: (70633, 48, 48, 1)\n",
            "iteration: 35293 train shape: (70634, 48, 48, 1)\n",
            "iteration: 35294 train shape: (70635, 48, 48, 1)\n",
            "iteration: 35295 train shape: (70636, 48, 48, 1)\n",
            "iteration: 35296 train shape: (70637, 48, 48, 1)\n",
            "iteration: 35297 train shape: (70638, 48, 48, 1)\n",
            "iteration: 35298 train shape: (70639, 48, 48, 1)\n",
            "iteration: 35299 train shape: (70640, 48, 48, 1)\n",
            "iteration: 35300 train shape: (70641, 48, 48, 1)\n",
            "iteration: 35301 train shape: (70642, 48, 48, 1)\n",
            "iteration: 35302 train shape: (70643, 48, 48, 1)\n",
            "iteration: 35303 train shape: (70644, 48, 48, 1)\n",
            "iteration: 35304 train shape: (70645, 48, 48, 1)\n",
            "iteration: 35305 train shape: (70646, 48, 48, 1)\n",
            "iteration: 35306 train shape: (70647, 48, 48, 1)\n",
            "iteration: 35307 train shape: (70648, 48, 48, 1)\n",
            "iteration: 35308 train shape: (70649, 48, 48, 1)\n",
            "iteration: 35309 train shape: (70650, 48, 48, 1)\n",
            "iteration: 35310 train shape: (70651, 48, 48, 1)\n",
            "iteration: 35311 train shape: (70652, 48, 48, 1)\n",
            "iteration: 35312 train shape: (70653, 48, 48, 1)\n",
            "iteration: 35313 train shape: (70654, 48, 48, 1)\n",
            "iteration: 35314 train shape: (70655, 48, 48, 1)\n",
            "iteration: 35315 train shape: (70656, 48, 48, 1)\n",
            "iteration: 35316 train shape: (70657, 48, 48, 1)\n",
            "iteration: 35317 train shape: (70658, 48, 48, 1)\n",
            "iteration: 35318 train shape: (70659, 48, 48, 1)\n",
            "iteration: 35319 train shape: (70660, 48, 48, 1)\n",
            "iteration: 35320 train shape: (70661, 48, 48, 1)\n",
            "iteration: 35321 train shape: (70662, 48, 48, 1)\n",
            "iteration: 35322 train shape: (70663, 48, 48, 1)\n",
            "iteration: 35323 train shape: (70664, 48, 48, 1)\n",
            "iteration: 35324 train shape: (70665, 48, 48, 1)\n",
            "iteration: 35325 train shape: (70666, 48, 48, 1)\n",
            "iteration: 35326 train shape: (70667, 48, 48, 1)\n",
            "iteration: 35327 train shape: (70668, 48, 48, 1)\n",
            "iteration: 35328 train shape: (70669, 48, 48, 1)\n",
            "iteration: 35329 train shape: (70670, 48, 48, 1)\n",
            "iteration: 35330 train shape: (70671, 48, 48, 1)\n",
            "iteration: 35331 train shape: (70672, 48, 48, 1)\n",
            "iteration: 35332 train shape: (70673, 48, 48, 1)\n",
            "iteration: 35333 train shape: (70674, 48, 48, 1)\n",
            "iteration: 35334 train shape: (70675, 48, 48, 1)\n",
            "iteration: 35335 train shape: (70676, 48, 48, 1)\n",
            "iteration: 35336 train shape: (70677, 48, 48, 1)\n",
            "iteration: 35337 train shape: (70678, 48, 48, 1)\n",
            "iteration: 35338 train shape: (70679, 48, 48, 1)\n",
            "iteration: 35339 train shape: (70680, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5nypb9PBqO"
      },
      "source": [
        "#brightness\n",
        "iterations_per_image = 2\n",
        "for k in range(len(X_train)):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "  for i in range(iterations_per_image):\n",
        "    contrast = iaa.GammaContrast(gamma=2.0 + i)\n",
        "    contrast_image =contrast.augment_image(img)\n",
        "    #contrast_image_reshaped = contrast_image.reshape((1, ) + contrast_image.shape) \n",
        "    contrasted_images.append(contrast_image)\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  #X_train.extend(contrasted_images)\n",
        "  X_train = np.concatenate((X_train, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]*iterations_per_image\n",
        "  #y_train.extend(emotions_list)\n",
        "  y_train = np.concatenate((y_train, emotions_list), axis=0)\n",
        "  print(\"iteration: {}train shape: {}\" ,k,X_train.shape)\n",
        "#plt.show()\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(r\"C:\\Users\\shir2\\Desktop\\Shir\\MSc\\%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sONoLUCKEOZR",
        "outputId": "e0c110bf-a42b-43a9-b157-31ba48dadf08"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=20000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 48, 48, 128)  3328        input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 2304)      13824       input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 100)          0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 2304)         0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3556)         0           flatten_6[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            3557        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4a5dd57b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0 [D loss: 0.700434, acc.: 53.91%] [G loss: 1.467685]\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4a5dd57b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "20 [D loss: 1.252126, acc.: 18.75%] [G loss: 0.721391]\n",
            "40 [D loss: 4.400112, acc.: 0.00%] [G loss: 0.443294]\n",
            "60 [D loss: 0.730817, acc.: 39.45%] [G loss: 2.092690]\n",
            "80 [D loss: 0.511958, acc.: 95.70%] [G loss: 2.511785]\n",
            "100 [D loss: 0.769442, acc.: 51.95%] [G loss: 2.044402]\n",
            "120 [D loss: 0.462046, acc.: 86.72%] [G loss: 2.658156]\n",
            "140 [D loss: 0.266575, acc.: 91.80%] [G loss: 6.591882]\n",
            "160 [D loss: 0.792736, acc.: 21.48%] [G loss: 1.618260]\n",
            "180 [D loss: 0.494554, acc.: 85.94%] [G loss: 2.481463]\n",
            "200 [D loss: 0.348663, acc.: 91.02%] [G loss: 5.188465]\n",
            "220 [D loss: 0.212924, acc.: 88.67%] [G loss: 8.758199]\n",
            "240 [D loss: 0.258529, acc.: 88.67%] [G loss: 12.860028]\n",
            "260 [D loss: 0.711553, acc.: 48.44%] [G loss: 2.995375]\n",
            "280 [D loss: 0.540235, acc.: 78.12%] [G loss: 2.222462]\n",
            "300 [D loss: 0.508876, acc.: 76.95%] [G loss: 3.608289]\n",
            "320 [D loss: 0.216259, acc.: 99.61%] [G loss: 6.394515]\n",
            "340 [D loss: 0.728442, acc.: 55.47%] [G loss: 2.795514]\n",
            "360 [D loss: 0.715313, acc.: 62.11%] [G loss: 2.274983]\n",
            "380 [D loss: 0.306810, acc.: 92.97%] [G loss: 4.154649]\n",
            "400 [D loss: 0.581242, acc.: 77.34%] [G loss: 3.013173]\n",
            "420 [D loss: 0.498744, acc.: 73.83%] [G loss: 4.839546]\n",
            "440 [D loss: 0.732198, acc.: 56.64%] [G loss: 3.602979]\n",
            "460 [D loss: 0.679032, acc.: 59.77%] [G loss: 3.275633]\n",
            "480 [D loss: 0.482416, acc.: 77.34%] [G loss: 3.331464]\n",
            "500 [D loss: 0.523641, acc.: 81.64%] [G loss: 1.687897]\n",
            "520 [D loss: 0.630248, acc.: 69.14%] [G loss: 4.011891]\n",
            "540 [D loss: 0.479106, acc.: 78.91%] [G loss: 2.555727]\n",
            "560 [D loss: 0.502139, acc.: 78.12%] [G loss: 2.797424]\n",
            "580 [D loss: 0.574865, acc.: 70.31%] [G loss: 3.340955]\n",
            "600 [D loss: 0.477788, acc.: 83.59%] [G loss: 3.451527]\n",
            "620 [D loss: 0.426337, acc.: 86.33%] [G loss: 3.193137]\n",
            "640 [D loss: 0.574872, acc.: 68.75%] [G loss: 3.281178]\n",
            "660 [D loss: 0.469788, acc.: 80.86%] [G loss: 3.282333]\n",
            "680 [D loss: 0.549176, acc.: 71.88%] [G loss: 2.870425]\n",
            "700 [D loss: 0.516476, acc.: 75.00%] [G loss: 2.709409]\n",
            "720 [D loss: 0.567604, acc.: 67.97%] [G loss: 3.033612]\n",
            "740 [D loss: 0.559478, acc.: 69.92%] [G loss: 3.003954]\n",
            "760 [D loss: 0.434976, acc.: 83.20%] [G loss: 3.295462]\n",
            "780 [D loss: 0.469579, acc.: 78.52%] [G loss: 3.327326]\n",
            "800 [D loss: 0.543089, acc.: 75.00%] [G loss: 3.137288]\n",
            "820 [D loss: 0.431920, acc.: 85.16%] [G loss: 3.085601]\n",
            "840 [D loss: 0.471709, acc.: 83.98%] [G loss: 3.145498]\n",
            "860 [D loss: 0.554860, acc.: 72.27%] [G loss: 3.229083]\n",
            "880 [D loss: 0.539221, acc.: 71.09%] [G loss: 3.218608]\n",
            "900 [D loss: 0.463637, acc.: 79.30%] [G loss: 3.260872]\n",
            "920 [D loss: 0.613659, acc.: 66.80%] [G loss: 2.717738]\n",
            "940 [D loss: 0.442565, acc.: 83.20%] [G loss: 3.182080]\n",
            "960 [D loss: 0.505468, acc.: 72.27%] [G loss: 3.389197]\n",
            "980 [D loss: 0.424530, acc.: 79.69%] [G loss: 3.947567]\n",
            "1000 [D loss: 0.505740, acc.: 76.56%] [G loss: 3.537664]\n",
            "1020 [D loss: 0.453953, acc.: 80.86%] [G loss: 3.352550]\n",
            "1040 [D loss: 0.581719, acc.: 69.92%] [G loss: 3.155952]\n",
            "1060 [D loss: 0.500033, acc.: 76.56%] [G loss: 2.881751]\n",
            "1080 [D loss: 0.564928, acc.: 72.66%] [G loss: 3.089260]\n",
            "1100 [D loss: 0.528902, acc.: 75.00%] [G loss: 2.940079]\n",
            "1120 [D loss: 0.516558, acc.: 76.56%] [G loss: 3.092759]\n",
            "1140 [D loss: 0.423878, acc.: 83.20%] [G loss: 3.139320]\n",
            "1160 [D loss: 0.498354, acc.: 78.91%] [G loss: 2.919463]\n",
            "1180 [D loss: 0.597165, acc.: 66.02%] [G loss: 2.851160]\n",
            "1200 [D loss: 0.552430, acc.: 69.53%] [G loss: 2.963300]\n",
            "1220 [D loss: 0.595389, acc.: 69.53%] [G loss: 2.846165]\n",
            "1260 [D loss: 0.463119, acc.: 80.47%] [G loss: 3.274584]\n",
            "1280 [D loss: 0.503033, acc.: 76.95%] [G loss: 2.906130]\n",
            "1300 [D loss: 0.539056, acc.: 73.83%] [G loss: 2.766418]\n",
            "1320 [D loss: 0.606781, acc.: 66.41%] [G loss: 3.941772]\n",
            "1340 [D loss: 0.528218, acc.: 73.83%] [G loss: 3.342978]\n",
            "1360 [D loss: 0.538956, acc.: 72.27%] [G loss: 3.201614]\n",
            "1380 [D loss: 0.563219, acc.: 69.53%] [G loss: 3.231246]\n",
            "1400 [D loss: 0.610427, acc.: 66.02%] [G loss: 2.833519]\n",
            "1420 [D loss: 0.756656, acc.: 63.67%] [G loss: 4.055398]\n",
            "1440 [D loss: 0.522971, acc.: 73.83%] [G loss: 3.891466]\n",
            "1460 [D loss: 0.412964, acc.: 84.38%] [G loss: 3.514786]\n",
            "1480 [D loss: 0.580581, acc.: 70.31%] [G loss: 3.243977]\n",
            "1500 [D loss: 0.520925, acc.: 75.00%] [G loss: 3.171255]\n",
            "1520 [D loss: 0.650411, acc.: 64.45%] [G loss: 2.916598]\n",
            "1540 [D loss: 0.497385, acc.: 76.56%] [G loss: 2.925524]\n",
            "1560 [D loss: 0.573231, acc.: 71.88%] [G loss: 3.634417]\n",
            "1580 [D loss: 0.523539, acc.: 75.00%] [G loss: 3.003258]\n",
            "1600 [D loss: 0.663605, acc.: 63.67%] [G loss: 2.881264]\n",
            "1620 [D loss: 0.457932, acc.: 80.47%] [G loss: 3.401469]\n",
            "1640 [D loss: 0.534944, acc.: 74.22%] [G loss: 3.094790]\n",
            "1660 [D loss: 0.457798, acc.: 83.20%] [G loss: 3.220596]\n",
            "1680 [D loss: 0.574562, acc.: 66.41%] [G loss: 2.945761]\n",
            "1700 [D loss: 0.493537, acc.: 73.05%] [G loss: 3.248261]\n",
            "1720 [D loss: 0.576778, acc.: 67.97%] [G loss: 3.084721]\n",
            "1740 [D loss: 0.404894, acc.: 83.20%] [G loss: 3.821087]\n",
            "1760 [D loss: 0.499201, acc.: 76.17%] [G loss: 3.086791]\n",
            "1780 [D loss: 0.477587, acc.: 77.34%] [G loss: 3.378293]\n",
            "1800 [D loss: 0.546237, acc.: 72.27%] [G loss: 2.789532]\n",
            "1820 [D loss: 0.505424, acc.: 74.22%] [G loss: 3.154507]\n",
            "1840 [D loss: 0.518259, acc.: 73.05%] [G loss: 3.428267]\n",
            "1860 [D loss: 0.454435, acc.: 76.56%] [G loss: 3.970617]\n",
            "1880 [D loss: 0.539048, acc.: 73.05%] [G loss: 3.296342]\n",
            "1900 [D loss: 0.488382, acc.: 76.56%] [G loss: 3.237591]\n",
            "1920 [D loss: 0.445261, acc.: 82.42%] [G loss: 3.573557]\n",
            "1940 [D loss: 0.465622, acc.: 78.52%] [G loss: 3.265401]\n",
            "1960 [D loss: 0.482366, acc.: 79.30%] [G loss: 2.937753]\n",
            "1980 [D loss: 0.511399, acc.: 76.17%] [G loss: 1.959323]\n",
            "2000 [D loss: 0.472380, acc.: 78.91%] [G loss: 3.897263]\n",
            "2020 [D loss: 0.582657, acc.: 73.05%] [G loss: 3.442030]\n",
            "2040 [D loss: 0.582329, acc.: 72.27%] [G loss: 3.607286]\n",
            "2060 [D loss: 0.553632, acc.: 73.83%] [G loss: 4.078200]\n",
            "2080 [D loss: 0.468492, acc.: 79.30%] [G loss: 3.550956]\n",
            "2100 [D loss: 0.488293, acc.: 75.00%] [G loss: 3.738218]\n",
            "2120 [D loss: 0.616013, acc.: 65.23%] [G loss: 3.252230]\n",
            "2140 [D loss: 0.524876, acc.: 74.22%] [G loss: 3.315946]\n",
            "2160 [D loss: 0.442799, acc.: 82.42%] [G loss: 3.802137]\n",
            "2180 [D loss: 0.465460, acc.: 77.73%] [G loss: 3.403452]\n",
            "2200 [D loss: 0.520219, acc.: 73.05%] [G loss: 3.559805]\n",
            "2220 [D loss: 0.462038, acc.: 80.86%] [G loss: 3.162084]\n",
            "2240 [D loss: 0.439204, acc.: 84.38%] [G loss: 3.530056]\n",
            "2260 [D loss: 0.494941, acc.: 73.83%] [G loss: 3.198147]\n",
            "2280 [D loss: 0.467633, acc.: 76.56%] [G loss: 3.450285]\n",
            "2300 [D loss: 0.468673, acc.: 78.12%] [G loss: 3.475707]\n",
            "2320 [D loss: 0.419520, acc.: 82.03%] [G loss: 3.658321]\n",
            "2340 [D loss: 0.508724, acc.: 76.56%] [G loss: 3.508678]\n",
            "2360 [D loss: 0.424388, acc.: 81.64%] [G loss: 3.708449]\n",
            "2380 [D loss: 0.598343, acc.: 68.75%] [G loss: 3.134171]\n",
            "2400 [D loss: 0.472173, acc.: 80.47%] [G loss: 3.439446]\n",
            "2420 [D loss: 0.565595, acc.: 69.53%] [G loss: 2.883696]\n",
            "2440 [D loss: 0.459629, acc.: 77.73%] [G loss: 3.641235]\n",
            "2460 [D loss: 0.450813, acc.: 79.69%] [G loss: 3.373738]\n",
            "2480 [D loss: 0.535569, acc.: 73.05%] [G loss: 3.483998]\n",
            "2500 [D loss: 0.499814, acc.: 74.61%] [G loss: 3.153636]\n",
            "2520 [D loss: 0.383711, acc.: 87.89%] [G loss: 3.687122]\n",
            "2540 [D loss: 0.495449, acc.: 75.39%] [G loss: 3.323463]\n",
            "2560 [D loss: 0.526364, acc.: 73.44%] [G loss: 3.278975]\n",
            "2580 [D loss: 0.455454, acc.: 82.03%] [G loss: 3.361021]\n",
            "2600 [D loss: 0.426952, acc.: 81.25%] [G loss: 3.638490]\n",
            "2620 [D loss: 0.612499, acc.: 72.27%] [G loss: 2.981585]\n",
            "2640 [D loss: 0.544376, acc.: 73.05%] [G loss: 3.192035]\n",
            "2660 [D loss: 0.656715, acc.: 64.45%] [G loss: 2.016367]\n",
            "2680 [D loss: 0.667194, acc.: 55.86%] [G loss: 1.641860]\n",
            "2700 [D loss: 0.702533, acc.: 54.30%] [G loss: 1.570392]\n",
            "2720 [D loss: 0.708536, acc.: 55.86%] [G loss: 1.571235]\n",
            "2740 [D loss: 0.568565, acc.: 74.61%] [G loss: 2.156453]\n",
            "2760 [D loss: 0.726067, acc.: 54.30%] [G loss: 1.725093]\n",
            "2780 [D loss: 0.538200, acc.: 76.95%] [G loss: 2.324985]\n",
            "2800 [D loss: 0.639901, acc.: 66.80%] [G loss: 2.266483]\n",
            "2820 [D loss: 0.631066, acc.: 69.14%] [G loss: 2.589213]\n",
            "2840 [D loss: 0.564681, acc.: 71.48%] [G loss: 2.673841]\n",
            "2860 [D loss: 0.584147, acc.: 67.19%] [G loss: 2.637341]\n",
            "2880 [D loss: 0.568047, acc.: 73.44%] [G loss: 2.586394]\n",
            "2900 [D loss: 0.539463, acc.: 75.78%] [G loss: 2.612110]\n",
            "2920 [D loss: 0.557765, acc.: 73.83%] [G loss: 2.677692]\n",
            "2940 [D loss: 0.547058, acc.: 74.61%] [G loss: 2.580311]\n",
            "2960 [D loss: 0.540166, acc.: 75.39%] [G loss: 2.649784]\n",
            "2980 [D loss: 0.536762, acc.: 72.27%] [G loss: 2.716412]\n",
            "3000 [D loss: 0.527131, acc.: 78.52%] [G loss: 2.770938]\n",
            "3020 [D loss: 0.529794, acc.: 73.83%] [G loss: 2.737725]\n",
            "3040 [D loss: 0.554610, acc.: 72.66%] [G loss: 2.591751]\n",
            "3060 [D loss: 0.467074, acc.: 81.64%] [G loss: 2.918468]\n",
            "3080 [D loss: 0.543686, acc.: 75.78%] [G loss: 2.649825]\n",
            "3100 [D loss: 0.483825, acc.: 81.25%] [G loss: 2.821656]\n",
            "3120 [D loss: 0.593796, acc.: 69.14%] [G loss: 2.723900]\n",
            "3140 [D loss: 0.572160, acc.: 71.88%] [G loss: 2.579295]\n",
            "3160 [D loss: 0.547724, acc.: 73.44%] [G loss: 2.619065]\n",
            "3180 [D loss: 0.551831, acc.: 69.53%] [G loss: 2.735200]\n",
            "3200 [D loss: 0.596627, acc.: 66.80%] [G loss: 2.637272]\n",
            "3220 [D loss: 0.538558, acc.: 73.83%] [G loss: 2.981643]\n",
            "3240 [D loss: 0.498644, acc.: 76.95%] [G loss: 2.668087]\n",
            "3260 [D loss: 0.570401, acc.: 69.92%] [G loss: 2.715104]\n",
            "3280 [D loss: 0.485167, acc.: 75.78%] [G loss: 2.876940]\n",
            "3300 [D loss: 0.564344, acc.: 71.88%] [G loss: 2.847174]\n",
            "3320 [D loss: 0.619109, acc.: 67.58%] [G loss: 2.501449]\n",
            "3340 [D loss: 0.564573, acc.: 74.61%] [G loss: 2.634479]\n",
            "3360 [D loss: 0.546204, acc.: 73.05%] [G loss: 2.810348]\n",
            "3380 [D loss: 0.554740, acc.: 73.44%] [G loss: 2.791648]\n",
            "3400 [D loss: 0.519845, acc.: 73.44%] [G loss: 3.039335]\n",
            "3420 [D loss: 0.549282, acc.: 68.75%] [G loss: 2.855623]\n",
            "3440 [D loss: 0.586543, acc.: 68.75%] [G loss: 2.518322]\n",
            "3460 [D loss: 0.590480, acc.: 66.80%] [G loss: 2.438975]\n",
            "3480 [D loss: 0.531604, acc.: 73.05%] [G loss: 2.689115]\n",
            "3500 [D loss: 0.535917, acc.: 73.05%] [G loss: 2.790954]\n",
            "3520 [D loss: 0.568578, acc.: 69.92%] [G loss: 2.755694]\n",
            "3540 [D loss: 0.522370, acc.: 72.66%] [G loss: 2.922705]\n",
            "3560 [D loss: 0.518562, acc.: 75.00%] [G loss: 2.796871]\n",
            "3580 [D loss: 0.547072, acc.: 69.14%] [G loss: 2.852136]\n",
            "3600 [D loss: 0.556928, acc.: 70.31%] [G loss: 2.373669]\n",
            "3620 [D loss: 0.408835, acc.: 85.16%] [G loss: 1.888309]\n",
            "3640 [D loss: 0.502393, acc.: 78.91%] [G loss: 1.714282]\n",
            "3660 [D loss: 0.985780, acc.: 47.66%] [G loss: 2.772002]\n",
            "3680 [D loss: 0.795481, acc.: 50.00%] [G loss: 2.995810]\n",
            "3700 [D loss: 0.570637, acc.: 69.92%] [G loss: 3.152113]\n",
            "3720 [D loss: 0.608024, acc.: 67.97%] [G loss: 2.624700]\n",
            "3740 [D loss: 0.701638, acc.: 61.33%] [G loss: 1.816732]\n",
            "3760 [D loss: 0.679787, acc.: 58.20%] [G loss: 2.754138]\n",
            "3780 [D loss: 0.690661, acc.: 58.98%] [G loss: 2.924712]\n",
            "3800 [D loss: 0.671830, acc.: 62.50%] [G loss: 2.816359]\n",
            "3820 [D loss: 0.468158, acc.: 78.12%] [G loss: 3.249133]\n",
            "3840 [D loss: 0.623183, acc.: 66.02%] [G loss: 3.250906]\n",
            "3860 [D loss: 0.445671, acc.: 80.08%] [G loss: 3.305415]\n",
            "3880 [D loss: 0.658017, acc.: 62.11%] [G loss: 2.640416]\n",
            "3900 [D loss: 0.577093, acc.: 70.70%] [G loss: 2.998224]\n",
            "3920 [D loss: 0.535688, acc.: 73.83%] [G loss: 3.226058]\n",
            "3940 [D loss: 0.585253, acc.: 68.75%] [G loss: 2.869814]\n",
            "3960 [D loss: 0.474272, acc.: 80.08%] [G loss: 3.070667]\n",
            "3980 [D loss: 0.644834, acc.: 66.80%] [G loss: 2.765801]\n",
            "4000 [D loss: 0.487226, acc.: 79.30%] [G loss: 3.065905]\n",
            "4020 [D loss: 0.595414, acc.: 65.62%] [G loss: 2.766820]\n",
            "4040 [D loss: 0.514578, acc.: 73.83%] [G loss: 2.951289]\n",
            "4060 [D loss: 0.508470, acc.: 77.73%] [G loss: 2.968307]\n",
            "4080 [D loss: 0.548259, acc.: 72.66%] [G loss: 2.921597]\n",
            "4100 [D loss: 0.548990, acc.: 73.83%] [G loss: 2.803533]\n",
            "4120 [D loss: 0.479322, acc.: 76.17%] [G loss: 3.232982]\n",
            "4140 [D loss: 0.546587, acc.: 71.88%] [G loss: 2.963748]\n",
            "4160 [D loss: 0.566682, acc.: 74.22%] [G loss: 3.072791]\n",
            "4180 [D loss: 0.513078, acc.: 76.17%] [G loss: 3.134123]\n",
            "4200 [D loss: 0.539832, acc.: 73.83%] [G loss: 2.949744]\n",
            "4220 [D loss: 0.522472, acc.: 72.27%] [G loss: 3.141422]\n",
            "4240 [D loss: 0.527653, acc.: 75.78%] [G loss: 3.180289]\n",
            "4260 [D loss: 0.552371, acc.: 73.05%] [G loss: 2.959339]\n",
            "4280 [D loss: 0.494671, acc.: 76.17%] [G loss: 2.873046]\n",
            "4300 [D loss: 0.539938, acc.: 76.17%] [G loss: 2.883640]\n",
            "4320 [D loss: 0.495305, acc.: 73.05%] [G loss: 3.334341]\n",
            "4340 [D loss: 0.495597, acc.: 76.95%] [G loss: 3.156517]\n",
            "4360 [D loss: 0.479309, acc.: 76.95%] [G loss: 3.258646]\n",
            "4380 [D loss: 0.543373, acc.: 70.31%] [G loss: 3.037992]\n",
            "4400 [D loss: 0.547406, acc.: 71.88%] [G loss: 3.122278]\n",
            "4420 [D loss: 0.504223, acc.: 76.17%] [G loss: 3.221269]\n",
            "4440 [D loss: 0.532992, acc.: 75.39%] [G loss: 3.460068]\n",
            "4460 [D loss: 0.435780, acc.: 80.08%] [G loss: 3.623299]\n",
            "4480 [D loss: 0.506874, acc.: 74.61%] [G loss: 2.924392]\n",
            "4500 [D loss: 0.607686, acc.: 68.75%] [G loss: 2.834673]\n",
            "4520 [D loss: 0.509443, acc.: 76.56%] [G loss: 2.960854]\n",
            "4540 [D loss: 0.484227, acc.: 80.08%] [G loss: 3.362706]\n",
            "4560 [D loss: 0.551260, acc.: 69.53%] [G loss: 3.009396]\n",
            "4580 [D loss: 0.518653, acc.: 75.00%] [G loss: 3.225392]\n",
            "4600 [D loss: 0.550133, acc.: 69.14%] [G loss: 3.149893]\n",
            "4620 [D loss: 0.510771, acc.: 77.34%] [G loss: 3.099575]\n",
            "4640 [D loss: 0.495754, acc.: 73.83%] [G loss: 3.316140]\n",
            "4660 [D loss: 0.518437, acc.: 73.83%] [G loss: 3.038269]\n",
            "4680 [D loss: 0.543906, acc.: 71.48%] [G loss: 3.026415]\n",
            "4700 [D loss: 0.498886, acc.: 76.56%] [G loss: 3.228366]\n",
            "4720 [D loss: 0.486667, acc.: 76.95%] [G loss: 3.185335]\n",
            "4740 [D loss: 0.484388, acc.: 76.56%] [G loss: 3.618409]\n",
            "4760 [D loss: 0.537183, acc.: 75.39%] [G loss: 3.020610]\n",
            "4780 [D loss: 0.528179, acc.: 74.61%] [G loss: 3.080171]\n",
            "4800 [D loss: 0.516637, acc.: 73.83%] [G loss: 3.013088]\n",
            "4820 [D loss: 0.439520, acc.: 82.03%] [G loss: 3.762843]\n",
            "4840 [D loss: 0.555626, acc.: 70.31%] [G loss: 3.088546]\n",
            "4860 [D loss: 0.572694, acc.: 70.31%] [G loss: 2.904438]\n",
            "4880 [D loss: 0.499022, acc.: 76.17%] [G loss: 3.325777]\n",
            "4900 [D loss: 0.465945, acc.: 76.17%] [G loss: 3.291158]\n",
            "4920 [D loss: 0.498045, acc.: 72.66%] [G loss: 3.053052]\n",
            "4940 [D loss: 0.550079, acc.: 73.05%] [G loss: 3.603507]\n",
            "4960 [D loss: 0.616297, acc.: 66.41%] [G loss: 3.402077]\n",
            "4980 [D loss: 0.491895, acc.: 75.78%] [G loss: 3.371536]\n",
            "5000 [D loss: 0.440570, acc.: 83.20%] [G loss: 3.534877]\n",
            "5020 [D loss: 0.501325, acc.: 74.61%] [G loss: 3.380649]\n",
            "5040 [D loss: 0.559729, acc.: 70.31%] [G loss: 3.563225]\n",
            "5060 [D loss: 0.471846, acc.: 79.30%] [G loss: 3.381116]\n",
            "5080 [D loss: 0.483291, acc.: 76.95%] [G loss: 3.415001]\n",
            "5100 [D loss: 0.490776, acc.: 78.91%] [G loss: 3.528653]\n",
            "5120 [D loss: 0.701687, acc.: 60.94%] [G loss: 2.785963]\n",
            "5140 [D loss: 0.400597, acc.: 86.33%] [G loss: 3.009515]\n",
            "5160 [D loss: 0.477922, acc.: 76.95%] [G loss: 3.541957]\n",
            "5180 [D loss: 0.493386, acc.: 76.17%] [G loss: 3.331196]\n",
            "5200 [D loss: 0.466249, acc.: 75.78%] [G loss: 3.488733]\n",
            "5220 [D loss: 0.444310, acc.: 78.12%] [G loss: 3.637371]\n",
            "5240 [D loss: 0.532611, acc.: 71.88%] [G loss: 3.232051]\n",
            "5260 [D loss: 0.445420, acc.: 78.12%] [G loss: 3.365701]\n",
            "5280 [D loss: 0.637727, acc.: 63.67%] [G loss: 2.646929]\n",
            "5300 [D loss: 0.700627, acc.: 59.77%] [G loss: 3.085218]\n",
            "5320 [D loss: 0.453553, acc.: 80.86%] [G loss: 3.657994]\n",
            "5340 [D loss: 0.611535, acc.: 67.19%] [G loss: 3.143475]\n",
            "5360 [D loss: 0.554004, acc.: 73.83%] [G loss: 3.143237]\n",
            "5380 [D loss: 0.522258, acc.: 72.66%] [G loss: 3.307638]\n",
            "5400 [D loss: 0.552605, acc.: 72.66%] [G loss: 3.052524]\n",
            "5420 [D loss: 0.491716, acc.: 76.56%] [G loss: 3.585880]\n",
            "5440 [D loss: 0.515785, acc.: 72.66%] [G loss: 3.474866]\n",
            "5460 [D loss: 0.515451, acc.: 71.09%] [G loss: 3.639800]\n",
            "5480 [D loss: 0.485155, acc.: 76.95%] [G loss: 3.411251]\n",
            "5500 [D loss: 0.576946, acc.: 70.31%] [G loss: 3.145051]\n",
            "5520 [D loss: 0.498211, acc.: 75.78%] [G loss: 3.729239]\n",
            "5540 [D loss: 0.564351, acc.: 69.14%] [G loss: 3.385039]\n",
            "5560 [D loss: 0.556861, acc.: 70.31%] [G loss: 3.249048]\n",
            "5580 [D loss: 0.506919, acc.: 75.78%] [G loss: 3.338856]\n",
            "5600 [D loss: 0.548224, acc.: 74.61%] [G loss: 3.353596]\n",
            "5620 [D loss: 0.649093, acc.: 65.23%] [G loss: 3.059271]\n",
            "5640 [D loss: 0.516035, acc.: 76.95%] [G loss: 3.347104]\n",
            "5660 [D loss: 0.541020, acc.: 74.22%] [G loss: 3.171703]\n",
            "5680 [D loss: 0.503400, acc.: 74.22%] [G loss: 3.253807]\n",
            "5700 [D loss: 0.568046, acc.: 71.88%] [G loss: 3.374863]\n",
            "5720 [D loss: 0.440286, acc.: 80.47%] [G loss: 3.895191]\n",
            "5740 [D loss: 0.495095, acc.: 75.78%] [G loss: 3.475708]\n",
            "5760 [D loss: 0.541793, acc.: 73.83%] [G loss: 3.391163]\n",
            "5780 [D loss: 0.501392, acc.: 74.61%] [G loss: 3.590703]\n",
            "5800 [D loss: 0.525571, acc.: 72.66%] [G loss: 3.623416]\n",
            "5820 [D loss: 0.541892, acc.: 69.53%] [G loss: 3.420203]\n",
            "5840 [D loss: 0.470031, acc.: 82.42%] [G loss: 3.461773]\n",
            "5860 [D loss: 0.427554, acc.: 82.42%] [G loss: 3.682395]\n",
            "5880 [D loss: 0.569901, acc.: 69.53%] [G loss: 3.320354]\n",
            "5900 [D loss: 0.521498, acc.: 72.27%] [G loss: 3.373895]\n",
            "5920 [D loss: 0.484811, acc.: 79.30%] [G loss: 3.599432]\n",
            "5940 [D loss: 0.512436, acc.: 75.78%] [G loss: 3.675545]\n",
            "5960 [D loss: 0.540885, acc.: 71.88%] [G loss: 3.383870]\n",
            "5980 [D loss: 0.472304, acc.: 77.34%] [G loss: 3.774349]\n",
            "6000 [D loss: 0.487823, acc.: 76.56%] [G loss: 3.472219]\n",
            "6020 [D loss: 0.502456, acc.: 76.56%] [G loss: 3.453554]\n",
            "6040 [D loss: 0.545590, acc.: 71.48%] [G loss: 3.083697]\n",
            "6060 [D loss: 0.528804, acc.: 73.05%] [G loss: 3.377475]\n",
            "6080 [D loss: 0.529553, acc.: 73.83%] [G loss: 3.320541]\n",
            "6100 [D loss: 0.508642, acc.: 76.56%] [G loss: 3.244349]\n",
            "6120 [D loss: 0.547629, acc.: 73.83%] [G loss: 3.516729]\n",
            "6140 [D loss: 0.508060, acc.: 76.17%] [G loss: 3.441060]\n",
            "6160 [D loss: 0.523664, acc.: 75.00%] [G loss: 3.335318]\n",
            "6180 [D loss: 0.447858, acc.: 81.25%] [G loss: 3.556962]\n",
            "6200 [D loss: 0.468891, acc.: 78.12%] [G loss: 3.546193]\n",
            "6220 [D loss: 0.488384, acc.: 76.17%] [G loss: 3.420362]\n",
            "6240 [D loss: 0.468018, acc.: 78.91%] [G loss: 3.357599]\n",
            "6260 [D loss: 0.539308, acc.: 75.00%] [G loss: 3.483194]\n",
            "6280 [D loss: 0.481252, acc.: 74.61%] [G loss: 3.397105]\n",
            "6300 [D loss: 0.413603, acc.: 84.77%] [G loss: 2.907132]\n",
            "6320 [D loss: 0.413631, acc.: 82.42%] [G loss: 2.374221]\n",
            "6340 [D loss: 0.710064, acc.: 61.33%] [G loss: 1.954531]\n",
            "6360 [D loss: 1.002769, acc.: 49.61%] [G loss: 2.093051]\n",
            "6380 [D loss: 1.102247, acc.: 42.97%] [G loss: 3.425496]\n",
            "6400 [D loss: 0.647164, acc.: 64.84%] [G loss: 3.572773]\n",
            "6420 [D loss: 0.691402, acc.: 65.62%] [G loss: 3.107646]\n",
            "6440 [D loss: 0.534238, acc.: 74.61%] [G loss: 3.508193]\n",
            "6460 [D loss: 0.423119, acc.: 81.64%] [G loss: 3.632246]\n",
            "6480 [D loss: 0.639163, acc.: 65.23%] [G loss: 3.204740]\n",
            "6500 [D loss: 0.379178, acc.: 85.55%] [G loss: 3.933474]\n",
            "6520 [D loss: 0.569204, acc.: 72.27%] [G loss: 3.263296]\n",
            "6540 [D loss: 0.549786, acc.: 69.92%] [G loss: 3.132354]\n",
            "6560 [D loss: 0.485188, acc.: 77.34%] [G loss: 3.379011]\n",
            "6580 [D loss: 0.489961, acc.: 76.17%] [G loss: 3.649284]\n",
            "6600 [D loss: 0.455123, acc.: 78.12%] [G loss: 3.569888]\n",
            "6620 [D loss: 0.585113, acc.: 68.36%] [G loss: 3.430307]\n",
            "6640 [D loss: 0.515146, acc.: 73.83%] [G loss: 3.326415]\n",
            "6660 [D loss: 0.542043, acc.: 72.66%] [G loss: 3.325861]\n",
            "6680 [D loss: 0.536556, acc.: 71.48%] [G loss: 3.229635]\n",
            "6700 [D loss: 0.494804, acc.: 77.34%] [G loss: 3.414742]\n",
            "6720 [D loss: 0.526499, acc.: 74.61%] [G loss: 3.261118]\n",
            "6740 [D loss: 0.498970, acc.: 72.27%] [G loss: 3.476306]\n",
            "6760 [D loss: 0.513907, acc.: 76.17%] [G loss: 3.626143]\n",
            "6780 [D loss: 0.488703, acc.: 76.17%] [G loss: 3.545323]\n",
            "6800 [D loss: 0.549826, acc.: 72.66%] [G loss: 3.287815]\n",
            "6820 [D loss: 0.501248, acc.: 76.56%] [G loss: 3.352342]\n",
            "6840 [D loss: 0.531161, acc.: 70.70%] [G loss: 3.399777]\n",
            "6860 [D loss: 0.566231, acc.: 73.05%] [G loss: 3.440760]\n",
            "6880 [D loss: 0.451468, acc.: 78.91%] [G loss: 3.860701]\n",
            "6900 [D loss: 0.514514, acc.: 73.83%] [G loss: 3.219082]\n",
            "6920 [D loss: 0.587306, acc.: 67.58%] [G loss: 3.085237]\n",
            "6940 [D loss: 0.476555, acc.: 79.69%] [G loss: 3.351914]\n",
            "6960 [D loss: 0.462928, acc.: 79.69%] [G loss: 3.339042]\n",
            "6980 [D loss: 0.490601, acc.: 78.12%] [G loss: 3.309331]\n",
            "7000 [D loss: 0.516610, acc.: 74.22%] [G loss: 3.483382]\n",
            "7020 [D loss: 0.556123, acc.: 70.31%] [G loss: 3.315993]\n",
            "7040 [D loss: 0.561435, acc.: 73.05%] [G loss: 3.056081]\n",
            "7060 [D loss: 0.578446, acc.: 67.97%] [G loss: 3.134484]\n",
            "7080 [D loss: 0.462470, acc.: 77.34%] [G loss: 3.701201]\n",
            "7100 [D loss: 0.459470, acc.: 78.12%] [G loss: 2.880359]\n",
            "7120 [D loss: 0.431414, acc.: 82.81%] [G loss: 2.181483]\n",
            "7140 [D loss: 0.718458, acc.: 58.20%] [G loss: 2.063774]\n",
            "7160 [D loss: 0.651497, acc.: 64.06%] [G loss: 2.936795]\n",
            "7180 [D loss: 0.659048, acc.: 64.84%] [G loss: 3.662257]\n",
            "7200 [D loss: 0.607790, acc.: 69.14%] [G loss: 3.189283]\n",
            "7220 [D loss: 0.551569, acc.: 74.61%] [G loss: 3.655275]\n",
            "7240 [D loss: 0.580590, acc.: 69.14%] [G loss: 3.300172]\n",
            "7260 [D loss: 0.514827, acc.: 71.88%] [G loss: 3.466946]\n",
            "7280 [D loss: 0.650662, acc.: 63.67%] [G loss: 3.194731]\n",
            "7300 [D loss: 0.548820, acc.: 72.27%] [G loss: 3.319758]\n",
            "7320 [D loss: 0.479277, acc.: 75.39%] [G loss: 3.531814]\n",
            "7340 [D loss: 0.482030, acc.: 76.56%] [G loss: 3.287907]\n",
            "7360 [D loss: 0.593273, acc.: 67.97%] [G loss: 3.339878]\n",
            "7380 [D loss: 0.460832, acc.: 77.34%] [G loss: 3.539673]\n",
            "7400 [D loss: 0.551642, acc.: 71.88%] [G loss: 3.266778]\n",
            "7420 [D loss: 0.434658, acc.: 82.42%] [G loss: 3.550536]\n",
            "7440 [D loss: 0.527812, acc.: 75.78%] [G loss: 3.262159]\n",
            "7460 [D loss: 0.501548, acc.: 75.78%] [G loss: 3.488325]\n",
            "7480 [D loss: 0.537262, acc.: 74.61%] [G loss: 3.327233]\n",
            "7500 [D loss: 0.433729, acc.: 80.47%] [G loss: 3.542349]\n",
            "7520 [D loss: 0.542200, acc.: 70.70%] [G loss: 3.269181]\n",
            "7540 [D loss: 0.557251, acc.: 69.92%] [G loss: 3.595996]\n",
            "7560 [D loss: 0.496689, acc.: 75.00%] [G loss: 3.362296]\n",
            "7580 [D loss: 0.468608, acc.: 77.73%] [G loss: 3.485495]\n",
            "7600 [D loss: 0.523986, acc.: 74.22%] [G loss: 3.225745]\n",
            "7620 [D loss: 0.449503, acc.: 80.08%] [G loss: 3.187682]\n",
            "7640 [D loss: 0.863284, acc.: 48.83%] [G loss: 1.934484]\n",
            "7660 [D loss: 0.589244, acc.: 67.97%] [G loss: 2.543393]\n",
            "7680 [D loss: 0.432228, acc.: 80.86%] [G loss: 3.797744]\n",
            "7700 [D loss: 0.671217, acc.: 62.50%] [G loss: 2.031878]\n",
            "7720 [D loss: 0.755379, acc.: 55.08%] [G loss: 2.855084]\n",
            "7740 [D loss: 0.569523, acc.: 69.53%] [G loss: 3.255945]\n",
            "7760 [D loss: 0.747956, acc.: 60.16%] [G loss: 4.463279]\n",
            "7780 [D loss: 0.743255, acc.: 56.25%] [G loss: 3.089159]\n",
            "7800 [D loss: 0.405998, acc.: 83.59%] [G loss: 3.534171]\n",
            "7820 [D loss: 0.500940, acc.: 75.78%] [G loss: 3.471044]\n",
            "7840 [D loss: 0.485528, acc.: 77.73%] [G loss: 3.601446]\n",
            "7860 [D loss: 0.584742, acc.: 69.53%] [G loss: 3.458277]\n",
            "7880 [D loss: 0.501382, acc.: 76.17%] [G loss: 3.511919]\n",
            "7900 [D loss: 0.474120, acc.: 78.91%] [G loss: 3.712646]\n",
            "7920 [D loss: 0.442644, acc.: 78.91%] [G loss: 3.737809]\n",
            "7940 [D loss: 0.497766, acc.: 76.95%] [G loss: 3.474928]\n",
            "7960 [D loss: 0.411475, acc.: 82.03%] [G loss: 3.907070]\n",
            "7980 [D loss: 0.555218, acc.: 70.31%] [G loss: 3.510782]\n",
            "8000 [D loss: 0.438322, acc.: 80.86%] [G loss: 3.793212]\n",
            "8020 [D loss: 0.515661, acc.: 75.39%] [G loss: 3.230874]\n",
            "8040 [D loss: 0.518805, acc.: 73.05%] [G loss: 3.556840]\n",
            "8060 [D loss: 0.515029, acc.: 74.22%] [G loss: 3.451200]\n",
            "8080 [D loss: 0.526674, acc.: 72.66%] [G loss: 3.329576]\n",
            "8100 [D loss: 0.464274, acc.: 77.34%] [G loss: 3.627245]\n",
            "8120 [D loss: 0.479923, acc.: 77.34%] [G loss: 3.467834]\n",
            "8140 [D loss: 0.526890, acc.: 73.83%] [G loss: 3.486554]\n",
            "8160 [D loss: 0.450103, acc.: 80.08%] [G loss: 3.755792]\n",
            "8180 [D loss: 0.510633, acc.: 73.05%] [G loss: 3.428512]\n",
            "8200 [D loss: 0.514759, acc.: 74.61%] [G loss: 3.357042]\n",
            "8220 [D loss: 0.487032, acc.: 77.34%] [G loss: 3.267457]\n",
            "8240 [D loss: 0.571420, acc.: 71.48%] [G loss: 3.309415]\n",
            "8260 [D loss: 0.459567, acc.: 80.08%] [G loss: 3.473465]\n",
            "8280 [D loss: 0.570065, acc.: 69.92%] [G loss: 3.714516]\n",
            "8300 [D loss: 0.504690, acc.: 76.17%] [G loss: 3.302264]\n",
            "8320 [D loss: 0.454211, acc.: 81.25%] [G loss: 3.418892]\n",
            "8340 [D loss: 0.547375, acc.: 74.22%] [G loss: 3.402963]\n",
            "8360 [D loss: 0.415873, acc.: 81.25%] [G loss: 4.005905]\n",
            "8380 [D loss: 0.500531, acc.: 75.00%] [G loss: 3.597455]\n",
            "8400 [D loss: 0.506486, acc.: 75.39%] [G loss: 2.990340]\n",
            "8420 [D loss: 0.451724, acc.: 78.91%] [G loss: 3.481854]\n",
            "8440 [D loss: 0.520703, acc.: 73.05%] [G loss: 3.212416]\n",
            "8460 [D loss: 0.472526, acc.: 78.91%] [G loss: 3.657242]\n",
            "8480 [D loss: 0.566098, acc.: 69.92%] [G loss: 3.030807]\n",
            "8500 [D loss: 0.518057, acc.: 76.56%] [G loss: 3.400129]\n",
            "8520 [D loss: 0.599871, acc.: 66.02%] [G loss: 3.035600]\n",
            "8540 [D loss: 0.798547, acc.: 55.86%] [G loss: 3.632720]\n",
            "8560 [D loss: 0.533136, acc.: 70.70%] [G loss: 3.459063]\n",
            "8580 [D loss: 0.595553, acc.: 66.41%] [G loss: 3.219668]\n",
            "8600 [D loss: 0.442107, acc.: 80.47%] [G loss: 3.511991]\n",
            "8620 [D loss: 0.443567, acc.: 81.25%] [G loss: 3.680820]\n",
            "8640 [D loss: 0.567585, acc.: 66.80%] [G loss: 3.314626]\n",
            "8660 [D loss: 0.464737, acc.: 79.69%] [G loss: 3.413890]\n",
            "8680 [D loss: 0.510816, acc.: 74.22%] [G loss: 3.633814]\n",
            "8700 [D loss: 0.434075, acc.: 83.20%] [G loss: 3.513039]\n",
            "8720 [D loss: 0.496874, acc.: 80.47%] [G loss: 3.523195]\n",
            "8740 [D loss: 0.456600, acc.: 76.56%] [G loss: 3.516590]\n",
            "8760 [D loss: 0.467681, acc.: 80.47%] [G loss: 3.849602]\n",
            "8780 [D loss: 0.481430, acc.: 76.56%] [G loss: 3.602056]\n",
            "8800 [D loss: 0.499618, acc.: 76.95%] [G loss: 3.717577]\n",
            "8820 [D loss: 0.531673, acc.: 73.83%] [G loss: 3.346916]\n",
            "8840 [D loss: 0.536364, acc.: 74.22%] [G loss: 3.468219]\n",
            "8860 [D loss: 0.511476, acc.: 75.78%] [G loss: 3.401849]\n",
            "8880 [D loss: 0.484332, acc.: 75.78%] [G loss: 3.622436]\n",
            "8900 [D loss: 0.468866, acc.: 78.52%] [G loss: 3.367935]\n",
            "8920 [D loss: 0.500068, acc.: 77.34%] [G loss: 3.775585]\n",
            "8940 [D loss: 0.497947, acc.: 75.00%] [G loss: 3.251059]\n",
            "8960 [D loss: 0.521147, acc.: 75.39%] [G loss: 3.282841]\n",
            "8980 [D loss: 0.475425, acc.: 76.95%] [G loss: 3.549880]\n",
            "9000 [D loss: 0.513702, acc.: 73.83%] [G loss: 3.716487]\n",
            "9020 [D loss: 0.468850, acc.: 78.91%] [G loss: 3.614244]\n",
            "9040 [D loss: 0.591487, acc.: 70.70%] [G loss: 3.204064]\n",
            "9060 [D loss: 0.575985, acc.: 69.14%] [G loss: 3.128700]\n",
            "9080 [D loss: 0.484183, acc.: 76.56%] [G loss: 3.380373]\n",
            "9100 [D loss: 0.460721, acc.: 78.52%] [G loss: 3.508584]\n",
            "9120 [D loss: 0.467411, acc.: 78.52%] [G loss: 3.581850]\n",
            "9140 [D loss: 0.583018, acc.: 69.53%] [G loss: 3.227175]\n",
            "9160 [D loss: 0.551827, acc.: 73.83%] [G loss: 3.528611]\n",
            "9180 [D loss: 0.426505, acc.: 79.30%] [G loss: 3.599606]\n",
            "9200 [D loss: 0.481032, acc.: 75.78%] [G loss: 3.595310]\n",
            "9220 [D loss: 0.523313, acc.: 73.44%] [G loss: 3.467247]\n",
            "9240 [D loss: 0.493528, acc.: 73.83%] [G loss: 3.457036]\n",
            "9260 [D loss: 0.446341, acc.: 80.08%] [G loss: 3.597196]\n",
            "9280 [D loss: 0.510240, acc.: 75.00%] [G loss: 3.746839]\n",
            "9300 [D loss: 0.526847, acc.: 74.22%] [G loss: 3.416654]\n",
            "9320 [D loss: 0.439435, acc.: 82.03%] [G loss: 3.673935]\n",
            "9340 [D loss: 0.462892, acc.: 82.03%] [G loss: 3.756204]\n",
            "9360 [D loss: 0.485120, acc.: 78.12%] [G loss: 3.482859]\n",
            "9380 [D loss: 0.467109, acc.: 77.34%] [G loss: 3.714691]\n",
            "9400 [D loss: 0.550894, acc.: 68.36%] [G loss: 3.492335]\n",
            "9420 [D loss: 0.417426, acc.: 82.81%] [G loss: 3.812988]\n",
            "9440 [D loss: 0.543729, acc.: 71.88%] [G loss: 3.338061]\n",
            "9460 [D loss: 0.413278, acc.: 82.81%] [G loss: 3.906657]\n",
            "9480 [D loss: 0.850003, acc.: 56.25%] [G loss: 3.137807]\n",
            "9500 [D loss: 0.465964, acc.: 78.52%] [G loss: 3.583370]\n",
            "9520 [D loss: 0.454824, acc.: 78.52%] [G loss: 3.597070]\n",
            "9540 [D loss: 0.395384, acc.: 85.55%] [G loss: 3.901070]\n",
            "9560 [D loss: 0.577669, acc.: 71.09%] [G loss: 3.570832]\n",
            "9580 [D loss: 0.478217, acc.: 79.30%] [G loss: 3.410625]\n",
            "9600 [D loss: 0.262890, acc.: 92.97%] [G loss: 2.374398]\n",
            "9620 [D loss: 0.163625, acc.: 96.48%] [G loss: 2.756777]\n",
            "9640 [D loss: 0.576720, acc.: 73.83%] [G loss: 2.130766]\n",
            "9660 [D loss: 1.341701, acc.: 33.59%] [G loss: 1.961000]\n",
            "9680 [D loss: 0.639287, acc.: 64.45%] [G loss: 2.712252]\n",
            "9700 [D loss: 0.645883, acc.: 64.84%] [G loss: 4.621830]\n",
            "9720 [D loss: 0.558199, acc.: 71.09%] [G loss: 3.500844]\n",
            "9740 [D loss: 0.414646, acc.: 80.86%] [G loss: 2.916336]\n",
            "9760 [D loss: 0.620272, acc.: 67.97%] [G loss: 3.949680]\n",
            "9780 [D loss: 0.658262, acc.: 63.67%] [G loss: 4.602592]\n",
            "9800 [D loss: 0.578194, acc.: 70.31%] [G loss: 3.284039]\n",
            "9820 [D loss: 0.656222, acc.: 64.45%] [G loss: 3.180072]\n",
            "9840 [D loss: 0.500130, acc.: 75.78%] [G loss: 4.424102]\n",
            "9860 [D loss: 0.625357, acc.: 65.62%] [G loss: 3.570227]\n",
            "9880 [D loss: 0.574963, acc.: 70.70%] [G loss: 3.608384]\n",
            "9900 [D loss: 0.600429, acc.: 69.14%] [G loss: 3.696014]\n",
            "9920 [D loss: 0.543516, acc.: 70.31%] [G loss: 3.763320]\n",
            "9940 [D loss: 0.528838, acc.: 73.44%] [G loss: 4.156817]\n",
            "9960 [D loss: 0.417719, acc.: 82.81%] [G loss: 3.849441]\n",
            "9980 [D loss: 0.537667, acc.: 74.61%] [G loss: 3.718623]\n",
            "10000 [D loss: 0.424873, acc.: 79.69%] [G loss: 4.082874]\n",
            "10020 [D loss: 0.492718, acc.: 78.12%] [G loss: 3.964959]\n",
            "10040 [D loss: 0.500869, acc.: 73.83%] [G loss: 3.874841]\n",
            "10060 [D loss: 0.574131, acc.: 69.14%] [G loss: 3.425413]\n",
            "10080 [D loss: 0.508588, acc.: 76.56%] [G loss: 3.655235]\n",
            "10100 [D loss: 0.489693, acc.: 78.52%] [G loss: 3.613548]\n",
            "10120 [D loss: 0.397042, acc.: 82.03%] [G loss: 3.916724]\n",
            "10140 [D loss: 0.503266, acc.: 75.00%] [G loss: 3.662885]\n",
            "10160 [D loss: 0.438934, acc.: 80.08%] [G loss: 3.816245]\n",
            "10180 [D loss: 0.456248, acc.: 80.08%] [G loss: 3.960026]\n",
            "10200 [D loss: 0.465494, acc.: 78.12%] [G loss: 3.634026]\n",
            "10220 [D loss: 0.525181, acc.: 75.00%] [G loss: 3.734253]\n",
            "10240 [D loss: 0.509364, acc.: 73.83%] [G loss: 3.922416]\n",
            "10260 [D loss: 0.462189, acc.: 78.12%] [G loss: 3.825472]\n",
            "10280 [D loss: 0.407819, acc.: 82.81%] [G loss: 3.990460]\n",
            "10300 [D loss: 0.460651, acc.: 79.69%] [G loss: 3.879179]\n",
            "10320 [D loss: 0.526644, acc.: 74.22%] [G loss: 3.597827]\n",
            "10340 [D loss: 0.544122, acc.: 73.83%] [G loss: 3.443520]\n",
            "10360 [D loss: 0.463800, acc.: 80.08%] [G loss: 3.775482]\n",
            "10380 [D loss: 0.523315, acc.: 73.44%] [G loss: 3.261258]\n",
            "10400 [D loss: 0.501390, acc.: 76.95%] [G loss: 3.628429]\n",
            "10420 [D loss: 0.474287, acc.: 76.56%] [G loss: 3.675703]\n",
            "10440 [D loss: 0.485068, acc.: 77.34%] [G loss: 4.070539]\n",
            "10460 [D loss: 0.449013, acc.: 78.52%] [G loss: 4.258018]\n",
            "10480 [D loss: 0.526937, acc.: 76.17%] [G loss: 3.398563]\n",
            "10500 [D loss: 0.454786, acc.: 76.95%] [G loss: 3.936033]\n",
            "10520 [D loss: 0.488368, acc.: 77.73%] [G loss: 3.679759]\n",
            "10540 [D loss: 0.536938, acc.: 73.44%] [G loss: 3.769600]\n",
            "10560 [D loss: 0.583171, acc.: 67.19%] [G loss: 3.546794]\n",
            "10580 [D loss: 0.591794, acc.: 70.70%] [G loss: 3.227754]\n",
            "10600 [D loss: 0.444401, acc.: 79.30%] [G loss: 3.973578]\n",
            "10620 [D loss: 0.573528, acc.: 73.44%] [G loss: 3.694883]\n",
            "10640 [D loss: 0.522837, acc.: 71.48%] [G loss: 3.801618]\n",
            "10660 [D loss: 0.509343, acc.: 77.73%] [G loss: 3.683197]\n",
            "10680 [D loss: 0.582361, acc.: 68.36%] [G loss: 3.963022]\n",
            "10700 [D loss: 0.394582, acc.: 84.77%] [G loss: 4.055211]\n",
            "10720 [D loss: 0.488204, acc.: 76.95%] [G loss: 3.789856]\n",
            "10740 [D loss: 0.527188, acc.: 74.61%] [G loss: 3.878586]\n",
            "10760 [D loss: 0.401289, acc.: 82.42%] [G loss: 3.921097]\n",
            "10780 [D loss: 0.504929, acc.: 76.95%] [G loss: 3.671168]\n",
            "10800 [D loss: 0.463725, acc.: 77.73%] [G loss: 3.775125]\n",
            "10820 [D loss: 0.509487, acc.: 74.61%] [G loss: 3.501379]\n",
            "10840 [D loss: 0.417606, acc.: 82.03%] [G loss: 4.398707]\n",
            "10860 [D loss: 0.414731, acc.: 82.03%] [G loss: 4.255301]\n",
            "10880 [D loss: 0.529645, acc.: 76.56%] [G loss: 3.809077]\n",
            "10900 [D loss: 0.443694, acc.: 81.25%] [G loss: 4.275208]\n",
            "10920 [D loss: 0.505756, acc.: 75.78%] [G loss: 3.595417]\n",
            "10940 [D loss: 0.415329, acc.: 82.42%] [G loss: 3.790812]\n",
            "10960 [D loss: 0.504291, acc.: 76.56%] [G loss: 3.842600]\n",
            "10980 [D loss: 0.411820, acc.: 82.81%] [G loss: 4.000692]\n",
            "11000 [D loss: 0.425155, acc.: 80.86%] [G loss: 4.166535]\n",
            "11020 [D loss: 0.512402, acc.: 76.56%] [G loss: 3.795498]\n",
            "11040 [D loss: 0.379561, acc.: 86.72%] [G loss: 3.936140]\n",
            "11060 [D loss: 0.491266, acc.: 76.17%] [G loss: 3.766811]\n",
            "11080 [D loss: 0.436217, acc.: 79.30%] [G loss: 3.992718]\n",
            "11100 [D loss: 0.460838, acc.: 80.86%] [G loss: 4.124922]\n",
            "11120 [D loss: 0.541633, acc.: 71.48%] [G loss: 3.606300]\n",
            "11140 [D loss: 0.521241, acc.: 73.44%] [G loss: 3.779936]\n",
            "11160 [D loss: 0.444106, acc.: 81.25%] [G loss: 4.116723]\n",
            "11180 [D loss: 0.390236, acc.: 85.94%] [G loss: 3.868373]\n",
            "11200 [D loss: 0.506056, acc.: 74.22%] [G loss: 3.825168]\n",
            "11220 [D loss: 0.522939, acc.: 73.44%] [G loss: 3.794503]\n",
            "11240 [D loss: 0.464345, acc.: 81.25%] [G loss: 4.038255]\n",
            "11260 [D loss: 0.484008, acc.: 76.17%] [G loss: 3.723883]\n",
            "11280 [D loss: 0.437446, acc.: 81.25%] [G loss: 4.140375]\n",
            "11300 [D loss: 0.497883, acc.: 75.00%] [G loss: 3.790669]\n",
            "11320 [D loss: 0.434865, acc.: 77.73%] [G loss: 3.812308]\n",
            "11340 [D loss: 0.439506, acc.: 79.69%] [G loss: 3.710773]\n",
            "11360 [D loss: 0.487246, acc.: 75.39%] [G loss: 3.893571]\n",
            "11380 [D loss: 0.477228, acc.: 76.56%] [G loss: 3.628004]\n",
            "11400 [D loss: 0.414729, acc.: 83.20%] [G loss: 3.915458]\n",
            "11420 [D loss: 0.429346, acc.: 80.47%] [G loss: 4.092714]\n",
            "11440 [D loss: 0.484543, acc.: 77.34%] [G loss: 4.326941]\n",
            "11460 [D loss: 0.416509, acc.: 81.64%] [G loss: 3.463286]\n",
            "11480 [D loss: 0.229691, acc.: 93.75%] [G loss: 4.395745]\n",
            "11500 [D loss: 0.592731, acc.: 68.36%] [G loss: 3.656908]\n",
            "11520 [D loss: 0.503848, acc.: 73.44%] [G loss: 4.186247]\n",
            "11540 [D loss: 0.438473, acc.: 81.64%] [G loss: 4.060326]\n",
            "11560 [D loss: 0.432603, acc.: 78.12%] [G loss: 4.375479]\n",
            "11580 [D loss: 0.474972, acc.: 76.95%] [G loss: 4.158549]\n",
            "11600 [D loss: 0.476841, acc.: 77.34%] [G loss: 4.200271]\n",
            "11620 [D loss: 0.571498, acc.: 68.75%] [G loss: 3.939059]\n",
            "11640 [D loss: 0.367602, acc.: 84.77%] [G loss: 4.706879]\n",
            "11660 [D loss: 0.523577, acc.: 71.88%] [G loss: 3.925599]\n",
            "11680 [D loss: 0.345987, acc.: 87.50%] [G loss: 4.357871]\n",
            "11700 [D loss: 0.448217, acc.: 80.08%] [G loss: 4.215873]\n",
            "11720 [D loss: 0.472846, acc.: 76.56%] [G loss: 3.991479]\n",
            "11740 [D loss: 0.374764, acc.: 86.33%] [G loss: 4.538159]\n",
            "11760 [D loss: 0.500490, acc.: 75.78%] [G loss: 4.038861]\n",
            "11800 [D loss: 0.462366, acc.: 80.08%] [G loss: 3.979362]\n",
            "11820 [D loss: 0.418569, acc.: 82.03%] [G loss: 4.214566]\n",
            "11840 [D loss: 0.412239, acc.: 84.77%] [G loss: 3.917542]\n",
            "11860 [D loss: 0.446183, acc.: 80.86%] [G loss: 4.223466]\n",
            "11880 [D loss: 0.454011, acc.: 77.73%] [G loss: 4.151854]\n",
            "11900 [D loss: 0.464591, acc.: 76.17%] [G loss: 3.808875]\n",
            "11920 [D loss: 0.453100, acc.: 79.69%] [G loss: 4.148604]\n",
            "11940 [D loss: 0.482403, acc.: 76.56%] [G loss: 3.953433]\n",
            "11960 [D loss: 0.456627, acc.: 78.12%] [G loss: 4.111288]\n",
            "11980 [D loss: 0.520556, acc.: 73.83%] [G loss: 3.832146]\n",
            "12000 [D loss: 0.441301, acc.: 78.12%] [G loss: 4.077620]\n",
            "12020 [D loss: 0.408230, acc.: 82.03%] [G loss: 4.186817]\n",
            "12040 [D loss: 0.470497, acc.: 78.91%] [G loss: 3.863176]\n",
            "12060 [D loss: 0.452820, acc.: 79.69%] [G loss: 4.056532]\n",
            "12080 [D loss: 0.469298, acc.: 78.91%] [G loss: 4.073647]\n",
            "12100 [D loss: 0.417118, acc.: 79.30%] [G loss: 4.484343]\n",
            "12120 [D loss: 0.466826, acc.: 77.73%] [G loss: 3.940286]\n",
            "12140 [D loss: 0.440110, acc.: 77.73%] [G loss: 4.278514]\n",
            "12160 [D loss: 0.417226, acc.: 80.86%] [G loss: 4.215484]\n",
            "12180 [D loss: 0.489547, acc.: 78.91%] [G loss: 3.937269]\n",
            "12200 [D loss: 0.406315, acc.: 82.03%] [G loss: 4.401111]\n",
            "12220 [D loss: 0.360760, acc.: 83.20%] [G loss: 4.695950]\n",
            "12240 [D loss: 0.434560, acc.: 77.73%] [G loss: 4.168984]\n",
            "12260 [D loss: 0.504142, acc.: 76.56%] [G loss: 3.943767]\n",
            "12280 [D loss: 0.423035, acc.: 81.25%] [G loss: 4.417848]\n",
            "12300 [D loss: 0.381386, acc.: 85.94%] [G loss: 4.600951]\n",
            "12320 [D loss: 0.440212, acc.: 81.64%] [G loss: 4.151081]\n",
            "12340 [D loss: 0.431574, acc.: 81.64%] [G loss: 4.715305]\n",
            "12360 [D loss: 0.389513, acc.: 84.38%] [G loss: 4.163562]\n",
            "12380 [D loss: 0.448655, acc.: 76.56%] [G loss: 4.444258]\n",
            "12400 [D loss: 0.454938, acc.: 77.73%] [G loss: 4.188633]\n",
            "12420 [D loss: 0.448704, acc.: 78.12%] [G loss: 4.192167]\n",
            "12440 [D loss: 0.481591, acc.: 75.78%] [G loss: 4.482973]\n",
            "12460 [D loss: 0.456346, acc.: 81.25%] [G loss: 4.525879]\n",
            "12480 [D loss: 0.450384, acc.: 78.12%] [G loss: 4.437299]\n",
            "12500 [D loss: 0.444984, acc.: 79.30%] [G loss: 3.926312]\n",
            "12520 [D loss: 0.363816, acc.: 85.94%] [G loss: 4.386283]\n",
            "12540 [D loss: 0.669070, acc.: 66.02%] [G loss: 2.718673]\n",
            "12560 [D loss: 0.117017, acc.: 98.83%] [G loss: 2.843195]\n",
            "12580 [D loss: 0.218461, acc.: 92.58%] [G loss: 2.688949]\n",
            "12600 [D loss: 0.243179, acc.: 91.80%] [G loss: 2.886991]\n",
            "12620 [D loss: 397.461060, acc.: 0.00%] [G loss: 326.427155]\n",
            "12640 [D loss: 2.149749, acc.: 20.31%] [G loss: 2.770271]\n",
            "12660 [D loss: 1.072323, acc.: 35.94%] [G loss: 2.214007]\n",
            "12680 [D loss: 1.228061, acc.: 19.53%] [G loss: 1.717431]\n",
            "12700 [D loss: 0.800919, acc.: 51.95%] [G loss: 2.161661]\n",
            "12720 [D loss: 0.956597, acc.: 34.38%] [G loss: 1.769968]\n",
            "12740 [D loss: 0.963064, acc.: 26.95%] [G loss: 1.512881]\n",
            "12760 [D loss: 0.834387, acc.: 39.06%] [G loss: 1.806138]\n",
            "12780 [D loss: 0.895437, acc.: 30.86%] [G loss: 1.541620]\n",
            "12800 [D loss: 0.732344, acc.: 50.00%] [G loss: 1.704265]\n",
            "12820 [D loss: 0.859906, acc.: 31.64%] [G loss: 1.516723]\n",
            "12840 [D loss: 0.773008, acc.: 41.80%] [G loss: 1.634925]\n",
            "12860 [D loss: 0.804490, acc.: 34.38%] [G loss: 1.551142]\n",
            "12880 [D loss: 0.814669, acc.: 36.33%] [G loss: 1.526012]\n",
            "12900 [D loss: 0.765075, acc.: 44.53%] [G loss: 1.659820]\n",
            "12920 [D loss: 0.810072, acc.: 39.84%] [G loss: 1.444650]\n",
            "12940 [D loss: 0.773944, acc.: 42.19%] [G loss: 1.498286]\n",
            "12960 [D loss: 0.714332, acc.: 55.86%] [G loss: 1.644644]\n",
            "12980 [D loss: 0.833988, acc.: 30.08%] [G loss: 1.358303]\n",
            "13000 [D loss: 0.781390, acc.: 36.72%] [G loss: 1.482744]\n",
            "13020 [D loss: 0.723231, acc.: 44.14%] [G loss: 1.565080]\n",
            "13040 [D loss: 0.767908, acc.: 39.45%] [G loss: 1.429929]\n",
            "13060 [D loss: 0.720941, acc.: 48.83%] [G loss: 1.554089]\n",
            "13080 [D loss: 0.726484, acc.: 48.83%] [G loss: 1.584226]\n",
            "13100 [D loss: 0.781273, acc.: 38.28%] [G loss: 1.493614]\n",
            "13120 [D loss: 0.727621, acc.: 48.83%] [G loss: 1.608826]\n",
            "13140 [D loss: 0.733567, acc.: 47.27%] [G loss: 1.615886]\n",
            "13160 [D loss: 0.739643, acc.: 50.78%] [G loss: 1.564963]\n",
            "13180 [D loss: 0.718276, acc.: 47.66%] [G loss: 1.529263]\n",
            "13200 [D loss: 0.753155, acc.: 43.75%] [G loss: 1.502788]\n",
            "13220 [D loss: 0.701235, acc.: 54.69%] [G loss: 1.611790]\n",
            "13240 [D loss: 0.753419, acc.: 40.23%] [G loss: 1.515982]\n",
            "13260 [D loss: 0.769311, acc.: 40.23%] [G loss: 1.476732]\n",
            "13280 [D loss: 0.768524, acc.: 43.36%] [G loss: 1.537280]\n",
            "13300 [D loss: 0.758209, acc.: 44.14%] [G loss: 1.494983]\n",
            "13320 [D loss: 0.724779, acc.: 50.39%] [G loss: 1.590746]\n",
            "13340 [D loss: 0.742431, acc.: 47.66%] [G loss: 1.548590]\n",
            "13360 [D loss: 0.728245, acc.: 50.39%] [G loss: 1.501239]\n",
            "13380 [D loss: 0.737923, acc.: 46.09%] [G loss: 1.607843]\n",
            "13400 [D loss: 0.689626, acc.: 53.91%] [G loss: 1.590126]\n",
            "13420 [D loss: 0.762313, acc.: 40.62%] [G loss: 1.585131]\n",
            "13440 [D loss: 0.725261, acc.: 50.39%] [G loss: 1.588187]\n",
            "13460 [D loss: 0.694747, acc.: 54.69%] [G loss: 1.709077]\n",
            "13480 [D loss: 0.726492, acc.: 51.17%] [G loss: 1.713423]\n",
            "13500 [D loss: 0.750072, acc.: 48.83%] [G loss: 1.638384]\n",
            "13520 [D loss: 0.653093, acc.: 60.55%] [G loss: 2.087216]\n",
            "13540 [D loss: 0.718744, acc.: 53.52%] [G loss: 1.782072]\n",
            "13560 [D loss: 0.693619, acc.: 55.08%] [G loss: 1.827576]\n",
            "13580 [D loss: 0.715356, acc.: 54.69%] [G loss: 1.781461]\n",
            "13600 [D loss: 0.702284, acc.: 58.20%] [G loss: 1.874670]\n",
            "13620 [D loss: 0.693995, acc.: 54.30%] [G loss: 1.863883]\n",
            "13640 [D loss: 0.667290, acc.: 57.03%] [G loss: 1.889675]\n",
            "13660 [D loss: 0.695650, acc.: 54.69%] [G loss: 1.885250]\n",
            "13680 [D loss: 0.702949, acc.: 56.64%] [G loss: 1.973986]\n",
            "13700 [D loss: 0.702645, acc.: 55.86%] [G loss: 1.826347]\n",
            "13720 [D loss: 0.639014, acc.: 61.33%] [G loss: 1.972666]\n",
            "13740 [D loss: 0.689889, acc.: 58.59%] [G loss: 2.029083]\n",
            "13760 [D loss: 0.680228, acc.: 55.86%] [G loss: 2.053537]\n",
            "13780 [D loss: 0.600335, acc.: 68.36%] [G loss: 2.177580]\n",
            "13800 [D loss: 0.697024, acc.: 56.64%] [G loss: 1.979789]\n",
            "13820 [D loss: 0.709487, acc.: 55.86%] [G loss: 1.892728]\n",
            "13840 [D loss: 0.615283, acc.: 65.62%] [G loss: 2.094283]\n",
            "13860 [D loss: 0.605602, acc.: 65.23%] [G loss: 2.076745]\n",
            "13880 [D loss: 0.688083, acc.: 53.91%] [G loss: 1.927692]\n",
            "13900 [D loss: 0.662636, acc.: 61.33%] [G loss: 2.028692]\n",
            "13920 [D loss: 0.633115, acc.: 65.62%] [G loss: 1.982381]\n",
            "13940 [D loss: 0.675154, acc.: 57.42%] [G loss: 1.924018]\n",
            "13960 [D loss: 0.630388, acc.: 66.80%] [G loss: 2.267234]\n",
            "13980 [D loss: 0.642776, acc.: 61.33%] [G loss: 2.116613]\n",
            "14000 [D loss: 0.649254, acc.: 62.11%] [G loss: 2.046911]\n",
            "14020 [D loss: 0.584532, acc.: 67.19%] [G loss: 2.280858]\n",
            "14040 [D loss: 0.650292, acc.: 63.28%] [G loss: 2.240298]\n",
            "14060 [D loss: 0.625845, acc.: 65.23%] [G loss: 2.111420]\n",
            "14080 [D loss: 0.566778, acc.: 72.66%] [G loss: 2.228313]\n",
            "14100 [D loss: 0.634586, acc.: 63.67%] [G loss: 2.041709]\n",
            "14120 [D loss: 0.611650, acc.: 67.19%] [G loss: 2.222427]\n",
            "14140 [D loss: 0.616520, acc.: 67.19%] [G loss: 2.192262]\n",
            "14160 [D loss: 0.656023, acc.: 60.55%] [G loss: 2.107264]\n",
            "14180 [D loss: 0.602011, acc.: 70.31%] [G loss: 2.152172]\n",
            "14200 [D loss: 0.636700, acc.: 64.06%] [G loss: 2.154852]\n",
            "14220 [D loss: 0.694476, acc.: 60.55%] [G loss: 1.992186]\n",
            "14240 [D loss: 0.615325, acc.: 65.62%] [G loss: 2.229295]\n",
            "14260 [D loss: 0.604695, acc.: 67.19%] [G loss: 2.414385]\n",
            "14280 [D loss: 0.599155, acc.: 66.80%] [G loss: 2.193115]\n",
            "14300 [D loss: 0.643062, acc.: 65.23%] [G loss: 2.259908]\n",
            "14320 [D loss: 0.620863, acc.: 67.19%] [G loss: 2.178571]\n",
            "14340 [D loss: 0.629957, acc.: 64.45%] [G loss: 2.321392]\n",
            "14360 [D loss: 0.618862, acc.: 66.41%] [G loss: 2.302438]\n",
            "14380 [D loss: 0.601788, acc.: 67.19%] [G loss: 2.395400]\n",
            "14400 [D loss: 0.613006, acc.: 66.80%] [G loss: 2.459732]\n",
            "14420 [D loss: 0.571759, acc.: 71.88%] [G loss: 2.383086]\n",
            "14440 [D loss: 0.532687, acc.: 72.27%] [G loss: 2.514733]\n",
            "14460 [D loss: 0.577693, acc.: 71.09%] [G loss: 2.406902]\n",
            "14480 [D loss: 0.585123, acc.: 68.75%] [G loss: 2.362775]\n",
            "14500 [D loss: 0.535856, acc.: 74.22%] [G loss: 2.430163]\n",
            "14520 [D loss: 0.583295, acc.: 71.09%] [G loss: 2.447805]\n",
            "14540 [D loss: 0.552316, acc.: 70.70%] [G loss: 2.412280]\n",
            "14560 [D loss: 0.548640, acc.: 72.27%] [G loss: 2.553636]\n",
            "14580 [D loss: 0.584299, acc.: 70.31%] [G loss: 2.382321]\n",
            "14600 [D loss: 0.525086, acc.: 74.22%] [G loss: 2.586702]\n",
            "14620 [D loss: 0.569783, acc.: 73.44%] [G loss: 2.644041]\n",
            "14640 [D loss: 0.542027, acc.: 72.27%] [G loss: 2.704900]\n",
            "14660 [D loss: 0.565188, acc.: 71.88%] [G loss: 2.429315]\n",
            "14680 [D loss: 0.572581, acc.: 69.53%] [G loss: 2.353931]\n",
            "14700 [D loss: 0.574277, acc.: 69.14%] [G loss: 2.350959]\n",
            "14720 [D loss: 0.593059, acc.: 69.14%] [G loss: 2.499839]\n",
            "14740 [D loss: 0.613715, acc.: 64.45%] [G loss: 2.512013]\n",
            "14760 [D loss: 0.686052, acc.: 61.33%] [G loss: 2.230486]\n",
            "14780 [D loss: 0.588877, acc.: 68.75%] [G loss: 2.817158]\n",
            "14800 [D loss: 0.530273, acc.: 74.22%] [G loss: 2.524532]\n",
            "14820 [D loss: 0.434146, acc.: 83.20%] [G loss: 2.629265]\n",
            "14840 [D loss: 0.611263, acc.: 66.80%] [G loss: 2.691007]\n",
            "14860 [D loss: 0.471998, acc.: 78.52%] [G loss: 2.490530]\n",
            "14880 [D loss: 0.557652, acc.: 71.09%] [G loss: 1.494334]\n",
            "14900 [D loss: 0.734179, acc.: 57.03%] [G loss: 1.485656]\n",
            "14920 [D loss: 0.658114, acc.: 66.02%] [G loss: 3.494201]\n",
            "14940 [D loss: 0.616878, acc.: 67.19%] [G loss: 2.867118]\n",
            "14960 [D loss: 0.571195, acc.: 68.36%] [G loss: 3.097041]\n",
            "14980 [D loss: 0.652506, acc.: 66.41%] [G loss: 2.784273]\n",
            "15000 [D loss: 0.545841, acc.: 71.09%] [G loss: 2.988213]\n",
            "15020 [D loss: 0.546377, acc.: 75.39%] [G loss: 2.721695]\n",
            "15040 [D loss: 0.534232, acc.: 75.00%] [G loss: 2.699474]\n",
            "15060 [D loss: 0.544342, acc.: 72.27%] [G loss: 2.803319]\n",
            "15080 [D loss: 0.547541, acc.: 71.48%] [G loss: 3.014448]\n",
            "15100 [D loss: 0.633754, acc.: 64.84%] [G loss: 2.580622]\n",
            "15120 [D loss: 0.515072, acc.: 75.00%] [G loss: 3.002444]\n",
            "15140 [D loss: 0.569991, acc.: 71.09%] [G loss: 2.842645]\n",
            "15160 [D loss: 0.487717, acc.: 75.39%] [G loss: 2.939958]\n",
            "15180 [D loss: 0.503310, acc.: 75.39%] [G loss: 3.072923]\n",
            "15200 [D loss: 0.535359, acc.: 73.83%] [G loss: 2.854047]\n",
            "15220 [D loss: 0.591751, acc.: 65.62%] [G loss: 2.779132]\n",
            "15240 [D loss: 0.497452, acc.: 77.34%] [G loss: 3.108869]\n",
            "15260 [D loss: 0.567112, acc.: 66.80%] [G loss: 2.962757]\n",
            "15280 [D loss: 0.530605, acc.: 74.61%] [G loss: 3.181646]\n",
            "15300 [D loss: 0.542680, acc.: 73.05%] [G loss: 3.046183]\n",
            "15320 [D loss: 0.504758, acc.: 75.00%] [G loss: 3.125658]\n",
            "15340 [D loss: 0.495558, acc.: 75.39%] [G loss: 2.946429]\n",
            "15360 [D loss: 0.547187, acc.: 69.14%] [G loss: 3.032266]\n",
            "15380 [D loss: 0.529900, acc.: 75.39%] [G loss: 3.059528]\n",
            "15400 [D loss: 0.535075, acc.: 72.27%] [G loss: 3.152778]\n",
            "15420 [D loss: 0.528736, acc.: 74.22%] [G loss: 2.943242]\n",
            "15440 [D loss: 0.545409, acc.: 74.61%] [G loss: 3.010587]\n",
            "15460 [D loss: 0.524097, acc.: 76.17%] [G loss: 2.980048]\n",
            "15480 [D loss: 0.571418, acc.: 73.44%] [G loss: 2.806247]\n",
            "15500 [D loss: 0.522539, acc.: 75.78%] [G loss: 3.293415]\n",
            "15520 [D loss: 0.571720, acc.: 70.70%] [G loss: 3.115919]\n",
            "15540 [D loss: 0.488991, acc.: 74.61%] [G loss: 3.303401]\n",
            "15560 [D loss: 0.534609, acc.: 70.31%] [G loss: 3.482697]\n",
            "15580 [D loss: 0.445997, acc.: 82.03%] [G loss: 3.468371]\n",
            "15600 [D loss: 0.507425, acc.: 74.22%] [G loss: 3.379269]\n",
            "15620 [D loss: 0.468957, acc.: 78.91%] [G loss: 3.293061]\n",
            "15640 [D loss: 0.510115, acc.: 75.00%] [G loss: 3.299730]\n",
            "15660 [D loss: 0.528977, acc.: 76.17%] [G loss: 3.318933]\n",
            "15680 [D loss: 0.547674, acc.: 73.44%] [G loss: 3.292364]\n",
            "15700 [D loss: 0.474375, acc.: 78.12%] [G loss: 3.075695]\n",
            "15720 [D loss: 0.454886, acc.: 77.34%] [G loss: 3.537590]\n",
            "15740 [D loss: 0.515616, acc.: 75.00%] [G loss: 3.364797]\n",
            "15760 [D loss: 0.458821, acc.: 79.30%] [G loss: 3.346215]\n",
            "15780 [D loss: 0.497385, acc.: 76.56%] [G loss: 2.998312]\n",
            "15800 [D loss: 0.447939, acc.: 79.69%] [G loss: 3.730515]\n",
            "15820 [D loss: 0.521695, acc.: 71.48%] [G loss: 3.333239]\n",
            "15840 [D loss: 0.547712, acc.: 74.22%] [G loss: 3.039105]\n",
            "15860 [D loss: 0.521660, acc.: 73.83%] [G loss: 3.277976]\n",
            "15880 [D loss: 0.484710, acc.: 78.12%] [G loss: 3.251577]\n",
            "15900 [D loss: 0.523252, acc.: 72.27%] [G loss: 3.330383]\n",
            "15920 [D loss: 0.429456, acc.: 81.25%] [G loss: 3.835911]\n",
            "15940 [D loss: 0.450525, acc.: 81.25%] [G loss: 3.613568]\n",
            "15960 [D loss: 0.571816, acc.: 73.44%] [G loss: 3.553242]\n",
            "15980 [D loss: 0.498624, acc.: 76.95%] [G loss: 3.534125]\n",
            "16000 [D loss: 0.533949, acc.: 71.48%] [G loss: 3.579718]\n",
            "16020 [D loss: 0.594780, acc.: 68.75%] [G loss: 3.128793]\n",
            "16040 [D loss: 0.530377, acc.: 72.27%] [G loss: 3.351895]\n",
            "16060 [D loss: 0.451046, acc.: 80.47%] [G loss: 3.369381]\n",
            "16080 [D loss: 0.464700, acc.: 78.52%] [G loss: 3.546370]\n",
            "16100 [D loss: 0.492597, acc.: 74.61%] [G loss: 3.475283]\n",
            "16120 [D loss: 0.443602, acc.: 81.64%] [G loss: 3.686546]\n",
            "16140 [D loss: 0.499557, acc.: 75.00%] [G loss: 3.186152]\n",
            "16160 [D loss: 0.443639, acc.: 80.86%] [G loss: 3.545908]\n",
            "16180 [D loss: 0.497987, acc.: 74.22%] [G loss: 3.695331]\n",
            "16200 [D loss: 0.479568, acc.: 77.34%] [G loss: 3.695174]\n",
            "16220 [D loss: 0.476779, acc.: 76.95%] [G loss: 3.597069]\n",
            "16240 [D loss: 0.549083, acc.: 72.27%] [G loss: 3.143379]\n",
            "16260 [D loss: 0.397992, acc.: 85.55%] [G loss: 3.715328]\n",
            "16280 [D loss: 0.411653, acc.: 81.25%] [G loss: 3.873999]\n",
            "16300 [D loss: 0.439230, acc.: 81.25%] [G loss: 3.878411]\n",
            "16320 [D loss: 0.495332, acc.: 73.05%] [G loss: 3.805642]\n",
            "16340 [D loss: 0.409919, acc.: 82.81%] [G loss: 3.658176]\n",
            "16360 [D loss: 0.493818, acc.: 73.83%] [G loss: 3.603335]\n",
            "16380 [D loss: 0.555108, acc.: 69.92%] [G loss: 3.302819]\n",
            "16400 [D loss: 0.448423, acc.: 76.56%] [G loss: 3.819104]\n",
            "16420 [D loss: 0.466996, acc.: 75.39%] [G loss: 3.956118]\n",
            "16440 [D loss: 0.474094, acc.: 76.17%] [G loss: 3.797653]\n",
            "16460 [D loss: 0.396217, acc.: 82.03%] [G loss: 3.762125]\n",
            "16480 [D loss: 0.456891, acc.: 76.95%] [G loss: 3.864359]\n",
            "16500 [D loss: 0.384846, acc.: 83.98%] [G loss: 4.205976]\n",
            "16520 [D loss: 0.473279, acc.: 78.91%] [G loss: 3.453546]\n",
            "16540 [D loss: 0.504048, acc.: 74.61%] [G loss: 3.560815]\n",
            "16560 [D loss: 0.464514, acc.: 76.56%] [G loss: 3.876060]\n",
            "16580 [D loss: 0.438641, acc.: 79.69%] [G loss: 3.777628]\n",
            "16600 [D loss: 0.446933, acc.: 78.12%] [G loss: 4.008136]\n",
            "16620 [D loss: 0.544848, acc.: 74.61%] [G loss: 3.776094]\n",
            "16640 [D loss: 0.488665, acc.: 77.73%] [G loss: 3.701022]\n",
            "16660 [D loss: 0.480496, acc.: 75.39%] [G loss: 4.081860]\n",
            "16680 [D loss: 0.381848, acc.: 83.20%] [G loss: 3.926901]\n",
            "16700 [D loss: 0.541082, acc.: 75.78%] [G loss: 3.567613]\n",
            "16720 [D loss: 0.437684, acc.: 81.64%] [G loss: 4.228920]\n",
            "16740 [D loss: 0.438626, acc.: 80.86%] [G loss: 3.470663]\n",
            "16760 [D loss: 0.422969, acc.: 82.42%] [G loss: 4.273767]\n",
            "16780 [D loss: 0.140915, acc.: 97.66%] [G loss: 2.929632]\n",
            "16800 [D loss: 0.095735, acc.: 98.83%] [G loss: 2.935702]\n",
            "16820 [D loss: 0.327457, acc.: 86.72%] [G loss: 2.316528]\n",
            "16840 [D loss: 0.200900, acc.: 97.27%] [G loss: 1.878231]\n",
            "16860 [D loss: 2.670322, acc.: 15.62%] [G loss: 2.715162]\n",
            "16880 [D loss: 1.126950, acc.: 51.17%] [G loss: 2.443721]\n",
            "16900 [D loss: 1.316010, acc.: 50.39%] [G loss: 3.548938]\n",
            "16920 [D loss: 0.500940, acc.: 75.78%] [G loss: 3.478302]\n",
            "16940 [D loss: 0.531101, acc.: 74.61%] [G loss: 3.345674]\n",
            "16960 [D loss: 0.583785, acc.: 69.14%] [G loss: 3.465172]\n",
            "16980 [D loss: 0.594839, acc.: 70.70%] [G loss: 3.184217]\n",
            "17000 [D loss: 0.448745, acc.: 79.30%] [G loss: 3.806947]\n",
            "17020 [D loss: 0.575555, acc.: 70.31%] [G loss: 3.365949]\n",
            "17040 [D loss: 0.517011, acc.: 75.00%] [G loss: 3.298144]\n",
            "17060 [D loss: 0.418440, acc.: 79.30%] [G loss: 3.859064]\n",
            "17080 [D loss: 0.489119, acc.: 76.95%] [G loss: 3.475952]\n",
            "17100 [D loss: 0.473869, acc.: 78.52%] [G loss: 3.633117]\n",
            "17120 [D loss: 0.498276, acc.: 77.34%] [G loss: 3.268645]\n",
            "17140 [D loss: 0.549765, acc.: 71.09%] [G loss: 3.445357]\n",
            "17160 [D loss: 0.523135, acc.: 72.66%] [G loss: 3.469413]\n",
            "17180 [D loss: 0.451284, acc.: 82.03%] [G loss: 3.680575]\n",
            "17200 [D loss: 0.482568, acc.: 77.73%] [G loss: 3.807078]\n",
            "17220 [D loss: 0.415679, acc.: 82.81%] [G loss: 3.970471]\n",
            "17240 [D loss: 0.482269, acc.: 78.12%] [G loss: 3.573400]\n",
            "17260 [D loss: 0.430401, acc.: 80.86%] [G loss: 3.897282]\n",
            "17280 [D loss: 0.529752, acc.: 73.83%] [G loss: 3.661973]\n",
            "17300 [D loss: 0.424977, acc.: 80.47%] [G loss: 4.051282]\n",
            "17320 [D loss: 0.422689, acc.: 80.08%] [G loss: 4.139228]\n",
            "17340 [D loss: 0.483086, acc.: 76.17%] [G loss: 3.724359]\n",
            "17360 [D loss: 0.427933, acc.: 81.64%] [G loss: 3.972797]\n",
            "17380 [D loss: 0.478965, acc.: 75.78%] [G loss: 3.685621]\n",
            "17400 [D loss: 0.425306, acc.: 79.69%] [G loss: 3.909443]\n",
            "17420 [D loss: 0.401038, acc.: 79.69%] [G loss: 4.014397]\n",
            "17440 [D loss: 0.456945, acc.: 76.95%] [G loss: 3.891011]\n",
            "17460 [D loss: 0.458380, acc.: 78.91%] [G loss: 3.774419]\n",
            "17480 [D loss: 0.442546, acc.: 79.69%] [G loss: 3.892166]\n",
            "17500 [D loss: 0.525184, acc.: 77.73%] [G loss: 3.972596]\n",
            "17520 [D loss: 0.398498, acc.: 83.20%] [G loss: 4.007174]\n",
            "17540 [D loss: 0.489216, acc.: 76.95%] [G loss: 3.881885]\n",
            "17560 [D loss: 0.507675, acc.: 75.00%] [G loss: 3.605468]\n",
            "17580 [D loss: 0.415378, acc.: 79.30%] [G loss: 3.979010]\n",
            "17600 [D loss: 0.353075, acc.: 86.33%] [G loss: 4.461925]\n",
            "17620 [D loss: 0.420220, acc.: 81.25%] [G loss: 3.721700]\n",
            "17640 [D loss: 0.401485, acc.: 81.64%] [G loss: 4.139170]\n",
            "17660 [D loss: 0.425625, acc.: 81.64%] [G loss: 4.235442]\n",
            "17680 [D loss: 0.480278, acc.: 78.52%] [G loss: 4.153822]\n",
            "17700 [D loss: 0.434465, acc.: 80.08%] [G loss: 3.816576]\n",
            "17720 [D loss: 0.531775, acc.: 72.27%] [G loss: 3.890561]\n",
            "17740 [D loss: 0.538159, acc.: 74.22%] [G loss: 4.241058]\n",
            "17760 [D loss: 0.450442, acc.: 77.34%] [G loss: 4.173909]\n",
            "17780 [D loss: 0.440888, acc.: 78.91%] [G loss: 4.079552]\n",
            "17800 [D loss: 0.429689, acc.: 81.64%] [G loss: 4.060982]\n",
            "17820 [D loss: 0.529083, acc.: 75.78%] [G loss: 3.852216]\n",
            "17840 [D loss: 0.513761, acc.: 74.61%] [G loss: 3.955640]\n",
            "17860 [D loss: 0.444273, acc.: 80.86%] [G loss: 2.473521]\n",
            "17880 [D loss: 0.450828, acc.: 76.17%] [G loss: 3.706024]\n",
            "17900 [D loss: 0.513751, acc.: 75.39%] [G loss: 3.550951]\n",
            "17920 [D loss: 0.332566, acc.: 86.72%] [G loss: 3.886613]\n",
            "17940 [D loss: 0.342621, acc.: 86.72%] [G loss: 4.883440]\n",
            "17960 [D loss: 0.469381, acc.: 79.30%] [G loss: 4.567422]\n",
            "17980 [D loss: 0.445935, acc.: 81.25%] [G loss: 4.127883]\n",
            "18000 [D loss: 0.395627, acc.: 85.16%] [G loss: 4.333349]\n",
            "18020 [D loss: 0.494582, acc.: 76.95%] [G loss: 4.438000]\n",
            "18040 [D loss: 0.425899, acc.: 82.42%] [G loss: 4.711637]\n",
            "18060 [D loss: 0.505778, acc.: 74.61%] [G loss: 4.213007]\n",
            "18080 [D loss: 0.474384, acc.: 76.17%] [G loss: 4.206460]\n",
            "18100 [D loss: 0.422520, acc.: 82.42%] [G loss: 4.155330]\n",
            "18120 [D loss: 0.416479, acc.: 84.38%] [G loss: 4.805680]\n",
            "18140 [D loss: 0.440658, acc.: 80.08%] [G loss: 4.352617]\n",
            "18160 [D loss: 0.432657, acc.: 79.30%] [G loss: 4.130453]\n",
            "18180 [D loss: 0.472778, acc.: 76.56%] [G loss: 4.212148]\n",
            "18200 [D loss: 0.396816, acc.: 82.42%] [G loss: 5.112321]\n",
            "18220 [D loss: 0.481563, acc.: 76.17%] [G loss: 4.147610]\n",
            "18240 [D loss: 0.420110, acc.: 80.47%] [G loss: 4.666373]\n",
            "18260 [D loss: 0.388194, acc.: 82.03%] [G loss: 4.539410]\n",
            "18280 [D loss: 0.396842, acc.: 81.25%] [G loss: 4.358639]\n",
            "18300 [D loss: 0.523322, acc.: 74.22%] [G loss: 3.936074]\n",
            "18320 [D loss: 0.450015, acc.: 79.69%] [G loss: 4.468688]\n",
            "18340 [D loss: 0.457133, acc.: 77.34%] [G loss: 4.299532]\n",
            "18360 [D loss: 0.420701, acc.: 78.52%] [G loss: 4.688275]\n",
            "18380 [D loss: 0.380876, acc.: 84.77%] [G loss: 4.490311]\n",
            "18400 [D loss: 0.360308, acc.: 83.98%] [G loss: 4.521237]\n",
            "18420 [D loss: 0.445186, acc.: 78.91%] [G loss: 4.441533]\n",
            "18440 [D loss: 0.440926, acc.: 79.69%] [G loss: 4.620003]\n",
            "18460 [D loss: 0.374126, acc.: 83.20%] [G loss: 4.787012]\n",
            "18480 [D loss: 0.452798, acc.: 78.91%] [G loss: 4.412710]\n",
            "18500 [D loss: 0.518348, acc.: 75.00%] [G loss: 4.373772]\n",
            "18520 [D loss: 0.363702, acc.: 85.55%] [G loss: 4.656341]\n",
            "18540 [D loss: 0.461616, acc.: 76.95%] [G loss: 4.678071]\n",
            "18560 [D loss: 0.422605, acc.: 80.08%] [G loss: 4.469112]\n",
            "18580 [D loss: 0.368804, acc.: 84.38%] [G loss: 5.013170]\n",
            "18600 [D loss: 0.353327, acc.: 84.38%] [G loss: 5.076804]\n",
            "18620 [D loss: 0.364131, acc.: 85.55%] [G loss: 4.645982]\n",
            "18640 [D loss: 0.332932, acc.: 85.16%] [G loss: 5.304235]\n",
            "18660 [D loss: 0.460010, acc.: 78.52%] [G loss: 4.442475]\n",
            "18680 [D loss: 0.393748, acc.: 83.59%] [G loss: 5.005321]\n",
            "18700 [D loss: 0.416917, acc.: 80.08%] [G loss: 4.531910]\n",
            "18720 [D loss: 0.525447, acc.: 73.83%] [G loss: 4.411332]\n",
            "18740 [D loss: 0.359224, acc.: 82.81%] [G loss: 4.616985]\n",
            "18760 [D loss: 0.395468, acc.: 84.38%] [G loss: 4.823833]\n",
            "18780 [D loss: 0.333402, acc.: 89.45%] [G loss: 4.605511]\n",
            "18800 [D loss: 0.430396, acc.: 79.69%] [G loss: 4.469258]\n",
            "18820 [D loss: 0.387128, acc.: 82.81%] [G loss: 4.971443]\n",
            "18840 [D loss: 0.329515, acc.: 83.59%] [G loss: 5.399846]\n",
            "18860 [D loss: 0.413335, acc.: 82.03%] [G loss: 4.787777]\n",
            "18880 [D loss: 0.348344, acc.: 85.55%] [G loss: 5.016345]\n",
            "18900 [D loss: 0.480398, acc.: 73.05%] [G loss: 4.317384]\n",
            "18920 [D loss: 0.381269, acc.: 83.20%] [G loss: 4.763235]\n",
            "18940 [D loss: 0.431107, acc.: 80.47%] [G loss: 4.825700]\n",
            "18960 [D loss: 0.365121, acc.: 85.94%] [G loss: 5.024138]\n",
            "18980 [D loss: 0.405408, acc.: 82.03%] [G loss: 4.399410]\n",
            "19000 [D loss: 0.412518, acc.: 80.47%] [G loss: 4.546829]\n",
            "19020 [D loss: 0.411497, acc.: 82.03%] [G loss: 4.790885]\n",
            "19040 [D loss: 0.367285, acc.: 82.03%] [G loss: 4.762962]\n",
            "19060 [D loss: 0.331231, acc.: 86.72%] [G loss: 4.733122]\n",
            "19080 [D loss: 0.429756, acc.: 82.03%] [G loss: 4.718849]\n",
            "19100 [D loss: 0.384943, acc.: 84.38%] [G loss: 4.704380]\n",
            "19120 [D loss: 0.392018, acc.: 83.59%] [G loss: 4.761758]\n",
            "19140 [D loss: 0.436348, acc.: 82.42%] [G loss: 4.553362]\n",
            "19160 [D loss: 0.436287, acc.: 76.56%] [G loss: 4.849899]\n",
            "19180 [D loss: 0.371065, acc.: 85.94%] [G loss: 4.776004]\n",
            "19200 [D loss: 0.421896, acc.: 80.86%] [G loss: 4.582654]\n",
            "19220 [D loss: 0.486532, acc.: 77.73%] [G loss: 4.577181]\n",
            "19240 [D loss: 0.392241, acc.: 81.25%] [G loss: 5.173867]\n",
            "19260 [D loss: 0.351962, acc.: 88.67%] [G loss: 4.750071]\n",
            "19280 [D loss: 0.456190, acc.: 82.81%] [G loss: 4.481617]\n",
            "19300 [D loss: 0.327666, acc.: 86.33%] [G loss: 4.756032]\n",
            "19320 [D loss: 0.269982, acc.: 90.62%] [G loss: 2.853055]\n",
            "19340 [D loss: 11.364231, acc.: 50.00%] [G loss: 84.083946]\n",
            "19360 [D loss: 1.440792, acc.: 57.42%] [G loss: 7.684786]\n",
            "19380 [D loss: 1.532775, acc.: 40.23%] [G loss: 2.988129]\n",
            "19400 [D loss: 0.551348, acc.: 75.39%] [G loss: 4.697005]\n",
            "19420 [D loss: 0.442347, acc.: 79.69%] [G loss: 4.072058]\n",
            "19440 [D loss: 0.553716, acc.: 73.83%] [G loss: 3.338627]\n",
            "19460 [D loss: 0.848049, acc.: 57.81%] [G loss: 2.737608]\n",
            "19480 [D loss: 0.642693, acc.: 66.41%] [G loss: 2.659514]\n",
            "19500 [D loss: 0.537567, acc.: 73.83%] [G loss: 3.387175]\n",
            "19520 [D loss: 0.613420, acc.: 71.09%] [G loss: 3.153891]\n",
            "19540 [D loss: 0.661218, acc.: 65.62%] [G loss: 3.199342]\n",
            "19560 [D loss: 0.641182, acc.: 65.23%] [G loss: 2.786673]\n",
            "19580 [D loss: 0.669367, acc.: 60.55%] [G loss: 2.944596]\n",
            "19600 [D loss: 0.608781, acc.: 67.58%] [G loss: 3.121792]\n",
            "19620 [D loss: 0.644335, acc.: 67.19%] [G loss: 2.932368]\n",
            "19640 [D loss: 0.707660, acc.: 58.59%] [G loss: 2.452814]\n",
            "19660 [D loss: 0.595574, acc.: 70.31%] [G loss: 2.496076]\n",
            "19680 [D loss: 0.649526, acc.: 66.80%] [G loss: 2.708781]\n",
            "19700 [D loss: 0.639289, acc.: 67.19%] [G loss: 2.500226]\n",
            "19720 [D loss: 0.575753, acc.: 66.80%] [G loss: 2.700844]\n",
            "19740 [D loss: 0.613375, acc.: 67.58%] [G loss: 2.466516]\n",
            "19760 [D loss: 0.658178, acc.: 64.45%] [G loss: 2.440794]\n",
            "19780 [D loss: 0.605321, acc.: 69.14%] [G loss: 2.583588]\n",
            "19800 [D loss: 0.573611, acc.: 69.92%] [G loss: 2.789363]\n",
            "19820 [D loss: 0.547718, acc.: 73.83%] [G loss: 2.460376]\n",
            "19840 [D loss: 0.691889, acc.: 59.38%] [G loss: 2.155219]\n",
            "19860 [D loss: 0.582809, acc.: 67.19%] [G loss: 2.685763]\n",
            "19880 [D loss: 0.528989, acc.: 74.22%] [G loss: 2.648357]\n",
            "19900 [D loss: 0.636178, acc.: 65.23%] [G loss: 2.390000]\n",
            "19920 [D loss: 0.568845, acc.: 70.31%] [G loss: 2.781032]\n",
            "19940 [D loss: 0.561666, acc.: 72.27%] [G loss: 2.584476]\n",
            "19960 [D loss: 0.547947, acc.: 76.56%] [G loss: 2.683856]\n",
            "19980 [D loss: 0.574006, acc.: 68.75%] [G loss: 2.755781]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7OvsALJTfRi",
        "outputId": "12a355d3-9d4b-49ca-ad2b-aba5aa6f7fad"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 48, 48, 128)  3328        input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 100)          0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          51712       flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 2304)      16128       input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 1152)         0           leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 2304)         0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3968)         0           dropout_2[0][0]                  \n",
            "                                                                 flatten_10[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 1)            3969        concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 4608)              465408    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)   (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 12, 12, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 48, 48, 1)         4609      \n",
            "=================================================================\n",
            "Total params: 1,256,833\n",
            "Trainable params: 1,256,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.692794, acc.: 41.80%] [G loss: 1.363244]\n",
            "20 [D loss: 0.997228, acc.: 49.22%] [G loss: 1.079654]\n",
            "40 [D loss: 10.192451, acc.: 38.28%] [G loss: 16.920563]\n",
            "60 [D loss: 4.623682, acc.: 29.30%] [G loss: 7.144755]\n",
            "80 [D loss: 0.145405, acc.: 96.88%] [G loss: 3.568455]\n",
            "100 [D loss: 0.171819, acc.: 96.09%] [G loss: 4.876417]\n",
            "120 [D loss: 0.059815, acc.: 100.00%] [G loss: 8.702084]\n",
            "140 [D loss: 0.277375, acc.: 89.45%] [G loss: 4.059882]\n",
            "160 [D loss: 0.139765, acc.: 94.53%] [G loss: 6.267387]\n",
            "180 [D loss: 0.098693, acc.: 96.48%] [G loss: 7.450905]\n",
            "200 [D loss: 0.075423, acc.: 96.88%] [G loss: 9.491319]\n",
            "220 [D loss: 0.860310, acc.: 76.17%] [G loss: 7.120339]\n",
            "240 [D loss: 0.021870, acc.: 100.00%] [G loss: 10.763273]\n",
            "260 [D loss: 0.283057, acc.: 98.83%] [G loss: 4.523032]\n",
            "280 [D loss: 0.064968, acc.: 99.22%] [G loss: 8.937262]\n",
            "300 [D loss: 0.257237, acc.: 98.05%] [G loss: 5.262499]\n",
            "320 [D loss: 0.638951, acc.: 46.88%] [G loss: 5.962174]\n",
            "340 [D loss: 0.419628, acc.: 89.84%] [G loss: 4.417611]\n",
            "360 [D loss: 0.347575, acc.: 93.36%] [G loss: 3.552187]\n",
            "380 [D loss: 0.969137, acc.: 64.84%] [G loss: 2.598330]\n",
            "400 [D loss: 0.632267, acc.: 78.52%] [G loss: 3.372662]\n",
            "420 [D loss: 0.525285, acc.: 83.98%] [G loss: 2.757557]\n",
            "440 [D loss: 0.400338, acc.: 88.28%] [G loss: 5.184896]\n",
            "460 [D loss: 0.484532, acc.: 89.84%] [G loss: 5.033338]\n",
            "480 [D loss: 0.526991, acc.: 77.34%] [G loss: 3.353721]\n",
            "500 [D loss: 0.484876, acc.: 87.11%] [G loss: 3.852768]\n",
            "520 [D loss: 0.238678, acc.: 94.14%] [G loss: 6.053322]\n",
            "540 [D loss: 0.310701, acc.: 91.41%] [G loss: 4.480524]\n",
            "560 [D loss: 0.386147, acc.: 92.19%] [G loss: 5.068025]\n",
            "580 [D loss: 0.441447, acc.: 82.42%] [G loss: 8.714972]\n",
            "600 [D loss: 2.068088, acc.: 35.16%] [G loss: 5.693320]\n",
            "620 [D loss: 0.076214, acc.: 96.88%] [G loss: 15.504116]\n",
            "640 [D loss: 0.402508, acc.: 89.84%] [G loss: 5.015415]\n",
            "660 [D loss: 0.291974, acc.: 86.33%] [G loss: 6.800432]\n",
            "680 [D loss: 0.365668, acc.: 85.94%] [G loss: 4.809776]\n",
            "700 [D loss: 0.375825, acc.: 87.89%] [G loss: 7.323451]\n",
            "720 [D loss: 0.584250, acc.: 77.34%] [G loss: 3.898723]\n",
            "740 [D loss: 0.480237, acc.: 77.34%] [G loss: 3.939720]\n",
            "760 [D loss: 0.615871, acc.: 68.75%] [G loss: 3.039476]\n",
            "780 [D loss: 0.518439, acc.: 75.39%] [G loss: 3.012846]\n",
            "800 [D loss: 0.591483, acc.: 67.58%] [G loss: 2.936316]\n",
            "820 [D loss: 0.580957, acc.: 68.75%] [G loss: 3.117126]\n",
            "840 [D loss: 0.453159, acc.: 84.77%] [G loss: 3.259461]\n",
            "860 [D loss: 0.419408, acc.: 83.20%] [G loss: 4.634182]\n",
            "880 [D loss: 0.313598, acc.: 90.62%] [G loss: 4.173212]\n",
            "900 [D loss: 0.333613, acc.: 86.33%] [G loss: 5.602916]\n",
            "920 [D loss: 0.195415, acc.: 96.48%] [G loss: 6.447292]\n",
            "940 [D loss: 0.307183, acc.: 89.45%] [G loss: 5.055152]\n",
            "960 [D loss: 0.452245, acc.: 81.64%] [G loss: 4.614908]\n",
            "980 [D loss: 0.375906, acc.: 84.38%] [G loss: 4.065977]\n",
            "1000 [D loss: 0.322876, acc.: 87.50%] [G loss: 4.418130]\n",
            "1020 [D loss: 0.586964, acc.: 73.05%] [G loss: 3.883324]\n",
            "1040 [D loss: 0.422864, acc.: 80.08%] [G loss: 3.679060]\n",
            "1060 [D loss: 0.452724, acc.: 79.30%] [G loss: 4.292919]\n",
            "1080 [D loss: 0.270108, acc.: 92.19%] [G loss: 5.464712]\n",
            "1100 [D loss: 0.379701, acc.: 83.98%] [G loss: 5.048302]\n",
            "1120 [D loss: 0.425299, acc.: 85.55%] [G loss: 4.107570]\n",
            "1140 [D loss: 0.346484, acc.: 86.33%] [G loss: 4.969882]\n",
            "1160 [D loss: 0.475160, acc.: 74.61%] [G loss: 4.161832]\n",
            "1180 [D loss: 0.556998, acc.: 73.83%] [G loss: 3.082120]\n",
            "1200 [D loss: 0.484912, acc.: 76.56%] [G loss: 3.433202]\n",
            "1220 [D loss: 0.389804, acc.: 84.77%] [G loss: 3.793457]\n",
            "1240 [D loss: 0.383044, acc.: 83.20%] [G loss: 4.044407]\n",
            "1260 [D loss: 0.557780, acc.: 74.61%] [G loss: 3.227710]\n",
            "1280 [D loss: 0.454065, acc.: 78.52%] [G loss: 3.505677]\n",
            "1300 [D loss: 0.447838, acc.: 81.25%] [G loss: 3.583960]\n",
            "1320 [D loss: 0.614067, acc.: 71.88%] [G loss: 3.790469]\n",
            "1340 [D loss: 0.397807, acc.: 83.59%] [G loss: 3.764839]\n",
            "1360 [D loss: 0.807834, acc.: 53.91%] [G loss: 3.551185]\n",
            "1380 [D loss: 0.439475, acc.: 81.64%] [G loss: 3.677790]\n",
            "1400 [D loss: 0.483154, acc.: 77.34%] [G loss: 3.529718]\n",
            "1420 [D loss: 0.475700, acc.: 76.56%] [G loss: 3.799642]\n",
            "1440 [D loss: 0.545951, acc.: 74.61%] [G loss: 3.470526]\n",
            "1460 [D loss: 0.441273, acc.: 81.64%] [G loss: 3.969632]\n",
            "1480 [D loss: 0.387720, acc.: 82.81%] [G loss: 4.333671]\n",
            "1500 [D loss: 0.557913, acc.: 71.88%] [G loss: 3.855313]\n",
            "1520 [D loss: 0.368076, acc.: 82.81%] [G loss: 4.123768]\n",
            "1540 [D loss: 0.410299, acc.: 82.42%] [G loss: 4.277707]\n",
            "1560 [D loss: 0.400476, acc.: 82.81%] [G loss: 3.751390]\n",
            "1580 [D loss: 0.449101, acc.: 79.30%] [G loss: 3.734212]\n",
            "1600 [D loss: 0.415674, acc.: 81.25%] [G loss: 3.873059]\n",
            "1620 [D loss: 0.265750, acc.: 92.19%] [G loss: 2.964638]\n",
            "1640 [D loss: 0.489255, acc.: 76.17%] [G loss: 3.519682]\n",
            "1660 [D loss: 0.477191, acc.: 78.91%] [G loss: 4.055447]\n",
            "1680 [D loss: 0.386211, acc.: 82.03%] [G loss: 4.089319]\n",
            "1700 [D loss: 0.438694, acc.: 80.86%] [G loss: 3.858937]\n",
            "1720 [D loss: 0.324894, acc.: 87.11%] [G loss: 4.758214]\n",
            "1740 [D loss: 0.508147, acc.: 73.83%] [G loss: 3.894225]\n",
            "1760 [D loss: 0.466030, acc.: 81.25%] [G loss: 4.541659]\n",
            "1780 [D loss: 0.499243, acc.: 77.34%] [G loss: 3.977340]\n",
            "1800 [D loss: 0.441533, acc.: 81.64%] [G loss: 3.896728]\n",
            "1820 [D loss: 0.476543, acc.: 77.34%] [G loss: 4.024343]\n",
            "1840 [D loss: 0.380871, acc.: 82.81%] [G loss: 4.468583]\n",
            "1860 [D loss: 0.521969, acc.: 74.22%] [G loss: 4.363077]\n",
            "1880 [D loss: 0.398990, acc.: 85.16%] [G loss: 4.545360]\n",
            "1900 [D loss: 0.352253, acc.: 84.77%] [G loss: 4.854161]\n",
            "1920 [D loss: 0.411604, acc.: 80.86%] [G loss: 4.394051]\n",
            "1940 [D loss: 0.348949, acc.: 86.72%] [G loss: 4.684855]\n",
            "1960 [D loss: 0.408454, acc.: 83.59%] [G loss: 4.370540]\n",
            "1980 [D loss: 0.445645, acc.: 78.12%] [G loss: 4.173965]\n",
            "2000 [D loss: 0.382557, acc.: 83.59%] [G loss: 4.587084]\n",
            "2020 [D loss: 0.528059, acc.: 77.73%] [G loss: 4.250171]\n",
            "2040 [D loss: 0.434418, acc.: 81.64%] [G loss: 3.984667]\n",
            "2060 [D loss: 0.396934, acc.: 83.59%] [G loss: 4.381786]\n",
            "2080 [D loss: 0.406715, acc.: 81.64%] [G loss: 4.359241]\n",
            "2100 [D loss: 0.498469, acc.: 76.17%] [G loss: 4.309250]\n",
            "2120 [D loss: 0.392552, acc.: 81.64%] [G loss: 4.542624]\n",
            "2140 [D loss: 0.497858, acc.: 75.39%] [G loss: 4.256995]\n",
            "2160 [D loss: 0.431669, acc.: 78.52%] [G loss: 4.374342]\n",
            "2180 [D loss: 0.386353, acc.: 82.03%] [G loss: 4.030497]\n",
            "2200 [D loss: 0.393525, acc.: 84.38%] [G loss: 4.327544]\n",
            "2220 [D loss: 0.471047, acc.: 76.17%] [G loss: 4.148203]\n",
            "2240 [D loss: 0.431076, acc.: 80.86%] [G loss: 4.362601]\n",
            "2260 [D loss: 0.376126, acc.: 82.81%] [G loss: 4.311936]\n",
            "2280 [D loss: 0.435135, acc.: 82.03%] [G loss: 4.216527]\n",
            "2300 [D loss: 0.412959, acc.: 80.47%] [G loss: 4.111159]\n",
            "2320 [D loss: 0.550280, acc.: 71.88%] [G loss: 3.931458]\n",
            "2340 [D loss: 0.335698, acc.: 87.50%] [G loss: 4.733627]\n",
            "2360 [D loss: 0.425331, acc.: 80.86%] [G loss: 4.688098]\n",
            "2380 [D loss: 0.424558, acc.: 82.81%] [G loss: 4.332030]\n",
            "2400 [D loss: 0.427221, acc.: 78.91%] [G loss: 4.370893]\n",
            "2420 [D loss: 0.318654, acc.: 88.28%] [G loss: 4.803578]\n",
            "2440 [D loss: 0.413908, acc.: 83.59%] [G loss: 4.504460]\n",
            "2460 [D loss: 0.421730, acc.: 79.69%] [G loss: 4.413567]\n",
            "2480 [D loss: 0.419884, acc.: 81.64%] [G loss: 4.405695]\n",
            "2500 [D loss: 0.424117, acc.: 82.42%] [G loss: 4.691920]\n",
            "2520 [D loss: 0.453394, acc.: 80.47%] [G loss: 4.406740]\n",
            "2540 [D loss: 0.374935, acc.: 84.77%] [G loss: 4.736284]\n",
            "2560 [D loss: 0.321794, acc.: 87.50%] [G loss: 5.267558]\n",
            "2580 [D loss: 0.518656, acc.: 72.27%] [G loss: 4.482098]\n",
            "2600 [D loss: 0.388319, acc.: 83.98%] [G loss: 4.813517]\n",
            "2620 [D loss: 0.328663, acc.: 86.33%] [G loss: 4.836767]\n",
            "2640 [D loss: 0.318099, acc.: 85.55%] [G loss: 5.641196]\n",
            "2660 [D loss: 0.326650, acc.: 87.50%] [G loss: 5.069518]\n",
            "2680 [D loss: 0.447819, acc.: 77.34%] [G loss: 5.226392]\n",
            "2700 [D loss: 0.347934, acc.: 87.50%] [G loss: 4.671406]\n",
            "2720 [D loss: 0.453368, acc.: 76.56%] [G loss: 4.273150]\n",
            "2740 [D loss: 0.432023, acc.: 78.12%] [G loss: 4.159589]\n",
            "2760 [D loss: 0.420880, acc.: 80.47%] [G loss: 4.599861]\n",
            "2780 [D loss: 0.566641, acc.: 69.92%] [G loss: 4.225712]\n",
            "2800 [D loss: 0.463328, acc.: 76.95%] [G loss: 4.600976]\n",
            "2820 [D loss: 0.350038, acc.: 83.98%] [G loss: 4.546372]\n",
            "2840 [D loss: 0.403359, acc.: 81.25%] [G loss: 5.237429]\n",
            "2860 [D loss: 0.345739, acc.: 85.94%] [G loss: 4.741522]\n",
            "2880 [D loss: 0.354460, acc.: 85.16%] [G loss: 4.596492]\n",
            "2900 [D loss: 0.375729, acc.: 80.86%] [G loss: 4.914245]\n",
            "2920 [D loss: 0.322705, acc.: 86.33%] [G loss: 5.045888]\n",
            "2940 [D loss: 0.247954, acc.: 91.41%] [G loss: 5.091659]\n",
            "2960 [D loss: 0.557924, acc.: 72.27%] [G loss: 4.150468]\n",
            "2980 [D loss: 0.327608, acc.: 89.06%] [G loss: 4.811913]\n",
            "3000 [D loss: 0.412486, acc.: 79.30%] [G loss: 4.731018]\n",
            "3020 [D loss: 0.344638, acc.: 87.11%] [G loss: 5.435119]\n",
            "3040 [D loss: 0.331938, acc.: 83.59%] [G loss: 4.884459]\n",
            "3060 [D loss: 0.328882, acc.: 83.98%] [G loss: 5.597978]\n",
            "3080 [D loss: 0.380475, acc.: 81.25%] [G loss: 4.764737]\n",
            "3100 [D loss: 0.393132, acc.: 81.64%] [G loss: 4.988447]\n",
            "3120 [D loss: 0.417215, acc.: 81.64%] [G loss: 4.992435]\n",
            "3140 [D loss: 0.358083, acc.: 83.59%] [G loss: 5.373530]\n",
            "3160 [D loss: 0.343837, acc.: 85.16%] [G loss: 5.502664]\n",
            "3180 [D loss: 0.242313, acc.: 91.41%] [G loss: 5.834956]\n",
            "3200 [D loss: 0.299378, acc.: 90.23%] [G loss: 5.601436]\n",
            "3220 [D loss: 0.500149, acc.: 76.17%] [G loss: 5.214738]\n",
            "3240 [D loss: 0.355832, acc.: 85.16%] [G loss: 5.202427]\n",
            "3260 [D loss: 0.290968, acc.: 88.28%] [G loss: 6.063551]\n",
            "3280 [D loss: 0.327741, acc.: 87.89%] [G loss: 5.059925]\n",
            "3300 [D loss: 0.451675, acc.: 79.30%] [G loss: 3.925858]\n",
            "3320 [D loss: 0.313318, acc.: 86.33%] [G loss: 5.141754]\n",
            "3340 [D loss: 0.371400, acc.: 84.77%] [G loss: 5.368063]\n",
            "3360 [D loss: 0.335157, acc.: 87.89%] [G loss: 5.477335]\n",
            "3380 [D loss: 0.408369, acc.: 82.81%] [G loss: 4.978508]\n",
            "3400 [D loss: 0.377747, acc.: 82.03%] [G loss: 5.000229]\n",
            "3420 [D loss: 0.411301, acc.: 80.08%] [G loss: 4.702883]\n",
            "3440 [D loss: 0.389751, acc.: 81.25%] [G loss: 5.163847]\n",
            "3460 [D loss: 0.364691, acc.: 84.38%] [G loss: 4.934480]\n",
            "3480 [D loss: 0.313698, acc.: 87.89%] [G loss: 5.449514]\n",
            "3500 [D loss: 0.315051, acc.: 88.67%] [G loss: 5.151597]\n",
            "3520 [D loss: 0.294548, acc.: 88.28%] [G loss: 5.393147]\n",
            "3540 [D loss: 0.360445, acc.: 83.20%] [G loss: 5.641194]\n",
            "3560 [D loss: 0.415940, acc.: 80.47%] [G loss: 5.054404]\n",
            "3580 [D loss: 0.353312, acc.: 84.38%] [G loss: 5.222958]\n",
            "3600 [D loss: 0.288212, acc.: 88.28%] [G loss: 5.577487]\n",
            "3620 [D loss: 0.343285, acc.: 85.55%] [G loss: 5.528058]\n",
            "3640 [D loss: 0.379959, acc.: 82.42%] [G loss: 4.946277]\n",
            "3660 [D loss: 0.299427, acc.: 85.94%] [G loss: 5.427139]\n",
            "3680 [D loss: 0.308425, acc.: 86.72%] [G loss: 5.204938]\n",
            "3700 [D loss: 0.379670, acc.: 83.98%] [G loss: 4.750484]\n",
            "3720 [D loss: 0.323518, acc.: 89.06%] [G loss: 6.012178]\n",
            "3740 [D loss: 0.311610, acc.: 88.67%] [G loss: 5.523414]\n",
            "3760 [D loss: 0.424161, acc.: 83.98%] [G loss: 5.111194]\n",
            "3780 [D loss: 0.404188, acc.: 80.47%] [G loss: 4.824166]\n",
            "3800 [D loss: 0.348751, acc.: 83.98%] [G loss: 5.565083]\n",
            "3820 [D loss: 0.362564, acc.: 82.03%] [G loss: 5.672406]\n",
            "3840 [D loss: 0.274154, acc.: 90.23%] [G loss: 5.396473]\n",
            "3860 [D loss: 0.397751, acc.: 82.42%] [G loss: 5.311002]\n",
            "3880 [D loss: 0.281521, acc.: 89.84%] [G loss: 5.293602]\n",
            "3900 [D loss: 0.239948, acc.: 93.36%] [G loss: 4.203655]\n",
            "3920 [D loss: 0.388629, acc.: 83.20%] [G loss: 5.298659]\n",
            "3940 [D loss: 0.416746, acc.: 82.42%] [G loss: 5.064167]\n",
            "3960 [D loss: 0.326796, acc.: 89.06%] [G loss: 5.618279]\n",
            "3980 [D loss: 0.406631, acc.: 81.64%] [G loss: 5.007459]\n",
            "4000 [D loss: 0.359046, acc.: 83.98%] [G loss: 5.233293]\n",
            "4020 [D loss: 0.309346, acc.: 87.11%] [G loss: 5.397418]\n",
            "4040 [D loss: 0.397237, acc.: 83.59%] [G loss: 5.136081]\n",
            "4060 [D loss: 0.263437, acc.: 89.45%] [G loss: 5.792728]\n",
            "4080 [D loss: 0.375356, acc.: 83.59%] [G loss: 5.597792]\n",
            "4100 [D loss: 0.425198, acc.: 77.73%] [G loss: 5.603860]\n",
            "4120 [D loss: 0.355144, acc.: 85.16%] [G loss: 4.942945]\n",
            "4140 [D loss: 0.310467, acc.: 87.50%] [G loss: 5.567299]\n",
            "4160 [D loss: 0.415152, acc.: 81.64%] [G loss: 5.416082]\n",
            "4180 [D loss: 0.411281, acc.: 82.42%] [G loss: 4.888762]\n",
            "4200 [D loss: 0.355689, acc.: 84.77%] [G loss: 5.024950]\n",
            "4220 [D loss: 0.364526, acc.: 83.59%] [G loss: 5.475099]\n",
            "4240 [D loss: 0.337517, acc.: 87.89%] [G loss: 5.470329]\n",
            "4260 [D loss: 0.354351, acc.: 84.77%] [G loss: 5.294339]\n",
            "4280 [D loss: 0.390846, acc.: 82.42%] [G loss: 5.764386]\n",
            "4300 [D loss: 0.399027, acc.: 82.81%] [G loss: 4.830018]\n",
            "4320 [D loss: 0.411984, acc.: 79.69%] [G loss: 5.266260]\n",
            "4340 [D loss: 0.371327, acc.: 83.98%] [G loss: 4.919018]\n",
            "4360 [D loss: 0.377299, acc.: 86.72%] [G loss: 5.610697]\n",
            "4380 [D loss: 0.404603, acc.: 83.20%] [G loss: 4.994227]\n",
            "4400 [D loss: 0.330091, acc.: 87.89%] [G loss: 5.244478]\n",
            "4420 [D loss: 0.374008, acc.: 82.81%] [G loss: 5.226240]\n",
            "4440 [D loss: 0.343358, acc.: 86.33%] [G loss: 5.445497]\n",
            "4460 [D loss: 0.418696, acc.: 81.64%] [G loss: 4.754268]\n",
            "4480 [D loss: 0.319797, acc.: 86.72%] [G loss: 5.319033]\n",
            "4500 [D loss: 0.287284, acc.: 88.67%] [G loss: 5.353791]\n",
            "4520 [D loss: 0.341780, acc.: 87.11%] [G loss: 4.987555]\n",
            "4540 [D loss: 0.390417, acc.: 85.16%] [G loss: 5.194599]\n",
            "4560 [D loss: 0.325254, acc.: 89.06%] [G loss: 5.506325]\n",
            "4580 [D loss: 0.397132, acc.: 81.64%] [G loss: 5.209028]\n",
            "4600 [D loss: 0.335165, acc.: 86.72%] [G loss: 5.436874]\n",
            "4620 [D loss: 0.194968, acc.: 94.53%] [G loss: 3.942992]\n",
            "4640 [D loss: 0.283390, acc.: 89.06%] [G loss: 5.332844]\n",
            "4660 [D loss: 0.310230, acc.: 87.89%] [G loss: 6.026187]\n",
            "4680 [D loss: 0.429914, acc.: 79.30%] [G loss: 5.205091]\n",
            "4700 [D loss: 0.346587, acc.: 86.33%] [G loss: 5.363153]\n",
            "4720 [D loss: 0.315344, acc.: 87.11%] [G loss: 5.414298]\n",
            "4740 [D loss: 0.379832, acc.: 81.64%] [G loss: 4.749491]\n",
            "4760 [D loss: 0.429490, acc.: 81.25%] [G loss: 5.200512]\n",
            "4780 [D loss: 0.391730, acc.: 85.16%] [G loss: 5.007379]\n",
            "4800 [D loss: 0.334838, acc.: 85.16%] [G loss: 5.388161]\n",
            "4820 [D loss: 0.325584, acc.: 88.67%] [G loss: 5.573364]\n",
            "4840 [D loss: 0.339214, acc.: 83.20%] [G loss: 5.809021]\n",
            "4860 [D loss: 0.338332, acc.: 84.77%] [G loss: 5.561607]\n",
            "4880 [D loss: 0.374262, acc.: 85.16%] [G loss: 5.383989]\n",
            "4900 [D loss: 0.379931, acc.: 86.33%] [G loss: 5.251416]\n",
            "4920 [D loss: 0.331883, acc.: 87.11%] [G loss: 5.405425]\n",
            "4940 [D loss: 0.334563, acc.: 84.38%] [G loss: 5.640156]\n",
            "4960 [D loss: 0.439441, acc.: 81.25%] [G loss: 5.522693]\n",
            "4980 [D loss: 0.289872, acc.: 89.84%] [G loss: 5.592683]\n",
            "5000 [D loss: 0.394560, acc.: 84.38%] [G loss: 5.213978]\n",
            "5020 [D loss: 0.306190, acc.: 87.11%] [G loss: 5.533834]\n",
            "5040 [D loss: 0.349813, acc.: 86.72%] [G loss: 5.110530]\n",
            "5060 [D loss: 0.300289, acc.: 91.02%] [G loss: 5.372245]\n",
            "5080 [D loss: 0.360505, acc.: 83.20%] [G loss: 5.911191]\n",
            "5100 [D loss: 0.319047, acc.: 86.72%] [G loss: 5.747505]\n",
            "5120 [D loss: 0.363069, acc.: 84.77%] [G loss: 5.584892]\n",
            "5140 [D loss: 0.385717, acc.: 83.59%] [G loss: 5.609719]\n",
            "5160 [D loss: 0.362564, acc.: 83.98%] [G loss: 5.778864]\n",
            "5180 [D loss: 0.317650, acc.: 85.55%] [G loss: 6.104589]\n",
            "5200 [D loss: 0.251684, acc.: 91.02%] [G loss: 5.762933]\n",
            "5220 [D loss: 0.301229, acc.: 87.89%] [G loss: 5.769100]\n",
            "5240 [D loss: 0.306466, acc.: 88.28%] [G loss: 5.430788]\n",
            "5260 [D loss: 0.298505, acc.: 87.50%] [G loss: 6.293819]\n",
            "5280 [D loss: 0.282071, acc.: 86.72%] [G loss: 5.871499]\n",
            "5300 [D loss: 0.314756, acc.: 87.11%] [G loss: 6.444508]\n",
            "5320 [D loss: 0.384698, acc.: 82.42%] [G loss: 5.484086]\n",
            "5340 [D loss: 0.300356, acc.: 88.28%] [G loss: 5.790524]\n",
            "5360 [D loss: 0.294371, acc.: 85.94%] [G loss: 5.791577]\n",
            "5380 [D loss: 0.342749, acc.: 85.16%] [G loss: 4.439356]\n",
            "5400 [D loss: 0.235910, acc.: 91.80%] [G loss: 5.700068]\n",
            "5420 [D loss: 0.365386, acc.: 83.98%] [G loss: 5.132436]\n",
            "5440 [D loss: 0.331750, acc.: 87.11%] [G loss: 5.891356]\n",
            "5460 [D loss: 0.262088, acc.: 89.06%] [G loss: 5.571550]\n",
            "5480 [D loss: 0.314808, acc.: 87.11%] [G loss: 5.646025]\n",
            "5500 [D loss: 0.233844, acc.: 92.19%] [G loss: 6.248126]\n",
            "5520 [D loss: 0.347749, acc.: 85.55%] [G loss: 5.817762]\n",
            "5540 [D loss: 0.362213, acc.: 84.77%] [G loss: 5.883071]\n",
            "5560 [D loss: 0.292542, acc.: 88.67%] [G loss: 6.394390]\n",
            "5580 [D loss: 0.268725, acc.: 89.45%] [G loss: 6.440895]\n",
            "5600 [D loss: 0.316412, acc.: 85.55%] [G loss: 6.040981]\n",
            "5620 [D loss: 0.222469, acc.: 92.19%] [G loss: 6.744613]\n",
            "5640 [D loss: 0.263532, acc.: 87.89%] [G loss: 6.116303]\n",
            "5660 [D loss: 0.286192, acc.: 89.45%] [G loss: 6.010793]\n",
            "5680 [D loss: 0.296819, acc.: 88.28%] [G loss: 5.903666]\n",
            "5700 [D loss: 0.282366, acc.: 87.89%] [G loss: 5.365356]\n",
            "5720 [D loss: 0.395449, acc.: 80.08%] [G loss: 5.242566]\n",
            "5740 [D loss: 0.326615, acc.: 85.55%] [G loss: 5.571678]\n",
            "5760 [D loss: 0.332601, acc.: 84.38%] [G loss: 5.675895]\n",
            "5780 [D loss: 0.385176, acc.: 81.25%] [G loss: 5.950945]\n",
            "5800 [D loss: 0.382811, acc.: 83.20%] [G loss: 5.254795]\n",
            "5820 [D loss: 0.333592, acc.: 87.50%] [G loss: 5.504701]\n",
            "5840 [D loss: 0.419590, acc.: 82.42%] [G loss: 5.453451]\n",
            "5860 [D loss: 0.273522, acc.: 91.80%] [G loss: 5.945286]\n",
            "5880 [D loss: 0.345261, acc.: 86.33%] [G loss: 5.456802]\n",
            "5900 [D loss: 0.327052, acc.: 83.59%] [G loss: 5.362304]\n",
            "5920 [D loss: 0.342705, acc.: 83.59%] [G loss: 5.935745]\n",
            "5940 [D loss: 0.264386, acc.: 89.06%] [G loss: 3.793453]\n",
            "5960 [D loss: 0.301584, acc.: 87.89%] [G loss: 5.095552]\n",
            "5980 [D loss: 0.363940, acc.: 84.77%] [G loss: 5.450327]\n",
            "6000 [D loss: 0.402290, acc.: 80.47%] [G loss: 4.837795]\n",
            "6020 [D loss: 0.348144, acc.: 83.98%] [G loss: 6.261810]\n",
            "6040 [D loss: 0.287552, acc.: 88.67%] [G loss: 5.299012]\n",
            "6060 [D loss: 0.445677, acc.: 77.34%] [G loss: 5.946408]\n",
            "6080 [D loss: 0.227243, acc.: 90.62%] [G loss: 6.162244]\n",
            "6100 [D loss: 0.353216, acc.: 83.20%] [G loss: 6.010668]\n",
            "6120 [D loss: 0.375577, acc.: 82.81%] [G loss: 5.656889]\n",
            "6140 [D loss: 0.345884, acc.: 86.33%] [G loss: 5.737312]\n",
            "6160 [D loss: 0.334394, acc.: 86.72%] [G loss: 5.291653]\n",
            "6180 [D loss: 0.300782, acc.: 85.94%] [G loss: 5.146754]\n",
            "6200 [D loss: 0.281913, acc.: 90.62%] [G loss: 5.748967]\n",
            "6220 [D loss: 0.334877, acc.: 85.94%] [G loss: 5.493914]\n",
            "6240 [D loss: 0.337611, acc.: 85.55%] [G loss: 5.100847]\n",
            "6260 [D loss: 0.324709, acc.: 84.77%] [G loss: 5.309240]\n",
            "6280 [D loss: 0.383528, acc.: 83.20%] [G loss: 5.204239]\n",
            "6300 [D loss: 0.456260, acc.: 78.12%] [G loss: 4.921412]\n",
            "6320 [D loss: 0.319257, acc.: 88.28%] [G loss: 4.922379]\n",
            "6340 [D loss: 0.347562, acc.: 85.16%] [G loss: 5.244738]\n",
            "6360 [D loss: 0.268125, acc.: 89.84%] [G loss: 5.325434]\n",
            "6380 [D loss: 0.284629, acc.: 87.50%] [G loss: 5.339823]\n",
            "6400 [D loss: 0.453858, acc.: 76.56%] [G loss: 5.390445]\n",
            "6420 [D loss: 0.307804, acc.: 86.72%] [G loss: 5.089771]\n",
            "6440 [D loss: 0.370477, acc.: 85.55%] [G loss: 5.368149]\n",
            "6460 [D loss: 0.339559, acc.: 85.16%] [G loss: 5.092026]\n",
            "6480 [D loss: 0.411319, acc.: 81.25%] [G loss: 4.833236]\n",
            "6500 [D loss: 0.332855, acc.: 87.50%] [G loss: 5.338369]\n",
            "6520 [D loss: 0.360680, acc.: 84.77%] [G loss: 5.107018]\n",
            "6540 [D loss: 0.360588, acc.: 82.81%] [G loss: 5.363896]\n",
            "6560 [D loss: 0.259306, acc.: 88.67%] [G loss: 5.369337]\n",
            "6580 [D loss: 0.339890, acc.: 83.20%] [G loss: 4.808709]\n",
            "6600 [D loss: 0.140767, acc.: 94.53%] [G loss: 5.386733]\n",
            "6620 [D loss: 0.347659, acc.: 87.50%] [G loss: 5.324479]\n",
            "6640 [D loss: 0.380079, acc.: 83.98%] [G loss: 4.730723]\n",
            "6660 [D loss: 0.366442, acc.: 83.59%] [G loss: 6.020202]\n",
            "6680 [D loss: 0.331211, acc.: 86.72%] [G loss: 5.635776]\n",
            "6700 [D loss: 0.316687, acc.: 87.11%] [G loss: 5.261915]\n",
            "6720 [D loss: 0.263666, acc.: 87.50%] [G loss: 5.789947]\n",
            "6740 [D loss: 0.450168, acc.: 77.34%] [G loss: 5.162899]\n",
            "6760 [D loss: 0.289071, acc.: 90.62%] [G loss: 5.552494]\n",
            "6780 [D loss: 0.316516, acc.: 86.33%] [G loss: 5.380622]\n",
            "6800 [D loss: 0.350055, acc.: 86.72%] [G loss: 5.656824]\n",
            "6820 [D loss: 0.322616, acc.: 87.50%] [G loss: 5.498057]\n",
            "6840 [D loss: 0.322218, acc.: 86.33%] [G loss: 5.597958]\n",
            "6860 [D loss: 0.340773, acc.: 84.77%] [G loss: 5.283210]\n",
            "6880 [D loss: 0.350483, acc.: 87.89%] [G loss: 5.216579]\n",
            "6900 [D loss: 0.244983, acc.: 89.84%] [G loss: 5.237999]\n",
            "6920 [D loss: 0.318166, acc.: 87.11%] [G loss: 5.810130]\n",
            "6940 [D loss: 0.416369, acc.: 79.69%] [G loss: 5.419020]\n",
            "6960 [D loss: 0.433403, acc.: 82.81%] [G loss: 6.122477]\n",
            "6980 [D loss: 0.337892, acc.: 86.33%] [G loss: 5.497224]\n",
            "7000 [D loss: 0.373318, acc.: 83.20%] [G loss: 5.608348]\n",
            "7020 [D loss: 0.269202, acc.: 88.67%] [G loss: 5.577579]\n",
            "7040 [D loss: 0.088703, acc.: 98.44%] [G loss: 3.912928]\n",
            "7060 [D loss: 0.328243, acc.: 85.94%] [G loss: 4.819490]\n",
            "7080 [D loss: 0.420681, acc.: 78.12%] [G loss: 5.446097]\n",
            "7100 [D loss: 0.448072, acc.: 79.69%] [G loss: 4.985517]\n",
            "7120 [D loss: 0.429468, acc.: 79.30%] [G loss: 5.316717]\n",
            "7140 [D loss: 0.312819, acc.: 87.89%] [G loss: 5.414249]\n",
            "7160 [D loss: 0.332111, acc.: 86.72%] [G loss: 5.827503]\n",
            "7180 [D loss: 0.238801, acc.: 89.45%] [G loss: 5.157704]\n",
            "7200 [D loss: 0.307728, acc.: 85.55%] [G loss: 5.356025]\n",
            "7220 [D loss: 0.321008, acc.: 83.98%] [G loss: 5.497461]\n",
            "7240 [D loss: 0.357018, acc.: 84.77%] [G loss: 5.168530]\n",
            "7260 [D loss: 0.404600, acc.: 82.03%] [G loss: 5.734508]\n",
            "7280 [D loss: 0.351773, acc.: 81.64%] [G loss: 5.576539]\n",
            "7300 [D loss: 0.271966, acc.: 88.67%] [G loss: 5.642989]\n",
            "7320 [D loss: 0.271960, acc.: 87.50%] [G loss: 5.433709]\n",
            "7340 [D loss: 0.310674, acc.: 85.55%] [G loss: 5.472804]\n",
            "7360 [D loss: 0.297896, acc.: 87.89%] [G loss: 5.940438]\n",
            "7380 [D loss: 0.315322, acc.: 85.55%] [G loss: 5.497225]\n",
            "7400 [D loss: 0.260705, acc.: 90.62%] [G loss: 5.451283]\n",
            "7420 [D loss: 0.347332, acc.: 83.20%] [G loss: 5.647743]\n",
            "7440 [D loss: 0.437393, acc.: 80.86%] [G loss: 5.248051]\n",
            "7460 [D loss: 0.309130, acc.: 87.89%] [G loss: 5.564451]\n",
            "7480 [D loss: 0.258613, acc.: 90.23%] [G loss: 5.398925]\n",
            "7500 [D loss: 0.373239, acc.: 85.55%] [G loss: 5.228782]\n",
            "7520 [D loss: 0.362169, acc.: 83.59%] [G loss: 5.316082]\n",
            "7540 [D loss: 0.237251, acc.: 89.45%] [G loss: 5.718632]\n",
            "7560 [D loss: 0.389103, acc.: 81.64%] [G loss: 5.932398]\n",
            "7580 [D loss: 0.286184, acc.: 89.06%] [G loss: 5.214523]\n",
            "7600 [D loss: 0.367061, acc.: 85.55%] [G loss: 5.895893]\n",
            "7620 [D loss: 0.338533, acc.: 85.94%] [G loss: 5.206831]\n",
            "7640 [D loss: 0.390912, acc.: 82.42%] [G loss: 5.079202]\n",
            "7660 [D loss: 0.307353, acc.: 89.45%] [G loss: 4.937565]\n",
            "7680 [D loss: 0.323087, acc.: 88.28%] [G loss: 5.009768]\n",
            "7700 [D loss: 0.308532, acc.: 89.06%] [G loss: 5.077505]\n",
            "7720 [D loss: 0.298557, acc.: 89.06%] [G loss: 5.603384]\n",
            "7740 [D loss: 0.280219, acc.: 88.28%] [G loss: 5.810865]\n",
            "7760 [D loss: 0.355162, acc.: 83.98%] [G loss: 5.248317]\n",
            "7780 [D loss: 0.324720, acc.: 87.50%] [G loss: 5.954989]\n",
            "7800 [D loss: 0.315485, acc.: 87.50%] [G loss: 5.386941]\n",
            "7820 [D loss: 0.510927, acc.: 75.00%] [G loss: 5.247611]\n",
            "7840 [D loss: 0.834379, acc.: 64.45%] [G loss: 4.532631]\n",
            "7860 [D loss: 0.543879, acc.: 72.27%] [G loss: 3.511292]\n",
            "7880 [D loss: 0.712902, acc.: 58.20%] [G loss: 2.758089]\n",
            "7900 [D loss: 0.697044, acc.: 58.98%] [G loss: 2.727075]\n",
            "7920 [D loss: 0.569789, acc.: 72.66%] [G loss: 3.217299]\n",
            "7940 [D loss: 0.593531, acc.: 68.36%] [G loss: 3.060927]\n",
            "7960 [D loss: 0.450351, acc.: 81.64%] [G loss: 3.634347]\n",
            "7980 [D loss: 0.427071, acc.: 76.95%] [G loss: 4.088515]\n",
            "8000 [D loss: 0.403474, acc.: 81.64%] [G loss: 4.158008]\n",
            "8020 [D loss: 0.264738, acc.: 89.06%] [G loss: 4.936428]\n",
            "8040 [D loss: 0.573446, acc.: 75.39%] [G loss: 3.190813]\n",
            "8060 [D loss: 0.286453, acc.: 86.33%] [G loss: 5.337687]\n",
            "8080 [D loss: 0.353029, acc.: 86.33%] [G loss: 5.581841]\n",
            "8100 [D loss: 0.340519, acc.: 84.38%] [G loss: 6.170039]\n",
            "8120 [D loss: 0.395155, acc.: 82.03%] [G loss: 5.883152]\n",
            "8140 [D loss: 0.319650, acc.: 83.98%] [G loss: 5.560670]\n",
            "8160 [D loss: 0.368742, acc.: 85.16%] [G loss: 5.223540]\n",
            "8180 [D loss: 0.317306, acc.: 86.72%] [G loss: 6.455637]\n",
            "8200 [D loss: 0.360140, acc.: 87.11%] [G loss: 5.370908]\n",
            "8220 [D loss: 0.297327, acc.: 88.67%] [G loss: 6.195693]\n",
            "8240 [D loss: 0.468766, acc.: 77.73%] [G loss: 4.452073]\n",
            "8260 [D loss: 0.342802, acc.: 83.98%] [G loss: 5.458196]\n",
            "8280 [D loss: 0.330584, acc.: 85.55%] [G loss: 4.843324]\n",
            "8300 [D loss: 0.348265, acc.: 86.33%] [G loss: 4.559250]\n",
            "8320 [D loss: 0.306799, acc.: 86.72%] [G loss: 5.830043]\n",
            "8340 [D loss: 0.432454, acc.: 82.03%] [G loss: 4.482040]\n",
            "8360 [D loss: 0.376301, acc.: 83.98%] [G loss: 5.358859]\n",
            "8380 [D loss: 0.282868, acc.: 88.67%] [G loss: 5.618600]\n",
            "8400 [D loss: 0.336804, acc.: 82.81%] [G loss: 5.274965]\n",
            "8420 [D loss: 0.363905, acc.: 83.20%] [G loss: 5.295593]\n",
            "8440 [D loss: 0.443801, acc.: 77.34%] [G loss: 4.648054]\n",
            "8460 [D loss: 0.347363, acc.: 86.72%] [G loss: 4.994941]\n",
            "8480 [D loss: 0.335500, acc.: 85.94%] [G loss: 4.914373]\n",
            "8500 [D loss: 0.316940, acc.: 87.89%] [G loss: 4.927286]\n",
            "8520 [D loss: 0.316858, acc.: 85.55%] [G loss: 4.926901]\n",
            "8540 [D loss: 0.358397, acc.: 84.38%] [G loss: 4.686515]\n",
            "8560 [D loss: 0.340775, acc.: 86.33%] [G loss: 4.704080]\n",
            "8580 [D loss: 0.328553, acc.: 84.77%] [G loss: 4.910675]\n",
            "8600 [D loss: 0.311557, acc.: 89.84%] [G loss: 4.430941]\n",
            "8620 [D loss: 0.300677, acc.: 88.28%] [G loss: 5.097423]\n",
            "8640 [D loss: 0.379789, acc.: 85.16%] [G loss: 4.683819]\n",
            "8660 [D loss: 0.383746, acc.: 85.55%] [G loss: 4.533529]\n",
            "8680 [D loss: 0.366701, acc.: 83.20%] [G loss: 4.618659]\n",
            "8700 [D loss: 0.438025, acc.: 81.25%] [G loss: 4.483653]\n",
            "8720 [D loss: 0.291162, acc.: 89.84%] [G loss: 4.547482]\n",
            "8740 [D loss: 0.272211, acc.: 89.45%] [G loss: 5.259771]\n",
            "8760 [D loss: 0.300338, acc.: 89.06%] [G loss: 4.994818]\n",
            "8780 [D loss: 0.165917, acc.: 95.70%] [G loss: 4.161804]\n",
            "8800 [D loss: 0.262181, acc.: 89.06%] [G loss: 4.381589]\n",
            "8820 [D loss: 0.366845, acc.: 84.77%] [G loss: 4.678288]\n",
            "8840 [D loss: 0.356838, acc.: 84.38%] [G loss: 4.391065]\n",
            "8860 [D loss: 0.317773, acc.: 86.72%] [G loss: 5.087899]\n",
            "8880 [D loss: 0.301357, acc.: 89.06%] [G loss: 5.207426]\n",
            "8900 [D loss: 0.415138, acc.: 81.25%] [G loss: 4.783024]\n",
            "8920 [D loss: 0.261798, acc.: 89.45%] [G loss: 4.700616]\n",
            "8940 [D loss: 0.290867, acc.: 87.89%] [G loss: 5.362225]\n",
            "8960 [D loss: 0.302861, acc.: 89.45%] [G loss: 5.130871]\n",
            "8980 [D loss: 0.405977, acc.: 82.42%] [G loss: 4.672053]\n",
            "9000 [D loss: 0.301594, acc.: 86.33%] [G loss: 4.924905]\n",
            "9020 [D loss: 0.374966, acc.: 85.16%] [G loss: 4.598363]\n",
            "9040 [D loss: 0.356573, acc.: 83.98%] [G loss: 4.320249]\n",
            "9060 [D loss: 0.291078, acc.: 89.84%] [G loss: 5.025998]\n",
            "9080 [D loss: 0.284955, acc.: 89.06%] [G loss: 4.873401]\n",
            "9100 [D loss: 0.340587, acc.: 84.38%] [G loss: 4.727383]\n",
            "9120 [D loss: 0.409597, acc.: 82.42%] [G loss: 4.274794]\n",
            "9140 [D loss: 0.340841, acc.: 85.94%] [G loss: 4.420088]\n",
            "9160 [D loss: 0.367653, acc.: 87.11%] [G loss: 4.632758]\n",
            "9180 [D loss: 0.343595, acc.: 85.55%] [G loss: 4.362648]\n",
            "9200 [D loss: 0.402040, acc.: 83.98%] [G loss: 4.331769]\n",
            "9220 [D loss: 0.338936, acc.: 85.94%] [G loss: 4.614621]\n",
            "9240 [D loss: 0.314944, acc.: 85.94%] [G loss: 4.928287]\n",
            "9260 [D loss: 0.313025, acc.: 87.11%] [G loss: 4.394022]\n",
            "9280 [D loss: 0.340353, acc.: 87.11%] [G loss: 4.729609]\n",
            "9300 [D loss: 0.388792, acc.: 82.81%] [G loss: 4.618552]\n",
            "9320 [D loss: 0.323104, acc.: 84.38%] [G loss: 4.993255]\n",
            "9340 [D loss: 0.388894, acc.: 85.94%] [G loss: 4.203055]\n",
            "9360 [D loss: 0.432078, acc.: 81.64%] [G loss: 4.226387]\n",
            "9380 [D loss: 0.400364, acc.: 81.25%] [G loss: 4.625824]\n",
            "9400 [D loss: 0.405556, acc.: 83.59%] [G loss: 4.488070]\n",
            "9420 [D loss: 0.323865, acc.: 85.94%] [G loss: 4.650316]\n",
            "9440 [D loss: 0.354810, acc.: 85.55%] [G loss: 4.565337]\n",
            "9460 [D loss: 0.413705, acc.: 81.64%] [G loss: 4.067022]\n",
            "9480 [D loss: 0.342281, acc.: 84.77%] [G loss: 4.250812]\n",
            "9500 [D loss: 0.310325, acc.: 87.50%] [G loss: 4.318416]\n",
            "9520 [D loss: 0.323131, acc.: 88.67%] [G loss: 4.726884]\n",
            "9540 [D loss: 0.360582, acc.: 84.38%] [G loss: 4.409189]\n",
            "9560 [D loss: 0.355458, acc.: 82.81%] [G loss: 4.455582]\n",
            "9580 [D loss: 0.338105, acc.: 85.55%] [G loss: 4.128982]\n",
            "9600 [D loss: 0.347263, acc.: 87.89%] [G loss: 4.534519]\n",
            "9620 [D loss: 0.298550, acc.: 87.89%] [G loss: 4.690908]\n",
            "9640 [D loss: 0.349257, acc.: 84.77%] [G loss: 4.295678]\n",
            "9660 [D loss: 0.279677, acc.: 88.67%] [G loss: 4.817104]\n",
            "9680 [D loss: 0.357021, acc.: 84.77%] [G loss: 4.612180]\n",
            "9700 [D loss: 0.225340, acc.: 91.02%] [G loss: 3.468170]\n",
            "9720 [D loss: 0.408100, acc.: 80.86%] [G loss: 4.110803]\n",
            "9740 [D loss: 0.354475, acc.: 87.89%] [G loss: 4.343664]\n",
            "9760 [D loss: 0.454871, acc.: 81.64%] [G loss: 4.114688]\n",
            "9780 [D loss: 0.265703, acc.: 89.84%] [G loss: 4.728211]\n",
            "9800 [D loss: 0.389367, acc.: 84.38%] [G loss: 4.563790]\n",
            "9820 [D loss: 0.375536, acc.: 83.20%] [G loss: 4.234245]\n",
            "9840 [D loss: 0.290670, acc.: 88.28%] [G loss: 4.791508]\n",
            "9860 [D loss: 0.300152, acc.: 87.89%] [G loss: 4.726147]\n",
            "9880 [D loss: 0.333102, acc.: 87.89%] [G loss: 4.696382]\n",
            "9900 [D loss: 0.356743, acc.: 85.55%] [G loss: 4.687780]\n",
            "9920 [D loss: 0.362884, acc.: 84.38%] [G loss: 4.338201]\n",
            "9940 [D loss: 0.310876, acc.: 86.72%] [G loss: 4.624803]\n",
            "9960 [D loss: 0.325089, acc.: 87.11%] [G loss: 5.075215]\n",
            "9980 [D loss: 0.354865, acc.: 85.55%] [G loss: 4.868052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKyOoPom5Zz",
        "outputId": "c8d3af2f-3729-4904-f9a0-9cb8c6d9a8f8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 48, 48, 128)  3328        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 100)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          51712       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 2304)      16128       input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1152)         0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2304)         0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_15[0][0]                 \n",
            "                                                                 flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            3969        concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.775621, acc.: 20.31%] [G loss: 1.527493]\n",
            "20 [D loss: 1.136945, acc.: 31.64%] [G loss: 1.106470]\n",
            "40 [D loss: 0.436015, acc.: 49.61%] [G loss: 17.784233]\n",
            "60 [D loss: 0.537491, acc.: 73.83%] [G loss: 4.473651]\n",
            "80 [D loss: 0.834621, acc.: 22.66%] [G loss: 1.869293]\n",
            "100 [D loss: 0.741704, acc.: 39.45%] [G loss: 1.638891]\n",
            "120 [D loss: 0.575083, acc.: 82.81%] [G loss: 2.042169]\n",
            "140 [D loss: 0.455398, acc.: 78.12%] [G loss: 3.064530]\n",
            "160 [D loss: 1.021226, acc.: 2.73%] [G loss: 1.166114]\n",
            "180 [D loss: 0.558926, acc.: 76.17%] [G loss: 2.472870]\n",
            "200 [D loss: 0.770760, acc.: 41.80%] [G loss: 1.451331]\n",
            "220 [D loss: 0.876846, acc.: 49.22%] [G loss: 3.595408]\n",
            "240 [D loss: 0.186598, acc.: 100.00%] [G loss: 7.332498]\n",
            "260 [D loss: 0.682056, acc.: 68.75%] [G loss: 1.890666]\n",
            "280 [D loss: 0.209264, acc.: 95.31%] [G loss: 10.532845]\n",
            "300 [D loss: 0.691736, acc.: 60.55%] [G loss: 1.672787]\n",
            "320 [D loss: 0.672510, acc.: 71.48%] [G loss: 1.888943]\n",
            "340 [D loss: 0.385103, acc.: 89.06%] [G loss: 4.332753]\n",
            "360 [D loss: 0.380074, acc.: 84.77%] [G loss: 4.375402]\n",
            "380 [D loss: 0.762896, acc.: 57.81%] [G loss: 1.722982]\n",
            "400 [D loss: 0.730825, acc.: 46.09%] [G loss: 2.234101]\n",
            "420 [D loss: 0.644904, acc.: 57.81%] [G loss: 2.380552]\n",
            "440 [D loss: 0.686306, acc.: 59.77%] [G loss: 2.222342]\n",
            "460 [D loss: 0.746045, acc.: 54.69%] [G loss: 1.957719]\n",
            "480 [D loss: 0.601959, acc.: 67.58%] [G loss: 2.125723]\n",
            "500 [D loss: 0.650282, acc.: 62.11%] [G loss: 2.268615]\n",
            "520 [D loss: 0.609233, acc.: 67.58%] [G loss: 2.602067]\n",
            "540 [D loss: 0.572285, acc.: 69.53%] [G loss: 2.928072]\n",
            "560 [D loss: 0.618016, acc.: 66.41%] [G loss: 2.138952]\n",
            "580 [D loss: 0.671801, acc.: 58.20%] [G loss: 1.966664]\n",
            "600 [D loss: 0.712011, acc.: 48.83%] [G loss: 1.836228]\n",
            "620 [D loss: 0.744234, acc.: 47.27%] [G loss: 1.852621]\n",
            "640 [D loss: 0.625981, acc.: 65.23%] [G loss: 2.130571]\n",
            "660 [D loss: 0.605573, acc.: 68.75%] [G loss: 2.238631]\n",
            "680 [D loss: 0.745590, acc.: 51.17%] [G loss: 1.863794]\n",
            "700 [D loss: 0.541753, acc.: 73.05%] [G loss: 2.539672]\n",
            "720 [D loss: 0.546152, acc.: 75.78%] [G loss: 2.414199]\n",
            "740 [D loss: 0.617598, acc.: 65.62%] [G loss: 2.347521]\n",
            "760 [D loss: 0.445802, acc.: 83.98%] [G loss: 3.243727]\n",
            "780 [D loss: 0.495237, acc.: 78.12%] [G loss: 2.732445]\n",
            "800 [D loss: 0.560501, acc.: 75.39%] [G loss: 2.522155]\n",
            "820 [D loss: 0.555277, acc.: 67.97%] [G loss: 2.861588]\n",
            "840 [D loss: 0.536461, acc.: 76.95%] [G loss: 2.788723]\n",
            "860 [D loss: 0.578135, acc.: 70.70%] [G loss: 3.652322]\n",
            "880 [D loss: 0.623166, acc.: 65.23%] [G loss: 2.995444]\n",
            "900 [D loss: 0.660175, acc.: 62.89%] [G loss: 2.713311]\n",
            "920 [D loss: 0.448838, acc.: 81.25%] [G loss: 3.552639]\n",
            "940 [D loss: 0.507613, acc.: 76.56%] [G loss: 3.297152]\n",
            "960 [D loss: 0.538144, acc.: 73.05%] [G loss: 3.129484]\n",
            "980 [D loss: 0.657594, acc.: 61.72%] [G loss: 3.054131]\n",
            "1000 [D loss: 0.435557, acc.: 84.77%] [G loss: 3.602971]\n",
            "1020 [D loss: 0.389269, acc.: 85.16%] [G loss: 4.215716]\n",
            "1040 [D loss: 0.441645, acc.: 80.47%] [G loss: 3.759854]\n",
            "1060 [D loss: 0.417450, acc.: 81.64%] [G loss: 3.896385]\n",
            "1080 [D loss: 0.648562, acc.: 64.84%] [G loss: 3.626264]\n",
            "1100 [D loss: 0.402189, acc.: 83.98%] [G loss: 3.678422]\n",
            "1120 [D loss: 0.418486, acc.: 82.81%] [G loss: 3.632628]\n",
            "1140 [D loss: 0.467013, acc.: 76.95%] [G loss: 3.824034]\n",
            "1160 [D loss: 0.570863, acc.: 70.31%] [G loss: 3.044311]\n",
            "1180 [D loss: 0.511243, acc.: 76.95%] [G loss: 3.353204]\n",
            "1200 [D loss: 0.478253, acc.: 78.12%] [G loss: 3.580424]\n",
            "1220 [D loss: 0.470860, acc.: 77.73%] [G loss: 3.401241]\n",
            "1240 [D loss: 0.509738, acc.: 75.00%] [G loss: 3.294276]\n",
            "1260 [D loss: 0.510494, acc.: 78.91%] [G loss: 3.284930]\n",
            "1280 [D loss: 0.589470, acc.: 67.19%] [G loss: 3.138620]\n",
            "1300 [D loss: 0.555125, acc.: 73.05%] [G loss: 2.883266]\n",
            "1320 [D loss: 0.479775, acc.: 79.30%] [G loss: 3.127534]\n",
            "1340 [D loss: 0.498140, acc.: 75.78%] [G loss: 3.167541]\n",
            "1360 [D loss: 0.530504, acc.: 76.56%] [G loss: 2.911742]\n",
            "1380 [D loss: 0.482003, acc.: 77.34%] [G loss: 3.266805]\n",
            "1400 [D loss: 0.470014, acc.: 80.47%] [G loss: 3.325715]\n",
            "1420 [D loss: 0.546466, acc.: 73.44%] [G loss: 3.093244]\n",
            "1440 [D loss: 0.548517, acc.: 73.83%] [G loss: 3.152225]\n",
            "1460 [D loss: 0.533497, acc.: 71.88%] [G loss: 3.297659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1-QlEvelsO",
        "outputId": "d5e6db9b-1b83-4118-fa9f-1173864d140b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      16128       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_73 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_33 (Flatten)            (None, 2304)         0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_32 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4708)         0           input_73[0][0]                   \n",
            "                                                                 flatten_33[0][0]                 \n",
            "                                                                 flatten_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 1024)         4822016     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, 1024)         0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 1024)         0           leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1024)         1049600     dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1024)         0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 1024)         0           leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 1024)         1049600     dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1024)         0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 1024)         0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 1)            1025        dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.849750, acc.: 30.47%] [G loss: 5.367685]\n",
            "20 [D loss: 0.341891, acc.: 91.41%] [G loss: 8.554846]\n",
            "40 [D loss: 0.710111, acc.: 44.14%] [G loss: 2.083147]\n",
            "60 [D loss: 0.715512, acc.: 46.48%] [G loss: 1.738583]\n",
            "80 [D loss: 0.648917, acc.: 56.25%] [G loss: 1.863081]\n",
            "100 [D loss: 0.701115, acc.: 44.92%] [G loss: 1.767049]\n",
            "120 [D loss: 0.732758, acc.: 40.23%] [G loss: 1.580268]\n",
            "140 [D loss: 0.706723, acc.: 48.05%] [G loss: 1.537512]\n",
            "160 [D loss: 0.717456, acc.: 40.23%] [G loss: 1.437199]\n",
            "180 [D loss: 0.710523, acc.: 42.97%] [G loss: 1.439812]\n",
            "200 [D loss: 0.711228, acc.: 45.70%] [G loss: 1.451416]\n",
            "220 [D loss: 0.721137, acc.: 35.55%] [G loss: 1.414149]\n",
            "240 [D loss: 0.714687, acc.: 42.58%] [G loss: 1.432792]\n",
            "260 [D loss: 0.703062, acc.: 46.09%] [G loss: 1.423483]\n",
            "280 [D loss: 0.724828, acc.: 36.72%] [G loss: 1.404998]\n",
            "300 [D loss: 0.716795, acc.: 42.58%] [G loss: 1.396966]\n",
            "320 [D loss: 0.720376, acc.: 42.97%] [G loss: 1.444355]\n",
            "340 [D loss: 0.706069, acc.: 46.88%] [G loss: 1.423882]\n",
            "360 [D loss: 0.709865, acc.: 39.06%] [G loss: 1.404603]\n",
            "380 [D loss: 0.714929, acc.: 43.75%] [G loss: 1.404289]\n",
            "400 [D loss: 0.711042, acc.: 42.97%] [G loss: 1.414505]\n",
            "420 [D loss: 0.717568, acc.: 40.23%] [G loss: 1.403865]\n",
            "440 [D loss: 0.711510, acc.: 40.23%] [G loss: 1.410104]\n",
            "460 [D loss: 0.714272, acc.: 41.41%] [G loss: 1.419285]\n",
            "480 [D loss: 0.708251, acc.: 41.02%] [G loss: 1.413074]\n",
            "500 [D loss: 0.706564, acc.: 40.62%] [G loss: 1.414759]\n",
            "520 [D loss: 0.711570, acc.: 39.84%] [G loss: 1.416901]\n",
            "540 [D loss: 0.707966, acc.: 44.14%] [G loss: 1.396562]\n",
            "560 [D loss: 0.714514, acc.: 40.62%] [G loss: 1.400588]\n",
            "580 [D loss: 0.708786, acc.: 39.06%] [G loss: 1.401240]\n",
            "600 [D loss: 0.715925, acc.: 37.50%] [G loss: 1.401176]\n",
            "620 [D loss: 0.700743, acc.: 42.19%] [G loss: 1.417309]\n",
            "640 [D loss: 0.706829, acc.: 40.23%] [G loss: 1.403668]\n",
            "660 [D loss: 0.707380, acc.: 39.84%] [G loss: 1.389831]\n",
            "680 [D loss: 0.707225, acc.: 39.84%] [G loss: 1.398783]\n",
            "700 [D loss: 0.703080, acc.: 40.62%] [G loss: 1.396986]\n",
            "720 [D loss: 0.718166, acc.: 34.38%] [G loss: 1.389264]\n",
            "740 [D loss: 0.713579, acc.: 40.23%] [G loss: 1.403226]\n",
            "760 [D loss: 0.705897, acc.: 45.31%] [G loss: 1.437489]\n",
            "780 [D loss: 0.707634, acc.: 40.62%] [G loss: 1.401700]\n",
            "800 [D loss: 0.701959, acc.: 39.06%] [G loss: 1.398977]\n",
            "820 [D loss: 0.706529, acc.: 42.19%] [G loss: 1.400484]\n",
            "840 [D loss: 0.700337, acc.: 41.02%] [G loss: 1.400969]\n",
            "860 [D loss: 0.706927, acc.: 38.67%] [G loss: 1.405506]\n",
            "880 [D loss: 0.708806, acc.: 38.28%] [G loss: 1.387069]\n",
            "900 [D loss: 0.706434, acc.: 41.41%] [G loss: 1.398114]\n",
            "920 [D loss: 0.704907, acc.: 35.94%] [G loss: 1.406425]\n",
            "940 [D loss: 0.696471, acc.: 47.66%] [G loss: 1.392628]\n",
            "960 [D loss: 0.705585, acc.: 41.02%] [G loss: 1.396953]\n",
            "980 [D loss: 0.705313, acc.: 39.84%] [G loss: 1.393413]\n",
            "1000 [D loss: 0.693701, acc.: 48.44%] [G loss: 1.410988]\n",
            "1020 [D loss: 0.707825, acc.: 38.28%] [G loss: 1.393575]\n",
            "1040 [D loss: 0.700687, acc.: 44.53%] [G loss: 1.400779]\n",
            "1060 [D loss: 0.703935, acc.: 39.84%] [G loss: 1.387593]\n",
            "1080 [D loss: 0.706629, acc.: 39.84%] [G loss: 1.389481]\n",
            "1100 [D loss: 0.707315, acc.: 37.89%] [G loss: 1.378371]\n",
            "1120 [D loss: 0.707380, acc.: 42.97%] [G loss: 1.383276]\n",
            "1140 [D loss: 0.701792, acc.: 45.31%] [G loss: 1.392983]\n",
            "1160 [D loss: 0.698018, acc.: 41.41%] [G loss: 1.396971]\n",
            "1180 [D loss: 0.710089, acc.: 35.94%] [G loss: 1.389826]\n",
            "1200 [D loss: 0.704007, acc.: 40.23%] [G loss: 1.392146]\n",
            "1220 [D loss: 0.704774, acc.: 42.97%] [G loss: 1.406959]\n",
            "1240 [D loss: 0.699959, acc.: 41.41%] [G loss: 1.412235]\n",
            "1260 [D loss: 0.702670, acc.: 44.53%] [G loss: 1.408520]\n",
            "1280 [D loss: 0.704078, acc.: 41.41%] [G loss: 1.384988]\n",
            "1300 [D loss: 0.707281, acc.: 40.62%] [G loss: 1.399330]\n",
            "1320 [D loss: 0.709640, acc.: 39.06%] [G loss: 1.400736]\n",
            "1340 [D loss: 0.697176, acc.: 46.88%] [G loss: 1.408559]\n",
            "1360 [D loss: 0.690890, acc.: 46.09%] [G loss: 1.407980]\n",
            "1380 [D loss: 0.695711, acc.: 48.44%] [G loss: 1.384939]\n",
            "1400 [D loss: 0.696284, acc.: 41.41%] [G loss: 1.391898]\n",
            "1420 [D loss: 0.702413, acc.: 41.02%] [G loss: 1.391516]\n",
            "1440 [D loss: 0.702305, acc.: 43.75%] [G loss: 1.404577]\n",
            "1460 [D loss: 0.702467, acc.: 40.62%] [G loss: 1.401769]\n",
            "1480 [D loss: 0.700135, acc.: 39.84%] [G loss: 1.391395]\n",
            "1500 [D loss: 0.701792, acc.: 41.80%] [G loss: 1.390833]\n",
            "1520 [D loss: 0.700585, acc.: 42.19%] [G loss: 1.394488]\n",
            "1540 [D loss: 0.704487, acc.: 41.41%] [G loss: 1.397046]\n",
            "1560 [D loss: 0.695279, acc.: 46.48%] [G loss: 1.408600]\n",
            "1580 [D loss: 0.695438, acc.: 48.83%] [G loss: 1.392757]\n",
            "1600 [D loss: 0.704662, acc.: 38.67%] [G loss: 1.390574]\n",
            "1620 [D loss: 0.699533, acc.: 46.09%] [G loss: 1.401004]\n",
            "1640 [D loss: 0.702735, acc.: 43.75%] [G loss: 1.398700]\n",
            "1660 [D loss: 0.697723, acc.: 43.75%] [G loss: 1.401226]\n",
            "1680 [D loss: 0.697773, acc.: 41.80%] [G loss: 1.410201]\n",
            "1700 [D loss: 0.696773, acc.: 43.36%] [G loss: 1.407255]\n",
            "1720 [D loss: 0.701552, acc.: 39.45%] [G loss: 1.395131]\n",
            "1740 [D loss: 0.696195, acc.: 48.44%] [G loss: 1.396674]\n",
            "1760 [D loss: 0.702511, acc.: 43.36%] [G loss: 1.383245]\n",
            "1780 [D loss: 0.704975, acc.: 40.62%] [G loss: 1.399527]\n",
            "1800 [D loss: 0.698895, acc.: 41.41%] [G loss: 1.391627]\n",
            "1820 [D loss: 0.702525, acc.: 37.50%] [G loss: 1.391825]\n",
            "1840 [D loss: 0.698295, acc.: 46.48%] [G loss: 1.401908]\n",
            "1860 [D loss: 0.699764, acc.: 42.97%] [G loss: 1.399934]\n",
            "1880 [D loss: 0.702305, acc.: 41.80%] [G loss: 1.396282]\n",
            "1900 [D loss: 0.705898, acc.: 42.19%] [G loss: 1.384895]\n",
            "1920 [D loss: 0.700531, acc.: 44.92%] [G loss: 1.391573]\n",
            "1940 [D loss: 0.707957, acc.: 41.02%] [G loss: 1.386891]\n",
            "1960 [D loss: 0.700286, acc.: 41.41%] [G loss: 1.396966]\n",
            "1980 [D loss: 0.697184, acc.: 42.19%] [G loss: 1.398432]\n",
            "2000 [D loss: 0.701151, acc.: 46.09%] [G loss: 1.408123]\n",
            "2020 [D loss: 0.707549, acc.: 41.80%] [G loss: 1.400710]\n",
            "2040 [D loss: 0.706370, acc.: 41.80%] [G loss: 1.382454]\n",
            "2060 [D loss: 0.696820, acc.: 44.14%] [G loss: 1.416820]\n",
            "2080 [D loss: 0.704532, acc.: 39.84%] [G loss: 1.388172]\n",
            "2100 [D loss: 0.698797, acc.: 41.80%] [G loss: 1.401481]\n",
            "2120 [D loss: 0.701321, acc.: 42.58%] [G loss: 1.405037]\n",
            "2140 [D loss: 0.700277, acc.: 42.58%] [G loss: 1.388465]\n",
            "2160 [D loss: 0.706183, acc.: 34.77%] [G loss: 1.395349]\n",
            "2180 [D loss: 0.700379, acc.: 44.53%] [G loss: 1.399946]\n",
            "2200 [D loss: 0.706776, acc.: 35.55%] [G loss: 1.397158]\n",
            "2220 [D loss: 0.690010, acc.: 49.61%] [G loss: 1.415059]\n",
            "2240 [D loss: 0.700388, acc.: 45.31%] [G loss: 1.396531]\n",
            "2260 [D loss: 0.698710, acc.: 41.41%] [G loss: 1.387498]\n",
            "2280 [D loss: 0.699097, acc.: 44.53%] [G loss: 1.400677]\n",
            "2300 [D loss: 0.698183, acc.: 41.41%] [G loss: 1.393741]\n",
            "2320 [D loss: 0.700072, acc.: 44.92%] [G loss: 1.387109]\n",
            "2340 [D loss: 0.702045, acc.: 43.75%] [G loss: 1.396456]\n",
            "2360 [D loss: 0.698545, acc.: 42.97%] [G loss: 1.386749]\n",
            "2380 [D loss: 0.701191, acc.: 44.53%] [G loss: 1.390651]\n",
            "2400 [D loss: 0.702897, acc.: 41.02%] [G loss: 1.385156]\n",
            "2420 [D loss: 0.705316, acc.: 41.02%] [G loss: 1.401502]\n",
            "2440 [D loss: 0.706111, acc.: 37.11%] [G loss: 1.388617]\n",
            "2460 [D loss: 0.702543, acc.: 41.02%] [G loss: 1.407373]\n",
            "2480 [D loss: 0.705135, acc.: 44.92%] [G loss: 1.386979]\n",
            "2500 [D loss: 0.699529, acc.: 43.75%] [G loss: 1.433604]\n",
            "2520 [D loss: 0.701719, acc.: 43.36%] [G loss: 1.395393]\n",
            "2540 [D loss: 0.699508, acc.: 42.58%] [G loss: 1.396002]\n",
            "2560 [D loss: 0.703406, acc.: 38.67%] [G loss: 1.390184]\n",
            "2580 [D loss: 0.702974, acc.: 38.67%] [G loss: 1.384311]\n",
            "2600 [D loss: 0.697291, acc.: 44.92%] [G loss: 1.391137]\n",
            "2620 [D loss: 0.698857, acc.: 46.48%] [G loss: 1.397241]\n",
            "2640 [D loss: 0.702192, acc.: 41.02%] [G loss: 1.391943]\n",
            "2660 [D loss: 0.703083, acc.: 35.94%] [G loss: 1.392532]\n",
            "2680 [D loss: 0.704199, acc.: 41.80%] [G loss: 1.383410]\n",
            "2700 [D loss: 0.705399, acc.: 40.62%] [G loss: 1.386579]\n",
            "2720 [D loss: 0.702500, acc.: 42.19%] [G loss: 1.398633]\n",
            "2740 [D loss: 0.701238, acc.: 41.41%] [G loss: 1.387710]\n",
            "2760 [D loss: 0.695434, acc.: 47.66%] [G loss: 1.398785]\n",
            "2780 [D loss: 0.698381, acc.: 46.48%] [G loss: 1.409770]\n",
            "2800 [D loss: 0.703224, acc.: 41.02%] [G loss: 1.398382]\n",
            "2820 [D loss: 0.704637, acc.: 42.97%] [G loss: 1.391324]\n",
            "2840 [D loss: 0.695273, acc.: 46.48%] [G loss: 1.403218]\n",
            "2860 [D loss: 0.703234, acc.: 45.31%] [G loss: 1.395308]\n",
            "2880 [D loss: 0.699889, acc.: 41.80%] [G loss: 1.389093]\n",
            "2900 [D loss: 0.704404, acc.: 42.19%] [G loss: 1.391987]\n",
            "2920 [D loss: 0.700413, acc.: 43.36%] [G loss: 1.394257]\n",
            "2940 [D loss: 0.701555, acc.: 45.31%] [G loss: 1.386916]\n",
            "2960 [D loss: 0.704574, acc.: 41.02%] [G loss: 1.397935]\n",
            "2980 [D loss: 0.701216, acc.: 38.28%] [G loss: 1.397118]\n",
            "3000 [D loss: 0.703859, acc.: 43.75%] [G loss: 1.386564]\n",
            "3020 [D loss: 0.697224, acc.: 47.27%] [G loss: 1.404107]\n",
            "3040 [D loss: 0.700284, acc.: 44.14%] [G loss: 1.408972]\n",
            "3060 [D loss: 0.710935, acc.: 39.45%] [G loss: 1.398036]\n",
            "3080 [D loss: 0.701705, acc.: 43.75%] [G loss: 1.385907]\n",
            "3100 [D loss: 0.703351, acc.: 42.19%] [G loss: 1.400969]\n",
            "3120 [D loss: 0.705543, acc.: 35.55%] [G loss: 1.395355]\n",
            "3140 [D loss: 0.705566, acc.: 41.02%] [G loss: 1.389833]\n",
            "3160 [D loss: 0.694014, acc.: 50.78%] [G loss: 1.399073]\n",
            "3180 [D loss: 0.696958, acc.: 47.27%] [G loss: 1.398360]\n",
            "3200 [D loss: 0.697163, acc.: 45.31%] [G loss: 1.396299]\n",
            "3220 [D loss: 0.704172, acc.: 39.45%] [G loss: 1.401600]\n",
            "3240 [D loss: 0.706146, acc.: 35.55%] [G loss: 1.386641]\n",
            "3260 [D loss: 0.693955, acc.: 46.88%] [G loss: 1.392742]\n",
            "3280 [D loss: 0.694101, acc.: 48.83%] [G loss: 1.400743]\n",
            "3300 [D loss: 0.699886, acc.: 42.58%] [G loss: 1.380824]\n",
            "3320 [D loss: 0.709387, acc.: 47.27%] [G loss: 1.398697]\n",
            "3340 [D loss: 0.699752, acc.: 48.44%] [G loss: 1.394596]\n",
            "3360 [D loss: 0.706852, acc.: 40.62%] [G loss: 1.405188]\n",
            "3380 [D loss: 0.699275, acc.: 44.53%] [G loss: 1.398795]\n",
            "3400 [D loss: 0.696006, acc.: 44.92%] [G loss: 1.421854]\n",
            "3420 [D loss: 0.696535, acc.: 48.05%] [G loss: 1.411503]\n",
            "3440 [D loss: 0.702331, acc.: 43.36%] [G loss: 1.399069]\n",
            "3460 [D loss: 0.705177, acc.: 42.19%] [G loss: 1.388886]\n",
            "3480 [D loss: 0.703387, acc.: 36.33%] [G loss: 1.393602]\n",
            "3500 [D loss: 0.701317, acc.: 38.67%] [G loss: 1.405229]\n",
            "3520 [D loss: 0.700779, acc.: 36.33%] [G loss: 1.394291]\n",
            "3540 [D loss: 0.704368, acc.: 43.75%] [G loss: 1.388223]\n",
            "3560 [D loss: 0.698320, acc.: 44.92%] [G loss: 1.408260]\n",
            "3580 [D loss: 0.699920, acc.: 41.80%] [G loss: 1.396066]\n",
            "3600 [D loss: 0.703215, acc.: 42.97%] [G loss: 1.393963]\n",
            "3620 [D loss: 0.704095, acc.: 42.58%] [G loss: 1.389001]\n",
            "3640 [D loss: 0.696503, acc.: 46.48%] [G loss: 1.391987]\n",
            "3660 [D loss: 0.706459, acc.: 39.45%] [G loss: 1.403823]\n",
            "3680 [D loss: 0.701097, acc.: 45.70%] [G loss: 1.391359]\n",
            "3700 [D loss: 0.700797, acc.: 40.62%] [G loss: 1.401428]\n",
            "3720 [D loss: 0.706842, acc.: 39.45%] [G loss: 1.401338]\n",
            "3740 [D loss: 0.695790, acc.: 48.44%] [G loss: 1.410640]\n",
            "3760 [D loss: 0.700871, acc.: 41.02%] [G loss: 1.389013]\n",
            "3780 [D loss: 0.698214, acc.: 45.31%] [G loss: 1.391579]\n",
            "3800 [D loss: 0.702306, acc.: 41.02%] [G loss: 1.390742]\n",
            "3820 [D loss: 0.699549, acc.: 44.92%] [G loss: 1.388456]\n",
            "3840 [D loss: 0.726881, acc.: 41.02%] [G loss: 1.427503]\n",
            "3860 [D loss: 0.706836, acc.: 40.23%] [G loss: 1.408166]\n",
            "3880 [D loss: 0.708650, acc.: 37.89%] [G loss: 1.388743]\n",
            "3900 [D loss: 0.703004, acc.: 42.19%] [G loss: 1.413317]\n",
            "3920 [D loss: 0.701894, acc.: 43.75%] [G loss: 1.391886]\n",
            "3940 [D loss: 0.695446, acc.: 49.22%] [G loss: 1.405801]\n",
            "3960 [D loss: 0.704857, acc.: 41.02%] [G loss: 1.401155]\n",
            "3980 [D loss: 0.699231, acc.: 42.58%] [G loss: 1.404975]\n",
            "4000 [D loss: 0.699956, acc.: 39.84%] [G loss: 1.397395]\n",
            "4020 [D loss: 0.693868, acc.: 48.83%] [G loss: 1.414370]\n",
            "4040 [D loss: 0.699592, acc.: 46.48%] [G loss: 1.402936]\n",
            "4060 [D loss: 0.697715, acc.: 45.31%] [G loss: 1.424391]\n",
            "4080 [D loss: 0.708574, acc.: 40.23%] [G loss: 1.381411]\n",
            "4100 [D loss: 1.072704, acc.: 84.38%] [G loss: 26.749468]\n",
            "4120 [D loss: 0.717382, acc.: 67.97%] [G loss: 6.265586]\n",
            "4140 [D loss: 1.001233, acc.: 50.00%] [G loss: 3.090046]\n",
            "4160 [D loss: 0.768635, acc.: 51.56%] [G loss: 2.125261]\n",
            "4180 [D loss: 0.740759, acc.: 46.48%] [G loss: 1.783273]\n",
            "4200 [D loss: 0.722959, acc.: 47.66%] [G loss: 1.520224]\n",
            "4220 [D loss: 0.734547, acc.: 45.31%] [G loss: 1.466105]\n",
            "4240 [D loss: 0.688544, acc.: 57.03%] [G loss: 1.615793]\n",
            "4260 [D loss: 0.738717, acc.: 43.75%] [G loss: 1.405132]\n",
            "4280 [D loss: 0.722575, acc.: 42.19%] [G loss: 1.447261]\n",
            "4300 [D loss: 0.690146, acc.: 49.61%] [G loss: 1.476141]\n",
            "4320 [D loss: 0.719582, acc.: 43.75%] [G loss: 1.429009]\n",
            "4340 [D loss: 0.720578, acc.: 45.70%] [G loss: 1.434604]\n",
            "4360 [D loss: 0.706739, acc.: 45.70%] [G loss: 1.414366]\n",
            "4380 [D loss: 0.702524, acc.: 48.44%] [G loss: 1.432420]\n",
            "4400 [D loss: 0.710964, acc.: 48.83%] [G loss: 1.441298]\n",
            "4420 [D loss: 0.711855, acc.: 46.48%] [G loss: 1.427701]\n",
            "4440 [D loss: 0.710840, acc.: 47.66%] [G loss: 1.458087]\n",
            "4460 [D loss: 0.705373, acc.: 44.53%] [G loss: 1.425750]\n",
            "4480 [D loss: 0.704472, acc.: 48.83%] [G loss: 1.433243]\n",
            "4500 [D loss: 0.698071, acc.: 51.17%] [G loss: 1.431295]\n",
            "4520 [D loss: 0.716092, acc.: 41.80%] [G loss: 1.414632]\n",
            "4540 [D loss: 0.709937, acc.: 44.53%] [G loss: 1.402517]\n",
            "4560 [D loss: 0.706339, acc.: 43.75%] [G loss: 1.417696]\n",
            "4580 [D loss: 0.701099, acc.: 49.61%] [G loss: 1.398619]\n",
            "4600 [D loss: 0.705118, acc.: 43.75%] [G loss: 1.420572]\n",
            "4620 [D loss: 0.706447, acc.: 43.75%] [G loss: 1.411010]\n",
            "4640 [D loss: 0.701780, acc.: 48.44%] [G loss: 1.398860]\n",
            "4660 [D loss: 0.705210, acc.: 48.05%] [G loss: 1.383090]\n",
            "4680 [D loss: 0.708879, acc.: 46.09%] [G loss: 1.397547]\n",
            "4700 [D loss: 0.710725, acc.: 42.97%] [G loss: 1.417038]\n",
            "4720 [D loss: 0.704634, acc.: 45.70%] [G loss: 1.393745]\n",
            "4740 [D loss: 0.696728, acc.: 51.56%] [G loss: 1.395010]\n",
            "4760 [D loss: 0.702551, acc.: 48.83%] [G loss: 1.404335]\n",
            "4780 [D loss: 0.699379, acc.: 46.48%] [G loss: 1.415344]\n",
            "4800 [D loss: 0.700585, acc.: 49.22%] [G loss: 1.406814]\n",
            "4820 [D loss: 0.698388, acc.: 46.88%] [G loss: 1.390556]\n",
            "4840 [D loss: 0.694551, acc.: 51.56%] [G loss: 1.385368]\n",
            "4860 [D loss: 0.694786, acc.: 53.12%] [G loss: 1.396678]\n",
            "4880 [D loss: 0.703839, acc.: 44.53%] [G loss: 1.405728]\n",
            "4900 [D loss: 0.705474, acc.: 44.53%] [G loss: 1.391244]\n",
            "4920 [D loss: 0.698867, acc.: 46.09%] [G loss: 1.391430]\n",
            "4940 [D loss: 0.705874, acc.: 39.45%] [G loss: 1.395215]\n",
            "4960 [D loss: 0.697893, acc.: 49.61%] [G loss: 1.396267]\n",
            "4980 [D loss: 0.709537, acc.: 39.84%] [G loss: 1.394031]\n",
            "5000 [D loss: 0.704992, acc.: 42.19%] [G loss: 1.393940]\n",
            "5020 [D loss: 0.702913, acc.: 48.05%] [G loss: 1.390844]\n",
            "5040 [D loss: 0.697565, acc.: 48.83%] [G loss: 1.388696]\n",
            "5060 [D loss: 0.700234, acc.: 42.58%] [G loss: 1.404058]\n",
            "5080 [D loss: 0.703479, acc.: 44.92%] [G loss: 1.406952]\n",
            "5100 [D loss: 0.693664, acc.: 49.22%] [G loss: 1.407932]\n",
            "5120 [D loss: 0.706131, acc.: 42.97%] [G loss: 1.404334]\n",
            "5140 [D loss: 0.700269, acc.: 48.44%] [G loss: 1.391555]\n",
            "5160 [D loss: 0.696768, acc.: 45.31%] [G loss: 1.396595]\n",
            "5180 [D loss: 0.699592, acc.: 46.09%] [G loss: 1.397417]\n",
            "5200 [D loss: 0.695383, acc.: 47.27%] [G loss: 1.387535]\n",
            "5220 [D loss: 0.693621, acc.: 47.27%] [G loss: 1.408859]\n",
            "5240 [D loss: 0.695037, acc.: 47.66%] [G loss: 1.403044]\n",
            "5260 [D loss: 0.700985, acc.: 41.41%] [G loss: 1.395171]\n",
            "5280 [D loss: 0.695701, acc.: 47.66%] [G loss: 1.403005]\n",
            "5300 [D loss: 0.700270, acc.: 42.19%] [G loss: 1.394384]\n",
            "5320 [D loss: 0.698988, acc.: 44.92%] [G loss: 1.401334]\n",
            "5340 [D loss: 0.694417, acc.: 53.12%] [G loss: 1.388224]\n",
            "5360 [D loss: 0.696847, acc.: 50.78%] [G loss: 1.402042]\n",
            "5380 [D loss: 0.699440, acc.: 46.48%] [G loss: 1.400354]\n",
            "5400 [D loss: 0.695333, acc.: 46.88%] [G loss: 1.389589]\n",
            "5420 [D loss: 0.693278, acc.: 52.34%] [G loss: 1.395435]\n",
            "5440 [D loss: 0.697515, acc.: 52.73%] [G loss: 1.403365]\n",
            "5460 [D loss: 0.700039, acc.: 44.92%] [G loss: 1.385313]\n",
            "5480 [D loss: 0.701342, acc.: 41.80%] [G loss: 1.400771]\n",
            "5500 [D loss: 0.691950, acc.: 52.73%] [G loss: 1.405167]\n",
            "5520 [D loss: 0.697928, acc.: 47.66%] [G loss: 1.407117]\n",
            "5540 [D loss: 0.694162, acc.: 50.39%] [G loss: 1.412841]\n",
            "5560 [D loss: 0.696665, acc.: 49.22%] [G loss: 1.398718]\n",
            "5580 [D loss: 0.700371, acc.: 45.31%] [G loss: 1.393484]\n",
            "5600 [D loss: 0.702304, acc.: 42.19%] [G loss: 1.393507]\n",
            "5620 [D loss: 0.694285, acc.: 50.39%] [G loss: 1.394578]\n",
            "5640 [D loss: 0.697163, acc.: 46.88%] [G loss: 1.395253]\n",
            "5660 [D loss: 0.693129, acc.: 50.78%] [G loss: 1.392592]\n",
            "5680 [D loss: 0.696335, acc.: 45.31%] [G loss: 1.386523]\n",
            "5700 [D loss: 0.697125, acc.: 50.00%] [G loss: 1.412799]\n",
            "5720 [D loss: 0.697893, acc.: 47.27%] [G loss: 1.399107]\n",
            "5740 [D loss: 0.695130, acc.: 49.22%] [G loss: 1.400753]\n",
            "5760 [D loss: 0.698657, acc.: 44.14%] [G loss: 1.395851]\n",
            "5780 [D loss: 0.697089, acc.: 47.66%] [G loss: 1.392132]\n",
            "5800 [D loss: 0.700125, acc.: 46.09%] [G loss: 1.401990]\n",
            "5820 [D loss: 0.697709, acc.: 47.27%] [G loss: 1.394505]\n",
            "5840 [D loss: 0.693451, acc.: 50.39%] [G loss: 1.400309]\n",
            "5860 [D loss: 0.697274, acc.: 45.31%] [G loss: 1.386672]\n",
            "5880 [D loss: 0.700695, acc.: 44.53%] [G loss: 1.398656]\n",
            "5900 [D loss: 0.697150, acc.: 47.27%] [G loss: 1.390164]\n",
            "5920 [D loss: 0.696779, acc.: 44.14%] [G loss: 1.396117]\n",
            "5940 [D loss: 0.693782, acc.: 50.00%] [G loss: 1.390036]\n",
            "5960 [D loss: 0.700256, acc.: 51.17%] [G loss: 1.398869]\n",
            "5980 [D loss: 0.696114, acc.: 49.22%] [G loss: 1.395630]\n",
            "6000 [D loss: 0.697736, acc.: 45.70%] [G loss: 1.393245]\n",
            "6020 [D loss: 0.699556, acc.: 43.75%] [G loss: 1.396108]\n",
            "6040 [D loss: 0.693481, acc.: 50.39%] [G loss: 1.397763]\n",
            "6060 [D loss: 0.703812, acc.: 44.14%] [G loss: 1.388477]\n",
            "6080 [D loss: 0.697451, acc.: 43.36%] [G loss: 1.389992]\n",
            "6100 [D loss: 0.692298, acc.: 46.88%] [G loss: 1.387400]\n",
            "6120 [D loss: 0.699595, acc.: 45.70%] [G loss: 1.405494]\n",
            "6140 [D loss: 0.697563, acc.: 45.70%] [G loss: 1.396657]\n",
            "6160 [D loss: 0.695493, acc.: 48.83%] [G loss: 1.383792]\n",
            "6180 [D loss: 0.697858, acc.: 43.75%] [G loss: 1.381215]\n",
            "6200 [D loss: 0.695829, acc.: 48.83%] [G loss: 1.407797]\n",
            "6220 [D loss: 0.717301, acc.: 37.11%] [G loss: 1.387314]\n",
            "6240 [D loss: 0.697440, acc.: 46.88%] [G loss: 1.404925]\n",
            "6260 [D loss: 0.693353, acc.: 48.05%] [G loss: 1.386393]\n",
            "6280 [D loss: 0.691351, acc.: 53.52%] [G loss: 1.411113]\n",
            "6300 [D loss: 0.697193, acc.: 48.44%] [G loss: 1.393546]\n",
            "6320 [D loss: 0.693712, acc.: 48.05%] [G loss: 1.386686]\n",
            "6340 [D loss: 0.701024, acc.: 43.75%] [G loss: 1.396646]\n",
            "6360 [D loss: 0.701343, acc.: 46.09%] [G loss: 1.392262]\n",
            "6380 [D loss: 0.696558, acc.: 48.05%] [G loss: 1.396049]\n",
            "6400 [D loss: 0.699096, acc.: 45.31%] [G loss: 1.396192]\n",
            "6420 [D loss: 0.695062, acc.: 48.44%] [G loss: 1.393358]\n",
            "6440 [D loss: 0.699274, acc.: 41.80%] [G loss: 1.398998]\n",
            "6460 [D loss: 0.698745, acc.: 42.97%] [G loss: 1.413985]\n",
            "6480 [D loss: 0.696595, acc.: 49.22%] [G loss: 1.407619]\n",
            "6500 [D loss: 0.699104, acc.: 44.53%] [G loss: 1.392327]\n",
            "6520 [D loss: 0.696238, acc.: 47.66%] [G loss: 1.400739]\n",
            "6540 [D loss: 0.694688, acc.: 49.61%] [G loss: 1.409534]\n",
            "6560 [D loss: 0.700760, acc.: 41.80%] [G loss: 1.394047]\n",
            "6580 [D loss: 0.690933, acc.: 51.56%] [G loss: 1.394747]\n",
            "6600 [D loss: 0.698814, acc.: 42.58%] [G loss: 1.389279]\n",
            "6620 [D loss: 0.700194, acc.: 44.53%] [G loss: 1.398422]\n",
            "6640 [D loss: 0.698942, acc.: 47.27%] [G loss: 1.386726]\n",
            "6660 [D loss: 0.700201, acc.: 48.05%] [G loss: 1.389932]\n",
            "6680 [D loss: 0.697296, acc.: 46.88%] [G loss: 1.393817]\n",
            "6700 [D loss: 0.700214, acc.: 42.97%] [G loss: 1.383901]\n",
            "6720 [D loss: 0.697019, acc.: 46.88%] [G loss: 1.392673]\n",
            "6740 [D loss: 0.701964, acc.: 48.05%] [G loss: 1.392654]\n",
            "6760 [D loss: 0.696323, acc.: 52.34%] [G loss: 1.397271]\n",
            "6780 [D loss: 0.699229, acc.: 43.75%] [G loss: 1.387532]\n",
            "6800 [D loss: 0.693560, acc.: 50.78%] [G loss: 1.384406]\n",
            "6820 [D loss: 0.699900, acc.: 46.48%] [G loss: 1.396095]\n",
            "6840 [D loss: 0.700351, acc.: 46.09%] [G loss: 1.400817]\n",
            "6860 [D loss: 0.696565, acc.: 50.39%] [G loss: 1.393807]\n",
            "6880 [D loss: 0.694871, acc.: 49.22%] [G loss: 1.389832]\n",
            "6900 [D loss: 0.700529, acc.: 46.88%] [G loss: 1.386945]\n",
            "6920 [D loss: 0.695679, acc.: 47.27%] [G loss: 1.394230]\n",
            "6940 [D loss: 0.697947, acc.: 43.75%] [G loss: 1.383168]\n",
            "6960 [D loss: 0.697765, acc.: 47.27%] [G loss: 1.401471]\n",
            "6980 [D loss: 0.699596, acc.: 42.97%] [G loss: 1.397855]\n",
            "7000 [D loss: 0.701152, acc.: 42.19%] [G loss: 1.395025]\n",
            "7020 [D loss: 0.697597, acc.: 44.53%] [G loss: 1.392945]\n",
            "7040 [D loss: 0.696670, acc.: 44.14%] [G loss: 1.395268]\n",
            "7060 [D loss: 0.701981, acc.: 41.80%] [G loss: 1.394562]\n",
            "7080 [D loss: 0.698433, acc.: 48.83%] [G loss: 1.398969]\n",
            "7100 [D loss: 0.699508, acc.: 44.92%] [G loss: 1.388082]\n",
            "7120 [D loss: 0.698022, acc.: 46.88%] [G loss: 1.393650]\n",
            "7140 [D loss: 0.694677, acc.: 44.53%] [G loss: 1.384647]\n",
            "7160 [D loss: 0.698689, acc.: 47.27%] [G loss: 1.392577]\n",
            "7180 [D loss: 0.698242, acc.: 42.58%] [G loss: 1.391645]\n",
            "7200 [D loss: 0.700687, acc.: 43.75%] [G loss: 1.396249]\n",
            "7220 [D loss: 0.696790, acc.: 44.14%] [G loss: 1.388211]\n",
            "7240 [D loss: 0.692794, acc.: 46.48%] [G loss: 1.397785]\n",
            "7260 [D loss: 0.698412, acc.: 43.36%] [G loss: 1.383541]\n",
            "7280 [D loss: 0.696848, acc.: 45.70%] [G loss: 1.394438]\n",
            "7300 [D loss: 0.694678, acc.: 45.70%] [G loss: 1.395237]\n",
            "7320 [D loss: 0.693550, acc.: 47.66%] [G loss: 1.395023]\n",
            "7340 [D loss: 0.697784, acc.: 46.09%] [G loss: 1.393231]\n",
            "7360 [D loss: 0.699125, acc.: 43.36%] [G loss: 1.386775]\n",
            "7380 [D loss: 0.698780, acc.: 42.97%] [G loss: 1.399514]\n",
            "7400 [D loss: 0.693878, acc.: 49.61%] [G loss: 1.394288]\n",
            "7420 [D loss: 0.694396, acc.: 50.39%] [G loss: 1.387681]\n",
            "7440 [D loss: 0.699487, acc.: 42.58%] [G loss: 1.391767]\n",
            "7460 [D loss: 0.695520, acc.: 47.27%] [G loss: 1.400790]\n",
            "7480 [D loss: 0.696784, acc.: 43.75%] [G loss: 1.389204]\n",
            "7500 [D loss: 0.705090, acc.: 40.23%] [G loss: 1.392731]\n",
            "7520 [D loss: 0.698871, acc.: 41.02%] [G loss: 1.388768]\n",
            "7540 [D loss: 0.697078, acc.: 46.09%] [G loss: 1.396813]\n",
            "7560 [D loss: 0.696970, acc.: 43.36%] [G loss: 1.395193]\n",
            "7580 [D loss: 0.696497, acc.: 46.09%] [G loss: 1.390957]\n",
            "7600 [D loss: 0.702041, acc.: 41.02%] [G loss: 1.395289]\n",
            "7620 [D loss: 0.697056, acc.: 43.75%] [G loss: 1.393222]\n",
            "7640 [D loss: 0.696136, acc.: 47.27%] [G loss: 1.394576]\n",
            "7660 [D loss: 0.694996, acc.: 47.27%] [G loss: 1.393033]\n",
            "7680 [D loss: 0.695782, acc.: 46.48%] [G loss: 1.385630]\n",
            "7700 [D loss: 0.693979, acc.: 50.39%] [G loss: 1.398277]\n",
            "7720 [D loss: 0.702364, acc.: 45.70%] [G loss: 1.391881]\n",
            "7740 [D loss: 0.699747, acc.: 38.67%] [G loss: 1.395062]\n",
            "7760 [D loss: 0.697299, acc.: 44.53%] [G loss: 1.393300]\n",
            "7780 [D loss: 0.697575, acc.: 44.14%] [G loss: 1.388515]\n",
            "7800 [D loss: 0.696327, acc.: 47.27%] [G loss: 1.399309]\n",
            "7820 [D loss: 0.699796, acc.: 39.45%] [G loss: 1.394260]\n",
            "7840 [D loss: 0.697553, acc.: 44.92%] [G loss: 1.389583]\n",
            "7860 [D loss: 0.698274, acc.: 40.62%] [G loss: 1.385870]\n",
            "7880 [D loss: 0.695591, acc.: 50.39%] [G loss: 1.394403]\n",
            "7900 [D loss: 0.697667, acc.: 42.97%] [G loss: 1.399807]\n",
            "7920 [D loss: 0.699197, acc.: 43.36%] [G loss: 1.389332]\n",
            "7940 [D loss: 0.695461, acc.: 43.75%] [G loss: 1.395153]\n",
            "7960 [D loss: 0.693985, acc.: 50.78%] [G loss: 1.396758]\n",
            "7980 [D loss: 0.694616, acc.: 49.61%] [G loss: 1.394727]\n",
            "8000 [D loss: 0.698273, acc.: 47.66%] [G loss: 1.394009]\n",
            "8020 [D loss: 0.697546, acc.: 46.09%] [G loss: 1.393506]\n",
            "8040 [D loss: 0.696341, acc.: 41.80%] [G loss: 1.394150]\n",
            "8060 [D loss: 0.697245, acc.: 48.44%] [G loss: 1.392660]\n",
            "8080 [D loss: 0.694503, acc.: 49.22%] [G loss: 1.399790]\n",
            "8100 [D loss: 0.699422, acc.: 40.23%] [G loss: 1.391258]\n",
            "8120 [D loss: 0.694864, acc.: 47.66%] [G loss: 1.396652]\n",
            "8140 [D loss: 0.694395, acc.: 45.70%] [G loss: 1.396786]\n",
            "8160 [D loss: 0.696896, acc.: 41.02%] [G loss: 1.390924]\n",
            "8180 [D loss: 0.697071, acc.: 45.31%] [G loss: 1.389972]\n",
            "8200 [D loss: 0.698414, acc.: 41.02%] [G loss: 1.395693]\n",
            "8220 [D loss: 0.696968, acc.: 42.97%] [G loss: 1.391768]\n",
            "8240 [D loss: 0.695341, acc.: 49.61%] [G loss: 1.391918]\n",
            "8260 [D loss: 0.696258, acc.: 42.97%] [G loss: 1.389055]\n",
            "8280 [D loss: 0.694510, acc.: 44.92%] [G loss: 1.391949]\n",
            "8300 [D loss: 0.693706, acc.: 51.17%] [G loss: 1.391831]\n",
            "8320 [D loss: 0.692619, acc.: 50.78%] [G loss: 1.394634]\n",
            "8340 [D loss: 0.692181, acc.: 45.31%] [G loss: 1.398439]\n",
            "8360 [D loss: 0.696840, acc.: 50.00%] [G loss: 1.386354]\n",
            "8380 [D loss: 0.696040, acc.: 45.70%] [G loss: 1.388847]\n",
            "8400 [D loss: 0.697098, acc.: 38.28%] [G loss: 1.385808]\n",
            "8420 [D loss: 0.696205, acc.: 48.44%] [G loss: 1.395315]\n",
            "8440 [D loss: 0.696577, acc.: 44.53%] [G loss: 1.395890]\n",
            "8460 [D loss: 0.698952, acc.: 40.23%] [G loss: 1.392299]\n",
            "8480 [D loss: 0.705718, acc.: 39.84%] [G loss: 1.425954]\n",
            "8500 [D loss: 0.692237, acc.: 47.66%] [G loss: 1.392538]\n",
            "8520 [D loss: 0.694271, acc.: 43.36%] [G loss: 1.400621]\n",
            "8540 [D loss: 0.697283, acc.: 43.36%] [G loss: 1.392912]\n",
            "8560 [D loss: 0.693932, acc.: 46.88%] [G loss: 1.389800]\n",
            "8580 [D loss: 0.698320, acc.: 41.02%] [G loss: 1.390708]\n",
            "8600 [D loss: 0.696863, acc.: 44.53%] [G loss: 1.393254]\n",
            "8620 [D loss: 0.693541, acc.: 48.83%] [G loss: 1.392359]\n",
            "8640 [D loss: 0.698448, acc.: 46.09%] [G loss: 1.390194]\n",
            "8660 [D loss: 0.698225, acc.: 43.36%] [G loss: 1.391702]\n",
            "8680 [D loss: 0.696173, acc.: 43.36%] [G loss: 1.393579]\n",
            "8700 [D loss: 0.698311, acc.: 39.84%] [G loss: 1.393775]\n",
            "8720 [D loss: 0.695746, acc.: 42.97%] [G loss: 1.389348]\n",
            "8740 [D loss: 0.697773, acc.: 45.31%] [G loss: 1.388839]\n",
            "8760 [D loss: 0.698051, acc.: 41.02%] [G loss: 1.387564]\n",
            "8780 [D loss: 0.695869, acc.: 41.80%] [G loss: 1.393906]\n",
            "8800 [D loss: 0.695220, acc.: 43.36%] [G loss: 1.384415]\n",
            "8820 [D loss: 0.694940, acc.: 43.36%] [G loss: 1.385797]\n",
            "8840 [D loss: 0.699507, acc.: 44.53%] [G loss: 1.392140]\n",
            "8860 [D loss: 0.698243, acc.: 41.02%] [G loss: 1.383003]\n",
            "8880 [D loss: 0.694200, acc.: 51.17%] [G loss: 1.389239]\n",
            "8900 [D loss: 0.700332, acc.: 39.45%] [G loss: 1.392415]\n",
            "8920 [D loss: 0.694468, acc.: 43.75%] [G loss: 1.398754]\n",
            "8940 [D loss: 0.693868, acc.: 46.09%] [G loss: 1.401220]\n",
            "8960 [D loss: 0.696201, acc.: 46.09%] [G loss: 1.390292]\n",
            "8980 [D loss: 0.700917, acc.: 37.11%] [G loss: 1.384135]\n",
            "9000 [D loss: 0.529419, acc.: 60.16%] [G loss: 3.451092]\n",
            "9020 [D loss: 0.714005, acc.: 34.77%] [G loss: 1.418368]\n",
            "9040 [D loss: 0.708870, acc.: 40.62%] [G loss: 1.395880]\n",
            "9060 [D loss: 0.705619, acc.: 41.80%] [G loss: 1.390461]\n",
            "9080 [D loss: 0.698528, acc.: 45.31%] [G loss: 1.404152]\n",
            "9100 [D loss: 0.699169, acc.: 39.06%] [G loss: 1.402751]\n",
            "9120 [D loss: 0.696621, acc.: 41.41%] [G loss: 1.406112]\n",
            "9140 [D loss: 0.698882, acc.: 44.92%] [G loss: 1.396840]\n",
            "9160 [D loss: 0.696500, acc.: 44.53%] [G loss: 1.399978]\n",
            "9180 [D loss: 0.694397, acc.: 50.39%] [G loss: 1.403586]\n",
            "9200 [D loss: 0.694435, acc.: 49.22%] [G loss: 1.399590]\n",
            "9220 [D loss: 0.696431, acc.: 43.36%] [G loss: 1.405650]\n",
            "9240 [D loss: 0.695898, acc.: 47.27%] [G loss: 1.392597]\n",
            "9260 [D loss: 0.698319, acc.: 42.58%] [G loss: 1.398521]\n",
            "9280 [D loss: 0.700153, acc.: 39.06%] [G loss: 1.388895]\n",
            "9300 [D loss: 0.693785, acc.: 47.27%] [G loss: 1.391394]\n",
            "9320 [D loss: 0.691352, acc.: 50.78%] [G loss: 1.394004]\n",
            "9340 [D loss: 0.694901, acc.: 43.36%] [G loss: 1.396414]\n",
            "9360 [D loss: 0.696790, acc.: 42.58%] [G loss: 1.395674]\n",
            "9380 [D loss: 0.692653, acc.: 48.83%] [G loss: 1.399105]\n",
            "9400 [D loss: 0.697056, acc.: 44.14%] [G loss: 1.392526]\n",
            "9420 [D loss: 0.699672, acc.: 42.97%] [G loss: 1.388630]\n",
            "9440 [D loss: 0.696520, acc.: 47.66%] [G loss: 1.388295]\n",
            "9460 [D loss: 0.692791, acc.: 49.22%] [G loss: 1.395600]\n",
            "9480 [D loss: 0.692895, acc.: 45.70%] [G loss: 1.389935]\n",
            "9500 [D loss: 0.697345, acc.: 44.92%] [G loss: 1.405686]\n",
            "9520 [D loss: 0.695038, acc.: 48.83%] [G loss: 1.398619]\n",
            "9540 [D loss: 0.693577, acc.: 49.22%] [G loss: 1.392874]\n",
            "9560 [D loss: 0.696086, acc.: 44.53%] [G loss: 1.395670]\n",
            "9580 [D loss: 0.694003, acc.: 47.66%] [G loss: 1.405685]\n",
            "9600 [D loss: 0.691519, acc.: 48.44%] [G loss: 1.388005]\n",
            "9620 [D loss: 0.701189, acc.: 41.41%] [G loss: 1.402854]\n",
            "9640 [D loss: 0.692627, acc.: 50.39%] [G loss: 1.407890]\n",
            "9660 [D loss: 0.699078, acc.: 45.31%] [G loss: 1.403761]\n",
            "9680 [D loss: 0.692119, acc.: 46.48%] [G loss: 1.422443]\n",
            "9700 [D loss: 0.694870, acc.: 47.27%] [G loss: 1.402087]\n",
            "9720 [D loss: 0.694653, acc.: 47.66%] [G loss: 1.401800]\n",
            "9740 [D loss: 0.691276, acc.: 53.52%] [G loss: 1.412094]\n",
            "9760 [D loss: 0.697540, acc.: 41.41%] [G loss: 1.401138]\n",
            "9780 [D loss: 0.697595, acc.: 46.09%] [G loss: 1.409874]\n",
            "9800 [D loss: 0.693005, acc.: 46.48%] [G loss: 1.400883]\n",
            "9820 [D loss: 0.692952, acc.: 54.30%] [G loss: 1.406455]\n",
            "9840 [D loss: 0.691773, acc.: 48.05%] [G loss: 1.401276]\n",
            "9860 [D loss: 0.691053, acc.: 51.17%] [G loss: 1.397547]\n",
            "9880 [D loss: 0.687381, acc.: 53.12%] [G loss: 1.407923]\n",
            "9900 [D loss: 0.692056, acc.: 47.27%] [G loss: 1.400826]\n",
            "9920 [D loss: 0.694512, acc.: 43.75%] [G loss: 1.410615]\n",
            "9940 [D loss: 0.693986, acc.: 45.70%] [G loss: 1.404719]\n",
            "9960 [D loss: 0.691524, acc.: 52.73%] [G loss: 1.402085]\n",
            "9980 [D loss: 0.695208, acc.: 48.05%] [G loss: 1.405380]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}