{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "12.Final_Model_BiCoGAN_Affine_Rotate.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS516rLy1LYQ",
        "outputId": "17ca2ef6-bbb7-473f-f986-8d0525300361"
      },
      "source": [
        "!unzip fer2013.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fer2013.zip\n",
            "  inflating: fer2013.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "# from __future__ import print_function, division\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "jF8QfAYPg_71",
        "outputId": "66fab483-360c-45b9-88a3-adfece31fc4c"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "fed5cfd4-8d98-4d74-f5f9-21b250d49202"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9swxs5P0w",
        "outputId": "9099bca9-873b-4c1f-b60b-3386d9897ed4"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "0205ee7a-7bfc-4725-e2e5-5dafdb8fef73"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG7mZac9cJlA"
      },
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXf6WO9KeeMb",
        "outputId": "396fcbfa-5bc4-4bb3-af4d-bcec1c19f491"
      },
      "source": [
        "epochs = X_train.shape[0]\n",
        "print(epochs)\n",
        "\n",
        "X_train_2 = X_train\n",
        "y_train_2 = y_train\n",
        "\n",
        "for k in range(epochs):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  # for i in range(1):\n",
        "  # contrast = iaa.Affine(rotate=(-50+i*5, 30+i*5))\n",
        "  contrast = iaa.Affine(rotate=(-50, 30))\n",
        "  contrast_image = contrast.augment_image(img)\n",
        "  contrasted_images.append(contrast_image)\n",
        "\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train_2 = np.concatenate((X_train_2, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]\n",
        "  y_train_2 = np.concatenate((y_train_2, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration: %d, train shape: %s\" % (k, X_train_2.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35340\n",
            "iteration: 0, train shape: (35341, 48, 48, 1)\n",
            "iteration: 100, train shape: (35441, 48, 48, 1)\n",
            "iteration: 200, train shape: (35541, 48, 48, 1)\n",
            "iteration: 300, train shape: (35641, 48, 48, 1)\n",
            "iteration: 400, train shape: (35741, 48, 48, 1)\n",
            "iteration: 500, train shape: (35841, 48, 48, 1)\n",
            "iteration: 600, train shape: (35941, 48, 48, 1)\n",
            "iteration: 700, train shape: (36041, 48, 48, 1)\n",
            "iteration: 800, train shape: (36141, 48, 48, 1)\n",
            "iteration: 900, train shape: (36241, 48, 48, 1)\n",
            "iteration: 1000, train shape: (36341, 48, 48, 1)\n",
            "iteration: 1100, train shape: (36441, 48, 48, 1)\n",
            "iteration: 1200, train shape: (36541, 48, 48, 1)\n",
            "iteration: 1300, train shape: (36641, 48, 48, 1)\n",
            "iteration: 1400, train shape: (36741, 48, 48, 1)\n",
            "iteration: 1500, train shape: (36841, 48, 48, 1)\n",
            "iteration: 1600, train shape: (36941, 48, 48, 1)\n",
            "iteration: 1700, train shape: (37041, 48, 48, 1)\n",
            "iteration: 1800, train shape: (37141, 48, 48, 1)\n",
            "iteration: 1900, train shape: (37241, 48, 48, 1)\n",
            "iteration: 2000, train shape: (37341, 48, 48, 1)\n",
            "iteration: 2100, train shape: (37441, 48, 48, 1)\n",
            "iteration: 2200, train shape: (37541, 48, 48, 1)\n",
            "iteration: 2300, train shape: (37641, 48, 48, 1)\n",
            "iteration: 2400, train shape: (37741, 48, 48, 1)\n",
            "iteration: 2500, train shape: (37841, 48, 48, 1)\n",
            "iteration: 2600, train shape: (37941, 48, 48, 1)\n",
            "iteration: 2700, train shape: (38041, 48, 48, 1)\n",
            "iteration: 2800, train shape: (38141, 48, 48, 1)\n",
            "iteration: 2900, train shape: (38241, 48, 48, 1)\n",
            "iteration: 3000, train shape: (38341, 48, 48, 1)\n",
            "iteration: 3100, train shape: (38441, 48, 48, 1)\n",
            "iteration: 3200, train shape: (38541, 48, 48, 1)\n",
            "iteration: 3300, train shape: (38641, 48, 48, 1)\n",
            "iteration: 3400, train shape: (38741, 48, 48, 1)\n",
            "iteration: 3500, train shape: (38841, 48, 48, 1)\n",
            "iteration: 3600, train shape: (38941, 48, 48, 1)\n",
            "iteration: 3700, train shape: (39041, 48, 48, 1)\n",
            "iteration: 3800, train shape: (39141, 48, 48, 1)\n",
            "iteration: 3900, train shape: (39241, 48, 48, 1)\n",
            "iteration: 4000, train shape: (39341, 48, 48, 1)\n",
            "iteration: 4100, train shape: (39441, 48, 48, 1)\n",
            "iteration: 4200, train shape: (39541, 48, 48, 1)\n",
            "iteration: 4300, train shape: (39641, 48, 48, 1)\n",
            "iteration: 4400, train shape: (39741, 48, 48, 1)\n",
            "iteration: 4500, train shape: (39841, 48, 48, 1)\n",
            "iteration: 4600, train shape: (39941, 48, 48, 1)\n",
            "iteration: 4700, train shape: (40041, 48, 48, 1)\n",
            "iteration: 4800, train shape: (40141, 48, 48, 1)\n",
            "iteration: 4900, train shape: (40241, 48, 48, 1)\n",
            "iteration: 5000, train shape: (40341, 48, 48, 1)\n",
            "iteration: 5100, train shape: (40441, 48, 48, 1)\n",
            "iteration: 5200, train shape: (40541, 48, 48, 1)\n",
            "iteration: 5300, train shape: (40641, 48, 48, 1)\n",
            "iteration: 5400, train shape: (40741, 48, 48, 1)\n",
            "iteration: 5500, train shape: (40841, 48, 48, 1)\n",
            "iteration: 5600, train shape: (40941, 48, 48, 1)\n",
            "iteration: 5700, train shape: (41041, 48, 48, 1)\n",
            "iteration: 5800, train shape: (41141, 48, 48, 1)\n",
            "iteration: 5900, train shape: (41241, 48, 48, 1)\n",
            "iteration: 6000, train shape: (41341, 48, 48, 1)\n",
            "iteration: 6100, train shape: (41441, 48, 48, 1)\n",
            "iteration: 6200, train shape: (41541, 48, 48, 1)\n",
            "iteration: 6300, train shape: (41641, 48, 48, 1)\n",
            "iteration: 6400, train shape: (41741, 48, 48, 1)\n",
            "iteration: 6500, train shape: (41841, 48, 48, 1)\n",
            "iteration: 6600, train shape: (41941, 48, 48, 1)\n",
            "iteration: 6700, train shape: (42041, 48, 48, 1)\n",
            "iteration: 6800, train shape: (42141, 48, 48, 1)\n",
            "iteration: 6900, train shape: (42241, 48, 48, 1)\n",
            "iteration: 7000, train shape: (42341, 48, 48, 1)\n",
            "iteration: 7100, train shape: (42441, 48, 48, 1)\n",
            "iteration: 7200, train shape: (42541, 48, 48, 1)\n",
            "iteration: 7300, train shape: (42641, 48, 48, 1)\n",
            "iteration: 7400, train shape: (42741, 48, 48, 1)\n",
            "iteration: 7500, train shape: (42841, 48, 48, 1)\n",
            "iteration: 7600, train shape: (42941, 48, 48, 1)\n",
            "iteration: 7700, train shape: (43041, 48, 48, 1)\n",
            "iteration: 7800, train shape: (43141, 48, 48, 1)\n",
            "iteration: 7900, train shape: (43241, 48, 48, 1)\n",
            "iteration: 8000, train shape: (43341, 48, 48, 1)\n",
            "iteration: 8100, train shape: (43441, 48, 48, 1)\n",
            "iteration: 8200, train shape: (43541, 48, 48, 1)\n",
            "iteration: 8300, train shape: (43641, 48, 48, 1)\n",
            "iteration: 8400, train shape: (43741, 48, 48, 1)\n",
            "iteration: 8500, train shape: (43841, 48, 48, 1)\n",
            "iteration: 8600, train shape: (43941, 48, 48, 1)\n",
            "iteration: 8700, train shape: (44041, 48, 48, 1)\n",
            "iteration: 8800, train shape: (44141, 48, 48, 1)\n",
            "iteration: 8900, train shape: (44241, 48, 48, 1)\n",
            "iteration: 9000, train shape: (44341, 48, 48, 1)\n",
            "iteration: 9100, train shape: (44441, 48, 48, 1)\n",
            "iteration: 9200, train shape: (44541, 48, 48, 1)\n",
            "iteration: 9300, train shape: (44641, 48, 48, 1)\n",
            "iteration: 9400, train shape: (44741, 48, 48, 1)\n",
            "iteration: 9500, train shape: (44841, 48, 48, 1)\n",
            "iteration: 9600, train shape: (44941, 48, 48, 1)\n",
            "iteration: 9700, train shape: (45041, 48, 48, 1)\n",
            "iteration: 9800, train shape: (45141, 48, 48, 1)\n",
            "iteration: 9900, train shape: (45241, 48, 48, 1)\n",
            "iteration: 10000, train shape: (45341, 48, 48, 1)\n",
            "iteration: 10100, train shape: (45441, 48, 48, 1)\n",
            "iteration: 10200, train shape: (45541, 48, 48, 1)\n",
            "iteration: 10300, train shape: (45641, 48, 48, 1)\n",
            "iteration: 10400, train shape: (45741, 48, 48, 1)\n",
            "iteration: 10500, train shape: (45841, 48, 48, 1)\n",
            "iteration: 10600, train shape: (45941, 48, 48, 1)\n",
            "iteration: 10700, train shape: (46041, 48, 48, 1)\n",
            "iteration: 10800, train shape: (46141, 48, 48, 1)\n",
            "iteration: 10900, train shape: (46241, 48, 48, 1)\n",
            "iteration: 11000, train shape: (46341, 48, 48, 1)\n",
            "iteration: 11100, train shape: (46441, 48, 48, 1)\n",
            "iteration: 11200, train shape: (46541, 48, 48, 1)\n",
            "iteration: 11300, train shape: (46641, 48, 48, 1)\n",
            "iteration: 11400, train shape: (46741, 48, 48, 1)\n",
            "iteration: 11500, train shape: (46841, 48, 48, 1)\n",
            "iteration: 11600, train shape: (46941, 48, 48, 1)\n",
            "iteration: 11700, train shape: (47041, 48, 48, 1)\n",
            "iteration: 11800, train shape: (47141, 48, 48, 1)\n",
            "iteration: 11900, train shape: (47241, 48, 48, 1)\n",
            "iteration: 12000, train shape: (47341, 48, 48, 1)\n",
            "iteration: 12100, train shape: (47441, 48, 48, 1)\n",
            "iteration: 12200, train shape: (47541, 48, 48, 1)\n",
            "iteration: 12300, train shape: (47641, 48, 48, 1)\n",
            "iteration: 12400, train shape: (47741, 48, 48, 1)\n",
            "iteration: 12500, train shape: (47841, 48, 48, 1)\n",
            "iteration: 12600, train shape: (47941, 48, 48, 1)\n",
            "iteration: 12700, train shape: (48041, 48, 48, 1)\n",
            "iteration: 12800, train shape: (48141, 48, 48, 1)\n",
            "iteration: 12900, train shape: (48241, 48, 48, 1)\n",
            "iteration: 13000, train shape: (48341, 48, 48, 1)\n",
            "iteration: 13100, train shape: (48441, 48, 48, 1)\n",
            "iteration: 13200, train shape: (48541, 48, 48, 1)\n",
            "iteration: 13300, train shape: (48641, 48, 48, 1)\n",
            "iteration: 13400, train shape: (48741, 48, 48, 1)\n",
            "iteration: 13500, train shape: (48841, 48, 48, 1)\n",
            "iteration: 13600, train shape: (48941, 48, 48, 1)\n",
            "iteration: 13700, train shape: (49041, 48, 48, 1)\n",
            "iteration: 13800, train shape: (49141, 48, 48, 1)\n",
            "iteration: 13900, train shape: (49241, 48, 48, 1)\n",
            "iteration: 14000, train shape: (49341, 48, 48, 1)\n",
            "iteration: 14100, train shape: (49441, 48, 48, 1)\n",
            "iteration: 14200, train shape: (49541, 48, 48, 1)\n",
            "iteration: 14300, train shape: (49641, 48, 48, 1)\n",
            "iteration: 14400, train shape: (49741, 48, 48, 1)\n",
            "iteration: 14500, train shape: (49841, 48, 48, 1)\n",
            "iteration: 14600, train shape: (49941, 48, 48, 1)\n",
            "iteration: 14700, train shape: (50041, 48, 48, 1)\n",
            "iteration: 14800, train shape: (50141, 48, 48, 1)\n",
            "iteration: 14900, train shape: (50241, 48, 48, 1)\n",
            "iteration: 15000, train shape: (50341, 48, 48, 1)\n",
            "iteration: 15100, train shape: (50441, 48, 48, 1)\n",
            "iteration: 15200, train shape: (50541, 48, 48, 1)\n",
            "iteration: 15300, train shape: (50641, 48, 48, 1)\n",
            "iteration: 15400, train shape: (50741, 48, 48, 1)\n",
            "iteration: 15500, train shape: (50841, 48, 48, 1)\n",
            "iteration: 15600, train shape: (50941, 48, 48, 1)\n",
            "iteration: 15700, train shape: (51041, 48, 48, 1)\n",
            "iteration: 15800, train shape: (51141, 48, 48, 1)\n",
            "iteration: 15900, train shape: (51241, 48, 48, 1)\n",
            "iteration: 16000, train shape: (51341, 48, 48, 1)\n",
            "iteration: 16100, train shape: (51441, 48, 48, 1)\n",
            "iteration: 16200, train shape: (51541, 48, 48, 1)\n",
            "iteration: 16300, train shape: (51641, 48, 48, 1)\n",
            "iteration: 16400, train shape: (51741, 48, 48, 1)\n",
            "iteration: 16500, train shape: (51841, 48, 48, 1)\n",
            "iteration: 16600, train shape: (51941, 48, 48, 1)\n",
            "iteration: 16700, train shape: (52041, 48, 48, 1)\n",
            "iteration: 16800, train shape: (52141, 48, 48, 1)\n",
            "iteration: 16900, train shape: (52241, 48, 48, 1)\n",
            "iteration: 17000, train shape: (52341, 48, 48, 1)\n",
            "iteration: 17100, train shape: (52441, 48, 48, 1)\n",
            "iteration: 17200, train shape: (52541, 48, 48, 1)\n",
            "iteration: 17300, train shape: (52641, 48, 48, 1)\n",
            "iteration: 17400, train shape: (52741, 48, 48, 1)\n",
            "iteration: 17500, train shape: (52841, 48, 48, 1)\n",
            "iteration: 17600, train shape: (52941, 48, 48, 1)\n",
            "iteration: 17700, train shape: (53041, 48, 48, 1)\n",
            "iteration: 17800, train shape: (53141, 48, 48, 1)\n",
            "iteration: 17900, train shape: (53241, 48, 48, 1)\n",
            "iteration: 18000, train shape: (53341, 48, 48, 1)\n",
            "iteration: 18100, train shape: (53441, 48, 48, 1)\n",
            "iteration: 18200, train shape: (53541, 48, 48, 1)\n",
            "iteration: 18300, train shape: (53641, 48, 48, 1)\n",
            "iteration: 18400, train shape: (53741, 48, 48, 1)\n",
            "iteration: 18500, train shape: (53841, 48, 48, 1)\n",
            "iteration: 18600, train shape: (53941, 48, 48, 1)\n",
            "iteration: 18700, train shape: (54041, 48, 48, 1)\n",
            "iteration: 18800, train shape: (54141, 48, 48, 1)\n",
            "iteration: 18900, train shape: (54241, 48, 48, 1)\n",
            "iteration: 19000, train shape: (54341, 48, 48, 1)\n",
            "iteration: 19100, train shape: (54441, 48, 48, 1)\n",
            "iteration: 19200, train shape: (54541, 48, 48, 1)\n",
            "iteration: 19300, train shape: (54641, 48, 48, 1)\n",
            "iteration: 19400, train shape: (54741, 48, 48, 1)\n",
            "iteration: 19500, train shape: (54841, 48, 48, 1)\n",
            "iteration: 19600, train shape: (54941, 48, 48, 1)\n",
            "iteration: 19700, train shape: (55041, 48, 48, 1)\n",
            "iteration: 19800, train shape: (55141, 48, 48, 1)\n",
            "iteration: 19900, train shape: (55241, 48, 48, 1)\n",
            "iteration: 20000, train shape: (55341, 48, 48, 1)\n",
            "iteration: 20100, train shape: (55441, 48, 48, 1)\n",
            "iteration: 20200, train shape: (55541, 48, 48, 1)\n",
            "iteration: 20300, train shape: (55641, 48, 48, 1)\n",
            "iteration: 20400, train shape: (55741, 48, 48, 1)\n",
            "iteration: 20500, train shape: (55841, 48, 48, 1)\n",
            "iteration: 20600, train shape: (55941, 48, 48, 1)\n",
            "iteration: 20700, train shape: (56041, 48, 48, 1)\n",
            "iteration: 20800, train shape: (56141, 48, 48, 1)\n",
            "iteration: 20900, train shape: (56241, 48, 48, 1)\n",
            "iteration: 21000, train shape: (56341, 48, 48, 1)\n",
            "iteration: 21100, train shape: (56441, 48, 48, 1)\n",
            "iteration: 21200, train shape: (56541, 48, 48, 1)\n",
            "iteration: 21300, train shape: (56641, 48, 48, 1)\n",
            "iteration: 21400, train shape: (56741, 48, 48, 1)\n",
            "iteration: 21500, train shape: (56841, 48, 48, 1)\n",
            "iteration: 21600, train shape: (56941, 48, 48, 1)\n",
            "iteration: 21700, train shape: (57041, 48, 48, 1)\n",
            "iteration: 21800, train shape: (57141, 48, 48, 1)\n",
            "iteration: 21900, train shape: (57241, 48, 48, 1)\n",
            "iteration: 22000, train shape: (57341, 48, 48, 1)\n",
            "iteration: 22100, train shape: (57441, 48, 48, 1)\n",
            "iteration: 22200, train shape: (57541, 48, 48, 1)\n",
            "iteration: 22300, train shape: (57641, 48, 48, 1)\n",
            "iteration: 22400, train shape: (57741, 48, 48, 1)\n",
            "iteration: 22500, train shape: (57841, 48, 48, 1)\n",
            "iteration: 22600, train shape: (57941, 48, 48, 1)\n",
            "iteration: 22700, train shape: (58041, 48, 48, 1)\n",
            "iteration: 22800, train shape: (58141, 48, 48, 1)\n",
            "iteration: 22900, train shape: (58241, 48, 48, 1)\n",
            "iteration: 23000, train shape: (58341, 48, 48, 1)\n",
            "iteration: 23100, train shape: (58441, 48, 48, 1)\n",
            "iteration: 23200, train shape: (58541, 48, 48, 1)\n",
            "iteration: 23300, train shape: (58641, 48, 48, 1)\n",
            "iteration: 23400, train shape: (58741, 48, 48, 1)\n",
            "iteration: 23500, train shape: (58841, 48, 48, 1)\n",
            "iteration: 23600, train shape: (58941, 48, 48, 1)\n",
            "iteration: 23700, train shape: (59041, 48, 48, 1)\n",
            "iteration: 23800, train shape: (59141, 48, 48, 1)\n",
            "iteration: 23900, train shape: (59241, 48, 48, 1)\n",
            "iteration: 24000, train shape: (59341, 48, 48, 1)\n",
            "iteration: 24100, train shape: (59441, 48, 48, 1)\n",
            "iteration: 24200, train shape: (59541, 48, 48, 1)\n",
            "iteration: 24300, train shape: (59641, 48, 48, 1)\n",
            "iteration: 24400, train shape: (59741, 48, 48, 1)\n",
            "iteration: 24500, train shape: (59841, 48, 48, 1)\n",
            "iteration: 24600, train shape: (59941, 48, 48, 1)\n",
            "iteration: 24700, train shape: (60041, 48, 48, 1)\n",
            "iteration: 24800, train shape: (60141, 48, 48, 1)\n",
            "iteration: 24900, train shape: (60241, 48, 48, 1)\n",
            "iteration: 25000, train shape: (60341, 48, 48, 1)\n",
            "iteration: 25100, train shape: (60441, 48, 48, 1)\n",
            "iteration: 25200, train shape: (60541, 48, 48, 1)\n",
            "iteration: 25300, train shape: (60641, 48, 48, 1)\n",
            "iteration: 25400, train shape: (60741, 48, 48, 1)\n",
            "iteration: 25500, train shape: (60841, 48, 48, 1)\n",
            "iteration: 25600, train shape: (60941, 48, 48, 1)\n",
            "iteration: 25700, train shape: (61041, 48, 48, 1)\n",
            "iteration: 25800, train shape: (61141, 48, 48, 1)\n",
            "iteration: 25900, train shape: (61241, 48, 48, 1)\n",
            "iteration: 26000, train shape: (61341, 48, 48, 1)\n",
            "iteration: 26100, train shape: (61441, 48, 48, 1)\n",
            "iteration: 26200, train shape: (61541, 48, 48, 1)\n",
            "iteration: 26300, train shape: (61641, 48, 48, 1)\n",
            "iteration: 26400, train shape: (61741, 48, 48, 1)\n",
            "iteration: 26500, train shape: (61841, 48, 48, 1)\n",
            "iteration: 26600, train shape: (61941, 48, 48, 1)\n",
            "iteration: 26700, train shape: (62041, 48, 48, 1)\n",
            "iteration: 26800, train shape: (62141, 48, 48, 1)\n",
            "iteration: 26900, train shape: (62241, 48, 48, 1)\n",
            "iteration: 27000, train shape: (62341, 48, 48, 1)\n",
            "iteration: 27100, train shape: (62441, 48, 48, 1)\n",
            "iteration: 27200, train shape: (62541, 48, 48, 1)\n",
            "iteration: 27300, train shape: (62641, 48, 48, 1)\n",
            "iteration: 27400, train shape: (62741, 48, 48, 1)\n",
            "iteration: 27500, train shape: (62841, 48, 48, 1)\n",
            "iteration: 27600, train shape: (62941, 48, 48, 1)\n",
            "iteration: 27700, train shape: (63041, 48, 48, 1)\n",
            "iteration: 27800, train shape: (63141, 48, 48, 1)\n",
            "iteration: 27900, train shape: (63241, 48, 48, 1)\n",
            "iteration: 28000, train shape: (63341, 48, 48, 1)\n",
            "iteration: 28100, train shape: (63441, 48, 48, 1)\n",
            "iteration: 28200, train shape: (63541, 48, 48, 1)\n",
            "iteration: 28300, train shape: (63641, 48, 48, 1)\n",
            "iteration: 28400, train shape: (63741, 48, 48, 1)\n",
            "iteration: 28500, train shape: (63841, 48, 48, 1)\n",
            "iteration: 28600, train shape: (63941, 48, 48, 1)\n",
            "iteration: 28700, train shape: (64041, 48, 48, 1)\n",
            "iteration: 28800, train shape: (64141, 48, 48, 1)\n",
            "iteration: 28900, train shape: (64241, 48, 48, 1)\n",
            "iteration: 29000, train shape: (64341, 48, 48, 1)\n",
            "iteration: 29100, train shape: (64441, 48, 48, 1)\n",
            "iteration: 29200, train shape: (64541, 48, 48, 1)\n",
            "iteration: 29300, train shape: (64641, 48, 48, 1)\n",
            "iteration: 29400, train shape: (64741, 48, 48, 1)\n",
            "iteration: 29500, train shape: (64841, 48, 48, 1)\n",
            "iteration: 29600, train shape: (64941, 48, 48, 1)\n",
            "iteration: 29700, train shape: (65041, 48, 48, 1)\n",
            "iteration: 29800, train shape: (65141, 48, 48, 1)\n",
            "iteration: 29900, train shape: (65241, 48, 48, 1)\n",
            "iteration: 30000, train shape: (65341, 48, 48, 1)\n",
            "iteration: 30100, train shape: (65441, 48, 48, 1)\n",
            "iteration: 30200, train shape: (65541, 48, 48, 1)\n",
            "iteration: 30300, train shape: (65641, 48, 48, 1)\n",
            "iteration: 30400, train shape: (65741, 48, 48, 1)\n",
            "iteration: 30500, train shape: (65841, 48, 48, 1)\n",
            "iteration: 30600, train shape: (65941, 48, 48, 1)\n",
            "iteration: 30700, train shape: (66041, 48, 48, 1)\n",
            "iteration: 30800, train shape: (66141, 48, 48, 1)\n",
            "iteration: 30900, train shape: (66241, 48, 48, 1)\n",
            "iteration: 31000, train shape: (66341, 48, 48, 1)\n",
            "iteration: 31100, train shape: (66441, 48, 48, 1)\n",
            "iteration: 31200, train shape: (66541, 48, 48, 1)\n",
            "iteration: 31300, train shape: (66641, 48, 48, 1)\n",
            "iteration: 31400, train shape: (66741, 48, 48, 1)\n",
            "iteration: 31500, train shape: (66841, 48, 48, 1)\n",
            "iteration: 31600, train shape: (66941, 48, 48, 1)\n",
            "iteration: 31700, train shape: (67041, 48, 48, 1)\n",
            "iteration: 31800, train shape: (67141, 48, 48, 1)\n",
            "iteration: 31900, train shape: (67241, 48, 48, 1)\n",
            "iteration: 32000, train shape: (67341, 48, 48, 1)\n",
            "iteration: 32100, train shape: (67441, 48, 48, 1)\n",
            "iteration: 32200, train shape: (67541, 48, 48, 1)\n",
            "iteration: 32300, train shape: (67641, 48, 48, 1)\n",
            "iteration: 32400, train shape: (67741, 48, 48, 1)\n",
            "iteration: 32500, train shape: (67841, 48, 48, 1)\n",
            "iteration: 32600, train shape: (67941, 48, 48, 1)\n",
            "iteration: 32700, train shape: (68041, 48, 48, 1)\n",
            "iteration: 32800, train shape: (68141, 48, 48, 1)\n",
            "iteration: 32900, train shape: (68241, 48, 48, 1)\n",
            "iteration: 33000, train shape: (68341, 48, 48, 1)\n",
            "iteration: 33100, train shape: (68441, 48, 48, 1)\n",
            "iteration: 33200, train shape: (68541, 48, 48, 1)\n",
            "iteration: 33300, train shape: (68641, 48, 48, 1)\n",
            "iteration: 33400, train shape: (68741, 48, 48, 1)\n",
            "iteration: 33500, train shape: (68841, 48, 48, 1)\n",
            "iteration: 33600, train shape: (68941, 48, 48, 1)\n",
            "iteration: 33700, train shape: (69041, 48, 48, 1)\n",
            "iteration: 33800, train shape: (69141, 48, 48, 1)\n",
            "iteration: 33900, train shape: (69241, 48, 48, 1)\n",
            "iteration: 34000, train shape: (69341, 48, 48, 1)\n",
            "iteration: 34100, train shape: (69441, 48, 48, 1)\n",
            "iteration: 34200, train shape: (69541, 48, 48, 1)\n",
            "iteration: 34300, train shape: (69641, 48, 48, 1)\n",
            "iteration: 34400, train shape: (69741, 48, 48, 1)\n",
            "iteration: 34500, train shape: (69841, 48, 48, 1)\n",
            "iteration: 34600, train shape: (69941, 48, 48, 1)\n",
            "iteration: 34700, train shape: (70041, 48, 48, 1)\n",
            "iteration: 34800, train shape: (70141, 48, 48, 1)\n",
            "iteration: 34900, train shape: (70241, 48, 48, 1)\n",
            "iteration: 35000, train shape: (70341, 48, 48, 1)\n",
            "iteration: 35100, train shape: (70441, 48, 48, 1)\n",
            "iteration: 35200, train shape: (70541, 48, 48, 1)\n",
            "iteration: 35300, train shape: (70641, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWsEbPyuvHO8",
        "outputId": "cfd7d743-8b92-4997-b147-5d4531ef89e1"
      },
      "source": [
        "print(X_train_2.shape)\n",
        "print(y_train_2.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(70680, 48, 48, 1)\n",
            "(70680, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train_2.shape[0], batch_size)\n",
        "            # imgs = datagen.flow(X_train_2[idx], y_train[idx], batch_size=batch_size)\n",
        "            # labels = y_train[idx]\n",
        "            imgs, labels = X_train_2[idx], y_train_2[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5TkoS2Nmt7o",
        "outputId": "db646037-1dd4-41b1-9e35-a8b9fd030558"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=20000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.703770, acc.: 47.66%] [G loss: 1.510614]\n",
            "20 [D loss: 1.264945, acc.: 17.19%] [G loss: 0.686439]\n",
            "40 [D loss: 1.380958, acc.: 0.78%] [G loss: 1.323033]\n",
            "60 [D loss: 1.741495, acc.: 24.61%] [G loss: 2.741020]\n",
            "80 [D loss: 0.689761, acc.: 39.45%] [G loss: 1.969091]\n",
            "100 [D loss: 0.604999, acc.: 69.92%] [G loss: 2.404637]\n",
            "120 [D loss: 0.951352, acc.: 10.55%] [G loss: 1.770444]\n",
            "140 [D loss: 0.481254, acc.: 86.72%] [G loss: 2.299339]\n",
            "160 [D loss: 0.266443, acc.: 97.27%] [G loss: 5.542087]\n",
            "180 [D loss: 0.781676, acc.: 51.17%] [G loss: 2.293643]\n",
            "200 [D loss: 1.056169, acc.: 58.59%] [G loss: 4.702920]\n",
            "220 [D loss: 0.534684, acc.: 77.73%] [G loss: 5.475939]\n",
            "240 [D loss: 0.132735, acc.: 98.83%] [G loss: 11.906663]\n",
            "260 [D loss: 0.450300, acc.: 91.80%] [G loss: 2.410875]\n",
            "280 [D loss: 0.751033, acc.: 58.20%] [G loss: 2.741867]\n",
            "300 [D loss: 0.758043, acc.: 57.81%] [G loss: 8.729086]\n",
            "320 [D loss: 0.699018, acc.: 57.81%] [G loss: 3.182218]\n",
            "340 [D loss: 0.290911, acc.: 91.02%] [G loss: 5.303382]\n",
            "360 [D loss: 0.174477, acc.: 98.44%] [G loss: 12.590182]\n",
            "380 [D loss: 0.558670, acc.: 76.56%] [G loss: 4.854518]\n",
            "400 [D loss: 0.651524, acc.: 63.67%] [G loss: 4.791213]\n",
            "420 [D loss: 0.581472, acc.: 69.92%] [G loss: 3.058154]\n",
            "440 [D loss: 0.252118, acc.: 88.28%] [G loss: 6.719268]\n",
            "460 [D loss: 0.556276, acc.: 70.31%] [G loss: 4.123643]\n",
            "480 [D loss: 0.353747, acc.: 85.94%] [G loss: 5.527024]\n",
            "500 [D loss: 0.461812, acc.: 81.25%] [G loss: 4.764277]\n",
            "520 [D loss: 0.530167, acc.: 76.95%] [G loss: 3.841655]\n",
            "540 [D loss: 0.555340, acc.: 71.09%] [G loss: 3.304632]\n",
            "560 [D loss: 0.483774, acc.: 76.17%] [G loss: 4.049955]\n",
            "580 [D loss: 0.421004, acc.: 83.59%] [G loss: 4.117683]\n",
            "600 [D loss: 0.348956, acc.: 87.11%] [G loss: 6.709509]\n",
            "620 [D loss: 0.434774, acc.: 81.25%] [G loss: 5.125196]\n",
            "640 [D loss: 0.442431, acc.: 82.42%] [G loss: 5.988045]\n",
            "660 [D loss: 0.249705, acc.: 92.97%] [G loss: 6.062648]\n",
            "680 [D loss: 0.376368, acc.: 84.77%] [G loss: 5.256093]\n",
            "700 [D loss: 0.355512, acc.: 84.38%] [G loss: 5.441913]\n",
            "720 [D loss: 0.441189, acc.: 80.08%] [G loss: 4.336955]\n",
            "740 [D loss: 0.367871, acc.: 84.38%] [G loss: 6.211957]\n",
            "760 [D loss: 0.578526, acc.: 74.61%] [G loss: 4.620744]\n",
            "780 [D loss: 0.464683, acc.: 78.12%] [G loss: 4.357867]\n",
            "800 [D loss: 0.521821, acc.: 73.44%] [G loss: 4.226654]\n",
            "820 [D loss: 0.450599, acc.: 80.86%] [G loss: 3.852619]\n",
            "840 [D loss: 0.335023, acc.: 87.89%] [G loss: 4.586185]\n",
            "860 [D loss: 0.316415, acc.: 88.28%] [G loss: 5.389889]\n",
            "880 [D loss: 0.378893, acc.: 86.72%] [G loss: 5.102079]\n",
            "900 [D loss: 0.226043, acc.: 91.41%] [G loss: 5.939994]\n",
            "920 [D loss: 0.569105, acc.: 75.78%] [G loss: 3.502247]\n",
            "940 [D loss: 0.403927, acc.: 80.86%] [G loss: 4.837775]\n",
            "960 [D loss: 0.443664, acc.: 80.47%] [G loss: 4.555588]\n",
            "980 [D loss: 0.518110, acc.: 76.95%] [G loss: 4.704926]\n",
            "1000 [D loss: 0.410143, acc.: 81.25%] [G loss: 4.636934]\n",
            "1020 [D loss: 0.474604, acc.: 77.34%] [G loss: 4.021203]\n",
            "1040 [D loss: 0.399894, acc.: 83.20%] [G loss: 4.067788]\n",
            "1060 [D loss: 0.430551, acc.: 83.20%] [G loss: 3.755662]\n",
            "1080 [D loss: 0.450736, acc.: 76.95%] [G loss: 4.119382]\n",
            "1100 [D loss: 0.307155, acc.: 86.72%] [G loss: 5.389581]\n",
            "1120 [D loss: 0.385605, acc.: 84.77%] [G loss: 4.346703]\n",
            "1140 [D loss: 0.309476, acc.: 89.06%] [G loss: 3.579614]\n",
            "1160 [D loss: 0.504216, acc.: 75.00%] [G loss: 4.725137]\n",
            "1180 [D loss: 0.490671, acc.: 78.52%] [G loss: 2.520722]\n",
            "1200 [D loss: 0.582752, acc.: 73.05%] [G loss: 4.495487]\n",
            "1220 [D loss: 0.554456, acc.: 72.27%] [G loss: 4.454854]\n",
            "1240 [D loss: 0.468929, acc.: 77.34%] [G loss: 5.125356]\n",
            "1260 [D loss: 0.449363, acc.: 81.25%] [G loss: 4.754329]\n",
            "1280 [D loss: 0.466651, acc.: 75.78%] [G loss: 4.662492]\n",
            "1300 [D loss: 0.459196, acc.: 80.86%] [G loss: 4.919710]\n",
            "1320 [D loss: 0.329756, acc.: 89.45%] [G loss: 4.802473]\n",
            "1340 [D loss: 0.390559, acc.: 83.98%] [G loss: 4.676436]\n",
            "1360 [D loss: 0.410639, acc.: 81.64%] [G loss: 4.854542]\n",
            "1380 [D loss: 0.365385, acc.: 87.89%] [G loss: 5.119552]\n",
            "1400 [D loss: 0.427065, acc.: 81.25%] [G loss: 4.573692]\n",
            "1420 [D loss: 0.455391, acc.: 76.17%] [G loss: 4.605950]\n",
            "1440 [D loss: 0.492478, acc.: 78.12%] [G loss: 4.142504]\n",
            "1460 [D loss: 0.388323, acc.: 83.98%] [G loss: 4.441108]\n",
            "1480 [D loss: 0.388182, acc.: 82.81%] [G loss: 4.444790]\n",
            "1500 [D loss: 0.425385, acc.: 83.59%] [G loss: 4.049767]\n",
            "1520 [D loss: 0.352237, acc.: 84.38%] [G loss: 4.820994]\n",
            "1540 [D loss: 0.498968, acc.: 78.12%] [G loss: 4.158381]\n",
            "1560 [D loss: 0.452879, acc.: 81.25%] [G loss: 4.094539]\n",
            "1580 [D loss: 0.478842, acc.: 78.91%] [G loss: 4.078547]\n",
            "1600 [D loss: 0.391623, acc.: 84.77%] [G loss: 4.474086]\n",
            "1620 [D loss: 0.479203, acc.: 76.56%] [G loss: 4.104395]\n",
            "1640 [D loss: 0.389700, acc.: 82.03%] [G loss: 4.408888]\n",
            "1660 [D loss: 0.446618, acc.: 80.86%] [G loss: 4.272148]\n",
            "1680 [D loss: 0.444769, acc.: 78.91%] [G loss: 4.212187]\n",
            "1700 [D loss: 0.431153, acc.: 80.47%] [G loss: 5.203035]\n",
            "1720 [D loss: 0.453965, acc.: 78.91%] [G loss: 4.712246]\n",
            "1740 [D loss: 0.508286, acc.: 75.00%] [G loss: 4.286992]\n",
            "1760 [D loss: 0.498963, acc.: 76.56%] [G loss: 2.842791]\n",
            "1780 [D loss: 0.619672, acc.: 67.58%] [G loss: 4.504304]\n",
            "1800 [D loss: 0.441505, acc.: 78.91%] [G loss: 5.014458]\n",
            "1820 [D loss: 0.419353, acc.: 78.91%] [G loss: 4.521346]\n",
            "1840 [D loss: 0.514239, acc.: 76.95%] [G loss: 4.382964]\n",
            "1860 [D loss: 0.545207, acc.: 72.66%] [G loss: 3.874986]\n",
            "1880 [D loss: 0.439717, acc.: 82.03%] [G loss: 4.104360]\n",
            "1900 [D loss: 0.418002, acc.: 80.86%] [G loss: 4.520987]\n",
            "1920 [D loss: 0.383301, acc.: 85.55%] [G loss: 4.684391]\n",
            "1940 [D loss: 0.429643, acc.: 79.69%] [G loss: 4.088441]\n",
            "1960 [D loss: 0.386277, acc.: 84.38%] [G loss: 5.025899]\n",
            "1980 [D loss: 0.490325, acc.: 78.91%] [G loss: 4.409038]\n",
            "2000 [D loss: 0.443531, acc.: 77.34%] [G loss: 4.263247]\n",
            "2020 [D loss: 0.471930, acc.: 77.73%] [G loss: 4.155853]\n",
            "2040 [D loss: 0.405282, acc.: 82.42%] [G loss: 4.153912]\n",
            "2060 [D loss: 0.564275, acc.: 73.05%] [G loss: 3.767768]\n",
            "2080 [D loss: 0.386269, acc.: 84.77%] [G loss: 4.131513]\n",
            "2100 [D loss: 0.477276, acc.: 79.30%] [G loss: 3.838953]\n",
            "2120 [D loss: 0.448158, acc.: 78.12%] [G loss: 4.505565]\n",
            "2140 [D loss: 0.378281, acc.: 84.38%] [G loss: 4.071297]\n",
            "2160 [D loss: 0.331712, acc.: 87.50%] [G loss: 4.664868]\n",
            "2180 [D loss: 0.430585, acc.: 81.64%] [G loss: 4.381789]\n",
            "2200 [D loss: 0.479673, acc.: 77.73%] [G loss: 3.755892]\n",
            "2220 [D loss: 0.493731, acc.: 79.69%] [G loss: 3.357916]\n",
            "2240 [D loss: 0.375713, acc.: 83.20%] [G loss: 3.409735]\n",
            "2260 [D loss: 0.521231, acc.: 76.56%] [G loss: 4.194673]\n",
            "2280 [D loss: 0.545999, acc.: 73.05%] [G loss: 4.055512]\n",
            "2300 [D loss: 0.464169, acc.: 79.69%] [G loss: 4.560557]\n",
            "2320 [D loss: 0.445399, acc.: 77.34%] [G loss: 4.427451]\n",
            "2340 [D loss: 0.435623, acc.: 79.30%] [G loss: 4.832202]\n",
            "2360 [D loss: 0.474513, acc.: 76.95%] [G loss: 4.188791]\n",
            "2380 [D loss: 0.636908, acc.: 66.80%] [G loss: 3.642601]\n",
            "2400 [D loss: 0.453961, acc.: 78.91%] [G loss: 4.379864]\n",
            "2420 [D loss: 0.441630, acc.: 78.12%] [G loss: 4.275010]\n",
            "2440 [D loss: 0.439762, acc.: 78.52%] [G loss: 4.387936]\n",
            "2460 [D loss: 0.383589, acc.: 82.03%] [G loss: 4.191079]\n",
            "2480 [D loss: 0.517784, acc.: 74.22%] [G loss: 3.935201]\n",
            "2500 [D loss: 0.417468, acc.: 79.30%] [G loss: 4.095732]\n",
            "2520 [D loss: 0.473212, acc.: 79.30%] [G loss: 4.148571]\n",
            "2540 [D loss: 0.356661, acc.: 87.89%] [G loss: 4.487892]\n",
            "2560 [D loss: 0.508093, acc.: 76.17%] [G loss: 4.345417]\n",
            "2580 [D loss: 0.439542, acc.: 82.81%] [G loss: 4.560102]\n",
            "2600 [D loss: 0.216789, acc.: 95.70%] [G loss: 2.690942]\n",
            "2620 [D loss: 0.731212, acc.: 61.33%] [G loss: 3.592071]\n",
            "2640 [D loss: 0.536847, acc.: 71.09%] [G loss: 4.104506]\n",
            "2660 [D loss: 0.362454, acc.: 84.77%] [G loss: 4.776493]\n",
            "2680 [D loss: 0.420660, acc.: 81.64%] [G loss: 4.962090]\n",
            "2700 [D loss: 0.390634, acc.: 83.20%] [G loss: 4.423559]\n",
            "2720 [D loss: 0.507321, acc.: 74.61%] [G loss: 4.024459]\n",
            "2740 [D loss: 0.446722, acc.: 79.69%] [G loss: 4.458115]\n",
            "2760 [D loss: 0.476247, acc.: 78.52%] [G loss: 3.912058]\n",
            "2780 [D loss: 0.511966, acc.: 75.39%] [G loss: 4.133390]\n",
            "2800 [D loss: 0.461249, acc.: 76.56%] [G loss: 4.121226]\n",
            "2820 [D loss: 0.423301, acc.: 81.25%] [G loss: 4.386965]\n",
            "2840 [D loss: 0.478192, acc.: 78.52%] [G loss: 4.652570]\n",
            "2860 [D loss: 0.538536, acc.: 73.44%] [G loss: 4.251510]\n",
            "2880 [D loss: 0.566911, acc.: 72.27%] [G loss: 3.897439]\n",
            "2900 [D loss: 0.503513, acc.: 74.22%] [G loss: 4.274117]\n",
            "2920 [D loss: 0.471095, acc.: 76.95%] [G loss: 4.527729]\n",
            "2940 [D loss: 0.509491, acc.: 73.83%] [G loss: 3.828006]\n",
            "2960 [D loss: 0.414633, acc.: 82.42%] [G loss: 4.173753]\n",
            "2980 [D loss: 0.419897, acc.: 80.86%] [G loss: 3.272243]\n",
            "3000 [D loss: 0.331920, acc.: 86.72%] [G loss: 4.162461]\n",
            "3020 [D loss: 0.401505, acc.: 83.98%] [G loss: 4.282913]\n",
            "3040 [D loss: 0.578782, acc.: 71.09%] [G loss: 4.051863]\n",
            "3060 [D loss: 0.520983, acc.: 76.17%] [G loss: 4.321439]\n",
            "3080 [D loss: 0.562389, acc.: 71.09%] [G loss: 4.515550]\n",
            "3100 [D loss: 0.392553, acc.: 82.03%] [G loss: 4.187614]\n",
            "3120 [D loss: 0.406051, acc.: 82.03%] [G loss: 4.667347]\n",
            "3140 [D loss: 0.439969, acc.: 82.42%] [G loss: 4.213274]\n",
            "3160 [D loss: 0.527634, acc.: 76.56%] [G loss: 4.088833]\n",
            "3180 [D loss: 0.438061, acc.: 79.69%] [G loss: 4.134293]\n",
            "3200 [D loss: 0.561129, acc.: 71.48%] [G loss: 3.942729]\n",
            "3220 [D loss: 0.423031, acc.: 82.42%] [G loss: 3.674069]\n",
            "3240 [D loss: 0.428707, acc.: 80.08%] [G loss: 4.689942]\n",
            "3260 [D loss: 0.477272, acc.: 76.95%] [G loss: 3.746596]\n",
            "3280 [D loss: 0.593261, acc.: 69.53%] [G loss: 3.441195]\n",
            "3300 [D loss: 0.498350, acc.: 76.95%] [G loss: 4.232565]\n",
            "3320 [D loss: 0.495037, acc.: 77.34%] [G loss: 3.806185]\n",
            "3340 [D loss: 0.400727, acc.: 82.81%] [G loss: 4.376815]\n",
            "3360 [D loss: 0.434275, acc.: 80.47%] [G loss: 4.003138]\n",
            "3380 [D loss: 0.483277, acc.: 77.34%] [G loss: 3.521255]\n",
            "3400 [D loss: 0.308028, acc.: 89.06%] [G loss: 4.608510]\n",
            "3420 [D loss: 0.540470, acc.: 71.88%] [G loss: 3.937994]\n",
            "3440 [D loss: 0.460247, acc.: 77.73%] [G loss: 4.426981]\n",
            "3460 [D loss: 0.487335, acc.: 77.34%] [G loss: 4.131818]\n",
            "3480 [D loss: 0.488182, acc.: 77.73%] [G loss: 4.156597]\n",
            "3500 [D loss: 0.550127, acc.: 70.70%] [G loss: 3.999867]\n",
            "3520 [D loss: 0.415385, acc.: 82.03%] [G loss: 4.072405]\n",
            "3540 [D loss: 0.480278, acc.: 74.22%] [G loss: 4.176293]\n",
            "3560 [D loss: 0.395484, acc.: 83.98%] [G loss: 4.407483]\n",
            "3580 [D loss: 0.404347, acc.: 80.08%] [G loss: 4.558132]\n",
            "3600 [D loss: 0.526902, acc.: 72.66%] [G loss: 4.117194]\n",
            "3620 [D loss: 0.512232, acc.: 76.17%] [G loss: 3.986052]\n",
            "3640 [D loss: 0.327206, acc.: 89.45%] [G loss: 2.877409]\n",
            "3660 [D loss: 0.792728, acc.: 63.28%] [G loss: 4.835374]\n",
            "3680 [D loss: 0.410874, acc.: 83.20%] [G loss: 5.261364]\n",
            "3700 [D loss: 0.536477, acc.: 75.78%] [G loss: 4.182532]\n",
            "3720 [D loss: 0.535142, acc.: 75.39%] [G loss: 3.942051]\n",
            "3740 [D loss: 0.334086, acc.: 89.06%] [G loss: 5.431086]\n",
            "3760 [D loss: 0.425293, acc.: 82.03%] [G loss: 4.654212]\n",
            "3780 [D loss: 0.438517, acc.: 79.69%] [G loss: 4.045759]\n",
            "3800 [D loss: 0.480406, acc.: 75.00%] [G loss: 4.297606]\n",
            "3820 [D loss: 0.406764, acc.: 84.77%] [G loss: 4.522343]\n",
            "3840 [D loss: 0.425133, acc.: 79.69%] [G loss: 4.058409]\n",
            "3860 [D loss: 0.405429, acc.: 82.03%] [G loss: 4.401647]\n",
            "3880 [D loss: 0.491282, acc.: 76.95%] [G loss: 3.845786]\n",
            "3900 [D loss: 0.466599, acc.: 78.12%] [G loss: 4.074653]\n",
            "3920 [D loss: 0.532860, acc.: 72.27%] [G loss: 3.684530]\n",
            "3940 [D loss: 0.530292, acc.: 72.66%] [G loss: 3.939835]\n",
            "3960 [D loss: 0.401591, acc.: 82.03%] [G loss: 3.881752]\n",
            "3980 [D loss: 0.581516, acc.: 68.75%] [G loss: 3.860662]\n",
            "4000 [D loss: 0.427375, acc.: 80.08%] [G loss: 3.877365]\n",
            "4020 [D loss: 0.540918, acc.: 74.22%] [G loss: 3.756370]\n",
            "4040 [D loss: 0.463080, acc.: 80.08%] [G loss: 4.305838]\n",
            "4060 [D loss: 0.476499, acc.: 78.52%] [G loss: 4.075773]\n",
            "4080 [D loss: 0.512597, acc.: 73.83%] [G loss: 4.284355]\n",
            "4100 [D loss: 0.427404, acc.: 80.86%] [G loss: 4.426580]\n",
            "4120 [D loss: 0.492242, acc.: 75.00%] [G loss: 3.969121]\n",
            "4140 [D loss: 0.444425, acc.: 78.91%] [G loss: 4.009696]\n",
            "4160 [D loss: 0.453538, acc.: 77.73%] [G loss: 3.953142]\n",
            "4180 [D loss: 0.435117, acc.: 83.98%] [G loss: 4.096146]\n",
            "4200 [D loss: 0.499150, acc.: 76.95%] [G loss: 3.997583]\n",
            "4220 [D loss: 0.494878, acc.: 77.34%] [G loss: 4.359426]\n",
            "4240 [D loss: 0.427009, acc.: 80.47%] [G loss: 4.348926]\n",
            "4260 [D loss: 0.515187, acc.: 71.88%] [G loss: 3.914362]\n",
            "4280 [D loss: 0.441361, acc.: 79.69%] [G loss: 4.746194]\n",
            "4300 [D loss: 0.495599, acc.: 76.17%] [G loss: 4.316066]\n",
            "4320 [D loss: 0.426878, acc.: 83.59%] [G loss: 4.247961]\n",
            "4340 [D loss: 0.639323, acc.: 64.45%] [G loss: 2.437271]\n",
            "4360 [D loss: 0.568334, acc.: 68.36%] [G loss: 2.911324]\n",
            "4380 [D loss: 0.544364, acc.: 74.22%] [G loss: 3.147603]\n",
            "4400 [D loss: 0.473773, acc.: 76.56%] [G loss: 3.629950]\n",
            "4420 [D loss: 0.598718, acc.: 68.36%] [G loss: 2.604429]\n",
            "4440 [D loss: 0.517180, acc.: 75.78%] [G loss: 3.228624]\n",
            "4460 [D loss: 0.466363, acc.: 76.95%] [G loss: 3.677693]\n",
            "4480 [D loss: 0.564348, acc.: 70.31%] [G loss: 3.196965]\n",
            "4500 [D loss: 0.426343, acc.: 81.25%] [G loss: 3.941340]\n",
            "4520 [D loss: 0.416817, acc.: 80.47%] [G loss: 3.916047]\n",
            "4540 [D loss: 0.558782, acc.: 73.44%] [G loss: 4.025647]\n",
            "4560 [D loss: 0.408034, acc.: 83.98%] [G loss: 4.185200]\n",
            "4580 [D loss: 0.481999, acc.: 76.17%] [G loss: 3.991818]\n",
            "4600 [D loss: 0.499504, acc.: 75.78%] [G loss: 4.312444]\n",
            "4620 [D loss: 0.430090, acc.: 80.08%] [G loss: 4.106532]\n",
            "4640 [D loss: 0.452346, acc.: 78.12%] [G loss: 4.407618]\n",
            "4660 [D loss: 0.608290, acc.: 65.62%] [G loss: 4.264530]\n",
            "4680 [D loss: 0.369565, acc.: 86.33%] [G loss: 3.295152]\n",
            "4700 [D loss: 0.499483, acc.: 76.95%] [G loss: 4.250195]\n",
            "4720 [D loss: 0.499709, acc.: 74.61%] [G loss: 3.986491]\n",
            "4740 [D loss: 0.602993, acc.: 73.05%] [G loss: 3.605597]\n",
            "4760 [D loss: 0.459115, acc.: 79.69%] [G loss: 4.067763]\n",
            "4780 [D loss: 0.427150, acc.: 80.86%] [G loss: 3.794364]\n",
            "4800 [D loss: 0.476261, acc.: 77.73%] [G loss: 3.773816]\n",
            "4820 [D loss: 0.415599, acc.: 80.86%] [G loss: 3.981258]\n",
            "4840 [D loss: 0.390491, acc.: 81.64%] [G loss: 4.366356]\n",
            "4860 [D loss: 0.586854, acc.: 69.14%] [G loss: 4.084357]\n",
            "4880 [D loss: 0.350640, acc.: 83.59%] [G loss: 5.151140]\n",
            "4900 [D loss: 0.486270, acc.: 76.56%] [G loss: 4.562852]\n",
            "4920 [D loss: 0.381943, acc.: 84.77%] [G loss: 4.672520]\n",
            "4940 [D loss: 0.518870, acc.: 74.22%] [G loss: 4.136309]\n",
            "4960 [D loss: 0.462939, acc.: 78.12%] [G loss: 4.398291]\n",
            "4980 [D loss: 0.522935, acc.: 75.78%] [G loss: 3.797438]\n",
            "5000 [D loss: 0.453190, acc.: 78.12%] [G loss: 4.274545]\n",
            "5020 [D loss: 0.465346, acc.: 75.00%] [G loss: 3.996341]\n",
            "5040 [D loss: 0.561815, acc.: 69.92%] [G loss: 3.652460]\n",
            "5060 [D loss: 0.545154, acc.: 73.44%] [G loss: 4.136659]\n",
            "5080 [D loss: 0.467085, acc.: 76.17%] [G loss: 3.958155]\n",
            "5100 [D loss: 0.503473, acc.: 76.17%] [G loss: 3.664615]\n",
            "5120 [D loss: 0.527655, acc.: 75.00%] [G loss: 4.895439]\n",
            "5140 [D loss: 0.498521, acc.: 76.56%] [G loss: 4.125403]\n",
            "5160 [D loss: 0.488412, acc.: 78.12%] [G loss: 4.253254]\n",
            "5180 [D loss: 0.447892, acc.: 80.47%] [G loss: 3.549400]\n",
            "5200 [D loss: 0.443360, acc.: 76.95%] [G loss: 4.343549]\n",
            "5220 [D loss: 0.491074, acc.: 74.22%] [G loss: 3.902834]\n",
            "5240 [D loss: 0.500246, acc.: 73.83%] [G loss: 3.808812]\n",
            "5260 [D loss: 0.430264, acc.: 79.30%] [G loss: 4.463462]\n",
            "5280 [D loss: 0.402872, acc.: 84.77%] [G loss: 4.915029]\n",
            "5300 [D loss: 0.420527, acc.: 82.03%] [G loss: 4.833241]\n",
            "5320 [D loss: 0.468783, acc.: 76.95%] [G loss: 4.857640]\n",
            "5340 [D loss: 0.416712, acc.: 80.47%] [G loss: 4.002873]\n",
            "5360 [D loss: 0.416486, acc.: 81.64%] [G loss: 4.546247]\n",
            "5380 [D loss: 0.530327, acc.: 74.22%] [G loss: 4.335669]\n",
            "5400 [D loss: 0.461124, acc.: 75.00%] [G loss: 3.782178]\n",
            "5420 [D loss: 0.481268, acc.: 76.17%] [G loss: 4.475326]\n",
            "5440 [D loss: 0.433932, acc.: 79.30%] [G loss: 4.582110]\n",
            "5460 [D loss: 0.444800, acc.: 78.91%] [G loss: 3.931216]\n",
            "5480 [D loss: 0.420835, acc.: 81.64%] [G loss: 4.240787]\n",
            "5500 [D loss: 0.634849, acc.: 67.97%] [G loss: 3.052122]\n",
            "5520 [D loss: 0.417105, acc.: 83.59%] [G loss: 4.840692]\n",
            "5540 [D loss: 0.539837, acc.: 73.44%] [G loss: 4.198668]\n",
            "5560 [D loss: 0.517309, acc.: 75.39%] [G loss: 4.055507]\n",
            "5580 [D loss: 0.530299, acc.: 73.05%] [G loss: 4.319265]\n",
            "5600 [D loss: 0.419041, acc.: 81.64%] [G loss: 4.279836]\n",
            "5620 [D loss: 0.545253, acc.: 72.27%] [G loss: 4.009728]\n",
            "5640 [D loss: 0.495153, acc.: 75.00%] [G loss: 4.396716]\n",
            "5660 [D loss: 0.570280, acc.: 73.44%] [G loss: 4.052394]\n",
            "5680 [D loss: 0.480883, acc.: 75.78%] [G loss: 3.998073]\n",
            "5700 [D loss: 0.435515, acc.: 78.91%] [G loss: 3.971066]\n",
            "5720 [D loss: 0.480389, acc.: 78.12%] [G loss: 3.847019]\n",
            "5740 [D loss: 0.435918, acc.: 80.47%] [G loss: 4.342231]\n",
            "5760 [D loss: 0.515387, acc.: 73.83%] [G loss: 3.921098]\n",
            "5780 [D loss: 0.420246, acc.: 81.64%] [G loss: 4.241726]\n",
            "5800 [D loss: 0.527451, acc.: 73.44%] [G loss: 4.459811]\n",
            "5820 [D loss: 0.489472, acc.: 75.78%] [G loss: 4.195115]\n",
            "5840 [D loss: 0.334280, acc.: 85.94%] [G loss: 5.181224]\n",
            "5860 [D loss: 0.558214, acc.: 72.66%] [G loss: 4.378734]\n",
            "5880 [D loss: 0.382991, acc.: 84.38%] [G loss: 4.697106]\n",
            "5900 [D loss: 0.494181, acc.: 73.83%] [G loss: 3.529606]\n",
            "5920 [D loss: 0.495572, acc.: 77.34%] [G loss: 3.208685]\n",
            "5940 [D loss: 0.446739, acc.: 79.30%] [G loss: 4.420391]\n",
            "5960 [D loss: 0.544341, acc.: 73.83%] [G loss: 4.504405]\n",
            "5980 [D loss: 0.426185, acc.: 81.25%] [G loss: 4.541722]\n",
            "6000 [D loss: 0.574089, acc.: 69.92%] [G loss: 4.523565]\n",
            "6020 [D loss: 0.452203, acc.: 78.12%] [G loss: 4.342885]\n",
            "6040 [D loss: 0.368858, acc.: 86.33%] [G loss: 5.184176]\n",
            "6060 [D loss: 0.573542, acc.: 70.31%] [G loss: 4.005151]\n",
            "6080 [D loss: 0.449174, acc.: 79.69%] [G loss: 4.552698]\n",
            "6100 [D loss: 0.449649, acc.: 81.64%] [G loss: 4.649699]\n",
            "6120 [D loss: 0.490286, acc.: 78.12%] [G loss: 4.512897]\n",
            "6140 [D loss: 0.497448, acc.: 74.61%] [G loss: 4.460680]\n",
            "6160 [D loss: 0.455986, acc.: 76.56%] [G loss: 4.122546]\n",
            "6180 [D loss: 0.435562, acc.: 78.52%] [G loss: 4.172760]\n",
            "6200 [D loss: 0.356183, acc.: 83.98%] [G loss: 4.781911]\n",
            "6220 [D loss: 0.463660, acc.: 75.00%] [G loss: 4.412989]\n",
            "6240 [D loss: 0.414186, acc.: 82.03%] [G loss: 4.831032]\n",
            "6260 [D loss: 0.372417, acc.: 85.16%] [G loss: 5.362244]\n",
            "6280 [D loss: 0.467880, acc.: 76.17%] [G loss: 4.360380]\n",
            "6300 [D loss: 0.400332, acc.: 82.81%] [G loss: 4.798859]\n",
            "6320 [D loss: 0.408534, acc.: 82.03%] [G loss: 4.388117]\n",
            "6340 [D loss: 0.461838, acc.: 76.56%] [G loss: 4.608136]\n",
            "6360 [D loss: 0.554654, acc.: 69.53%] [G loss: 4.221312]\n",
            "6380 [D loss: 0.455452, acc.: 80.08%] [G loss: 4.245480]\n",
            "6400 [D loss: 0.435096, acc.: 81.25%] [G loss: 4.739724]\n",
            "6420 [D loss: 0.486670, acc.: 76.17%] [G loss: 4.400792]\n",
            "6440 [D loss: 0.397072, acc.: 81.25%] [G loss: 4.724797]\n",
            "6460 [D loss: 0.474373, acc.: 76.56%] [G loss: 4.318533]\n",
            "6480 [D loss: 0.466955, acc.: 76.56%] [G loss: 4.036880]\n",
            "6500 [D loss: 0.430213, acc.: 81.64%] [G loss: 4.957911]\n",
            "6520 [D loss: 0.380296, acc.: 84.38%] [G loss: 4.335943]\n",
            "6540 [D loss: 0.359202, acc.: 85.94%] [G loss: 5.080183]\n",
            "6560 [D loss: 0.455271, acc.: 79.69%] [G loss: 4.850559]\n",
            "6580 [D loss: 0.455480, acc.: 77.34%] [G loss: 4.772310]\n",
            "6600 [D loss: 0.395046, acc.: 84.38%] [G loss: 5.063589]\n",
            "6620 [D loss: 0.482585, acc.: 76.56%] [G loss: 4.393572]\n",
            "6640 [D loss: 0.411519, acc.: 82.81%] [G loss: 4.253354]\n",
            "6660 [D loss: 0.380085, acc.: 86.33%] [G loss: 3.355166]\n",
            "6680 [D loss: 0.618617, acc.: 65.23%] [G loss: 3.723468]\n",
            "6700 [D loss: 0.479589, acc.: 79.30%] [G loss: 5.173312]\n",
            "6720 [D loss: 0.399319, acc.: 85.55%] [G loss: 4.599520]\n",
            "6740 [D loss: 0.545668, acc.: 71.88%] [G loss: 3.999143]\n",
            "6760 [D loss: 0.387638, acc.: 84.77%] [G loss: 5.160907]\n",
            "6780 [D loss: 0.361547, acc.: 83.59%] [G loss: 5.018031]\n",
            "6800 [D loss: 0.440656, acc.: 81.64%] [G loss: 5.037896]\n",
            "6820 [D loss: 0.434474, acc.: 79.69%] [G loss: 4.727262]\n",
            "6840 [D loss: 0.596491, acc.: 66.80%] [G loss: 4.193734]\n",
            "6860 [D loss: 0.422367, acc.: 80.08%] [G loss: 4.742515]\n",
            "6880 [D loss: 0.422333, acc.: 79.69%] [G loss: 5.133650]\n",
            "6900 [D loss: 0.471970, acc.: 76.95%] [G loss: 4.696536]\n",
            "6920 [D loss: 0.494275, acc.: 76.17%] [G loss: 4.803502]\n",
            "6940 [D loss: 0.430861, acc.: 78.52%] [G loss: 4.893723]\n",
            "6960 [D loss: 0.480185, acc.: 75.78%] [G loss: 4.586589]\n",
            "6980 [D loss: 0.394835, acc.: 84.77%] [G loss: 4.788989]\n",
            "7000 [D loss: 0.422100, acc.: 81.25%] [G loss: 5.173104]\n",
            "7020 [D loss: 0.372996, acc.: 83.98%] [G loss: 5.151712]\n",
            "7040 [D loss: 0.452881, acc.: 76.17%] [G loss: 4.508017]\n",
            "7060 [D loss: 0.365007, acc.: 86.72%] [G loss: 4.782337]\n",
            "7080 [D loss: 0.449266, acc.: 80.47%] [G loss: 4.573940]\n",
            "7100 [D loss: 0.374245, acc.: 83.59%] [G loss: 5.167995]\n",
            "7120 [D loss: 0.425651, acc.: 80.47%] [G loss: 4.627223]\n",
            "7140 [D loss: 0.511015, acc.: 73.44%] [G loss: 4.050308]\n",
            "7160 [D loss: 0.363580, acc.: 85.94%] [G loss: 4.933572]\n",
            "7180 [D loss: 0.435191, acc.: 82.81%] [G loss: 4.449340]\n",
            "7200 [D loss: 93.008862, acc.: 33.98%] [G loss: 6.433070]\n",
            "7220 [D loss: 1.082236, acc.: 27.73%] [G loss: 1.665594]\n",
            "7240 [D loss: 1.161771, acc.: 24.22%] [G loss: 1.800719]\n",
            "7260 [D loss: 0.780439, acc.: 46.88%] [G loss: 1.384255]\n",
            "7280 [D loss: 0.966307, acc.: 32.42%] [G loss: 2.024988]\n",
            "7300 [D loss: 0.819228, acc.: 46.09%] [G loss: 1.941742]\n",
            "7320 [D loss: 0.818466, acc.: 47.27%] [G loss: 1.839249]\n",
            "7340 [D loss: 0.747050, acc.: 56.64%] [G loss: 1.947170]\n",
            "7360 [D loss: 0.694975, acc.: 59.77%] [G loss: 2.193023]\n",
            "7380 [D loss: 0.596408, acc.: 67.58%] [G loss: 2.627534]\n",
            "7400 [D loss: 0.676913, acc.: 60.94%] [G loss: 2.301745]\n",
            "7420 [D loss: 0.648126, acc.: 59.77%] [G loss: 2.367830]\n",
            "7440 [D loss: 0.648464, acc.: 63.67%] [G loss: 2.268638]\n",
            "7460 [D loss: 0.626339, acc.: 65.62%] [G loss: 2.325616]\n",
            "7480 [D loss: 0.621956, acc.: 66.41%] [G loss: 2.347293]\n",
            "7500 [D loss: 0.576653, acc.: 66.02%] [G loss: 2.658233]\n",
            "7520 [D loss: 0.570155, acc.: 71.88%] [G loss: 2.848550]\n",
            "7540 [D loss: 0.523893, acc.: 75.00%] [G loss: 2.680045]\n",
            "7560 [D loss: 0.678904, acc.: 63.28%] [G loss: 1.947879]\n",
            "7580 [D loss: 0.678473, acc.: 58.59%] [G loss: 2.311602]\n",
            "7600 [D loss: 0.675170, acc.: 57.42%] [G loss: 2.336698]\n",
            "7620 [D loss: 0.675074, acc.: 61.72%] [G loss: 2.611678]\n",
            "7640 [D loss: 0.590346, acc.: 66.41%] [G loss: 2.969736]\n",
            "7660 [D loss: 0.509903, acc.: 74.61%] [G loss: 3.031069]\n",
            "7680 [D loss: 0.592270, acc.: 67.58%] [G loss: 2.608442]\n",
            "7700 [D loss: 0.488741, acc.: 77.73%] [G loss: 2.943946]\n",
            "7720 [D loss: 0.576908, acc.: 69.14%] [G loss: 3.318761]\n",
            "7740 [D loss: 0.524383, acc.: 74.61%] [G loss: 2.886499]\n",
            "7760 [D loss: 0.555215, acc.: 71.88%] [G loss: 2.654543]\n",
            "7780 [D loss: 0.520132, acc.: 75.39%] [G loss: 3.351314]\n",
            "7800 [D loss: 0.602197, acc.: 70.70%] [G loss: 2.938179]\n",
            "7820 [D loss: 0.529868, acc.: 73.83%] [G loss: 2.966575]\n",
            "7840 [D loss: 0.512180, acc.: 74.22%] [G loss: 3.830128]\n",
            "7860 [D loss: 0.574921, acc.: 69.14%] [G loss: 3.047951]\n",
            "7880 [D loss: 0.493401, acc.: 76.95%] [G loss: 3.285118]\n",
            "7900 [D loss: 0.558279, acc.: 71.88%] [G loss: 3.116910]\n",
            "7920 [D loss: 0.490184, acc.: 78.12%] [G loss: 3.635678]\n",
            "7940 [D loss: 0.529040, acc.: 73.05%] [G loss: 3.464617]\n",
            "7960 [D loss: 0.508088, acc.: 73.05%] [G loss: 3.568749]\n",
            "7980 [D loss: 0.475294, acc.: 78.52%] [G loss: 3.660685]\n",
            "8000 [D loss: 0.465687, acc.: 79.30%] [G loss: 4.465701]\n",
            "8020 [D loss: 0.577261, acc.: 67.58%] [G loss: 3.965286]\n",
            "8040 [D loss: 0.501819, acc.: 76.17%] [G loss: 3.306842]\n",
            "8060 [D loss: 0.506293, acc.: 73.83%] [G loss: 4.249799]\n",
            "8080 [D loss: 0.448338, acc.: 80.08%] [G loss: 4.264654]\n",
            "8100 [D loss: 0.534540, acc.: 73.44%] [G loss: 3.278389]\n",
            "8120 [D loss: 0.457930, acc.: 76.95%] [G loss: 4.320199]\n",
            "8140 [D loss: 0.557190, acc.: 71.09%] [G loss: 2.918166]\n",
            "8160 [D loss: 0.467177, acc.: 77.34%] [G loss: 3.455966]\n",
            "8180 [D loss: 0.610672, acc.: 69.14%] [G loss: 3.183360]\n",
            "8200 [D loss: 0.461481, acc.: 78.91%] [G loss: 3.777412]\n",
            "8220 [D loss: 0.486920, acc.: 71.48%] [G loss: 4.076422]\n",
            "8240 [D loss: 0.506653, acc.: 73.83%] [G loss: 3.672002]\n",
            "8260 [D loss: 0.552002, acc.: 70.70%] [G loss: 2.949643]\n",
            "8280 [D loss: 0.414068, acc.: 80.08%] [G loss: 4.215812]\n",
            "8300 [D loss: 0.494668, acc.: 77.73%] [G loss: 3.675337]\n",
            "8320 [D loss: 0.589216, acc.: 66.80%] [G loss: 3.121263]\n",
            "8340 [D loss: 0.490102, acc.: 75.78%] [G loss: 3.307297]\n",
            "8360 [D loss: 0.502944, acc.: 74.61%] [G loss: 3.674286]\n",
            "8380 [D loss: 0.436642, acc.: 80.08%] [G loss: 4.169909]\n",
            "8400 [D loss: 0.469008, acc.: 79.30%] [G loss: 3.572909]\n",
            "8420 [D loss: 0.482852, acc.: 75.78%] [G loss: 3.393866]\n",
            "8440 [D loss: 0.591594, acc.: 70.70%] [G loss: 2.660285]\n",
            "8460 [D loss: 0.487727, acc.: 76.56%] [G loss: 3.786855]\n",
            "8480 [D loss: 0.588033, acc.: 70.70%] [G loss: 4.064078]\n",
            "8500 [D loss: 0.561217, acc.: 70.70%] [G loss: 3.537051]\n",
            "8520 [D loss: 0.529635, acc.: 70.70%] [G loss: 3.839049]\n",
            "8540 [D loss: 0.532337, acc.: 73.44%] [G loss: 4.458511]\n",
            "8560 [D loss: 0.489783, acc.: 74.22%] [G loss: 3.878422]\n",
            "8580 [D loss: 0.526897, acc.: 76.17%] [G loss: 3.451127]\n",
            "8600 [D loss: 0.488135, acc.: 74.61%] [G loss: 3.686475]\n",
            "8620 [D loss: 0.559798, acc.: 72.27%] [G loss: 3.520478]\n",
            "8640 [D loss: 0.428729, acc.: 81.25%] [G loss: 4.009608]\n",
            "8660 [D loss: 0.526730, acc.: 73.44%] [G loss: 4.090393]\n",
            "8680 [D loss: 0.431749, acc.: 80.08%] [G loss: 4.397148]\n",
            "8700 [D loss: 0.461498, acc.: 80.08%] [G loss: 4.038422]\n",
            "8720 [D loss: 0.508642, acc.: 73.05%] [G loss: 4.477991]\n",
            "8740 [D loss: 0.618403, acc.: 67.19%] [G loss: 3.840793]\n",
            "8760 [D loss: 0.532822, acc.: 74.22%] [G loss: 4.354614]\n",
            "8780 [D loss: 0.475769, acc.: 76.95%] [G loss: 3.972570]\n",
            "8800 [D loss: 0.444487, acc.: 79.30%] [G loss: 4.407744]\n",
            "8820 [D loss: 0.505855, acc.: 73.44%] [G loss: 4.336640]\n",
            "8840 [D loss: 0.521333, acc.: 71.09%] [G loss: 3.837093]\n",
            "8860 [D loss: 0.447927, acc.: 80.47%] [G loss: 3.559058]\n",
            "8880 [D loss: 0.514541, acc.: 75.78%] [G loss: 4.298106]\n",
            "8900 [D loss: 0.421530, acc.: 82.42%] [G loss: 4.140854]\n",
            "8920 [D loss: 0.565912, acc.: 71.88%] [G loss: 3.871551]\n",
            "8940 [D loss: 0.450803, acc.: 79.30%] [G loss: 4.014162]\n",
            "8960 [D loss: 0.569475, acc.: 69.92%] [G loss: 4.234999]\n",
            "8980 [D loss: 0.407795, acc.: 83.20%] [G loss: 4.972523]\n",
            "9000 [D loss: 0.504159, acc.: 75.78%] [G loss: 4.384655]\n",
            "9020 [D loss: 0.443010, acc.: 78.12%] [G loss: 3.870542]\n",
            "9040 [D loss: 0.520477, acc.: 73.05%] [G loss: 3.554924]\n",
            "9060 [D loss: 0.469677, acc.: 77.34%] [G loss: 3.714620]\n",
            "9080 [D loss: 0.411901, acc.: 81.25%] [G loss: 4.520152]\n",
            "9100 [D loss: 0.582923, acc.: 68.36%] [G loss: 3.902362]\n",
            "9120 [D loss: 0.415486, acc.: 82.03%] [G loss: 3.983042]\n",
            "9140 [D loss: 0.512428, acc.: 74.22%] [G loss: 4.260905]\n",
            "9160 [D loss: 0.470925, acc.: 80.08%] [G loss: 3.393573]\n",
            "9180 [D loss: 0.464467, acc.: 77.73%] [G loss: 4.501943]\n",
            "9200 [D loss: 0.452724, acc.: 80.08%] [G loss: 4.262903]\n",
            "9220 [D loss: 0.494720, acc.: 77.34%] [G loss: 3.765383]\n",
            "9240 [D loss: 0.512667, acc.: 73.05%] [G loss: 3.772371]\n",
            "9260 [D loss: 0.417422, acc.: 80.86%] [G loss: 4.855905]\n",
            "9280 [D loss: 0.492838, acc.: 74.61%] [G loss: 4.112430]\n",
            "9300 [D loss: 0.446068, acc.: 78.91%] [G loss: 4.441000]\n",
            "9320 [D loss: 0.506981, acc.: 75.39%] [G loss: 3.644724]\n",
            "9340 [D loss: 0.416127, acc.: 81.25%] [G loss: 4.414407]\n",
            "9360 [D loss: 0.518369, acc.: 74.61%] [G loss: 3.829485]\n",
            "9380 [D loss: 0.354843, acc.: 86.72%] [G loss: 5.357634]\n",
            "9400 [D loss: 0.451693, acc.: 79.69%] [G loss: 3.715940]\n",
            "9420 [D loss: 0.431418, acc.: 80.47%] [G loss: 4.559270]\n",
            "9440 [D loss: 0.432290, acc.: 82.03%] [G loss: 4.093608]\n",
            "9460 [D loss: 0.523145, acc.: 71.09%] [G loss: 4.163167]\n",
            "9480 [D loss: 0.437573, acc.: 80.86%] [G loss: 4.658608]\n",
            "9500 [D loss: 0.514255, acc.: 75.39%] [G loss: 3.676113]\n",
            "9520 [D loss: 0.377480, acc.: 85.94%] [G loss: 4.497245]\n",
            "9540 [D loss: 0.489691, acc.: 76.56%] [G loss: 3.935603]\n",
            "9560 [D loss: 0.486709, acc.: 79.30%] [G loss: 5.533547]\n",
            "9580 [D loss: 0.408381, acc.: 80.08%] [G loss: 4.730929]\n",
            "9600 [D loss: 0.437920, acc.: 79.69%] [G loss: 4.299321]\n",
            "9620 [D loss: 0.480623, acc.: 77.34%] [G loss: 3.938702]\n",
            "9640 [D loss: 0.456292, acc.: 80.47%] [G loss: 4.197849]\n",
            "9660 [D loss: 0.451994, acc.: 77.34%] [G loss: 3.831512]\n",
            "9680 [D loss: 0.424929, acc.: 82.03%] [G loss: 4.576011]\n",
            "9700 [D loss: 0.489933, acc.: 77.34%] [G loss: 4.109711]\n",
            "9720 [D loss: 0.395240, acc.: 83.59%] [G loss: 4.531324]\n",
            "9740 [D loss: 0.432758, acc.: 80.47%] [G loss: 4.461974]\n",
            "9760 [D loss: 0.443328, acc.: 79.69%] [G loss: 4.458419]\n",
            "9780 [D loss: 0.451786, acc.: 80.47%] [G loss: 4.180616]\n",
            "9800 [D loss: 0.437896, acc.: 80.86%] [G loss: 3.105220]\n",
            "9820 [D loss: 0.429956, acc.: 78.91%] [G loss: 5.089725]\n",
            "9840 [D loss: 0.618707, acc.: 69.92%] [G loss: 3.111432]\n",
            "9860 [D loss: 0.398330, acc.: 82.42%] [G loss: 4.563382]\n",
            "9880 [D loss: 0.441010, acc.: 82.81%] [G loss: 5.034310]\n",
            "9900 [D loss: 0.426148, acc.: 80.86%] [G loss: 5.244269]\n",
            "9920 [D loss: 0.422411, acc.: 82.03%] [G loss: 4.404534]\n",
            "9940 [D loss: 0.462902, acc.: 80.47%] [G loss: 4.661936]\n",
            "9960 [D loss: 0.584217, acc.: 71.88%] [G loss: 2.665930]\n",
            "9980 [D loss: 0.556666, acc.: 73.05%] [G loss: 4.699268]\n",
            "10000 [D loss: 0.496088, acc.: 75.00%] [G loss: 4.701067]\n",
            "10020 [D loss: 0.585284, acc.: 71.88%] [G loss: 4.188406]\n",
            "10040 [D loss: 0.378083, acc.: 85.55%] [G loss: 4.352952]\n",
            "10060 [D loss: 0.440564, acc.: 80.86%] [G loss: 4.932684]\n",
            "10080 [D loss: 0.407784, acc.: 80.47%] [G loss: 5.594504]\n",
            "10100 [D loss: 0.472643, acc.: 76.56%] [G loss: 4.925526]\n",
            "10120 [D loss: 0.453263, acc.: 77.34%] [G loss: 4.015405]\n",
            "10140 [D loss: 0.497309, acc.: 77.73%] [G loss: 4.180937]\n",
            "10160 [D loss: 0.414588, acc.: 83.59%] [G loss: 4.881147]\n",
            "10180 [D loss: 0.448705, acc.: 78.12%] [G loss: 4.564996]\n",
            "10200 [D loss: 0.357451, acc.: 85.55%] [G loss: 4.356048]\n",
            "10220 [D loss: 0.397448, acc.: 82.03%] [G loss: 4.937330]\n",
            "10240 [D loss: 0.440293, acc.: 80.47%] [G loss: 4.748587]\n",
            "10260 [D loss: 0.378383, acc.: 83.59%] [G loss: 4.451899]\n",
            "10280 [D loss: 0.390426, acc.: 84.77%] [G loss: 5.114259]\n",
            "10300 [D loss: 0.498859, acc.: 76.17%] [G loss: 4.356938]\n",
            "10320 [D loss: 0.408494, acc.: 80.47%] [G loss: 4.150654]\n",
            "10340 [D loss: 0.457711, acc.: 80.08%] [G loss: 4.432890]\n",
            "10360 [D loss: 0.497446, acc.: 75.39%] [G loss: 4.849862]\n",
            "10380 [D loss: 0.415453, acc.: 80.47%] [G loss: 5.096115]\n",
            "10400 [D loss: 0.439674, acc.: 80.47%] [G loss: 4.677988]\n",
            "10420 [D loss: 0.457835, acc.: 77.34%] [G loss: 4.599969]\n",
            "10440 [D loss: 0.395079, acc.: 81.64%] [G loss: 5.974300]\n",
            "10460 [D loss: 0.387914, acc.: 81.64%] [G loss: 5.279893]\n",
            "10480 [D loss: 0.366662, acc.: 84.77%] [G loss: 5.904010]\n",
            "10500 [D loss: 0.592025, acc.: 67.19%] [G loss: 3.888296]\n",
            "10520 [D loss: 0.361790, acc.: 82.42%] [G loss: 5.130326]\n",
            "10540 [D loss: 0.437560, acc.: 80.47%] [G loss: 4.837314]\n",
            "10560 [D loss: 0.409387, acc.: 81.64%] [G loss: 4.580488]\n",
            "10580 [D loss: 0.443822, acc.: 81.25%] [G loss: 4.604065]\n",
            "10600 [D loss: 0.426773, acc.: 80.86%] [G loss: 5.160069]\n",
            "10620 [D loss: 0.459629, acc.: 81.25%] [G loss: 4.617586]\n",
            "10640 [D loss: 0.420732, acc.: 80.47%] [G loss: 5.225354]\n",
            "10660 [D loss: 0.412021, acc.: 81.25%] [G loss: 5.078468]\n",
            "10680 [D loss: 0.503181, acc.: 75.00%] [G loss: 3.935503]\n",
            "10700 [D loss: 0.356123, acc.: 83.20%] [G loss: 5.115108]\n",
            "10720 [D loss: 0.399181, acc.: 80.08%] [G loss: 4.570167]\n",
            "10740 [D loss: 0.394620, acc.: 83.98%] [G loss: 4.968446]\n",
            "10760 [D loss: 0.450544, acc.: 81.25%] [G loss: 4.509284]\n",
            "10780 [D loss: 0.413772, acc.: 80.08%] [G loss: 4.773662]\n",
            "10800 [D loss: 0.486374, acc.: 75.39%] [G loss: 4.298763]\n",
            "10820 [D loss: 0.396396, acc.: 81.64%] [G loss: 4.530687]\n",
            "10840 [D loss: 0.394600, acc.: 80.47%] [G loss: 5.083557]\n",
            "10860 [D loss: 0.483551, acc.: 76.95%] [G loss: 4.374646]\n",
            "10880 [D loss: 0.467539, acc.: 75.39%] [G loss: 5.254607]\n",
            "10900 [D loss: 0.400731, acc.: 81.64%] [G loss: 5.202421]\n",
            "10920 [D loss: 0.380204, acc.: 83.59%] [G loss: 4.352855]\n",
            "10940 [D loss: 0.436174, acc.: 79.69%] [G loss: 4.975749]\n",
            "10960 [D loss: 0.375454, acc.: 80.47%] [G loss: 5.080150]\n",
            "10980 [D loss: 0.430540, acc.: 80.47%] [G loss: 5.274837]\n",
            "11000 [D loss: 0.534118, acc.: 75.39%] [G loss: 4.731630]\n",
            "11020 [D loss: 0.386277, acc.: 83.98%] [G loss: 5.953914]\n",
            "11040 [D loss: 0.411866, acc.: 82.81%] [G loss: 5.946226]\n",
            "11060 [D loss: 0.436619, acc.: 80.86%] [G loss: 5.440408]\n",
            "11080 [D loss: 0.407264, acc.: 82.81%] [G loss: 5.036678]\n",
            "11100 [D loss: 0.394744, acc.: 82.03%] [G loss: 5.567089]\n",
            "11120 [D loss: 0.375413, acc.: 83.98%] [G loss: 5.658683]\n",
            "11140 [D loss: 0.404382, acc.: 80.08%] [G loss: 5.555416]\n",
            "11160 [D loss: 0.473211, acc.: 76.56%] [G loss: 5.637985]\n",
            "11180 [D loss: 0.452384, acc.: 78.91%] [G loss: 5.091068]\n",
            "11200 [D loss: 0.402082, acc.: 82.03%] [G loss: 5.211745]\n",
            "11220 [D loss: 0.412823, acc.: 83.98%] [G loss: 5.226840]\n",
            "11240 [D loss: 0.416964, acc.: 79.69%] [G loss: 5.737350]\n",
            "11260 [D loss: 0.405713, acc.: 82.03%] [G loss: 4.726535]\n",
            "11280 [D loss: 0.381403, acc.: 84.77%] [G loss: 4.869131]\n",
            "11300 [D loss: 0.343646, acc.: 85.16%] [G loss: 5.585864]\n",
            "11320 [D loss: 0.291956, acc.: 88.28%] [G loss: 3.036586]\n",
            "11340 [D loss: 0.527726, acc.: 74.22%] [G loss: 4.698700]\n",
            "11360 [D loss: 0.407057, acc.: 83.20%] [G loss: 5.856875]\n",
            "11380 [D loss: 0.482222, acc.: 75.39%] [G loss: 5.182225]\n",
            "11400 [D loss: 0.402474, acc.: 81.25%] [G loss: 5.580120]\n",
            "11420 [D loss: 0.510718, acc.: 76.56%] [G loss: 5.630170]\n",
            "11440 [D loss: 0.385471, acc.: 82.03%] [G loss: 4.862363]\n",
            "11460 [D loss: 0.353502, acc.: 85.94%] [G loss: 6.147778]\n",
            "11480 [D loss: 0.450847, acc.: 75.00%] [G loss: 6.582708]\n",
            "11500 [D loss: 0.289124, acc.: 91.02%] [G loss: 6.737858]\n",
            "11520 [D loss: 0.416413, acc.: 81.64%] [G loss: 5.740519]\n",
            "11540 [D loss: 0.329059, acc.: 87.11%] [G loss: 6.125436]\n",
            "11560 [D loss: 0.454253, acc.: 78.91%] [G loss: 5.429161]\n",
            "11580 [D loss: 0.360768, acc.: 82.81%] [G loss: 5.661659]\n",
            "11600 [D loss: 0.445107, acc.: 77.34%] [G loss: 5.160774]\n",
            "11620 [D loss: 0.379224, acc.: 80.47%] [G loss: 5.367666]\n",
            "11640 [D loss: 0.377602, acc.: 82.42%] [G loss: 5.405951]\n",
            "11660 [D loss: 0.376743, acc.: 83.98%] [G loss: 5.556513]\n",
            "11680 [D loss: 0.408972, acc.: 81.25%] [G loss: 5.172992]\n",
            "11700 [D loss: 0.384649, acc.: 83.98%] [G loss: 6.309635]\n",
            "11720 [D loss: 0.367332, acc.: 83.98%] [G loss: 5.911287]\n",
            "11740 [D loss: 0.283625, acc.: 89.45%] [G loss: 6.173672]\n",
            "11760 [D loss: 0.432697, acc.: 78.52%] [G loss: 6.978231]\n",
            "11780 [D loss: 0.343791, acc.: 86.72%] [G loss: 6.248479]\n",
            "11800 [D loss: 0.389739, acc.: 86.33%] [G loss: 6.136621]\n",
            "11820 [D loss: 0.341841, acc.: 84.77%] [G loss: 6.622736]\n",
            "11840 [D loss: 0.324744, acc.: 85.55%] [G loss: 5.168955]\n",
            "11860 [D loss: 0.480753, acc.: 76.17%] [G loss: 5.300055]\n",
            "11880 [D loss: 0.456399, acc.: 75.78%] [G loss: 6.160108]\n",
            "11900 [D loss: 0.485597, acc.: 77.34%] [G loss: 5.227773]\n",
            "11920 [D loss: 0.343988, acc.: 84.77%] [G loss: 5.490447]\n",
            "11940 [D loss: 0.373034, acc.: 82.03%] [G loss: 5.519077]\n",
            "11960 [D loss: 0.316637, acc.: 87.89%] [G loss: 5.802330]\n",
            "11980 [D loss: 0.404428, acc.: 84.38%] [G loss: 6.978159]\n",
            "12000 [D loss: 0.505426, acc.: 75.78%] [G loss: 7.175419]\n",
            "12020 [D loss: 0.394219, acc.: 84.38%] [G loss: 4.450908]\n",
            "12040 [D loss: 0.416695, acc.: 77.73%] [G loss: 6.884238]\n",
            "12060 [D loss: 0.350695, acc.: 84.77%] [G loss: 6.286865]\n",
            "12080 [D loss: 0.345585, acc.: 83.59%] [G loss: 6.321768]\n",
            "12100 [D loss: 0.348088, acc.: 87.11%] [G loss: 6.008907]\n",
            "12120 [D loss: 0.457983, acc.: 79.30%] [G loss: 6.018282]\n",
            "12140 [D loss: 0.351441, acc.: 83.59%] [G loss: 5.714504]\n",
            "12160 [D loss: 0.285240, acc.: 88.67%] [G loss: 6.891268]\n",
            "12180 [D loss: 0.468097, acc.: 77.34%] [G loss: 5.490290]\n",
            "12200 [D loss: 0.358974, acc.: 87.11%] [G loss: 5.919963]\n",
            "12220 [D loss: 0.334826, acc.: 86.72%] [G loss: 6.324811]\n",
            "12240 [D loss: 0.333600, acc.: 87.50%] [G loss: 6.139942]\n",
            "12260 [D loss: 0.311515, acc.: 86.72%] [G loss: 6.804759]\n",
            "12280 [D loss: 0.268235, acc.: 89.45%] [G loss: 5.504233]\n",
            "12300 [D loss: 0.587687, acc.: 75.78%] [G loss: 5.170318]\n",
            "12320 [D loss: 0.469703, acc.: 80.47%] [G loss: 4.069772]\n",
            "12340 [D loss: 0.405061, acc.: 80.47%] [G loss: 6.423919]\n",
            "12360 [D loss: 0.522996, acc.: 76.95%] [G loss: 6.377960]\n",
            "12380 [D loss: 0.456781, acc.: 78.12%] [G loss: 5.948242]\n",
            "12400 [D loss: 0.364247, acc.: 84.77%] [G loss: 6.676058]\n",
            "12420 [D loss: 0.422939, acc.: 82.42%] [G loss: 6.057257]\n",
            "12440 [D loss: 0.436665, acc.: 80.08%] [G loss: 7.301912]\n",
            "12460 [D loss: 0.470520, acc.: 75.00%] [G loss: 5.879971]\n",
            "12480 [D loss: 0.405881, acc.: 82.81%] [G loss: 6.338185]\n",
            "12500 [D loss: 0.442102, acc.: 76.95%] [G loss: 5.237364]\n",
            "12520 [D loss: 0.428022, acc.: 80.08%] [G loss: 6.188371]\n",
            "12540 [D loss: 0.308746, acc.: 87.50%] [G loss: 6.925527]\n",
            "12560 [D loss: 0.438819, acc.: 79.69%] [G loss: 6.337472]\n",
            "12580 [D loss: 0.292141, acc.: 87.89%] [G loss: 6.494462]\n",
            "12600 [D loss: 0.317964, acc.: 85.16%] [G loss: 7.208646]\n",
            "12620 [D loss: 0.298120, acc.: 90.23%] [G loss: 7.842165]\n",
            "12640 [D loss: 0.294593, acc.: 89.45%] [G loss: 5.810407]\n",
            "12660 [D loss: 0.388190, acc.: 82.42%] [G loss: 5.331986]\n",
            "12680 [D loss: 0.331511, acc.: 87.89%] [G loss: 6.215546]\n",
            "12700 [D loss: 0.407417, acc.: 82.03%] [G loss: 6.939997]\n",
            "12720 [D loss: 0.396075, acc.: 82.03%] [G loss: 6.707378]\n",
            "12740 [D loss: 0.397537, acc.: 83.20%] [G loss: 6.750228]\n",
            "12760 [D loss: 0.383069, acc.: 83.98%] [G loss: 6.414960]\n",
            "12780 [D loss: 0.369634, acc.: 82.03%] [G loss: 6.531647]\n",
            "12800 [D loss: 0.410014, acc.: 81.64%] [G loss: 6.703175]\n",
            "12820 [D loss: 0.538611, acc.: 76.95%] [G loss: 7.252933]\n",
            "12840 [D loss: 0.356644, acc.: 85.55%] [G loss: 6.582463]\n",
            "12860 [D loss: 0.415291, acc.: 81.25%] [G loss: 6.900125]\n",
            "12880 [D loss: 0.291876, acc.: 87.50%] [G loss: 7.339811]\n",
            "12900 [D loss: 0.324514, acc.: 87.89%] [G loss: 7.332344]\n",
            "12920 [D loss: 0.469940, acc.: 76.17%] [G loss: 7.367971]\n",
            "12940 [D loss: 0.423928, acc.: 80.86%] [G loss: 7.084244]\n",
            "12960 [D loss: 0.373397, acc.: 83.98%] [G loss: 6.949248]\n",
            "12980 [D loss: 0.322414, acc.: 85.55%] [G loss: 7.259893]\n",
            "13000 [D loss: 0.317022, acc.: 85.94%] [G loss: 7.284443]\n",
            "13020 [D loss: 0.394388, acc.: 81.25%] [G loss: 5.743835]\n",
            "13040 [D loss: 0.347336, acc.: 87.50%] [G loss: 6.258044]\n",
            "13060 [D loss: 0.373567, acc.: 84.38%] [G loss: 7.833761]\n",
            "13080 [D loss: 0.357101, acc.: 83.59%] [G loss: 7.591195]\n",
            "13100 [D loss: 0.404570, acc.: 83.20%] [G loss: 6.132127]\n",
            "13120 [D loss: 0.379586, acc.: 82.42%] [G loss: 7.273404]\n",
            "13140 [D loss: 0.322452, acc.: 87.11%] [G loss: 7.575310]\n",
            "13160 [D loss: 0.326097, acc.: 85.55%] [G loss: 6.795119]\n",
            "13180 [D loss: 0.394773, acc.: 81.64%] [G loss: 7.107541]\n",
            "13200 [D loss: 0.342276, acc.: 85.94%] [G loss: 6.029406]\n",
            "13220 [D loss: 0.295459, acc.: 89.06%] [G loss: 6.854098]\n",
            "13240 [D loss: 0.385172, acc.: 82.42%] [G loss: 6.320241]\n",
            "13260 [D loss: 0.314269, acc.: 87.50%] [G loss: 6.864094]\n",
            "13280 [D loss: 0.409090, acc.: 80.47%] [G loss: 6.268842]\n",
            "13300 [D loss: 0.364161, acc.: 83.98%] [G loss: 7.167533]\n",
            "13320 [D loss: 0.357281, acc.: 84.77%] [G loss: 6.826403]\n",
            "13340 [D loss: 0.255951, acc.: 89.45%] [G loss: 7.580193]\n",
            "13360 [D loss: 0.351028, acc.: 84.77%] [G loss: 6.920796]\n",
            "13380 [D loss: 0.328269, acc.: 83.98%] [G loss: 6.953189]\n",
            "13400 [D loss: 0.444759, acc.: 79.30%] [G loss: 5.195118]\n",
            "13420 [D loss: 0.264403, acc.: 89.84%] [G loss: 7.592134]\n",
            "13440 [D loss: 0.395371, acc.: 82.81%] [G loss: 7.357364]\n",
            "13460 [D loss: 0.314653, acc.: 86.72%] [G loss: 6.310154]\n",
            "13480 [D loss: 0.419416, acc.: 78.91%] [G loss: 7.279246]\n",
            "13500 [D loss: 0.315576, acc.: 86.72%] [G loss: 7.345734]\n",
            "13520 [D loss: 0.397021, acc.: 83.59%] [G loss: 6.249782]\n",
            "13540 [D loss: 0.310016, acc.: 85.16%] [G loss: 6.928164]\n",
            "13560 [D loss: 0.362967, acc.: 83.59%] [G loss: 7.830127]\n",
            "13580 [D loss: 0.319393, acc.: 86.33%] [G loss: 6.337083]\n",
            "13600 [D loss: 0.330733, acc.: 85.94%] [G loss: 6.581656]\n",
            "13620 [D loss: 0.354177, acc.: 86.33%] [G loss: 7.594417]\n",
            "13640 [D loss: 0.360824, acc.: 83.98%] [G loss: 6.978655]\n",
            "13660 [D loss: 0.290027, acc.: 87.89%] [G loss: 6.844589]\n",
            "13680 [D loss: 0.324389, acc.: 85.55%] [G loss: 7.397511]\n",
            "13700 [D loss: 0.283793, acc.: 88.67%] [G loss: 9.677359]\n",
            "13720 [D loss: 0.340112, acc.: 86.72%] [G loss: 7.459762]\n",
            "13740 [D loss: 0.277937, acc.: 89.84%] [G loss: 6.778852]\n",
            "13760 [D loss: 0.399546, acc.: 80.08%] [G loss: 7.743240]\n",
            "13780 [D loss: 0.382466, acc.: 83.59%] [G loss: 6.626602]\n",
            "13800 [D loss: 0.306980, acc.: 87.11%] [G loss: 7.455739]\n",
            "13820 [D loss: 0.356930, acc.: 85.55%] [G loss: 6.391320]\n",
            "13840 [D loss: 0.312915, acc.: 86.72%] [G loss: 6.826787]\n",
            "13860 [D loss: 0.211501, acc.: 91.02%] [G loss: 3.547670]\n",
            "13880 [D loss: 0.455395, acc.: 79.69%] [G loss: 5.413429]\n",
            "13900 [D loss: 0.464546, acc.: 79.69%] [G loss: 7.738646]\n",
            "13920 [D loss: 0.404823, acc.: 80.47%] [G loss: 8.345417]\n",
            "13940 [D loss: 0.379287, acc.: 83.98%] [G loss: 7.776661]\n",
            "13960 [D loss: 0.424347, acc.: 82.42%] [G loss: 7.200886]\n",
            "13980 [D loss: 0.255350, acc.: 91.02%] [G loss: 7.588039]\n",
            "14000 [D loss: 0.342897, acc.: 84.38%] [G loss: 6.632424]\n",
            "14020 [D loss: 0.381034, acc.: 82.81%] [G loss: 7.413245]\n",
            "14040 [D loss: 0.263740, acc.: 89.84%] [G loss: 6.882921]\n",
            "14060 [D loss: 0.305427, acc.: 84.38%] [G loss: 8.489262]\n",
            "14080 [D loss: 0.377128, acc.: 85.16%] [G loss: 8.065702]\n",
            "14100 [D loss: 0.293346, acc.: 89.84%] [G loss: 7.653934]\n",
            "14120 [D loss: 0.425676, acc.: 80.08%] [G loss: 7.577480]\n",
            "14140 [D loss: 0.350777, acc.: 88.28%] [G loss: 8.531383]\n",
            "14160 [D loss: 0.371217, acc.: 83.59%] [G loss: 8.432186]\n",
            "14180 [D loss: 0.283794, acc.: 89.06%] [G loss: 8.051974]\n",
            "14200 [D loss: 0.350522, acc.: 83.98%] [G loss: 7.434947]\n",
            "14220 [D loss: 0.352354, acc.: 84.38%] [G loss: 8.619605]\n",
            "14240 [D loss: 0.322215, acc.: 86.33%] [G loss: 9.497088]\n",
            "14260 [D loss: 0.419700, acc.: 82.03%] [G loss: 7.577460]\n",
            "14280 [D loss: 0.269756, acc.: 89.84%] [G loss: 10.039961]\n",
            "14300 [D loss: 0.278729, acc.: 89.06%] [G loss: 9.131018]\n",
            "14320 [D loss: 0.307258, acc.: 86.33%] [G loss: 8.579001]\n",
            "14340 [D loss: 0.338204, acc.: 87.89%] [G loss: 7.856684]\n",
            "14360 [D loss: 0.336947, acc.: 85.55%] [G loss: 7.151581]\n",
            "14380 [D loss: 0.378592, acc.: 80.08%] [G loss: 7.944240]\n",
            "14400 [D loss: 0.347418, acc.: 84.77%] [G loss: 7.367421]\n",
            "14420 [D loss: 0.461766, acc.: 78.52%] [G loss: 7.166309]\n",
            "14440 [D loss: 0.288089, acc.: 88.28%] [G loss: 7.142981]\n",
            "14460 [D loss: 0.373779, acc.: 83.59%] [G loss: 7.460708]\n",
            "14480 [D loss: 0.338601, acc.: 85.16%] [G loss: 8.339933]\n",
            "14500 [D loss: 0.249901, acc.: 89.45%] [G loss: 9.481032]\n",
            "14520 [D loss: 0.286948, acc.: 87.89%] [G loss: 6.988026]\n",
            "14540 [D loss: 0.387599, acc.: 80.08%] [G loss: 7.447049]\n",
            "14560 [D loss: 0.297777, acc.: 89.06%] [G loss: 7.382974]\n",
            "14580 [D loss: 0.312541, acc.: 88.67%] [G loss: 7.436281]\n",
            "14600 [D loss: 0.291230, acc.: 85.94%] [G loss: 7.316807]\n",
            "14620 [D loss: 0.288872, acc.: 89.06%] [G loss: 8.163324]\n",
            "14640 [D loss: 0.395813, acc.: 82.03%] [G loss: 6.777693]\n",
            "14660 [D loss: 0.342538, acc.: 85.94%] [G loss: 8.377609]\n",
            "14680 [D loss: 0.269599, acc.: 87.89%] [G loss: 7.870429]\n",
            "14700 [D loss: 0.425692, acc.: 80.47%] [G loss: 6.395660]\n",
            "14720 [D loss: 0.327085, acc.: 85.16%] [G loss: 7.022179]\n",
            "14740 [D loss: 0.298354, acc.: 87.89%] [G loss: 7.861921]\n",
            "14760 [D loss: 0.350437, acc.: 86.72%] [G loss: 7.450790]\n",
            "14780 [D loss: 0.300372, acc.: 86.72%] [G loss: 7.248696]\n",
            "14800 [D loss: 0.267854, acc.: 88.28%] [G loss: 8.048431]\n",
            "14820 [D loss: 0.352534, acc.: 83.98%] [G loss: 9.523913]\n",
            "14840 [D loss: 0.444268, acc.: 80.08%] [G loss: 7.034032]\n",
            "14860 [D loss: 0.339760, acc.: 84.77%] [G loss: 8.170843]\n",
            "14880 [D loss: 0.348978, acc.: 82.42%] [G loss: 6.910745]\n",
            "14900 [D loss: 0.260393, acc.: 90.62%] [G loss: 7.271688]\n",
            "14920 [D loss: 0.303208, acc.: 85.55%] [G loss: 8.141439]\n",
            "14940 [D loss: 0.278856, acc.: 88.67%] [G loss: 8.763189]\n",
            "14960 [D loss: 0.294035, acc.: 90.23%] [G loss: 7.937849]\n",
            "14980 [D loss: 0.340609, acc.: 84.38%] [G loss: 8.585423]\n",
            "15000 [D loss: 0.491741, acc.: 77.34%] [G loss: 8.142202]\n",
            "15020 [D loss: 0.616997, acc.: 74.61%] [G loss: 8.068479]\n",
            "15040 [D loss: 1.142190, acc.: 61.33%] [G loss: 9.162878]\n",
            "15060 [D loss: 1.411670, acc.: 51.95%] [G loss: 5.477548]\n",
            "15080 [D loss: 0.861898, acc.: 68.36%] [G loss: 5.313634]\n",
            "15100 [D loss: 0.669836, acc.: 65.62%] [G loss: 3.741374]\n",
            "15120 [D loss: 0.765680, acc.: 61.33%] [G loss: 3.140267]\n",
            "15140 [D loss: 0.802132, acc.: 63.28%] [G loss: 3.381205]\n",
            "15160 [D loss: 0.726443, acc.: 63.67%] [G loss: 3.546134]\n",
            "15180 [D loss: 0.687877, acc.: 65.62%] [G loss: 3.482314]\n",
            "15200 [D loss: 0.671689, acc.: 62.89%] [G loss: 3.033410]\n",
            "15220 [D loss: 0.694577, acc.: 66.80%] [G loss: 3.118874]\n",
            "15240 [D loss: 0.658047, acc.: 65.23%] [G loss: 2.982007]\n",
            "15260 [D loss: 0.537648, acc.: 75.00%] [G loss: 3.241133]\n",
            "15280 [D loss: 0.694261, acc.: 60.94%] [G loss: 2.779635]\n",
            "15300 [D loss: 0.592243, acc.: 65.23%] [G loss: 3.013899]\n",
            "15320 [D loss: 0.635349, acc.: 61.72%] [G loss: 2.858375]\n",
            "15340 [D loss: 0.606285, acc.: 66.80%] [G loss: 2.978028]\n",
            "15360 [D loss: 0.663534, acc.: 62.50%] [G loss: 2.987072]\n",
            "15380 [D loss: 0.651618, acc.: 63.67%] [G loss: 2.959951]\n",
            "15400 [D loss: 0.605379, acc.: 70.31%] [G loss: 3.129327]\n",
            "15420 [D loss: 0.647424, acc.: 62.11%] [G loss: 2.967430]\n",
            "15440 [D loss: 0.645425, acc.: 64.06%] [G loss: 2.855186]\n",
            "15460 [D loss: 0.518083, acc.: 75.78%] [G loss: 3.176057]\n",
            "15480 [D loss: 0.569859, acc.: 70.31%] [G loss: 3.036350]\n",
            "15500 [D loss: 0.625359, acc.: 65.23%] [G loss: 3.134859]\n",
            "15520 [D loss: 0.564631, acc.: 71.09%] [G loss: 3.414405]\n",
            "15540 [D loss: 0.540626, acc.: 70.70%] [G loss: 3.518453]\n",
            "15560 [D loss: 0.584052, acc.: 69.14%] [G loss: 3.290771]\n",
            "15580 [D loss: 0.620184, acc.: 68.75%] [G loss: 3.462420]\n",
            "15600 [D loss: 0.575126, acc.: 73.44%] [G loss: 3.446009]\n",
            "15620 [D loss: 0.557342, acc.: 71.09%] [G loss: 3.571162]\n",
            "15640 [D loss: 0.542150, acc.: 72.66%] [G loss: 3.526735]\n",
            "15660 [D loss: 0.554936, acc.: 72.27%] [G loss: 3.493771]\n",
            "15680 [D loss: 0.590264, acc.: 69.92%] [G loss: 3.654294]\n",
            "15700 [D loss: 0.546067, acc.: 73.44%] [G loss: 3.327076]\n",
            "15720 [D loss: 0.531344, acc.: 75.00%] [G loss: 3.452464]\n",
            "15740 [D loss: 0.621844, acc.: 68.75%] [G loss: 3.309040]\n",
            "15760 [D loss: 0.484731, acc.: 82.03%] [G loss: 3.716144]\n",
            "15780 [D loss: 0.534158, acc.: 73.83%] [G loss: 3.821672]\n",
            "15800 [D loss: 0.557104, acc.: 72.27%] [G loss: 3.738095]\n",
            "15820 [D loss: 0.506533, acc.: 75.00%] [G loss: 3.150208]\n",
            "15840 [D loss: 0.486726, acc.: 73.83%] [G loss: 4.040384]\n",
            "15860 [D loss: 0.484679, acc.: 76.95%] [G loss: 3.772824]\n",
            "15880 [D loss: 0.469963, acc.: 79.30%] [G loss: 3.954294]\n",
            "15900 [D loss: 0.525520, acc.: 72.27%] [G loss: 3.711624]\n",
            "15920 [D loss: 0.439077, acc.: 79.69%] [G loss: 4.211228]\n",
            "15940 [D loss: 0.440602, acc.: 81.25%] [G loss: 4.063867]\n",
            "15960 [D loss: 0.463642, acc.: 79.30%] [G loss: 3.760472]\n",
            "15980 [D loss: 0.495674, acc.: 76.95%] [G loss: 3.869098]\n",
            "16000 [D loss: 0.520372, acc.: 73.05%] [G loss: 3.651154]\n",
            "16020 [D loss: 0.560304, acc.: 72.27%] [G loss: 3.720550]\n",
            "16040 [D loss: 0.432243, acc.: 80.08%] [G loss: 4.370526]\n",
            "16060 [D loss: 0.521187, acc.: 72.66%] [G loss: 3.613790]\n",
            "16080 [D loss: 0.455743, acc.: 79.69%] [G loss: 3.991901]\n",
            "16100 [D loss: 0.478871, acc.: 78.12%] [G loss: 3.896418]\n",
            "16120 [D loss: 0.516712, acc.: 76.95%] [G loss: 3.641126]\n",
            "16140 [D loss: 0.532979, acc.: 75.00%] [G loss: 3.728743]\n",
            "16160 [D loss: 0.454779, acc.: 80.86%] [G loss: 4.163646]\n",
            "16180 [D loss: 0.594388, acc.: 71.88%] [G loss: 3.537246]\n",
            "16200 [D loss: 0.478834, acc.: 78.52%] [G loss: 3.763774]\n",
            "16220 [D loss: 0.483265, acc.: 75.78%] [G loss: 4.419334]\n",
            "16240 [D loss: 0.427224, acc.: 82.03%] [G loss: 4.221557]\n",
            "16260 [D loss: 0.434614, acc.: 79.30%] [G loss: 4.604084]\n",
            "16280 [D loss: 0.436222, acc.: 82.03%] [G loss: 4.372426]\n",
            "16300 [D loss: 0.421282, acc.: 83.20%] [G loss: 4.178100]\n",
            "16320 [D loss: 0.437902, acc.: 80.08%] [G loss: 4.181934]\n",
            "16340 [D loss: 0.471899, acc.: 78.91%] [G loss: 4.306708]\n",
            "16360 [D loss: 0.461526, acc.: 78.12%] [G loss: 4.766950]\n",
            "16380 [D loss: 0.481314, acc.: 76.95%] [G loss: 4.101850]\n",
            "16400 [D loss: 0.417274, acc.: 80.08%] [G loss: 4.032704]\n",
            "16420 [D loss: 0.416561, acc.: 79.69%] [G loss: 4.539693]\n",
            "16440 [D loss: 0.403131, acc.: 83.98%] [G loss: 4.134830]\n",
            "16460 [D loss: 0.491810, acc.: 74.61%] [G loss: 4.545390]\n",
            "16480 [D loss: 0.424606, acc.: 81.25%] [G loss: 4.632340]\n",
            "16500 [D loss: 0.407945, acc.: 78.91%] [G loss: 4.511482]\n",
            "16520 [D loss: 0.418328, acc.: 82.03%] [G loss: 4.708535]\n",
            "16540 [D loss: 0.423844, acc.: 82.42%] [G loss: 4.927540]\n",
            "16560 [D loss: 0.453216, acc.: 78.91%] [G loss: 4.420005]\n",
            "16580 [D loss: 0.435458, acc.: 80.47%] [G loss: 4.208908]\n",
            "16600 [D loss: 0.287047, acc.: 89.06%] [G loss: 5.400107]\n",
            "16620 [D loss: 0.368672, acc.: 84.77%] [G loss: 3.380308]\n",
            "16640 [D loss: 0.634134, acc.: 71.48%] [G loss: 4.853149]\n",
            "16660 [D loss: 0.410745, acc.: 82.42%] [G loss: 2.632699]\n",
            "16680 [D loss: 1.171628, acc.: 48.83%] [G loss: 4.702309]\n",
            "16700 [D loss: 0.666158, acc.: 69.14%] [G loss: 4.636933]\n",
            "16720 [D loss: 0.457194, acc.: 77.34%] [G loss: 5.648641]\n",
            "16740 [D loss: 0.456424, acc.: 76.56%] [G loss: 4.871570]\n",
            "16760 [D loss: 0.530110, acc.: 73.83%] [G loss: 4.898804]\n",
            "16780 [D loss: 0.409494, acc.: 80.08%] [G loss: 5.889758]\n",
            "16800 [D loss: 0.432526, acc.: 81.25%] [G loss: 4.733052]\n",
            "16820 [D loss: 0.360091, acc.: 83.98%] [G loss: 5.641132]\n",
            "16840 [D loss: 0.371555, acc.: 85.94%] [G loss: 5.361587]\n",
            "16860 [D loss: 0.389762, acc.: 83.98%] [G loss: 5.769684]\n",
            "16880 [D loss: 0.488312, acc.: 76.56%] [G loss: 5.365999]\n",
            "16900 [D loss: 0.426417, acc.: 80.08%] [G loss: 5.016115]\n",
            "16920 [D loss: 0.384462, acc.: 82.42%] [G loss: 4.474331]\n",
            "16940 [D loss: 0.441508, acc.: 77.73%] [G loss: 4.941057]\n",
            "16960 [D loss: 0.369268, acc.: 85.16%] [G loss: 5.777617]\n",
            "16980 [D loss: 0.375541, acc.: 83.98%] [G loss: 5.547795]\n",
            "17000 [D loss: 0.361256, acc.: 80.47%] [G loss: 5.594180]\n",
            "17020 [D loss: 0.440369, acc.: 77.73%] [G loss: 5.637636]\n",
            "17040 [D loss: 0.374583, acc.: 83.98%] [G loss: 5.615772]\n",
            "17060 [D loss: 0.352518, acc.: 84.77%] [G loss: 5.342440]\n",
            "17080 [D loss: 0.333523, acc.: 83.98%] [G loss: 5.168500]\n",
            "17100 [D loss: 0.390427, acc.: 82.42%] [G loss: 5.802145]\n",
            "17120 [D loss: 0.392335, acc.: 83.59%] [G loss: 5.925859]\n",
            "17140 [D loss: 0.435386, acc.: 83.20%] [G loss: 5.574489]\n",
            "17160 [D loss: 0.405405, acc.: 83.98%] [G loss: 5.422453]\n",
            "17180 [D loss: 0.432524, acc.: 79.69%] [G loss: 5.362757]\n",
            "17200 [D loss: 0.420907, acc.: 83.59%] [G loss: 4.723732]\n",
            "17220 [D loss: 0.370016, acc.: 82.42%] [G loss: 5.603199]\n",
            "17240 [D loss: 0.383149, acc.: 80.47%] [G loss: 5.993039]\n",
            "17260 [D loss: 0.342901, acc.: 85.16%] [G loss: 5.626287]\n",
            "17280 [D loss: 0.392656, acc.: 82.42%] [G loss: 5.549652]\n",
            "17300 [D loss: 0.379125, acc.: 82.42%] [G loss: 5.703914]\n",
            "17320 [D loss: 0.522534, acc.: 72.27%] [G loss: 5.088090]\n",
            "17340 [D loss: 0.362835, acc.: 83.59%] [G loss: 6.373034]\n",
            "17360 [D loss: 0.373739, acc.: 83.98%] [G loss: 6.396958]\n",
            "17380 [D loss: 0.397834, acc.: 82.81%] [G loss: 5.642082]\n",
            "17400 [D loss: 0.314705, acc.: 88.28%] [G loss: 6.948339]\n",
            "17420 [D loss: 0.302139, acc.: 89.45%] [G loss: 5.898987]\n",
            "17440 [D loss: 0.340073, acc.: 85.16%] [G loss: 5.561586]\n",
            "17460 [D loss: 0.346004, acc.: 83.59%] [G loss: 6.389208]\n",
            "17480 [D loss: 0.317199, acc.: 88.28%] [G loss: 6.264254]\n",
            "17500 [D loss: 0.504924, acc.: 75.00%] [G loss: 4.429911]\n",
            "17520 [D loss: 0.358632, acc.: 82.81%] [G loss: 6.198256]\n",
            "17540 [D loss: 0.433438, acc.: 81.25%] [G loss: 4.826341]\n",
            "17560 [D loss: 0.344270, acc.: 85.55%] [G loss: 6.738696]\n",
            "17580 [D loss: 0.377793, acc.: 82.42%] [G loss: 6.085816]\n",
            "17600 [D loss: 0.343167, acc.: 84.77%] [G loss: 6.130071]\n",
            "17620 [D loss: 0.345882, acc.: 86.72%] [G loss: 6.713158]\n",
            "17640 [D loss: 0.353682, acc.: 86.33%] [G loss: 5.425261]\n",
            "17660 [D loss: 0.445300, acc.: 78.91%] [G loss: 5.153594]\n",
            "17680 [D loss: 0.367044, acc.: 83.98%] [G loss: 7.027284]\n",
            "17700 [D loss: 0.373192, acc.: 84.38%] [G loss: 6.181095]\n",
            "17720 [D loss: 0.418275, acc.: 85.94%] [G loss: 7.504472]\n",
            "17740 [D loss: 0.296073, acc.: 89.84%] [G loss: 6.182404]\n",
            "17760 [D loss: 0.435133, acc.: 78.12%] [G loss: 5.794170]\n",
            "17780 [D loss: 0.352656, acc.: 82.42%] [G loss: 6.580448]\n",
            "17800 [D loss: 0.312013, acc.: 86.72%] [G loss: 6.056778]\n",
            "17820 [D loss: 0.372289, acc.: 85.16%] [G loss: 6.086576]\n",
            "17840 [D loss: 0.299493, acc.: 85.94%] [G loss: 6.774362]\n",
            "17860 [D loss: 0.337293, acc.: 85.55%] [G loss: 7.282459]\n",
            "17880 [D loss: 0.360551, acc.: 84.77%] [G loss: 5.649211]\n",
            "17900 [D loss: 0.329633, acc.: 84.77%] [G loss: 6.757336]\n",
            "17920 [D loss: 0.370142, acc.: 83.59%] [G loss: 5.935635]\n",
            "17940 [D loss: 0.363934, acc.: 83.98%] [G loss: 6.527356]\n",
            "17960 [D loss: 0.374468, acc.: 83.20%] [G loss: 6.413429]\n",
            "17980 [D loss: 0.359569, acc.: 80.47%] [G loss: 5.792001]\n",
            "18000 [D loss: 0.278696, acc.: 89.45%] [G loss: 6.304343]\n",
            "18020 [D loss: 0.412054, acc.: 80.86%] [G loss: 5.526112]\n",
            "18040 [D loss: 0.358977, acc.: 85.94%] [G loss: 6.981989]\n",
            "18060 [D loss: 0.343068, acc.: 85.16%] [G loss: 5.812169]\n",
            "18080 [D loss: 0.448976, acc.: 77.73%] [G loss: 6.040600]\n",
            "18100 [D loss: 0.313409, acc.: 85.94%] [G loss: 5.997387]\n",
            "18120 [D loss: 0.414814, acc.: 80.08%] [G loss: 5.127721]\n",
            "18140 [D loss: 0.333927, acc.: 84.77%] [G loss: 7.407190]\n",
            "18160 [D loss: 0.408654, acc.: 81.64%] [G loss: 5.949815]\n",
            "18180 [D loss: 0.345381, acc.: 86.72%] [G loss: 6.606937]\n",
            "18200 [D loss: 0.353385, acc.: 86.33%] [G loss: 6.857212]\n",
            "18220 [D loss: 0.379917, acc.: 82.42%] [G loss: 6.552008]\n",
            "18240 [D loss: 0.535313, acc.: 74.22%] [G loss: 6.899839]\n",
            "18260 [D loss: 0.299520, acc.: 87.89%] [G loss: 6.390575]\n",
            "18280 [D loss: 0.359124, acc.: 85.55%] [G loss: 7.122738]\n",
            "18300 [D loss: 0.388463, acc.: 81.64%] [G loss: 6.872061]\n",
            "18320 [D loss: 0.375210, acc.: 83.20%] [G loss: 7.659486]\n",
            "18340 [D loss: 0.273612, acc.: 88.67%] [G loss: 7.713293]\n",
            "18360 [D loss: 0.432985, acc.: 80.86%] [G loss: 7.475254]\n",
            "18380 [D loss: 0.358873, acc.: 84.38%] [G loss: 7.028105]\n",
            "18400 [D loss: 0.249628, acc.: 89.84%] [G loss: 8.130473]\n",
            "18420 [D loss: 0.400069, acc.: 83.98%] [G loss: 7.231112]\n",
            "18440 [D loss: 0.344456, acc.: 84.77%] [G loss: 7.236972]\n",
            "18460 [D loss: 0.294266, acc.: 88.28%] [G loss: 6.951975]\n",
            "18480 [D loss: 0.379498, acc.: 82.81%] [G loss: 7.140552]\n",
            "18500 [D loss: 0.280282, acc.: 89.06%] [G loss: 7.087630]\n",
            "18520 [D loss: 0.366536, acc.: 83.59%] [G loss: 5.550246]\n",
            "18540 [D loss: 0.399532, acc.: 82.03%] [G loss: 6.472261]\n",
            "18560 [D loss: 0.322396, acc.: 84.38%] [G loss: 6.813694]\n",
            "18580 [D loss: 0.338212, acc.: 83.98%] [G loss: 7.005143]\n",
            "18600 [D loss: 0.335022, acc.: 85.16%] [G loss: 7.601974]\n",
            "18620 [D loss: 0.358863, acc.: 82.42%] [G loss: 7.454278]\n",
            "18640 [D loss: 0.340809, acc.: 85.55%] [G loss: 8.247665]\n",
            "18660 [D loss: 0.327959, acc.: 85.55%] [G loss: 7.359578]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
