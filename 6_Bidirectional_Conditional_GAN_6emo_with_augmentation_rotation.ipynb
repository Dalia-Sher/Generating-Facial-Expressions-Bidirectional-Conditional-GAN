{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "6_Bidirectional_Conditional_GAN_6emo_with_augmentation_rotation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dalia-Sher/Generating-Facial-Expressions-Bidirectional-Conditional-GAN/blob/Shir/6_Bidirectional_Conditional_GAN_6emo_with_augmentation_rotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt_ZQaFJO9F5"
      },
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "b70130d5-0c72-4569-9cac-5a913ac9f58e"
      },
      "source": [
        "data = pd.read_csv('fer2013.csv')\n",
        "data = data[data.emotion != 1]\n",
        "data.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIL-PyaBP_oP",
        "outputId": "b8760d4b-d578-4ccb-e20d-d100b5a23263"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35340, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHR9swxs5P0w"
      },
      "source": [
        "data['emotion'] = data.emotion.replace(6, 1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQj2Szg75fm5",
        "outputId": "c57538da-0f37-4827-9342-d9d9e624b6c2"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    9017\n",
              "1    6217\n",
              "4    6099\n",
              "2    5142\n",
              "0    4974\n",
              "5    4014\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yxZyZWONmc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "e05d5683-f81d-4cfc-9d6c-af8d1708573c"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tX-9YaZPDyx"
      },
      "source": [
        "Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoYxTe66PY9N",
        "outputId": "27c61b37-9457-493e-f1aa-6d5ed406b84f"
      },
      "source": [
        "#rotating image \n",
        "iterations_per_image = 2\n",
        "X_train_aug = np.copy(X_train)\n",
        "y_train_aug = np.copy(y_train)\n",
        "for k in range(len(X_train)):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  rotated_images = []\n",
        "  emotions_list = []\n",
        "  for i in range(iterations_per_image):\n",
        "    rotate=iaa.Affine(rotate=(-50+(i+1)*10, 30+(i+1)*10))\n",
        "    rotated_img = rotate.augment_image(img)\n",
        "    rotated_images.append(rotated_img)\n",
        "  rotated_images = np.array(rotated_images)\n",
        "  X_train_aug = np.concatenate((X_train_aug, rotated_images), axis=0)\n",
        "  emotions_list = [emotion]*iterations_per_image\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  y_train_aug = np.concatenate((y_train_aug, emotions_list), axis=0)\n",
        "  print(\"iteration:\" , k ,\"train shape:\",X_train_aug.shape)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration: 30335 train shape: (96012, 48, 48, 1)\n",
            "iteration: 30336 train shape: (96014, 48, 48, 1)\n",
            "iteration: 30337 train shape: (96016, 48, 48, 1)\n",
            "iteration: 30338 train shape: (96018, 48, 48, 1)\n",
            "iteration: 30339 train shape: (96020, 48, 48, 1)\n",
            "iteration: 30340 train shape: (96022, 48, 48, 1)\n",
            "iteration: 30341 train shape: (96024, 48, 48, 1)\n",
            "iteration: 30342 train shape: (96026, 48, 48, 1)\n",
            "iteration: 30343 train shape: (96028, 48, 48, 1)\n",
            "iteration: 30344 train shape: (96030, 48, 48, 1)\n",
            "iteration: 30345 train shape: (96032, 48, 48, 1)\n",
            "iteration: 30346 train shape: (96034, 48, 48, 1)\n",
            "iteration: 30347 train shape: (96036, 48, 48, 1)\n",
            "iteration: 30348 train shape: (96038, 48, 48, 1)\n",
            "iteration: 30349 train shape: (96040, 48, 48, 1)\n",
            "iteration: 30350 train shape: (96042, 48, 48, 1)\n",
            "iteration: 30351 train shape: (96044, 48, 48, 1)\n",
            "iteration: 30352 train shape: (96046, 48, 48, 1)\n",
            "iteration: 30353 train shape: (96048, 48, 48, 1)\n",
            "iteration: 30354 train shape: (96050, 48, 48, 1)\n",
            "iteration: 30355 train shape: (96052, 48, 48, 1)\n",
            "iteration: 30356 train shape: (96054, 48, 48, 1)\n",
            "iteration: 30357 train shape: (96056, 48, 48, 1)\n",
            "iteration: 30358 train shape: (96058, 48, 48, 1)\n",
            "iteration: 30359 train shape: (96060, 48, 48, 1)\n",
            "iteration: 30360 train shape: (96062, 48, 48, 1)\n",
            "iteration: 30361 train shape: (96064, 48, 48, 1)\n",
            "iteration: 30362 train shape: (96066, 48, 48, 1)\n",
            "iteration: 30363 train shape: (96068, 48, 48, 1)\n",
            "iteration: 30364 train shape: (96070, 48, 48, 1)\n",
            "iteration: 30365 train shape: (96072, 48, 48, 1)\n",
            "iteration: 30366 train shape: (96074, 48, 48, 1)\n",
            "iteration: 30367 train shape: (96076, 48, 48, 1)\n",
            "iteration: 30368 train shape: (96078, 48, 48, 1)\n",
            "iteration: 30369 train shape: (96080, 48, 48, 1)\n",
            "iteration: 30370 train shape: (96082, 48, 48, 1)\n",
            "iteration: 30371 train shape: (96084, 48, 48, 1)\n",
            "iteration: 30372 train shape: (96086, 48, 48, 1)\n",
            "iteration: 30373 train shape: (96088, 48, 48, 1)\n",
            "iteration: 30374 train shape: (96090, 48, 48, 1)\n",
            "iteration: 30375 train shape: (96092, 48, 48, 1)\n",
            "iteration: 30376 train shape: (96094, 48, 48, 1)\n",
            "iteration: 30377 train shape: (96096, 48, 48, 1)\n",
            "iteration: 30378 train shape: (96098, 48, 48, 1)\n",
            "iteration: 30379 train shape: (96100, 48, 48, 1)\n",
            "iteration: 30380 train shape: (96102, 48, 48, 1)\n",
            "iteration: 30381 train shape: (96104, 48, 48, 1)\n",
            "iteration: 30382 train shape: (96106, 48, 48, 1)\n",
            "iteration: 30383 train shape: (96108, 48, 48, 1)\n",
            "iteration: 30384 train shape: (96110, 48, 48, 1)\n",
            "iteration: 30385 train shape: (96112, 48, 48, 1)\n",
            "iteration: 30386 train shape: (96114, 48, 48, 1)\n",
            "iteration: 30387 train shape: (96116, 48, 48, 1)\n",
            "iteration: 30388 train shape: (96118, 48, 48, 1)\n",
            "iteration: 30389 train shape: (96120, 48, 48, 1)\n",
            "iteration: 30390 train shape: (96122, 48, 48, 1)\n",
            "iteration: 30391 train shape: (96124, 48, 48, 1)\n",
            "iteration: 30392 train shape: (96126, 48, 48, 1)\n",
            "iteration: 30393 train shape: (96128, 48, 48, 1)\n",
            "iteration: 30394 train shape: (96130, 48, 48, 1)\n",
            "iteration: 30395 train shape: (96132, 48, 48, 1)\n",
            "iteration: 30396 train shape: (96134, 48, 48, 1)\n",
            "iteration: 30397 train shape: (96136, 48, 48, 1)\n",
            "iteration: 30398 train shape: (96138, 48, 48, 1)\n",
            "iteration: 30399 train shape: (96140, 48, 48, 1)\n",
            "iteration: 30400 train shape: (96142, 48, 48, 1)\n",
            "iteration: 30401 train shape: (96144, 48, 48, 1)\n",
            "iteration: 30402 train shape: (96146, 48, 48, 1)\n",
            "iteration: 30403 train shape: (96148, 48, 48, 1)\n",
            "iteration: 30404 train shape: (96150, 48, 48, 1)\n",
            "iteration: 30405 train shape: (96152, 48, 48, 1)\n",
            "iteration: 30406 train shape: (96154, 48, 48, 1)\n",
            "iteration: 30407 train shape: (96156, 48, 48, 1)\n",
            "iteration: 30408 train shape: (96158, 48, 48, 1)\n",
            "iteration: 30409 train shape: (96160, 48, 48, 1)\n",
            "iteration: 30410 train shape: (96162, 48, 48, 1)\n",
            "iteration: 30411 train shape: (96164, 48, 48, 1)\n",
            "iteration: 30412 train shape: (96166, 48, 48, 1)\n",
            "iteration: 30413 train shape: (96168, 48, 48, 1)\n",
            "iteration: 30414 train shape: (96170, 48, 48, 1)\n",
            "iteration: 30415 train shape: (96172, 48, 48, 1)\n",
            "iteration: 30416 train shape: (96174, 48, 48, 1)\n",
            "iteration: 30417 train shape: (96176, 48, 48, 1)\n",
            "iteration: 30418 train shape: (96178, 48, 48, 1)\n",
            "iteration: 30419 train shape: (96180, 48, 48, 1)\n",
            "iteration: 30420 train shape: (96182, 48, 48, 1)\n",
            "iteration: 30421 train shape: (96184, 48, 48, 1)\n",
            "iteration: 30422 train shape: (96186, 48, 48, 1)\n",
            "iteration: 30423 train shape: (96188, 48, 48, 1)\n",
            "iteration: 30424 train shape: (96190, 48, 48, 1)\n",
            "iteration: 30425 train shape: (96192, 48, 48, 1)\n",
            "iteration: 30426 train shape: (96194, 48, 48, 1)\n",
            "iteration: 30427 train shape: (96196, 48, 48, 1)\n",
            "iteration: 30428 train shape: (96198, 48, 48, 1)\n",
            "iteration: 30429 train shape: (96200, 48, 48, 1)\n",
            "iteration: 30430 train shape: (96202, 48, 48, 1)\n",
            "iteration: 30431 train shape: (96204, 48, 48, 1)\n",
            "iteration: 30432 train shape: (96206, 48, 48, 1)\n",
            "iteration: 30433 train shape: (96208, 48, 48, 1)\n",
            "iteration: 30434 train shape: (96210, 48, 48, 1)\n",
            "iteration: 30435 train shape: (96212, 48, 48, 1)\n",
            "iteration: 30436 train shape: (96214, 48, 48, 1)\n",
            "iteration: 30437 train shape: (96216, 48, 48, 1)\n",
            "iteration: 30438 train shape: (96218, 48, 48, 1)\n",
            "iteration: 30439 train shape: (96220, 48, 48, 1)\n",
            "iteration: 30440 train shape: (96222, 48, 48, 1)\n",
            "iteration: 30441 train shape: (96224, 48, 48, 1)\n",
            "iteration: 30442 train shape: (96226, 48, 48, 1)\n",
            "iteration: 30443 train shape: (96228, 48, 48, 1)\n",
            "iteration: 30444 train shape: (96230, 48, 48, 1)\n",
            "iteration: 30445 train shape: (96232, 48, 48, 1)\n",
            "iteration: 30446 train shape: (96234, 48, 48, 1)\n",
            "iteration: 30447 train shape: (96236, 48, 48, 1)\n",
            "iteration: 30448 train shape: (96238, 48, 48, 1)\n",
            "iteration: 30449 train shape: (96240, 48, 48, 1)\n",
            "iteration: 30450 train shape: (96242, 48, 48, 1)\n",
            "iteration: 30451 train shape: (96244, 48, 48, 1)\n",
            "iteration: 30452 train shape: (96246, 48, 48, 1)\n",
            "iteration: 30453 train shape: (96248, 48, 48, 1)\n",
            "iteration: 30454 train shape: (96250, 48, 48, 1)\n",
            "iteration: 30455 train shape: (96252, 48, 48, 1)\n",
            "iteration: 30456 train shape: (96254, 48, 48, 1)\n",
            "iteration: 30457 train shape: (96256, 48, 48, 1)\n",
            "iteration: 30458 train shape: (96258, 48, 48, 1)\n",
            "iteration: 30459 train shape: (96260, 48, 48, 1)\n",
            "iteration: 30460 train shape: (96262, 48, 48, 1)\n",
            "iteration: 30461 train shape: (96264, 48, 48, 1)\n",
            "iteration: 30462 train shape: (96266, 48, 48, 1)\n",
            "iteration: 30463 train shape: (96268, 48, 48, 1)\n",
            "iteration: 30464 train shape: (96270, 48, 48, 1)\n",
            "iteration: 30465 train shape: (96272, 48, 48, 1)\n",
            "iteration: 30466 train shape: (96274, 48, 48, 1)\n",
            "iteration: 30467 train shape: (96276, 48, 48, 1)\n",
            "iteration: 30468 train shape: (96278, 48, 48, 1)\n",
            "iteration: 30469 train shape: (96280, 48, 48, 1)\n",
            "iteration: 30470 train shape: (96282, 48, 48, 1)\n",
            "iteration: 30471 train shape: (96284, 48, 48, 1)\n",
            "iteration: 30472 train shape: (96286, 48, 48, 1)\n",
            "iteration: 30473 train shape: (96288, 48, 48, 1)\n",
            "iteration: 30474 train shape: (96290, 48, 48, 1)\n",
            "iteration: 30475 train shape: (96292, 48, 48, 1)\n",
            "iteration: 30476 train shape: (96294, 48, 48, 1)\n",
            "iteration: 30477 train shape: (96296, 48, 48, 1)\n",
            "iteration: 30478 train shape: (96298, 48, 48, 1)\n",
            "iteration: 30479 train shape: (96300, 48, 48, 1)\n",
            "iteration: 30480 train shape: (96302, 48, 48, 1)\n",
            "iteration: 30481 train shape: (96304, 48, 48, 1)\n",
            "iteration: 30482 train shape: (96306, 48, 48, 1)\n",
            "iteration: 30483 train shape: (96308, 48, 48, 1)\n",
            "iteration: 30484 train shape: (96310, 48, 48, 1)\n",
            "iteration: 30485 train shape: (96312, 48, 48, 1)\n",
            "iteration: 30486 train shape: (96314, 48, 48, 1)\n",
            "iteration: 30487 train shape: (96316, 48, 48, 1)\n",
            "iteration: 30488 train shape: (96318, 48, 48, 1)\n",
            "iteration: 30489 train shape: (96320, 48, 48, 1)\n",
            "iteration: 30490 train shape: (96322, 48, 48, 1)\n",
            "iteration: 30491 train shape: (96324, 48, 48, 1)\n",
            "iteration: 30492 train shape: (96326, 48, 48, 1)\n",
            "iteration: 30493 train shape: (96328, 48, 48, 1)\n",
            "iteration: 30494 train shape: (96330, 48, 48, 1)\n",
            "iteration: 30495 train shape: (96332, 48, 48, 1)\n",
            "iteration: 30496 train shape: (96334, 48, 48, 1)\n",
            "iteration: 30497 train shape: (96336, 48, 48, 1)\n",
            "iteration: 30498 train shape: (96338, 48, 48, 1)\n",
            "iteration: 30499 train shape: (96340, 48, 48, 1)\n",
            "iteration: 30500 train shape: (96342, 48, 48, 1)\n",
            "iteration: 30501 train shape: (96344, 48, 48, 1)\n",
            "iteration: 30502 train shape: (96346, 48, 48, 1)\n",
            "iteration: 30503 train shape: (96348, 48, 48, 1)\n",
            "iteration: 30504 train shape: (96350, 48, 48, 1)\n",
            "iteration: 30505 train shape: (96352, 48, 48, 1)\n",
            "iteration: 30506 train shape: (96354, 48, 48, 1)\n",
            "iteration: 30507 train shape: (96356, 48, 48, 1)\n",
            "iteration: 30508 train shape: (96358, 48, 48, 1)\n",
            "iteration: 30509 train shape: (96360, 48, 48, 1)\n",
            "iteration: 30510 train shape: (96362, 48, 48, 1)\n",
            "iteration: 30511 train shape: (96364, 48, 48, 1)\n",
            "iteration: 30512 train shape: (96366, 48, 48, 1)\n",
            "iteration: 30513 train shape: (96368, 48, 48, 1)\n",
            "iteration: 30514 train shape: (96370, 48, 48, 1)\n",
            "iteration: 30515 train shape: (96372, 48, 48, 1)\n",
            "iteration: 30516 train shape: (96374, 48, 48, 1)\n",
            "iteration: 30517 train shape: (96376, 48, 48, 1)\n",
            "iteration: 30518 train shape: (96378, 48, 48, 1)\n",
            "iteration: 30519 train shape: (96380, 48, 48, 1)\n",
            "iteration: 30520 train shape: (96382, 48, 48, 1)\n",
            "iteration: 30521 train shape: (96384, 48, 48, 1)\n",
            "iteration: 30522 train shape: (96386, 48, 48, 1)\n",
            "iteration: 30523 train shape: (96388, 48, 48, 1)\n",
            "iteration: 30524 train shape: (96390, 48, 48, 1)\n",
            "iteration: 30525 train shape: (96392, 48, 48, 1)\n",
            "iteration: 30526 train shape: (96394, 48, 48, 1)\n",
            "iteration: 30527 train shape: (96396, 48, 48, 1)\n",
            "iteration: 30528 train shape: (96398, 48, 48, 1)\n",
            "iteration: 30529 train shape: (96400, 48, 48, 1)\n",
            "iteration: 30530 train shape: (96402, 48, 48, 1)\n",
            "iteration: 30531 train shape: (96404, 48, 48, 1)\n",
            "iteration: 30532 train shape: (96406, 48, 48, 1)\n",
            "iteration: 30533 train shape: (96408, 48, 48, 1)\n",
            "iteration: 30534 train shape: (96410, 48, 48, 1)\n",
            "iteration: 30535 train shape: (96412, 48, 48, 1)\n",
            "iteration: 30536 train shape: (96414, 48, 48, 1)\n",
            "iteration: 30537 train shape: (96416, 48, 48, 1)\n",
            "iteration: 30538 train shape: (96418, 48, 48, 1)\n",
            "iteration: 30539 train shape: (96420, 48, 48, 1)\n",
            "iteration: 30540 train shape: (96422, 48, 48, 1)\n",
            "iteration: 30541 train shape: (96424, 48, 48, 1)\n",
            "iteration: 30542 train shape: (96426, 48, 48, 1)\n",
            "iteration: 30543 train shape: (96428, 48, 48, 1)\n",
            "iteration: 30544 train shape: (96430, 48, 48, 1)\n",
            "iteration: 30545 train shape: (96432, 48, 48, 1)\n",
            "iteration: 30546 train shape: (96434, 48, 48, 1)\n",
            "iteration: 30547 train shape: (96436, 48, 48, 1)\n",
            "iteration: 30548 train shape: (96438, 48, 48, 1)\n",
            "iteration: 30549 train shape: (96440, 48, 48, 1)\n",
            "iteration: 30550 train shape: (96442, 48, 48, 1)\n",
            "iteration: 30551 train shape: (96444, 48, 48, 1)\n",
            "iteration: 30552 train shape: (96446, 48, 48, 1)\n",
            "iteration: 30553 train shape: (96448, 48, 48, 1)\n",
            "iteration: 30554 train shape: (96450, 48, 48, 1)\n",
            "iteration: 30555 train shape: (96452, 48, 48, 1)\n",
            "iteration: 30556 train shape: (96454, 48, 48, 1)\n",
            "iteration: 30557 train shape: (96456, 48, 48, 1)\n",
            "iteration: 30558 train shape: (96458, 48, 48, 1)\n",
            "iteration: 30559 train shape: (96460, 48, 48, 1)\n",
            "iteration: 30560 train shape: (96462, 48, 48, 1)\n",
            "iteration: 30561 train shape: (96464, 48, 48, 1)\n",
            "iteration: 30562 train shape: (96466, 48, 48, 1)\n",
            "iteration: 30563 train shape: (96468, 48, 48, 1)\n",
            "iteration: 30564 train shape: (96470, 48, 48, 1)\n",
            "iteration: 30565 train shape: (96472, 48, 48, 1)\n",
            "iteration: 30566 train shape: (96474, 48, 48, 1)\n",
            "iteration: 30567 train shape: (96476, 48, 48, 1)\n",
            "iteration: 30568 train shape: (96478, 48, 48, 1)\n",
            "iteration: 30569 train shape: (96480, 48, 48, 1)\n",
            "iteration: 30570 train shape: (96482, 48, 48, 1)\n",
            "iteration: 30571 train shape: (96484, 48, 48, 1)\n",
            "iteration: 30572 train shape: (96486, 48, 48, 1)\n",
            "iteration: 30573 train shape: (96488, 48, 48, 1)\n",
            "iteration: 30574 train shape: (96490, 48, 48, 1)\n",
            "iteration: 30575 train shape: (96492, 48, 48, 1)\n",
            "iteration: 30576 train shape: (96494, 48, 48, 1)\n",
            "iteration: 30577 train shape: (96496, 48, 48, 1)\n",
            "iteration: 30578 train shape: (96498, 48, 48, 1)\n",
            "iteration: 30579 train shape: (96500, 48, 48, 1)\n",
            "iteration: 30580 train shape: (96502, 48, 48, 1)\n",
            "iteration: 30581 train shape: (96504, 48, 48, 1)\n",
            "iteration: 30582 train shape: (96506, 48, 48, 1)\n",
            "iteration: 30583 train shape: (96508, 48, 48, 1)\n",
            "iteration: 30584 train shape: (96510, 48, 48, 1)\n",
            "iteration: 30585 train shape: (96512, 48, 48, 1)\n",
            "iteration: 30586 train shape: (96514, 48, 48, 1)\n",
            "iteration: 30587 train shape: (96516, 48, 48, 1)\n",
            "iteration: 30588 train shape: (96518, 48, 48, 1)\n",
            "iteration: 30589 train shape: (96520, 48, 48, 1)\n",
            "iteration: 30590 train shape: (96522, 48, 48, 1)\n",
            "iteration: 30591 train shape: (96524, 48, 48, 1)\n",
            "iteration: 30592 train shape: (96526, 48, 48, 1)\n",
            "iteration: 30593 train shape: (96528, 48, 48, 1)\n",
            "iteration: 30594 train shape: (96530, 48, 48, 1)\n",
            "iteration: 30595 train shape: (96532, 48, 48, 1)\n",
            "iteration: 30596 train shape: (96534, 48, 48, 1)\n",
            "iteration: 30597 train shape: (96536, 48, 48, 1)\n",
            "iteration: 30598 train shape: (96538, 48, 48, 1)\n",
            "iteration: 30599 train shape: (96540, 48, 48, 1)\n",
            "iteration: 30600 train shape: (96542, 48, 48, 1)\n",
            "iteration: 30601 train shape: (96544, 48, 48, 1)\n",
            "iteration: 30602 train shape: (96546, 48, 48, 1)\n",
            "iteration: 30603 train shape: (96548, 48, 48, 1)\n",
            "iteration: 30604 train shape: (96550, 48, 48, 1)\n",
            "iteration: 30605 train shape: (96552, 48, 48, 1)\n",
            "iteration: 30606 train shape: (96554, 48, 48, 1)\n",
            "iteration: 30607 train shape: (96556, 48, 48, 1)\n",
            "iteration: 30608 train shape: (96558, 48, 48, 1)\n",
            "iteration: 30609 train shape: (96560, 48, 48, 1)\n",
            "iteration: 30610 train shape: (96562, 48, 48, 1)\n",
            "iteration: 30611 train shape: (96564, 48, 48, 1)\n",
            "iteration: 30612 train shape: (96566, 48, 48, 1)\n",
            "iteration: 30613 train shape: (96568, 48, 48, 1)\n",
            "iteration: 30614 train shape: (96570, 48, 48, 1)\n",
            "iteration: 30615 train shape: (96572, 48, 48, 1)\n",
            "iteration: 30616 train shape: (96574, 48, 48, 1)\n",
            "iteration: 30617 train shape: (96576, 48, 48, 1)\n",
            "iteration: 30618 train shape: (96578, 48, 48, 1)\n",
            "iteration: 30619 train shape: (96580, 48, 48, 1)\n",
            "iteration: 30620 train shape: (96582, 48, 48, 1)\n",
            "iteration: 30621 train shape: (96584, 48, 48, 1)\n",
            "iteration: 30622 train shape: (96586, 48, 48, 1)\n",
            "iteration: 30623 train shape: (96588, 48, 48, 1)\n",
            "iteration: 30624 train shape: (96590, 48, 48, 1)\n",
            "iteration: 30625 train shape: (96592, 48, 48, 1)\n",
            "iteration: 30626 train shape: (96594, 48, 48, 1)\n",
            "iteration: 30627 train shape: (96596, 48, 48, 1)\n",
            "iteration: 30628 train shape: (96598, 48, 48, 1)\n",
            "iteration: 30629 train shape: (96600, 48, 48, 1)\n",
            "iteration: 30630 train shape: (96602, 48, 48, 1)\n",
            "iteration: 30631 train shape: (96604, 48, 48, 1)\n",
            "iteration: 30632 train shape: (96606, 48, 48, 1)\n",
            "iteration: 30633 train shape: (96608, 48, 48, 1)\n",
            "iteration: 30634 train shape: (96610, 48, 48, 1)\n",
            "iteration: 30635 train shape: (96612, 48, 48, 1)\n",
            "iteration: 30636 train shape: (96614, 48, 48, 1)\n",
            "iteration: 30637 train shape: (96616, 48, 48, 1)\n",
            "iteration: 30638 train shape: (96618, 48, 48, 1)\n",
            "iteration: 30639 train shape: (96620, 48, 48, 1)\n",
            "iteration: 30640 train shape: (96622, 48, 48, 1)\n",
            "iteration: 30641 train shape: (96624, 48, 48, 1)\n",
            "iteration: 30642 train shape: (96626, 48, 48, 1)\n",
            "iteration: 30643 train shape: (96628, 48, 48, 1)\n",
            "iteration: 30644 train shape: (96630, 48, 48, 1)\n",
            "iteration: 30645 train shape: (96632, 48, 48, 1)\n",
            "iteration: 30646 train shape: (96634, 48, 48, 1)\n",
            "iteration: 30647 train shape: (96636, 48, 48, 1)\n",
            "iteration: 30648 train shape: (96638, 48, 48, 1)\n",
            "iteration: 30649 train shape: (96640, 48, 48, 1)\n",
            "iteration: 30650 train shape: (96642, 48, 48, 1)\n",
            "iteration: 30651 train shape: (96644, 48, 48, 1)\n",
            "iteration: 30652 train shape: (96646, 48, 48, 1)\n",
            "iteration: 30653 train shape: (96648, 48, 48, 1)\n",
            "iteration: 30654 train shape: (96650, 48, 48, 1)\n",
            "iteration: 30655 train shape: (96652, 48, 48, 1)\n",
            "iteration: 30656 train shape: (96654, 48, 48, 1)\n",
            "iteration: 30657 train shape: (96656, 48, 48, 1)\n",
            "iteration: 30658 train shape: (96658, 48, 48, 1)\n",
            "iteration: 30659 train shape: (96660, 48, 48, 1)\n",
            "iteration: 30660 train shape: (96662, 48, 48, 1)\n",
            "iteration: 30661 train shape: (96664, 48, 48, 1)\n",
            "iteration: 30662 train shape: (96666, 48, 48, 1)\n",
            "iteration: 30663 train shape: (96668, 48, 48, 1)\n",
            "iteration: 30664 train shape: (96670, 48, 48, 1)\n",
            "iteration: 30665 train shape: (96672, 48, 48, 1)\n",
            "iteration: 30666 train shape: (96674, 48, 48, 1)\n",
            "iteration: 30667 train shape: (96676, 48, 48, 1)\n",
            "iteration: 30668 train shape: (96678, 48, 48, 1)\n",
            "iteration: 30669 train shape: (96680, 48, 48, 1)\n",
            "iteration: 30670 train shape: (96682, 48, 48, 1)\n",
            "iteration: 30671 train shape: (96684, 48, 48, 1)\n",
            "iteration: 30672 train shape: (96686, 48, 48, 1)\n",
            "iteration: 30673 train shape: (96688, 48, 48, 1)\n",
            "iteration: 30674 train shape: (96690, 48, 48, 1)\n",
            "iteration: 30675 train shape: (96692, 48, 48, 1)\n",
            "iteration: 30676 train shape: (96694, 48, 48, 1)\n",
            "iteration: 30677 train shape: (96696, 48, 48, 1)\n",
            "iteration: 30678 train shape: (96698, 48, 48, 1)\n",
            "iteration: 30679 train shape: (96700, 48, 48, 1)\n",
            "iteration: 30680 train shape: (96702, 48, 48, 1)\n",
            "iteration: 30681 train shape: (96704, 48, 48, 1)\n",
            "iteration: 30682 train shape: (96706, 48, 48, 1)\n",
            "iteration: 30683 train shape: (96708, 48, 48, 1)\n",
            "iteration: 30684 train shape: (96710, 48, 48, 1)\n",
            "iteration: 30685 train shape: (96712, 48, 48, 1)\n",
            "iteration: 30686 train shape: (96714, 48, 48, 1)\n",
            "iteration: 30687 train shape: (96716, 48, 48, 1)\n",
            "iteration: 30688 train shape: (96718, 48, 48, 1)\n",
            "iteration: 30689 train shape: (96720, 48, 48, 1)\n",
            "iteration: 30690 train shape: (96722, 48, 48, 1)\n",
            "iteration: 30691 train shape: (96724, 48, 48, 1)\n",
            "iteration: 30692 train shape: (96726, 48, 48, 1)\n",
            "iteration: 30693 train shape: (96728, 48, 48, 1)\n",
            "iteration: 30694 train shape: (96730, 48, 48, 1)\n",
            "iteration: 30695 train shape: (96732, 48, 48, 1)\n",
            "iteration: 30696 train shape: (96734, 48, 48, 1)\n",
            "iteration: 30697 train shape: (96736, 48, 48, 1)\n",
            "iteration: 30698 train shape: (96738, 48, 48, 1)\n",
            "iteration: 30699 train shape: (96740, 48, 48, 1)\n",
            "iteration: 30700 train shape: (96742, 48, 48, 1)\n",
            "iteration: 30701 train shape: (96744, 48, 48, 1)\n",
            "iteration: 30702 train shape: (96746, 48, 48, 1)\n",
            "iteration: 30703 train shape: (96748, 48, 48, 1)\n",
            "iteration: 30704 train shape: (96750, 48, 48, 1)\n",
            "iteration: 30705 train shape: (96752, 48, 48, 1)\n",
            "iteration: 30706 train shape: (96754, 48, 48, 1)\n",
            "iteration: 30707 train shape: (96756, 48, 48, 1)\n",
            "iteration: 30708 train shape: (96758, 48, 48, 1)\n",
            "iteration: 30709 train shape: (96760, 48, 48, 1)\n",
            "iteration: 30710 train shape: (96762, 48, 48, 1)\n",
            "iteration: 30711 train shape: (96764, 48, 48, 1)\n",
            "iteration: 30712 train shape: (96766, 48, 48, 1)\n",
            "iteration: 30713 train shape: (96768, 48, 48, 1)\n",
            "iteration: 30714 train shape: (96770, 48, 48, 1)\n",
            "iteration: 30715 train shape: (96772, 48, 48, 1)\n",
            "iteration: 30716 train shape: (96774, 48, 48, 1)\n",
            "iteration: 30717 train shape: (96776, 48, 48, 1)\n",
            "iteration: 30718 train shape: (96778, 48, 48, 1)\n",
            "iteration: 30719 train shape: (96780, 48, 48, 1)\n",
            "iteration: 30720 train shape: (96782, 48, 48, 1)\n",
            "iteration: 30721 train shape: (96784, 48, 48, 1)\n",
            "iteration: 30722 train shape: (96786, 48, 48, 1)\n",
            "iteration: 30723 train shape: (96788, 48, 48, 1)\n",
            "iteration: 30724 train shape: (96790, 48, 48, 1)\n",
            "iteration: 30725 train shape: (96792, 48, 48, 1)\n",
            "iteration: 30726 train shape: (96794, 48, 48, 1)\n",
            "iteration: 30727 train shape: (96796, 48, 48, 1)\n",
            "iteration: 30728 train shape: (96798, 48, 48, 1)\n",
            "iteration: 30729 train shape: (96800, 48, 48, 1)\n",
            "iteration: 30730 train shape: (96802, 48, 48, 1)\n",
            "iteration: 30731 train shape: (96804, 48, 48, 1)\n",
            "iteration: 30732 train shape: (96806, 48, 48, 1)\n",
            "iteration: 30733 train shape: (96808, 48, 48, 1)\n",
            "iteration: 30734 train shape: (96810, 48, 48, 1)\n",
            "iteration: 30735 train shape: (96812, 48, 48, 1)\n",
            "iteration: 30736 train shape: (96814, 48, 48, 1)\n",
            "iteration: 30737 train shape: (96816, 48, 48, 1)\n",
            "iteration: 30738 train shape: (96818, 48, 48, 1)\n",
            "iteration: 30739 train shape: (96820, 48, 48, 1)\n",
            "iteration: 30740 train shape: (96822, 48, 48, 1)\n",
            "iteration: 30741 train shape: (96824, 48, 48, 1)\n",
            "iteration: 30742 train shape: (96826, 48, 48, 1)\n",
            "iteration: 30743 train shape: (96828, 48, 48, 1)\n",
            "iteration: 30744 train shape: (96830, 48, 48, 1)\n",
            "iteration: 30745 train shape: (96832, 48, 48, 1)\n",
            "iteration: 30746 train shape: (96834, 48, 48, 1)\n",
            "iteration: 30747 train shape: (96836, 48, 48, 1)\n",
            "iteration: 30748 train shape: (96838, 48, 48, 1)\n",
            "iteration: 30749 train shape: (96840, 48, 48, 1)\n",
            "iteration: 30750 train shape: (96842, 48, 48, 1)\n",
            "iteration: 30751 train shape: (96844, 48, 48, 1)\n",
            "iteration: 30752 train shape: (96846, 48, 48, 1)\n",
            "iteration: 30753 train shape: (96848, 48, 48, 1)\n",
            "iteration: 30754 train shape: (96850, 48, 48, 1)\n",
            "iteration: 30755 train shape: (96852, 48, 48, 1)\n",
            "iteration: 30756 train shape: (96854, 48, 48, 1)\n",
            "iteration: 30757 train shape: (96856, 48, 48, 1)\n",
            "iteration: 30758 train shape: (96858, 48, 48, 1)\n",
            "iteration: 30759 train shape: (96860, 48, 48, 1)\n",
            "iteration: 30760 train shape: (96862, 48, 48, 1)\n",
            "iteration: 30761 train shape: (96864, 48, 48, 1)\n",
            "iteration: 30762 train shape: (96866, 48, 48, 1)\n",
            "iteration: 30763 train shape: (96868, 48, 48, 1)\n",
            "iteration: 30764 train shape: (96870, 48, 48, 1)\n",
            "iteration: 30765 train shape: (96872, 48, 48, 1)\n",
            "iteration: 30766 train shape: (96874, 48, 48, 1)\n",
            "iteration: 30767 train shape: (96876, 48, 48, 1)\n",
            "iteration: 30768 train shape: (96878, 48, 48, 1)\n",
            "iteration: 30769 train shape: (96880, 48, 48, 1)\n",
            "iteration: 30770 train shape: (96882, 48, 48, 1)\n",
            "iteration: 30771 train shape: (96884, 48, 48, 1)\n",
            "iteration: 30772 train shape: (96886, 48, 48, 1)\n",
            "iteration: 30773 train shape: (96888, 48, 48, 1)\n",
            "iteration: 30774 train shape: (96890, 48, 48, 1)\n",
            "iteration: 30775 train shape: (96892, 48, 48, 1)\n",
            "iteration: 30776 train shape: (96894, 48, 48, 1)\n",
            "iteration: 30777 train shape: (96896, 48, 48, 1)\n",
            "iteration: 30778 train shape: (96898, 48, 48, 1)\n",
            "iteration: 30779 train shape: (96900, 48, 48, 1)\n",
            "iteration: 30780 train shape: (96902, 48, 48, 1)\n",
            "iteration: 30781 train shape: (96904, 48, 48, 1)\n",
            "iteration: 30782 train shape: (96906, 48, 48, 1)\n",
            "iteration: 30783 train shape: (96908, 48, 48, 1)\n",
            "iteration: 30784 train shape: (96910, 48, 48, 1)\n",
            "iteration: 30785 train shape: (96912, 48, 48, 1)\n",
            "iteration: 30786 train shape: (96914, 48, 48, 1)\n",
            "iteration: 30787 train shape: (96916, 48, 48, 1)\n",
            "iteration: 30788 train shape: (96918, 48, 48, 1)\n",
            "iteration: 30789 train shape: (96920, 48, 48, 1)\n",
            "iteration: 30790 train shape: (96922, 48, 48, 1)\n",
            "iteration: 30791 train shape: (96924, 48, 48, 1)\n",
            "iteration: 30792 train shape: (96926, 48, 48, 1)\n",
            "iteration: 30793 train shape: (96928, 48, 48, 1)\n",
            "iteration: 30794 train shape: (96930, 48, 48, 1)\n",
            "iteration: 30795 train shape: (96932, 48, 48, 1)\n",
            "iteration: 30796 train shape: (96934, 48, 48, 1)\n",
            "iteration: 30797 train shape: (96936, 48, 48, 1)\n",
            "iteration: 30798 train shape: (96938, 48, 48, 1)\n",
            "iteration: 30799 train shape: (96940, 48, 48, 1)\n",
            "iteration: 30800 train shape: (96942, 48, 48, 1)\n",
            "iteration: 30801 train shape: (96944, 48, 48, 1)\n",
            "iteration: 30802 train shape: (96946, 48, 48, 1)\n",
            "iteration: 30803 train shape: (96948, 48, 48, 1)\n",
            "iteration: 30804 train shape: (96950, 48, 48, 1)\n",
            "iteration: 30807 train shape: (96956, 48, 48, 1)\n",
            "iteration: 30808 train shape: (96958, 48, 48, 1)\n",
            "iteration: 30809 train shape: (96960, 48, 48, 1)\n",
            "iteration: 30810 train shape: (96962, 48, 48, 1)\n",
            "iteration: 30811 train shape: (96964, 48, 48, 1)\n",
            "iteration: 30812 train shape: (96966, 48, 48, 1)\n",
            "iteration: 30813 train shape: (96968, 48, 48, 1)\n",
            "iteration: 30814 train shape: (96970, 48, 48, 1)\n",
            "iteration: 30815 train shape: (96972, 48, 48, 1)\n",
            "iteration: 30816 train shape: (96974, 48, 48, 1)\n",
            "iteration: 30817 train shape: (96976, 48, 48, 1)\n",
            "iteration: 30818 train shape: (96978, 48, 48, 1)\n",
            "iteration: 30819 train shape: (96980, 48, 48, 1)\n",
            "iteration: 30820 train shape: (96982, 48, 48, 1)\n",
            "iteration: 30821 train shape: (96984, 48, 48, 1)\n",
            "iteration: 30822 train shape: (96986, 48, 48, 1)\n",
            "iteration: 30823 train shape: (96988, 48, 48, 1)\n",
            "iteration: 30824 train shape: (96990, 48, 48, 1)\n",
            "iteration: 30825 train shape: (96992, 48, 48, 1)\n",
            "iteration: 30826 train shape: (96994, 48, 48, 1)\n",
            "iteration: 30827 train shape: (96996, 48, 48, 1)\n",
            "iteration: 30828 train shape: (96998, 48, 48, 1)\n",
            "iteration: 30829 train shape: (97000, 48, 48, 1)\n",
            "iteration: 30830 train shape: (97002, 48, 48, 1)\n",
            "iteration: 30831 train shape: (97004, 48, 48, 1)\n",
            "iteration: 30832 train shape: (97006, 48, 48, 1)\n",
            "iteration: 30833 train shape: (97008, 48, 48, 1)\n",
            "iteration: 30834 train shape: (97010, 48, 48, 1)\n",
            "iteration: 30835 train shape: (97012, 48, 48, 1)\n",
            "iteration: 30836 train shape: (97014, 48, 48, 1)\n",
            "iteration: 30837 train shape: (97016, 48, 48, 1)\n",
            "iteration: 30838 train shape: (97018, 48, 48, 1)\n",
            "iteration: 30839 train shape: (97020, 48, 48, 1)\n",
            "iteration: 30840 train shape: (97022, 48, 48, 1)\n",
            "iteration: 30841 train shape: (97024, 48, 48, 1)\n",
            "iteration: 30842 train shape: (97026, 48, 48, 1)\n",
            "iteration: 30843 train shape: (97028, 48, 48, 1)\n",
            "iteration: 30844 train shape: (97030, 48, 48, 1)\n",
            "iteration: 30845 train shape: (97032, 48, 48, 1)\n",
            "iteration: 30846 train shape: (97034, 48, 48, 1)\n",
            "iteration: 30847 train shape: (97036, 48, 48, 1)\n",
            "iteration: 30848 train shape: (97038, 48, 48, 1)\n",
            "iteration: 30849 train shape: (97040, 48, 48, 1)\n",
            "iteration: 30850 train shape: (97042, 48, 48, 1)\n",
            "iteration: 30851 train shape: (97044, 48, 48, 1)\n",
            "iteration: 30852 train shape: (97046, 48, 48, 1)\n",
            "iteration: 30853 train shape: (97048, 48, 48, 1)\n",
            "iteration: 30854 train shape: (97050, 48, 48, 1)\n",
            "iteration: 30855 train shape: (97052, 48, 48, 1)\n",
            "iteration: 30856 train shape: (97054, 48, 48, 1)\n",
            "iteration: 30857 train shape: (97056, 48, 48, 1)\n",
            "iteration: 30858 train shape: (97058, 48, 48, 1)\n",
            "iteration: 30859 train shape: (97060, 48, 48, 1)\n",
            "iteration: 30860 train shape: (97062, 48, 48, 1)\n",
            "iteration: 30861 train shape: (97064, 48, 48, 1)\n",
            "iteration: 30862 train shape: (97066, 48, 48, 1)\n",
            "iteration: 30863 train shape: (97068, 48, 48, 1)\n",
            "iteration: 30864 train shape: (97070, 48, 48, 1)\n",
            "iteration: 30865 train shape: (97072, 48, 48, 1)\n",
            "iteration: 30866 train shape: (97074, 48, 48, 1)\n",
            "iteration: 30867 train shape: (97076, 48, 48, 1)\n",
            "iteration: 30868 train shape: (97078, 48, 48, 1)\n",
            "iteration: 30869 train shape: (97080, 48, 48, 1)\n",
            "iteration: 30870 train shape: (97082, 48, 48, 1)\n",
            "iteration: 30871 train shape: (97084, 48, 48, 1)\n",
            "iteration: 30872 train shape: (97086, 48, 48, 1)\n",
            "iteration: 30873 train shape: (97088, 48, 48, 1)\n",
            "iteration: 30874 train shape: (97090, 48, 48, 1)\n",
            "iteration: 30875 train shape: (97092, 48, 48, 1)\n",
            "iteration: 30876 train shape: (97094, 48, 48, 1)\n",
            "iteration: 30877 train shape: (97096, 48, 48, 1)\n",
            "iteration: 30878 train shape: (97098, 48, 48, 1)\n",
            "iteration: 30879 train shape: (97100, 48, 48, 1)\n",
            "iteration: 30880 train shape: (97102, 48, 48, 1)\n",
            "iteration: 30881 train shape: (97104, 48, 48, 1)\n",
            "iteration: 30882 train shape: (97106, 48, 48, 1)\n",
            "iteration: 30883 train shape: (97108, 48, 48, 1)\n",
            "iteration: 30884 train shape: (97110, 48, 48, 1)\n",
            "iteration: 30885 train shape: (97112, 48, 48, 1)\n",
            "iteration: 30886 train shape: (97114, 48, 48, 1)\n",
            "iteration: 30887 train shape: (97116, 48, 48, 1)\n",
            "iteration: 30888 train shape: (97118, 48, 48, 1)\n",
            "iteration: 30889 train shape: (97120, 48, 48, 1)\n",
            "iteration: 30890 train shape: (97122, 48, 48, 1)\n",
            "iteration: 30891 train shape: (97124, 48, 48, 1)\n",
            "iteration: 30892 train shape: (97126, 48, 48, 1)\n",
            "iteration: 30893 train shape: (97128, 48, 48, 1)\n",
            "iteration: 30894 train shape: (97130, 48, 48, 1)\n",
            "iteration: 30895 train shape: (97132, 48, 48, 1)\n",
            "iteration: 30896 train shape: (97134, 48, 48, 1)\n",
            "iteration: 30897 train shape: (97136, 48, 48, 1)\n",
            "iteration: 30898 train shape: (97138, 48, 48, 1)\n",
            "iteration: 30899 train shape: (97140, 48, 48, 1)\n",
            "iteration: 30900 train shape: (97142, 48, 48, 1)\n",
            "iteration: 30901 train shape: (97144, 48, 48, 1)\n",
            "iteration: 30902 train shape: (97146, 48, 48, 1)\n",
            "iteration: 30903 train shape: (97148, 48, 48, 1)\n",
            "iteration: 30904 train shape: (97150, 48, 48, 1)\n",
            "iteration: 30905 train shape: (97152, 48, 48, 1)\n",
            "iteration: 30906 train shape: (97154, 48, 48, 1)\n",
            "iteration: 30907 train shape: (97156, 48, 48, 1)\n",
            "iteration: 30908 train shape: (97158, 48, 48, 1)\n",
            "iteration: 30909 train shape: (97160, 48, 48, 1)\n",
            "iteration: 30910 train shape: (97162, 48, 48, 1)\n",
            "iteration: 30911 train shape: (97164, 48, 48, 1)\n",
            "iteration: 30912 train shape: (97166, 48, 48, 1)\n",
            "iteration: 30913 train shape: (97168, 48, 48, 1)\n",
            "iteration: 30914 train shape: (97170, 48, 48, 1)\n",
            "iteration: 30915 train shape: (97172, 48, 48, 1)\n",
            "iteration: 30916 train shape: (97174, 48, 48, 1)\n",
            "iteration: 30917 train shape: (97176, 48, 48, 1)\n",
            "iteration: 30918 train shape: (97178, 48, 48, 1)\n",
            "iteration: 30919 train shape: (97180, 48, 48, 1)\n",
            "iteration: 30920 train shape: (97182, 48, 48, 1)\n",
            "iteration: 30921 train shape: (97184, 48, 48, 1)\n",
            "iteration: 30922 train shape: (97186, 48, 48, 1)\n",
            "iteration: 30923 train shape: (97188, 48, 48, 1)\n",
            "iteration: 30924 train shape: (97190, 48, 48, 1)\n",
            "iteration: 30925 train shape: (97192, 48, 48, 1)\n",
            "iteration: 30926 train shape: (97194, 48, 48, 1)\n",
            "iteration: 30927 train shape: (97196, 48, 48, 1)\n",
            "iteration: 30928 train shape: (97198, 48, 48, 1)\n",
            "iteration: 30929 train shape: (97200, 48, 48, 1)\n",
            "iteration: 30930 train shape: (97202, 48, 48, 1)\n",
            "iteration: 30931 train shape: (97204, 48, 48, 1)\n",
            "iteration: 30932 train shape: (97206, 48, 48, 1)\n",
            "iteration: 30933 train shape: (97208, 48, 48, 1)\n",
            "iteration: 30934 train shape: (97210, 48, 48, 1)\n",
            "iteration: 30935 train shape: (97212, 48, 48, 1)\n",
            "iteration: 30936 train shape: (97214, 48, 48, 1)\n",
            "iteration: 30937 train shape: (97216, 48, 48, 1)\n",
            "iteration: 30938 train shape: (97218, 48, 48, 1)\n",
            "iteration: 30939 train shape: (97220, 48, 48, 1)\n",
            "iteration: 30940 train shape: (97222, 48, 48, 1)\n",
            "iteration: 30941 train shape: (97224, 48, 48, 1)\n",
            "iteration: 30942 train shape: (97226, 48, 48, 1)\n",
            "iteration: 30943 train shape: (97228, 48, 48, 1)\n",
            "iteration: 30944 train shape: (97230, 48, 48, 1)\n",
            "iteration: 30945 train shape: (97232, 48, 48, 1)\n",
            "iteration: 30946 train shape: (97234, 48, 48, 1)\n",
            "iteration: 30947 train shape: (97236, 48, 48, 1)\n",
            "iteration: 30948 train shape: (97238, 48, 48, 1)\n",
            "iteration: 30949 train shape: (97240, 48, 48, 1)\n",
            "iteration: 30950 train shape: (97242, 48, 48, 1)\n",
            "iteration: 30951 train shape: (97244, 48, 48, 1)\n",
            "iteration: 30952 train shape: (97246, 48, 48, 1)\n",
            "iteration: 30953 train shape: (97248, 48, 48, 1)\n",
            "iteration: 30954 train shape: (97250, 48, 48, 1)\n",
            "iteration: 30955 train shape: (97252, 48, 48, 1)\n",
            "iteration: 30956 train shape: (97254, 48, 48, 1)\n",
            "iteration: 30957 train shape: (97256, 48, 48, 1)\n",
            "iteration: 30958 train shape: (97258, 48, 48, 1)\n",
            "iteration: 30959 train shape: (97260, 48, 48, 1)\n",
            "iteration: 30960 train shape: (97262, 48, 48, 1)\n",
            "iteration: 30961 train shape: (97264, 48, 48, 1)\n",
            "iteration: 30962 train shape: (97266, 48, 48, 1)\n",
            "iteration: 30963 train shape: (97268, 48, 48, 1)\n",
            "iteration: 30964 train shape: (97270, 48, 48, 1)\n",
            "iteration: 30965 train shape: (97272, 48, 48, 1)\n",
            "iteration: 30966 train shape: (97274, 48, 48, 1)\n",
            "iteration: 30967 train shape: (97276, 48, 48, 1)\n",
            "iteration: 30968 train shape: (97278, 48, 48, 1)\n",
            "iteration: 30969 train shape: (97280, 48, 48, 1)\n",
            "iteration: 30970 train shape: (97282, 48, 48, 1)\n",
            "iteration: 30971 train shape: (97284, 48, 48, 1)\n",
            "iteration: 30972 train shape: (97286, 48, 48, 1)\n",
            "iteration: 30973 train shape: (97288, 48, 48, 1)\n",
            "iteration: 30974 train shape: (97290, 48, 48, 1)\n",
            "iteration: 30975 train shape: (97292, 48, 48, 1)\n",
            "iteration: 30976 train shape: (97294, 48, 48, 1)\n",
            "iteration: 30977 train shape: (97296, 48, 48, 1)\n",
            "iteration: 30978 train shape: (97298, 48, 48, 1)\n",
            "iteration: 30979 train shape: (97300, 48, 48, 1)\n",
            "iteration: 30980 train shape: (97302, 48, 48, 1)\n",
            "iteration: 30981 train shape: (97304, 48, 48, 1)\n",
            "iteration: 30982 train shape: (97306, 48, 48, 1)\n",
            "iteration: 30983 train shape: (97308, 48, 48, 1)\n",
            "iteration: 30984 train shape: (97310, 48, 48, 1)\n",
            "iteration: 30985 train shape: (97312, 48, 48, 1)\n",
            "iteration: 30986 train shape: (97314, 48, 48, 1)\n",
            "iteration: 30987 train shape: (97316, 48, 48, 1)\n",
            "iteration: 30988 train shape: (97318, 48, 48, 1)\n",
            "iteration: 30989 train shape: (97320, 48, 48, 1)\n",
            "iteration: 30990 train shape: (97322, 48, 48, 1)\n",
            "iteration: 30991 train shape: (97324, 48, 48, 1)\n",
            "iteration: 30992 train shape: (97326, 48, 48, 1)\n",
            "iteration: 30993 train shape: (97328, 48, 48, 1)\n",
            "iteration: 30994 train shape: (97330, 48, 48, 1)\n",
            "iteration: 30995 train shape: (97332, 48, 48, 1)\n",
            "iteration: 30996 train shape: (97334, 48, 48, 1)\n",
            "iteration: 30997 train shape: (97336, 48, 48, 1)\n",
            "iteration: 30998 train shape: (97338, 48, 48, 1)\n",
            "iteration: 30999 train shape: (97340, 48, 48, 1)\n",
            "iteration: 31000 train shape: (97342, 48, 48, 1)\n",
            "iteration: 31001 train shape: (97344, 48, 48, 1)\n",
            "iteration: 31002 train shape: (97346, 48, 48, 1)\n",
            "iteration: 31003 train shape: (97348, 48, 48, 1)\n",
            "iteration: 31004 train shape: (97350, 48, 48, 1)\n",
            "iteration: 31005 train shape: (97352, 48, 48, 1)\n",
            "iteration: 31006 train shape: (97354, 48, 48, 1)\n",
            "iteration: 31007 train shape: (97356, 48, 48, 1)\n",
            "iteration: 31008 train shape: (97358, 48, 48, 1)\n",
            "iteration: 31009 train shape: (97360, 48, 48, 1)\n",
            "iteration: 31010 train shape: (97362, 48, 48, 1)\n",
            "iteration: 31011 train shape: (97364, 48, 48, 1)\n",
            "iteration: 31012 train shape: (97366, 48, 48, 1)\n",
            "iteration: 31013 train shape: (97368, 48, 48, 1)\n",
            "iteration: 31014 train shape: (97370, 48, 48, 1)\n",
            "iteration: 31015 train shape: (97372, 48, 48, 1)\n",
            "iteration: 31016 train shape: (97374, 48, 48, 1)\n",
            "iteration: 31017 train shape: (97376, 48, 48, 1)\n",
            "iteration: 31018 train shape: (97378, 48, 48, 1)\n",
            "iteration: 31019 train shape: (97380, 48, 48, 1)\n",
            "iteration: 31020 train shape: (97382, 48, 48, 1)\n",
            "iteration: 31021 train shape: (97384, 48, 48, 1)\n",
            "iteration: 31022 train shape: (97386, 48, 48, 1)\n",
            "iteration: 31023 train shape: (97388, 48, 48, 1)\n",
            "iteration: 31024 train shape: (97390, 48, 48, 1)\n",
            "iteration: 31025 train shape: (97392, 48, 48, 1)\n",
            "iteration: 31026 train shape: (97394, 48, 48, 1)\n",
            "iteration: 31027 train shape: (97396, 48, 48, 1)\n",
            "iteration: 31028 train shape: (97398, 48, 48, 1)\n",
            "iteration: 31029 train shape: (97400, 48, 48, 1)\n",
            "iteration: 31030 train shape: (97402, 48, 48, 1)\n",
            "iteration: 31031 train shape: (97404, 48, 48, 1)\n",
            "iteration: 31032 train shape: (97406, 48, 48, 1)\n",
            "iteration: 31033 train shape: (97408, 48, 48, 1)\n",
            "iteration: 31034 train shape: (97410, 48, 48, 1)\n",
            "iteration: 31035 train shape: (97412, 48, 48, 1)\n",
            "iteration: 31036 train shape: (97414, 48, 48, 1)\n",
            "iteration: 31037 train shape: (97416, 48, 48, 1)\n",
            "iteration: 31038 train shape: (97418, 48, 48, 1)\n",
            "iteration: 31039 train shape: (97420, 48, 48, 1)\n",
            "iteration: 31040 train shape: (97422, 48, 48, 1)\n",
            "iteration: 31041 train shape: (97424, 48, 48, 1)\n",
            "iteration: 31042 train shape: (97426, 48, 48, 1)\n",
            "iteration: 31043 train shape: (97428, 48, 48, 1)\n",
            "iteration: 31044 train shape: (97430, 48, 48, 1)\n",
            "iteration: 31045 train shape: (97432, 48, 48, 1)\n",
            "iteration: 31046 train shape: (97434, 48, 48, 1)\n",
            "iteration: 31047 train shape: (97436, 48, 48, 1)\n",
            "iteration: 31048 train shape: (97438, 48, 48, 1)\n",
            "iteration: 31049 train shape: (97440, 48, 48, 1)\n",
            "iteration: 31050 train shape: (97442, 48, 48, 1)\n",
            "iteration: 31051 train shape: (97444, 48, 48, 1)\n",
            "iteration: 31052 train shape: (97446, 48, 48, 1)\n",
            "iteration: 31053 train shape: (97448, 48, 48, 1)\n",
            "iteration: 31054 train shape: (97450, 48, 48, 1)\n",
            "iteration: 31055 train shape: (97452, 48, 48, 1)\n",
            "iteration: 31056 train shape: (97454, 48, 48, 1)\n",
            "iteration: 31057 train shape: (97456, 48, 48, 1)\n",
            "iteration: 31058 train shape: (97458, 48, 48, 1)\n",
            "iteration: 31059 train shape: (97460, 48, 48, 1)\n",
            "iteration: 31060 train shape: (97462, 48, 48, 1)\n",
            "iteration: 31061 train shape: (97464, 48, 48, 1)\n",
            "iteration: 31062 train shape: (97466, 48, 48, 1)\n",
            "iteration: 31063 train shape: (97468, 48, 48, 1)\n",
            "iteration: 31064 train shape: (97470, 48, 48, 1)\n",
            "iteration: 31065 train shape: (97472, 48, 48, 1)\n",
            "iteration: 31066 train shape: (97474, 48, 48, 1)\n",
            "iteration: 31067 train shape: (97476, 48, 48, 1)\n",
            "iteration: 31068 train shape: (97478, 48, 48, 1)\n",
            "iteration: 31069 train shape: (97480, 48, 48, 1)\n",
            "iteration: 31070 train shape: (97482, 48, 48, 1)\n",
            "iteration: 31071 train shape: (97484, 48, 48, 1)\n",
            "iteration: 31072 train shape: (97486, 48, 48, 1)\n",
            "iteration: 31073 train shape: (97488, 48, 48, 1)\n",
            "iteration: 31074 train shape: (97490, 48, 48, 1)\n",
            "iteration: 31075 train shape: (97492, 48, 48, 1)\n",
            "iteration: 31076 train shape: (97494, 48, 48, 1)\n",
            "iteration: 31077 train shape: (97496, 48, 48, 1)\n",
            "iteration: 31078 train shape: (97498, 48, 48, 1)\n",
            "iteration: 31079 train shape: (97500, 48, 48, 1)\n",
            "iteration: 31080 train shape: (97502, 48, 48, 1)\n",
            "iteration: 31081 train shape: (97504, 48, 48, 1)\n",
            "iteration: 31082 train shape: (97506, 48, 48, 1)\n",
            "iteration: 31083 train shape: (97508, 48, 48, 1)\n",
            "iteration: 31084 train shape: (97510, 48, 48, 1)\n",
            "iteration: 31085 train shape: (97512, 48, 48, 1)\n",
            "iteration: 31086 train shape: (97514, 48, 48, 1)\n",
            "iteration: 31087 train shape: (97516, 48, 48, 1)\n",
            "iteration: 31088 train shape: (97518, 48, 48, 1)\n",
            "iteration: 31089 train shape: (97520, 48, 48, 1)\n",
            "iteration: 31090 train shape: (97522, 48, 48, 1)\n",
            "iteration: 31091 train shape: (97524, 48, 48, 1)\n",
            "iteration: 31092 train shape: (97526, 48, 48, 1)\n",
            "iteration: 31093 train shape: (97528, 48, 48, 1)\n",
            "iteration: 31094 train shape: (97530, 48, 48, 1)\n",
            "iteration: 31095 train shape: (97532, 48, 48, 1)\n",
            "iteration: 31096 train shape: (97534, 48, 48, 1)\n",
            "iteration: 31097 train shape: (97536, 48, 48, 1)\n",
            "iteration: 31098 train shape: (97538, 48, 48, 1)\n",
            "iteration: 31099 train shape: (97540, 48, 48, 1)\n",
            "iteration: 31100 train shape: (97542, 48, 48, 1)\n",
            "iteration: 31101 train shape: (97544, 48, 48, 1)\n",
            "iteration: 31102 train shape: (97546, 48, 48, 1)\n",
            "iteration: 31103 train shape: (97548, 48, 48, 1)\n",
            "iteration: 31104 train shape: (97550, 48, 48, 1)\n",
            "iteration: 31105 train shape: (97552, 48, 48, 1)\n",
            "iteration: 31106 train shape: (97554, 48, 48, 1)\n",
            "iteration: 31107 train shape: (97556, 48, 48, 1)\n",
            "iteration: 31108 train shape: (97558, 48, 48, 1)\n",
            "iteration: 31109 train shape: (97560, 48, 48, 1)\n",
            "iteration: 31110 train shape: (97562, 48, 48, 1)\n",
            "iteration: 31111 train shape: (97564, 48, 48, 1)\n",
            "iteration: 31112 train shape: (97566, 48, 48, 1)\n",
            "iteration: 31113 train shape: (97568, 48, 48, 1)\n",
            "iteration: 31114 train shape: (97570, 48, 48, 1)\n",
            "iteration: 31115 train shape: (97572, 48, 48, 1)\n",
            "iteration: 31116 train shape: (97574, 48, 48, 1)\n",
            "iteration: 31117 train shape: (97576, 48, 48, 1)\n",
            "iteration: 31118 train shape: (97578, 48, 48, 1)\n",
            "iteration: 31119 train shape: (97580, 48, 48, 1)\n",
            "iteration: 31120 train shape: (97582, 48, 48, 1)\n",
            "iteration: 31121 train shape: (97584, 48, 48, 1)\n",
            "iteration: 31122 train shape: (97586, 48, 48, 1)\n",
            "iteration: 31123 train shape: (97588, 48, 48, 1)\n",
            "iteration: 31124 train shape: (97590, 48, 48, 1)\n",
            "iteration: 31125 train shape: (97592, 48, 48, 1)\n",
            "iteration: 31126 train shape: (97594, 48, 48, 1)\n",
            "iteration: 31127 train shape: (97596, 48, 48, 1)\n",
            "iteration: 31128 train shape: (97598, 48, 48, 1)\n",
            "iteration: 31129 train shape: (97600, 48, 48, 1)\n",
            "iteration: 31130 train shape: (97602, 48, 48, 1)\n",
            "iteration: 31131 train shape: (97604, 48, 48, 1)\n",
            "iteration: 31132 train shape: (97606, 48, 48, 1)\n",
            "iteration: 31133 train shape: (97608, 48, 48, 1)\n",
            "iteration: 31134 train shape: (97610, 48, 48, 1)\n",
            "iteration: 31135 train shape: (97612, 48, 48, 1)\n",
            "iteration: 31136 train shape: (97614, 48, 48, 1)\n",
            "iteration: 31137 train shape: (97616, 48, 48, 1)\n",
            "iteration: 31138 train shape: (97618, 48, 48, 1)\n",
            "iteration: 31139 train shape: (97620, 48, 48, 1)\n",
            "iteration: 31140 train shape: (97622, 48, 48, 1)\n",
            "iteration: 31141 train shape: (97624, 48, 48, 1)\n",
            "iteration: 31142 train shape: (97626, 48, 48, 1)\n",
            "iteration: 31143 train shape: (97628, 48, 48, 1)\n",
            "iteration: 31144 train shape: (97630, 48, 48, 1)\n",
            "iteration: 31145 train shape: (97632, 48, 48, 1)\n",
            "iteration: 31146 train shape: (97634, 48, 48, 1)\n",
            "iteration: 31147 train shape: (97636, 48, 48, 1)\n",
            "iteration: 31148 train shape: (97638, 48, 48, 1)\n",
            "iteration: 31149 train shape: (97640, 48, 48, 1)\n",
            "iteration: 31150 train shape: (97642, 48, 48, 1)\n",
            "iteration: 31151 train shape: (97644, 48, 48, 1)\n",
            "iteration: 31152 train shape: (97646, 48, 48, 1)\n",
            "iteration: 31153 train shape: (97648, 48, 48, 1)\n",
            "iteration: 31154 train shape: (97650, 48, 48, 1)\n",
            "iteration: 31155 train shape: (97652, 48, 48, 1)\n",
            "iteration: 31156 train shape: (97654, 48, 48, 1)\n",
            "iteration: 31157 train shape: (97656, 48, 48, 1)\n",
            "iteration: 31158 train shape: (97658, 48, 48, 1)\n",
            "iteration: 31159 train shape: (97660, 48, 48, 1)\n",
            "iteration: 31160 train shape: (97662, 48, 48, 1)\n",
            "iteration: 31161 train shape: (97664, 48, 48, 1)\n",
            "iteration: 31162 train shape: (97666, 48, 48, 1)\n",
            "iteration: 31163 train shape: (97668, 48, 48, 1)\n",
            "iteration: 31164 train shape: (97670, 48, 48, 1)\n",
            "iteration: 31165 train shape: (97672, 48, 48, 1)\n",
            "iteration: 31166 train shape: (97674, 48, 48, 1)\n",
            "iteration: 31167 train shape: (97676, 48, 48, 1)\n",
            "iteration: 31168 train shape: (97678, 48, 48, 1)\n",
            "iteration: 31169 train shape: (97680, 48, 48, 1)\n",
            "iteration: 31170 train shape: (97682, 48, 48, 1)\n",
            "iteration: 31171 train shape: (97684, 48, 48, 1)\n",
            "iteration: 31172 train shape: (97686, 48, 48, 1)\n",
            "iteration: 31173 train shape: (97688, 48, 48, 1)\n",
            "iteration: 31174 train shape: (97690, 48, 48, 1)\n",
            "iteration: 31175 train shape: (97692, 48, 48, 1)\n",
            "iteration: 31176 train shape: (97694, 48, 48, 1)\n",
            "iteration: 31177 train shape: (97696, 48, 48, 1)\n",
            "iteration: 31178 train shape: (97698, 48, 48, 1)\n",
            "iteration: 31179 train shape: (97700, 48, 48, 1)\n",
            "iteration: 31180 train shape: (97702, 48, 48, 1)\n",
            "iteration: 31181 train shape: (97704, 48, 48, 1)\n",
            "iteration: 31182 train shape: (97706, 48, 48, 1)\n",
            "iteration: 31183 train shape: (97708, 48, 48, 1)\n",
            "iteration: 31184 train shape: (97710, 48, 48, 1)\n",
            "iteration: 31185 train shape: (97712, 48, 48, 1)\n",
            "iteration: 31186 train shape: (97714, 48, 48, 1)\n",
            "iteration: 31187 train shape: (97716, 48, 48, 1)\n",
            "iteration: 31188 train shape: (97718, 48, 48, 1)\n",
            "iteration: 31189 train shape: (97720, 48, 48, 1)\n",
            "iteration: 31190 train shape: (97722, 48, 48, 1)\n",
            "iteration: 31191 train shape: (97724, 48, 48, 1)\n",
            "iteration: 31192 train shape: (97726, 48, 48, 1)\n",
            "iteration: 31193 train shape: (97728, 48, 48, 1)\n",
            "iteration: 31194 train shape: (97730, 48, 48, 1)\n",
            "iteration: 31195 train shape: (97732, 48, 48, 1)\n",
            "iteration: 31196 train shape: (97734, 48, 48, 1)\n",
            "iteration: 31197 train shape: (97736, 48, 48, 1)\n",
            "iteration: 31198 train shape: (97738, 48, 48, 1)\n",
            "iteration: 31199 train shape: (97740, 48, 48, 1)\n",
            "iteration: 31200 train shape: (97742, 48, 48, 1)\n",
            "iteration: 31201 train shape: (97744, 48, 48, 1)\n",
            "iteration: 31202 train shape: (97746, 48, 48, 1)\n",
            "iteration: 31203 train shape: (97748, 48, 48, 1)\n",
            "iteration: 31204 train shape: (97750, 48, 48, 1)\n",
            "iteration: 31205 train shape: (97752, 48, 48, 1)\n",
            "iteration: 31206 train shape: (97754, 48, 48, 1)\n",
            "iteration: 31207 train shape: (97756, 48, 48, 1)\n",
            "iteration: 31208 train shape: (97758, 48, 48, 1)\n",
            "iteration: 31209 train shape: (97760, 48, 48, 1)\n",
            "iteration: 31210 train shape: (97762, 48, 48, 1)\n",
            "iteration: 31211 train shape: (97764, 48, 48, 1)\n",
            "iteration: 31212 train shape: (97766, 48, 48, 1)\n",
            "iteration: 31213 train shape: (97768, 48, 48, 1)\n",
            "iteration: 31214 train shape: (97770, 48, 48, 1)\n",
            "iteration: 31215 train shape: (97772, 48, 48, 1)\n",
            "iteration: 31216 train shape: (97774, 48, 48, 1)\n",
            "iteration: 31217 train shape: (97776, 48, 48, 1)\n",
            "iteration: 31218 train shape: (97778, 48, 48, 1)\n",
            "iteration: 31219 train shape: (97780, 48, 48, 1)\n",
            "iteration: 31220 train shape: (97782, 48, 48, 1)\n",
            "iteration: 31221 train shape: (97784, 48, 48, 1)\n",
            "iteration: 31222 train shape: (97786, 48, 48, 1)\n",
            "iteration: 31223 train shape: (97788, 48, 48, 1)\n",
            "iteration: 31224 train shape: (97790, 48, 48, 1)\n",
            "iteration: 31225 train shape: (97792, 48, 48, 1)\n",
            "iteration: 31226 train shape: (97794, 48, 48, 1)\n",
            "iteration: 31227 train shape: (97796, 48, 48, 1)\n",
            "iteration: 31228 train shape: (97798, 48, 48, 1)\n",
            "iteration: 31229 train shape: (97800, 48, 48, 1)\n",
            "iteration: 31230 train shape: (97802, 48, 48, 1)\n",
            "iteration: 31231 train shape: (97804, 48, 48, 1)\n",
            "iteration: 31232 train shape: (97806, 48, 48, 1)\n",
            "iteration: 31233 train shape: (97808, 48, 48, 1)\n",
            "iteration: 31234 train shape: (97810, 48, 48, 1)\n",
            "iteration: 31235 train shape: (97812, 48, 48, 1)\n",
            "iteration: 31236 train shape: (97814, 48, 48, 1)\n",
            "iteration: 31237 train shape: (97816, 48, 48, 1)\n",
            "iteration: 31238 train shape: (97818, 48, 48, 1)\n",
            "iteration: 31239 train shape: (97820, 48, 48, 1)\n",
            "iteration: 31240 train shape: (97822, 48, 48, 1)\n",
            "iteration: 31241 train shape: (97824, 48, 48, 1)\n",
            "iteration: 31242 train shape: (97826, 48, 48, 1)\n",
            "iteration: 31243 train shape: (97828, 48, 48, 1)\n",
            "iteration: 31244 train shape: (97830, 48, 48, 1)\n",
            "iteration: 31245 train shape: (97832, 48, 48, 1)\n",
            "iteration: 31246 train shape: (97834, 48, 48, 1)\n",
            "iteration: 31247 train shape: (97836, 48, 48, 1)\n",
            "iteration: 31248 train shape: (97838, 48, 48, 1)\n",
            "iteration: 31249 train shape: (97840, 48, 48, 1)\n",
            "iteration: 31250 train shape: (97842, 48, 48, 1)\n",
            "iteration: 31251 train shape: (97844, 48, 48, 1)\n",
            "iteration: 31252 train shape: (97846, 48, 48, 1)\n",
            "iteration: 31253 train shape: (97848, 48, 48, 1)\n",
            "iteration: 31254 train shape: (97850, 48, 48, 1)\n",
            "iteration: 31255 train shape: (97852, 48, 48, 1)\n",
            "iteration: 31256 train shape: (97854, 48, 48, 1)\n",
            "iteration: 31257 train shape: (97856, 48, 48, 1)\n",
            "iteration: 31258 train shape: (97858, 48, 48, 1)\n",
            "iteration: 31259 train shape: (97860, 48, 48, 1)\n",
            "iteration: 31260 train shape: (97862, 48, 48, 1)\n",
            "iteration: 31261 train shape: (97864, 48, 48, 1)\n",
            "iteration: 31262 train shape: (97866, 48, 48, 1)\n",
            "iteration: 31263 train shape: (97868, 48, 48, 1)\n",
            "iteration: 31264 train shape: (97870, 48, 48, 1)\n",
            "iteration: 31265 train shape: (97872, 48, 48, 1)\n",
            "iteration: 31266 train shape: (97874, 48, 48, 1)\n",
            "iteration: 31267 train shape: (97876, 48, 48, 1)\n",
            "iteration: 31268 train shape: (97878, 48, 48, 1)\n",
            "iteration: 31269 train shape: (97880, 48, 48, 1)\n",
            "iteration: 31270 train shape: (97882, 48, 48, 1)\n",
            "iteration: 31271 train shape: (97884, 48, 48, 1)\n",
            "iteration: 31272 train shape: (97886, 48, 48, 1)\n",
            "iteration: 31273 train shape: (97888, 48, 48, 1)\n",
            "iteration: 31274 train shape: (97890, 48, 48, 1)\n",
            "iteration: 31275 train shape: (97892, 48, 48, 1)\n",
            "iteration: 31276 train shape: (97894, 48, 48, 1)\n",
            "iteration: 31277 train shape: (97896, 48, 48, 1)\n",
            "iteration: 31278 train shape: (97898, 48, 48, 1)\n",
            "iteration: 31279 train shape: (97900, 48, 48, 1)\n",
            "iteration: 31280 train shape: (97902, 48, 48, 1)\n",
            "iteration: 31281 train shape: (97904, 48, 48, 1)\n",
            "iteration: 31282 train shape: (97906, 48, 48, 1)\n",
            "iteration: 31283 train shape: (97908, 48, 48, 1)\n",
            "iteration: 31284 train shape: (97910, 48, 48, 1)\n",
            "iteration: 31285 train shape: (97912, 48, 48, 1)\n",
            "iteration: 31286 train shape: (97914, 48, 48, 1)\n",
            "iteration: 31287 train shape: (97916, 48, 48, 1)\n",
            "iteration: 31288 train shape: (97918, 48, 48, 1)\n",
            "iteration: 31289 train shape: (97920, 48, 48, 1)\n",
            "iteration: 31290 train shape: (97922, 48, 48, 1)\n",
            "iteration: 31291 train shape: (97924, 48, 48, 1)\n",
            "iteration: 31292 train shape: (97926, 48, 48, 1)\n",
            "iteration: 31293 train shape: (97928, 48, 48, 1)\n",
            "iteration: 31294 train shape: (97930, 48, 48, 1)\n",
            "iteration: 31295 train shape: (97932, 48, 48, 1)\n",
            "iteration: 31296 train shape: (97934, 48, 48, 1)\n",
            "iteration: 31297 train shape: (97936, 48, 48, 1)\n",
            "iteration: 31298 train shape: (97938, 48, 48, 1)\n",
            "iteration: 31299 train shape: (97940, 48, 48, 1)\n",
            "iteration: 31300 train shape: (97942, 48, 48, 1)\n",
            "iteration: 31301 train shape: (97944, 48, 48, 1)\n",
            "iteration: 31302 train shape: (97946, 48, 48, 1)\n",
            "iteration: 31303 train shape: (97948, 48, 48, 1)\n",
            "iteration: 31304 train shape: (97950, 48, 48, 1)\n",
            "iteration: 31305 train shape: (97952, 48, 48, 1)\n",
            "iteration: 31306 train shape: (97954, 48, 48, 1)\n",
            "iteration: 31307 train shape: (97956, 48, 48, 1)\n",
            "iteration: 31308 train shape: (97958, 48, 48, 1)\n",
            "iteration: 31309 train shape: (97960, 48, 48, 1)\n",
            "iteration: 31310 train shape: (97962, 48, 48, 1)\n",
            "iteration: 31311 train shape: (97964, 48, 48, 1)\n",
            "iteration: 31312 train shape: (97966, 48, 48, 1)\n",
            "iteration: 31313 train shape: (97968, 48, 48, 1)\n",
            "iteration: 31314 train shape: (97970, 48, 48, 1)\n",
            "iteration: 31315 train shape: (97972, 48, 48, 1)\n",
            "iteration: 31316 train shape: (97974, 48, 48, 1)\n",
            "iteration: 31317 train shape: (97976, 48, 48, 1)\n",
            "iteration: 31318 train shape: (97978, 48, 48, 1)\n",
            "iteration: 31319 train shape: (97980, 48, 48, 1)\n",
            "iteration: 31320 train shape: (97982, 48, 48, 1)\n",
            "iteration: 31321 train shape: (97984, 48, 48, 1)\n",
            "iteration: 31322 train shape: (97986, 48, 48, 1)\n",
            "iteration: 31323 train shape: (97988, 48, 48, 1)\n",
            "iteration: 31324 train shape: (97990, 48, 48, 1)\n",
            "iteration: 31325 train shape: (97992, 48, 48, 1)\n",
            "iteration: 31326 train shape: (97994, 48, 48, 1)\n",
            "iteration: 31327 train shape: (97996, 48, 48, 1)\n",
            "iteration: 31328 train shape: (97998, 48, 48, 1)\n",
            "iteration: 31329 train shape: (98000, 48, 48, 1)\n",
            "iteration: 31330 train shape: (98002, 48, 48, 1)\n",
            "iteration: 31331 train shape: (98004, 48, 48, 1)\n",
            "iteration: 31332 train shape: (98006, 48, 48, 1)\n",
            "iteration: 31333 train shape: (98008, 48, 48, 1)\n",
            "iteration: 31334 train shape: (98010, 48, 48, 1)\n",
            "iteration: 31335 train shape: (98012, 48, 48, 1)\n",
            "iteration: 31336 train shape: (98014, 48, 48, 1)\n",
            "iteration: 31337 train shape: (98016, 48, 48, 1)\n",
            "iteration: 31338 train shape: (98018, 48, 48, 1)\n",
            "iteration: 31339 train shape: (98020, 48, 48, 1)\n",
            "iteration: 31340 train shape: (98022, 48, 48, 1)\n",
            "iteration: 31341 train shape: (98024, 48, 48, 1)\n",
            "iteration: 31342 train shape: (98026, 48, 48, 1)\n",
            "iteration: 31343 train shape: (98028, 48, 48, 1)\n",
            "iteration: 31344 train shape: (98030, 48, 48, 1)\n",
            "iteration: 31345 train shape: (98032, 48, 48, 1)\n",
            "iteration: 31346 train shape: (98034, 48, 48, 1)\n",
            "iteration: 31347 train shape: (98036, 48, 48, 1)\n",
            "iteration: 31348 train shape: (98038, 48, 48, 1)\n",
            "iteration: 31349 train shape: (98040, 48, 48, 1)\n",
            "iteration: 31350 train shape: (98042, 48, 48, 1)\n",
            "iteration: 31351 train shape: (98044, 48, 48, 1)\n",
            "iteration: 31352 train shape: (98046, 48, 48, 1)\n",
            "iteration: 31353 train shape: (98048, 48, 48, 1)\n",
            "iteration: 31354 train shape: (98050, 48, 48, 1)\n",
            "iteration: 31355 train shape: (98052, 48, 48, 1)\n",
            "iteration: 31356 train shape: (98054, 48, 48, 1)\n",
            "iteration: 31357 train shape: (98056, 48, 48, 1)\n",
            "iteration: 31358 train shape: (98058, 48, 48, 1)\n",
            "iteration: 31359 train shape: (98060, 48, 48, 1)\n",
            "iteration: 31360 train shape: (98062, 48, 48, 1)\n",
            "iteration: 31361 train shape: (98064, 48, 48, 1)\n",
            "iteration: 31362 train shape: (98066, 48, 48, 1)\n",
            "iteration: 31363 train shape: (98068, 48, 48, 1)\n",
            "iteration: 31364 train shape: (98070, 48, 48, 1)\n",
            "iteration: 31365 train shape: (98072, 48, 48, 1)\n",
            "iteration: 31366 train shape: (98074, 48, 48, 1)\n",
            "iteration: 31367 train shape: (98076, 48, 48, 1)\n",
            "iteration: 31368 train shape: (98078, 48, 48, 1)\n",
            "iteration: 31369 train shape: (98080, 48, 48, 1)\n",
            "iteration: 31370 train shape: (98082, 48, 48, 1)\n",
            "iteration: 31371 train shape: (98084, 48, 48, 1)\n",
            "iteration: 31372 train shape: (98086, 48, 48, 1)\n",
            "iteration: 31373 train shape: (98088, 48, 48, 1)\n",
            "iteration: 31374 train shape: (98090, 48, 48, 1)\n",
            "iteration: 31375 train shape: (98092, 48, 48, 1)\n",
            "iteration: 31376 train shape: (98094, 48, 48, 1)\n",
            "iteration: 31377 train shape: (98096, 48, 48, 1)\n",
            "iteration: 31378 train shape: (98098, 48, 48, 1)\n",
            "iteration: 31379 train shape: (98100, 48, 48, 1)\n",
            "iteration: 31380 train shape: (98102, 48, 48, 1)\n",
            "iteration: 31381 train shape: (98104, 48, 48, 1)\n",
            "iteration: 31382 train shape: (98106, 48, 48, 1)\n",
            "iteration: 31383 train shape: (98108, 48, 48, 1)\n",
            "iteration: 31384 train shape: (98110, 48, 48, 1)\n",
            "iteration: 31385 train shape: (98112, 48, 48, 1)\n",
            "iteration: 31386 train shape: (98114, 48, 48, 1)\n",
            "iteration: 31387 train shape: (98116, 48, 48, 1)\n",
            "iteration: 31388 train shape: (98118, 48, 48, 1)\n",
            "iteration: 31389 train shape: (98120, 48, 48, 1)\n",
            "iteration: 31390 train shape: (98122, 48, 48, 1)\n",
            "iteration: 31391 train shape: (98124, 48, 48, 1)\n",
            "iteration: 31392 train shape: (98126, 48, 48, 1)\n",
            "iteration: 31393 train shape: (98128, 48, 48, 1)\n",
            "iteration: 31394 train shape: (98130, 48, 48, 1)\n",
            "iteration: 31395 train shape: (98132, 48, 48, 1)\n",
            "iteration: 31396 train shape: (98134, 48, 48, 1)\n",
            "iteration: 31397 train shape: (98136, 48, 48, 1)\n",
            "iteration: 31398 train shape: (98138, 48, 48, 1)\n",
            "iteration: 31399 train shape: (98140, 48, 48, 1)\n",
            "iteration: 31400 train shape: (98142, 48, 48, 1)\n",
            "iteration: 31401 train shape: (98144, 48, 48, 1)\n",
            "iteration: 31402 train shape: (98146, 48, 48, 1)\n",
            "iteration: 31403 train shape: (98148, 48, 48, 1)\n",
            "iteration: 31404 train shape: (98150, 48, 48, 1)\n",
            "iteration: 31405 train shape: (98152, 48, 48, 1)\n",
            "iteration: 31406 train shape: (98154, 48, 48, 1)\n",
            "iteration: 31407 train shape: (98156, 48, 48, 1)\n",
            "iteration: 31408 train shape: (98158, 48, 48, 1)\n",
            "iteration: 31409 train shape: (98160, 48, 48, 1)\n",
            "iteration: 31410 train shape: (98162, 48, 48, 1)\n",
            "iteration: 31411 train shape: (98164, 48, 48, 1)\n",
            "iteration: 31412 train shape: (98166, 48, 48, 1)\n",
            "iteration: 31413 train shape: (98168, 48, 48, 1)\n",
            "iteration: 31414 train shape: (98170, 48, 48, 1)\n",
            "iteration: 31415 train shape: (98172, 48, 48, 1)\n",
            "iteration: 31416 train shape: (98174, 48, 48, 1)\n",
            "iteration: 31417 train shape: (98176, 48, 48, 1)\n",
            "iteration: 31418 train shape: (98178, 48, 48, 1)\n",
            "iteration: 31419 train shape: (98180, 48, 48, 1)\n",
            "iteration: 31420 train shape: (98182, 48, 48, 1)\n",
            "iteration: 31421 train shape: (98184, 48, 48, 1)\n",
            "iteration: 31422 train shape: (98186, 48, 48, 1)\n",
            "iteration: 31423 train shape: (98188, 48, 48, 1)\n",
            "iteration: 31424 train shape: (98190, 48, 48, 1)\n",
            "iteration: 31425 train shape: (98192, 48, 48, 1)\n",
            "iteration: 31426 train shape: (98194, 48, 48, 1)\n",
            "iteration: 31427 train shape: (98196, 48, 48, 1)\n",
            "iteration: 31428 train shape: (98198, 48, 48, 1)\n",
            "iteration: 31429 train shape: (98200, 48, 48, 1)\n",
            "iteration: 31430 train shape: (98202, 48, 48, 1)\n",
            "iteration: 31431 train shape: (98204, 48, 48, 1)\n",
            "iteration: 31432 train shape: (98206, 48, 48, 1)\n",
            "iteration: 31433 train shape: (98208, 48, 48, 1)\n",
            "iteration: 31434 train shape: (98210, 48, 48, 1)\n",
            "iteration: 31435 train shape: (98212, 48, 48, 1)\n",
            "iteration: 31436 train shape: (98214, 48, 48, 1)\n",
            "iteration: 31437 train shape: (98216, 48, 48, 1)\n",
            "iteration: 31438 train shape: (98218, 48, 48, 1)\n",
            "iteration: 31439 train shape: (98220, 48, 48, 1)\n",
            "iteration: 31440 train shape: (98222, 48, 48, 1)\n",
            "iteration: 31441 train shape: (98224, 48, 48, 1)\n",
            "iteration: 31442 train shape: (98226, 48, 48, 1)\n",
            "iteration: 31443 train shape: (98228, 48, 48, 1)\n",
            "iteration: 31444 train shape: (98230, 48, 48, 1)\n",
            "iteration: 31445 train shape: (98232, 48, 48, 1)\n",
            "iteration: 31446 train shape: (98234, 48, 48, 1)\n",
            "iteration: 31447 train shape: (98236, 48, 48, 1)\n",
            "iteration: 31448 train shape: (98238, 48, 48, 1)\n",
            "iteration: 31449 train shape: (98240, 48, 48, 1)\n",
            "iteration: 31450 train shape: (98242, 48, 48, 1)\n",
            "iteration: 31451 train shape: (98244, 48, 48, 1)\n",
            "iteration: 31452 train shape: (98246, 48, 48, 1)\n",
            "iteration: 31453 train shape: (98248, 48, 48, 1)\n",
            "iteration: 31454 train shape: (98250, 48, 48, 1)\n",
            "iteration: 31455 train shape: (98252, 48, 48, 1)\n",
            "iteration: 31456 train shape: (98254, 48, 48, 1)\n",
            "iteration: 31457 train shape: (98256, 48, 48, 1)\n",
            "iteration: 31458 train shape: (98258, 48, 48, 1)\n",
            "iteration: 31459 train shape: (98260, 48, 48, 1)\n",
            "iteration: 31460 train shape: (98262, 48, 48, 1)\n",
            "iteration: 31461 train shape: (98264, 48, 48, 1)\n",
            "iteration: 31462 train shape: (98266, 48, 48, 1)\n",
            "iteration: 31463 train shape: (98268, 48, 48, 1)\n",
            "iteration: 31464 train shape: (98270, 48, 48, 1)\n",
            "iteration: 31465 train shape: (98272, 48, 48, 1)\n",
            "iteration: 31466 train shape: (98274, 48, 48, 1)\n",
            "iteration: 31467 train shape: (98276, 48, 48, 1)\n",
            "iteration: 31468 train shape: (98278, 48, 48, 1)\n",
            "iteration: 31469 train shape: (98280, 48, 48, 1)\n",
            "iteration: 31470 train shape: (98282, 48, 48, 1)\n",
            "iteration: 31471 train shape: (98284, 48, 48, 1)\n",
            "iteration: 31472 train shape: (98286, 48, 48, 1)\n",
            "iteration: 31473 train shape: (98288, 48, 48, 1)\n",
            "iteration: 31474 train shape: (98290, 48, 48, 1)\n",
            "iteration: 31475 train shape: (98292, 48, 48, 1)\n",
            "iteration: 31476 train shape: (98294, 48, 48, 1)\n",
            "iteration: 31477 train shape: (98296, 48, 48, 1)\n",
            "iteration: 31478 train shape: (98298, 48, 48, 1)\n",
            "iteration: 31479 train shape: (98300, 48, 48, 1)\n",
            "iteration: 31480 train shape: (98302, 48, 48, 1)\n",
            "iteration: 31481 train shape: (98304, 48, 48, 1)\n",
            "iteration: 31482 train shape: (98306, 48, 48, 1)\n",
            "iteration: 31483 train shape: (98308, 48, 48, 1)\n",
            "iteration: 31484 train shape: (98310, 48, 48, 1)\n",
            "iteration: 31485 train shape: (98312, 48, 48, 1)\n",
            "iteration: 31486 train shape: (98314, 48, 48, 1)\n",
            "iteration: 31487 train shape: (98316, 48, 48, 1)\n",
            "iteration: 31488 train shape: (98318, 48, 48, 1)\n",
            "iteration: 31489 train shape: (98320, 48, 48, 1)\n",
            "iteration: 31490 train shape: (98322, 48, 48, 1)\n",
            "iteration: 31491 train shape: (98324, 48, 48, 1)\n",
            "iteration: 31492 train shape: (98326, 48, 48, 1)\n",
            "iteration: 31493 train shape: (98328, 48, 48, 1)\n",
            "iteration: 31494 train shape: (98330, 48, 48, 1)\n",
            "iteration: 31495 train shape: (98332, 48, 48, 1)\n",
            "iteration: 31496 train shape: (98334, 48, 48, 1)\n",
            "iteration: 31497 train shape: (98336, 48, 48, 1)\n",
            "iteration: 31498 train shape: (98338, 48, 48, 1)\n",
            "iteration: 31499 train shape: (98340, 48, 48, 1)\n",
            "iteration: 31500 train shape: (98342, 48, 48, 1)\n",
            "iteration: 31501 train shape: (98344, 48, 48, 1)\n",
            "iteration: 31502 train shape: (98346, 48, 48, 1)\n",
            "iteration: 31503 train shape: (98348, 48, 48, 1)\n",
            "iteration: 31504 train shape: (98350, 48, 48, 1)\n",
            "iteration: 31505 train shape: (98352, 48, 48, 1)\n",
            "iteration: 31506 train shape: (98354, 48, 48, 1)\n",
            "iteration: 31507 train shape: (98356, 48, 48, 1)\n",
            "iteration: 31508 train shape: (98358, 48, 48, 1)\n",
            "iteration: 31509 train shape: (98360, 48, 48, 1)\n",
            "iteration: 31510 train shape: (98362, 48, 48, 1)\n",
            "iteration: 31511 train shape: (98364, 48, 48, 1)\n",
            "iteration: 31512 train shape: (98366, 48, 48, 1)\n",
            "iteration: 31513 train shape: (98368, 48, 48, 1)\n",
            "iteration: 31514 train shape: (98370, 48, 48, 1)\n",
            "iteration: 31515 train shape: (98372, 48, 48, 1)\n",
            "iteration: 31516 train shape: (98374, 48, 48, 1)\n",
            "iteration: 31517 train shape: (98376, 48, 48, 1)\n",
            "iteration: 31518 train shape: (98378, 48, 48, 1)\n",
            "iteration: 31519 train shape: (98380, 48, 48, 1)\n",
            "iteration: 31520 train shape: (98382, 48, 48, 1)\n",
            "iteration: 31521 train shape: (98384, 48, 48, 1)\n",
            "iteration: 31522 train shape: (98386, 48, 48, 1)\n",
            "iteration: 31523 train shape: (98388, 48, 48, 1)\n",
            "iteration: 31524 train shape: (98390, 48, 48, 1)\n",
            "iteration: 31525 train shape: (98392, 48, 48, 1)\n",
            "iteration: 31526 train shape: (98394, 48, 48, 1)\n",
            "iteration: 31527 train shape: (98396, 48, 48, 1)\n",
            "iteration: 31528 train shape: (98398, 48, 48, 1)\n",
            "iteration: 31529 train shape: (98400, 48, 48, 1)\n",
            "iteration: 31530 train shape: (98402, 48, 48, 1)\n",
            "iteration: 31531 train shape: (98404, 48, 48, 1)\n",
            "iteration: 31532 train shape: (98406, 48, 48, 1)\n",
            "iteration: 31533 train shape: (98408, 48, 48, 1)\n",
            "iteration: 31534 train shape: (98410, 48, 48, 1)\n",
            "iteration: 31535 train shape: (98412, 48, 48, 1)\n",
            "iteration: 31536 train shape: (98414, 48, 48, 1)\n",
            "iteration: 31537 train shape: (98416, 48, 48, 1)\n",
            "iteration: 31538 train shape: (98418, 48, 48, 1)\n",
            "iteration: 31539 train shape: (98420, 48, 48, 1)\n",
            "iteration: 31540 train shape: (98422, 48, 48, 1)\n",
            "iteration: 31541 train shape: (98424, 48, 48, 1)\n",
            "iteration: 31542 train shape: (98426, 48, 48, 1)\n",
            "iteration: 31543 train shape: (98428, 48, 48, 1)\n",
            "iteration: 31544 train shape: (98430, 48, 48, 1)\n",
            "iteration: 31545 train shape: (98432, 48, 48, 1)\n",
            "iteration: 31546 train shape: (98434, 48, 48, 1)\n",
            "iteration: 31547 train shape: (98436, 48, 48, 1)\n",
            "iteration: 31548 train shape: (98438, 48, 48, 1)\n",
            "iteration: 31549 train shape: (98440, 48, 48, 1)\n",
            "iteration: 31550 train shape: (98442, 48, 48, 1)\n",
            "iteration: 31551 train shape: (98444, 48, 48, 1)\n",
            "iteration: 31552 train shape: (98446, 48, 48, 1)\n",
            "iteration: 31553 train shape: (98448, 48, 48, 1)\n",
            "iteration: 31554 train shape: (98450, 48, 48, 1)\n",
            "iteration: 31555 train shape: (98452, 48, 48, 1)\n",
            "iteration: 31556 train shape: (98454, 48, 48, 1)\n",
            "iteration: 31557 train shape: (98456, 48, 48, 1)\n",
            "iteration: 31558 train shape: (98458, 48, 48, 1)\n",
            "iteration: 31559 train shape: (98460, 48, 48, 1)\n",
            "iteration: 31560 train shape: (98462, 48, 48, 1)\n",
            "iteration: 31561 train shape: (98464, 48, 48, 1)\n",
            "iteration: 31562 train shape: (98466, 48, 48, 1)\n",
            "iteration: 31563 train shape: (98468, 48, 48, 1)\n",
            "iteration: 31564 train shape: (98470, 48, 48, 1)\n",
            "iteration: 31565 train shape: (98472, 48, 48, 1)\n",
            "iteration: 31566 train shape: (98474, 48, 48, 1)\n",
            "iteration: 31567 train shape: (98476, 48, 48, 1)\n",
            "iteration: 31568 train shape: (98478, 48, 48, 1)\n",
            "iteration: 31569 train shape: (98480, 48, 48, 1)\n",
            "iteration: 31570 train shape: (98482, 48, 48, 1)\n",
            "iteration: 31571 train shape: (98484, 48, 48, 1)\n",
            "iteration: 31572 train shape: (98486, 48, 48, 1)\n",
            "iteration: 31573 train shape: (98488, 48, 48, 1)\n",
            "iteration: 31574 train shape: (98490, 48, 48, 1)\n",
            "iteration: 31575 train shape: (98492, 48, 48, 1)\n",
            "iteration: 31576 train shape: (98494, 48, 48, 1)\n",
            "iteration: 31577 train shape: (98496, 48, 48, 1)\n",
            "iteration: 31578 train shape: (98498, 48, 48, 1)\n",
            "iteration: 31579 train shape: (98500, 48, 48, 1)\n",
            "iteration: 31580 train shape: (98502, 48, 48, 1)\n",
            "iteration: 31581 train shape: (98504, 48, 48, 1)\n",
            "iteration: 31582 train shape: (98506, 48, 48, 1)\n",
            "iteration: 31583 train shape: (98508, 48, 48, 1)\n",
            "iteration: 31584 train shape: (98510, 48, 48, 1)\n",
            "iteration: 31585 train shape: (98512, 48, 48, 1)\n",
            "iteration: 31586 train shape: (98514, 48, 48, 1)\n",
            "iteration: 31587 train shape: (98516, 48, 48, 1)\n",
            "iteration: 31588 train shape: (98518, 48, 48, 1)\n",
            "iteration: 31589 train shape: (98520, 48, 48, 1)\n",
            "iteration: 31590 train shape: (98522, 48, 48, 1)\n",
            "iteration: 31591 train shape: (98524, 48, 48, 1)\n",
            "iteration: 31592 train shape: (98526, 48, 48, 1)\n",
            "iteration: 31593 train shape: (98528, 48, 48, 1)\n",
            "iteration: 31594 train shape: (98530, 48, 48, 1)\n",
            "iteration: 31595 train shape: (98532, 48, 48, 1)\n",
            "iteration: 31596 train shape: (98534, 48, 48, 1)\n",
            "iteration: 31597 train shape: (98536, 48, 48, 1)\n",
            "iteration: 31598 train shape: (98538, 48, 48, 1)\n",
            "iteration: 31599 train shape: (98540, 48, 48, 1)\n",
            "iteration: 31600 train shape: (98542, 48, 48, 1)\n",
            "iteration: 31601 train shape: (98544, 48, 48, 1)\n",
            "iteration: 31602 train shape: (98546, 48, 48, 1)\n",
            "iteration: 31603 train shape: (98548, 48, 48, 1)\n",
            "iteration: 31604 train shape: (98550, 48, 48, 1)\n",
            "iteration: 31605 train shape: (98552, 48, 48, 1)\n",
            "iteration: 31606 train shape: (98554, 48, 48, 1)\n",
            "iteration: 31607 train shape: (98556, 48, 48, 1)\n",
            "iteration: 31608 train shape: (98558, 48, 48, 1)\n",
            "iteration: 31609 train shape: (98560, 48, 48, 1)\n",
            "iteration: 31610 train shape: (98562, 48, 48, 1)\n",
            "iteration: 31611 train shape: (98564, 48, 48, 1)\n",
            "iteration: 31612 train shape: (98566, 48, 48, 1)\n",
            "iteration: 31613 train shape: (98568, 48, 48, 1)\n",
            "iteration: 31614 train shape: (98570, 48, 48, 1)\n",
            "iteration: 31615 train shape: (98572, 48, 48, 1)\n",
            "iteration: 31616 train shape: (98574, 48, 48, 1)\n",
            "iteration: 31617 train shape: (98576, 48, 48, 1)\n",
            "iteration: 31618 train shape: (98578, 48, 48, 1)\n",
            "iteration: 31619 train shape: (98580, 48, 48, 1)\n",
            "iteration: 31620 train shape: (98582, 48, 48, 1)\n",
            "iteration: 31621 train shape: (98584, 48, 48, 1)\n",
            "iteration: 31622 train shape: (98586, 48, 48, 1)\n",
            "iteration: 31623 train shape: (98588, 48, 48, 1)\n",
            "iteration: 31624 train shape: (98590, 48, 48, 1)\n",
            "iteration: 31625 train shape: (98592, 48, 48, 1)\n",
            "iteration: 31626 train shape: (98594, 48, 48, 1)\n",
            "iteration: 31627 train shape: (98596, 48, 48, 1)\n",
            "iteration: 31628 train shape: (98598, 48, 48, 1)\n",
            "iteration: 31629 train shape: (98600, 48, 48, 1)\n",
            "iteration: 31630 train shape: (98602, 48, 48, 1)\n",
            "iteration: 31631 train shape: (98604, 48, 48, 1)\n",
            "iteration: 31632 train shape: (98606, 48, 48, 1)\n",
            "iteration: 31633 train shape: (98608, 48, 48, 1)\n",
            "iteration: 31634 train shape: (98610, 48, 48, 1)\n",
            "iteration: 31635 train shape: (98612, 48, 48, 1)\n",
            "iteration: 31636 train shape: (98614, 48, 48, 1)\n",
            "iteration: 31637 train shape: (98616, 48, 48, 1)\n",
            "iteration: 31638 train shape: (98618, 48, 48, 1)\n",
            "iteration: 31639 train shape: (98620, 48, 48, 1)\n",
            "iteration: 31640 train shape: (98622, 48, 48, 1)\n",
            "iteration: 31641 train shape: (98624, 48, 48, 1)\n",
            "iteration: 31642 train shape: (98626, 48, 48, 1)\n",
            "iteration: 31643 train shape: (98628, 48, 48, 1)\n",
            "iteration: 31644 train shape: (98630, 48, 48, 1)\n",
            "iteration: 31645 train shape: (98632, 48, 48, 1)\n",
            "iteration: 31646 train shape: (98634, 48, 48, 1)\n",
            "iteration: 31647 train shape: (98636, 48, 48, 1)\n",
            "iteration: 31648 train shape: (98638, 48, 48, 1)\n",
            "iteration: 31649 train shape: (98640, 48, 48, 1)\n",
            "iteration: 31650 train shape: (98642, 48, 48, 1)\n",
            "iteration: 31651 train shape: (98644, 48, 48, 1)\n",
            "iteration: 31652 train shape: (98646, 48, 48, 1)\n",
            "iteration: 31653 train shape: (98648, 48, 48, 1)\n",
            "iteration: 31654 train shape: (98650, 48, 48, 1)\n",
            "iteration: 31655 train shape: (98652, 48, 48, 1)\n",
            "iteration: 31656 train shape: (98654, 48, 48, 1)\n",
            "iteration: 31657 train shape: (98656, 48, 48, 1)\n",
            "iteration: 31658 train shape: (98658, 48, 48, 1)\n",
            "iteration: 31659 train shape: (98660, 48, 48, 1)\n",
            "iteration: 31660 train shape: (98662, 48, 48, 1)\n",
            "iteration: 31661 train shape: (98664, 48, 48, 1)\n",
            "iteration: 31662 train shape: (98666, 48, 48, 1)\n",
            "iteration: 31663 train shape: (98668, 48, 48, 1)\n",
            "iteration: 31664 train shape: (98670, 48, 48, 1)\n",
            "iteration: 31665 train shape: (98672, 48, 48, 1)\n",
            "iteration: 31666 train shape: (98674, 48, 48, 1)\n",
            "iteration: 31667 train shape: (98676, 48, 48, 1)\n",
            "iteration: 31668 train shape: (98678, 48, 48, 1)\n",
            "iteration: 31669 train shape: (98680, 48, 48, 1)\n",
            "iteration: 31670 train shape: (98682, 48, 48, 1)\n",
            "iteration: 31671 train shape: (98684, 48, 48, 1)\n",
            "iteration: 31672 train shape: (98686, 48, 48, 1)\n",
            "iteration: 31673 train shape: (98688, 48, 48, 1)\n",
            "iteration: 31674 train shape: (98690, 48, 48, 1)\n",
            "iteration: 31675 train shape: (98692, 48, 48, 1)\n",
            "iteration: 31676 train shape: (98694, 48, 48, 1)\n",
            "iteration: 31677 train shape: (98696, 48, 48, 1)\n",
            "iteration: 31678 train shape: (98698, 48, 48, 1)\n",
            "iteration: 31679 train shape: (98700, 48, 48, 1)\n",
            "iteration: 31680 train shape: (98702, 48, 48, 1)\n",
            "iteration: 31681 train shape: (98704, 48, 48, 1)\n",
            "iteration: 31682 train shape: (98706, 48, 48, 1)\n",
            "iteration: 31683 train shape: (98708, 48, 48, 1)\n",
            "iteration: 31684 train shape: (98710, 48, 48, 1)\n",
            "iteration: 31685 train shape: (98712, 48, 48, 1)\n",
            "iteration: 31686 train shape: (98714, 48, 48, 1)\n",
            "iteration: 31687 train shape: (98716, 48, 48, 1)\n",
            "iteration: 31688 train shape: (98718, 48, 48, 1)\n",
            "iteration: 31689 train shape: (98720, 48, 48, 1)\n",
            "iteration: 31690 train shape: (98722, 48, 48, 1)\n",
            "iteration: 31691 train shape: (98724, 48, 48, 1)\n",
            "iteration: 31692 train shape: (98726, 48, 48, 1)\n",
            "iteration: 31693 train shape: (98728, 48, 48, 1)\n",
            "iteration: 31694 train shape: (98730, 48, 48, 1)\n",
            "iteration: 31695 train shape: (98732, 48, 48, 1)\n",
            "iteration: 31696 train shape: (98734, 48, 48, 1)\n",
            "iteration: 31697 train shape: (98736, 48, 48, 1)\n",
            "iteration: 31698 train shape: (98738, 48, 48, 1)\n",
            "iteration: 31699 train shape: (98740, 48, 48, 1)\n",
            "iteration: 31700 train shape: (98742, 48, 48, 1)\n",
            "iteration: 31701 train shape: (98744, 48, 48, 1)\n",
            "iteration: 31702 train shape: (98746, 48, 48, 1)\n",
            "iteration: 31703 train shape: (98748, 48, 48, 1)\n",
            "iteration: 31704 train shape: (98750, 48, 48, 1)\n",
            "iteration: 31705 train shape: (98752, 48, 48, 1)\n",
            "iteration: 31706 train shape: (98754, 48, 48, 1)\n",
            "iteration: 31707 train shape: (98756, 48, 48, 1)\n",
            "iteration: 31708 train shape: (98758, 48, 48, 1)\n",
            "iteration: 31709 train shape: (98760, 48, 48, 1)\n",
            "iteration: 31710 train shape: (98762, 48, 48, 1)\n",
            "iteration: 31711 train shape: (98764, 48, 48, 1)\n",
            "iteration: 31712 train shape: (98766, 48, 48, 1)\n",
            "iteration: 31713 train shape: (98768, 48, 48, 1)\n",
            "iteration: 31714 train shape: (98770, 48, 48, 1)\n",
            "iteration: 31715 train shape: (98772, 48, 48, 1)\n",
            "iteration: 31716 train shape: (98774, 48, 48, 1)\n",
            "iteration: 31717 train shape: (98776, 48, 48, 1)\n",
            "iteration: 31718 train shape: (98778, 48, 48, 1)\n",
            "iteration: 31719 train shape: (98780, 48, 48, 1)\n",
            "iteration: 31720 train shape: (98782, 48, 48, 1)\n",
            "iteration: 31721 train shape: (98784, 48, 48, 1)\n",
            "iteration: 31722 train shape: (98786, 48, 48, 1)\n",
            "iteration: 31723 train shape: (98788, 48, 48, 1)\n",
            "iteration: 31724 train shape: (98790, 48, 48, 1)\n",
            "iteration: 31725 train shape: (98792, 48, 48, 1)\n",
            "iteration: 31726 train shape: (98794, 48, 48, 1)\n",
            "iteration: 31727 train shape: (98796, 48, 48, 1)\n",
            "iteration: 31728 train shape: (98798, 48, 48, 1)\n",
            "iteration: 31729 train shape: (98800, 48, 48, 1)\n",
            "iteration: 31730 train shape: (98802, 48, 48, 1)\n",
            "iteration: 31731 train shape: (98804, 48, 48, 1)\n",
            "iteration: 31732 train shape: (98806, 48, 48, 1)\n",
            "iteration: 31733 train shape: (98808, 48, 48, 1)\n",
            "iteration: 31734 train shape: (98810, 48, 48, 1)\n",
            "iteration: 31735 train shape: (98812, 48, 48, 1)\n",
            "iteration: 31736 train shape: (98814, 48, 48, 1)\n",
            "iteration: 31737 train shape: (98816, 48, 48, 1)\n",
            "iteration: 31738 train shape: (98818, 48, 48, 1)\n",
            "iteration: 31739 train shape: (98820, 48, 48, 1)\n",
            "iteration: 31740 train shape: (98822, 48, 48, 1)\n",
            "iteration: 31741 train shape: (98824, 48, 48, 1)\n",
            "iteration: 31742 train shape: (98826, 48, 48, 1)\n",
            "iteration: 31743 train shape: (98828, 48, 48, 1)\n",
            "iteration: 31744 train shape: (98830, 48, 48, 1)\n",
            "iteration: 31745 train shape: (98832, 48, 48, 1)\n",
            "iteration: 31746 train shape: (98834, 48, 48, 1)\n",
            "iteration: 31747 train shape: (98836, 48, 48, 1)\n",
            "iteration: 31748 train shape: (98838, 48, 48, 1)\n",
            "iteration: 31749 train shape: (98840, 48, 48, 1)\n",
            "iteration: 31750 train shape: (98842, 48, 48, 1)\n",
            "iteration: 31751 train shape: (98844, 48, 48, 1)\n",
            "iteration: 31752 train shape: (98846, 48, 48, 1)\n",
            "iteration: 31753 train shape: (98848, 48, 48, 1)\n",
            "iteration: 31754 train shape: (98850, 48, 48, 1)\n",
            "iteration: 31755 train shape: (98852, 48, 48, 1)\n",
            "iteration: 31756 train shape: (98854, 48, 48, 1)\n",
            "iteration: 31757 train shape: (98856, 48, 48, 1)\n",
            "iteration: 31758 train shape: (98858, 48, 48, 1)\n",
            "iteration: 31759 train shape: (98860, 48, 48, 1)\n",
            "iteration: 31760 train shape: (98862, 48, 48, 1)\n",
            "iteration: 31761 train shape: (98864, 48, 48, 1)\n",
            "iteration: 31762 train shape: (98866, 48, 48, 1)\n",
            "iteration: 31763 train shape: (98868, 48, 48, 1)\n",
            "iteration: 31764 train shape: (98870, 48, 48, 1)\n",
            "iteration: 31765 train shape: (98872, 48, 48, 1)\n",
            "iteration: 31766 train shape: (98874, 48, 48, 1)\n",
            "iteration: 31767 train shape: (98876, 48, 48, 1)\n",
            "iteration: 31768 train shape: (98878, 48, 48, 1)\n",
            "iteration: 31769 train shape: (98880, 48, 48, 1)\n",
            "iteration: 31770 train shape: (98882, 48, 48, 1)\n",
            "iteration: 31771 train shape: (98884, 48, 48, 1)\n",
            "iteration: 31772 train shape: (98886, 48, 48, 1)\n",
            "iteration: 31773 train shape: (98888, 48, 48, 1)\n",
            "iteration: 31774 train shape: (98890, 48, 48, 1)\n",
            "iteration: 31775 train shape: (98892, 48, 48, 1)\n",
            "iteration: 31776 train shape: (98894, 48, 48, 1)\n",
            "iteration: 31777 train shape: (98896, 48, 48, 1)\n",
            "iteration: 31778 train shape: (98898, 48, 48, 1)\n",
            "iteration: 31779 train shape: (98900, 48, 48, 1)\n",
            "iteration: 31780 train shape: (98902, 48, 48, 1)\n",
            "iteration: 31781 train shape: (98904, 48, 48, 1)\n",
            "iteration: 31782 train shape: (98906, 48, 48, 1)\n",
            "iteration: 31783 train shape: (98908, 48, 48, 1)\n",
            "iteration: 31784 train shape: (98910, 48, 48, 1)\n",
            "iteration: 31785 train shape: (98912, 48, 48, 1)\n",
            "iteration: 31786 train shape: (98914, 48, 48, 1)\n",
            "iteration: 31787 train shape: (98916, 48, 48, 1)\n",
            "iteration: 31788 train shape: (98918, 48, 48, 1)\n",
            "iteration: 31789 train shape: (98920, 48, 48, 1)\n",
            "iteration: 31790 train shape: (98922, 48, 48, 1)\n",
            "iteration: 31791 train shape: (98924, 48, 48, 1)\n",
            "iteration: 31792 train shape: (98926, 48, 48, 1)\n",
            "iteration: 31793 train shape: (98928, 48, 48, 1)\n",
            "iteration: 31794 train shape: (98930, 48, 48, 1)\n",
            "iteration: 31795 train shape: (98932, 48, 48, 1)\n",
            "iteration: 31796 train shape: (98934, 48, 48, 1)\n",
            "iteration: 31797 train shape: (98936, 48, 48, 1)\n",
            "iteration: 31798 train shape: (98938, 48, 48, 1)\n",
            "iteration: 31799 train shape: (98940, 48, 48, 1)\n",
            "iteration: 31800 train shape: (98942, 48, 48, 1)\n",
            "iteration: 31801 train shape: (98944, 48, 48, 1)\n",
            "iteration: 31802 train shape: (98946, 48, 48, 1)\n",
            "iteration: 31803 train shape: (98948, 48, 48, 1)\n",
            "iteration: 31804 train shape: (98950, 48, 48, 1)\n",
            "iteration: 31805 train shape: (98952, 48, 48, 1)\n",
            "iteration: 31806 train shape: (98954, 48, 48, 1)\n",
            "iteration: 31807 train shape: (98956, 48, 48, 1)\n",
            "iteration: 31808 train shape: (98958, 48, 48, 1)\n",
            "iteration: 31809 train shape: (98960, 48, 48, 1)\n",
            "iteration: 31810 train shape: (98962, 48, 48, 1)\n",
            "iteration: 31811 train shape: (98964, 48, 48, 1)\n",
            "iteration: 31812 train shape: (98966, 48, 48, 1)\n",
            "iteration: 31813 train shape: (98968, 48, 48, 1)\n",
            "iteration: 31814 train shape: (98970, 48, 48, 1)\n",
            "iteration: 31815 train shape: (98972, 48, 48, 1)\n",
            "iteration: 31816 train shape: (98974, 48, 48, 1)\n",
            "iteration: 31817 train shape: (98976, 48, 48, 1)\n",
            "iteration: 31818 train shape: (98978, 48, 48, 1)\n",
            "iteration: 31819 train shape: (98980, 48, 48, 1)\n",
            "iteration: 31820 train shape: (98982, 48, 48, 1)\n",
            "iteration: 31821 train shape: (98984, 48, 48, 1)\n",
            "iteration: 31822 train shape: (98986, 48, 48, 1)\n",
            "iteration: 31823 train shape: (98988, 48, 48, 1)\n",
            "iteration: 31824 train shape: (98990, 48, 48, 1)\n",
            "iteration: 31825 train shape: (98992, 48, 48, 1)\n",
            "iteration: 31826 train shape: (98994, 48, 48, 1)\n",
            "iteration: 31827 train shape: (98996, 48, 48, 1)\n",
            "iteration: 31828 train shape: (98998, 48, 48, 1)\n",
            "iteration: 31829 train shape: (99000, 48, 48, 1)\n",
            "iteration: 31830 train shape: (99002, 48, 48, 1)\n",
            "iteration: 31831 train shape: (99004, 48, 48, 1)\n",
            "iteration: 31832 train shape: (99006, 48, 48, 1)\n",
            "iteration: 31833 train shape: (99008, 48, 48, 1)\n",
            "iteration: 31834 train shape: (99010, 48, 48, 1)\n",
            "iteration: 31835 train shape: (99012, 48, 48, 1)\n",
            "iteration: 31836 train shape: (99014, 48, 48, 1)\n",
            "iteration: 31837 train shape: (99016, 48, 48, 1)\n",
            "iteration: 31838 train shape: (99018, 48, 48, 1)\n",
            "iteration: 31839 train shape: (99020, 48, 48, 1)\n",
            "iteration: 31840 train shape: (99022, 48, 48, 1)\n",
            "iteration: 31841 train shape: (99024, 48, 48, 1)\n",
            "iteration: 31842 train shape: (99026, 48, 48, 1)\n",
            "iteration: 31843 train shape: (99028, 48, 48, 1)\n",
            "iteration: 31844 train shape: (99030, 48, 48, 1)\n",
            "iteration: 31845 train shape: (99032, 48, 48, 1)\n",
            "iteration: 31846 train shape: (99034, 48, 48, 1)\n",
            "iteration: 31847 train shape: (99036, 48, 48, 1)\n",
            "iteration: 31848 train shape: (99038, 48, 48, 1)\n",
            "iteration: 31849 train shape: (99040, 48, 48, 1)\n",
            "iteration: 31850 train shape: (99042, 48, 48, 1)\n",
            "iteration: 31851 train shape: (99044, 48, 48, 1)\n",
            "iteration: 31852 train shape: (99046, 48, 48, 1)\n",
            "iteration: 31853 train shape: (99048, 48, 48, 1)\n",
            "iteration: 31854 train shape: (99050, 48, 48, 1)\n",
            "iteration: 31855 train shape: (99052, 48, 48, 1)\n",
            "iteration: 31856 train shape: (99054, 48, 48, 1)\n",
            "iteration: 31857 train shape: (99056, 48, 48, 1)\n",
            "iteration: 31858 train shape: (99058, 48, 48, 1)\n",
            "iteration: 31859 train shape: (99060, 48, 48, 1)\n",
            "iteration: 31860 train shape: (99062, 48, 48, 1)\n",
            "iteration: 31861 train shape: (99064, 48, 48, 1)\n",
            "iteration: 31862 train shape: (99066, 48, 48, 1)\n",
            "iteration: 31863 train shape: (99068, 48, 48, 1)\n",
            "iteration: 31864 train shape: (99070, 48, 48, 1)\n",
            "iteration: 31865 train shape: (99072, 48, 48, 1)\n",
            "iteration: 31866 train shape: (99074, 48, 48, 1)\n",
            "iteration: 31867 train shape: (99076, 48, 48, 1)\n",
            "iteration: 31868 train shape: (99078, 48, 48, 1)\n",
            "iteration: 31869 train shape: (99080, 48, 48, 1)\n",
            "iteration: 31870 train shape: (99082, 48, 48, 1)\n",
            "iteration: 31871 train shape: (99084, 48, 48, 1)\n",
            "iteration: 31872 train shape: (99086, 48, 48, 1)\n",
            "iteration: 31873 train shape: (99088, 48, 48, 1)\n",
            "iteration: 31874 train shape: (99090, 48, 48, 1)\n",
            "iteration: 31875 train shape: (99092, 48, 48, 1)\n",
            "iteration: 31876 train shape: (99094, 48, 48, 1)\n",
            "iteration: 31877 train shape: (99096, 48, 48, 1)\n",
            "iteration: 31878 train shape: (99098, 48, 48, 1)\n",
            "iteration: 31879 train shape: (99100, 48, 48, 1)\n",
            "iteration: 31880 train shape: (99102, 48, 48, 1)\n",
            "iteration: 31881 train shape: (99104, 48, 48, 1)\n",
            "iteration: 31882 train shape: (99106, 48, 48, 1)\n",
            "iteration: 31883 train shape: (99108, 48, 48, 1)\n",
            "iteration: 31884 train shape: (99110, 48, 48, 1)\n",
            "iteration: 31885 train shape: (99112, 48, 48, 1)\n",
            "iteration: 31886 train shape: (99114, 48, 48, 1)\n",
            "iteration: 31887 train shape: (99116, 48, 48, 1)\n",
            "iteration: 31888 train shape: (99118, 48, 48, 1)\n",
            "iteration: 31889 train shape: (99120, 48, 48, 1)\n",
            "iteration: 31890 train shape: (99122, 48, 48, 1)\n",
            "iteration: 31891 train shape: (99124, 48, 48, 1)\n",
            "iteration: 31892 train shape: (99126, 48, 48, 1)\n",
            "iteration: 31893 train shape: (99128, 48, 48, 1)\n",
            "iteration: 31894 train shape: (99130, 48, 48, 1)\n",
            "iteration: 31895 train shape: (99132, 48, 48, 1)\n",
            "iteration: 31896 train shape: (99134, 48, 48, 1)\n",
            "iteration: 31897 train shape: (99136, 48, 48, 1)\n",
            "iteration: 31898 train shape: (99138, 48, 48, 1)\n",
            "iteration: 31899 train shape: (99140, 48, 48, 1)\n",
            "iteration: 31900 train shape: (99142, 48, 48, 1)\n",
            "iteration: 31901 train shape: (99144, 48, 48, 1)\n",
            "iteration: 31902 train shape: (99146, 48, 48, 1)\n",
            "iteration: 31903 train shape: (99148, 48, 48, 1)\n",
            "iteration: 31904 train shape: (99150, 48, 48, 1)\n",
            "iteration: 31905 train shape: (99152, 48, 48, 1)\n",
            "iteration: 31906 train shape: (99154, 48, 48, 1)\n",
            "iteration: 31907 train shape: (99156, 48, 48, 1)\n",
            "iteration: 31908 train shape: (99158, 48, 48, 1)\n",
            "iteration: 31909 train shape: (99160, 48, 48, 1)\n",
            "iteration: 31910 train shape: (99162, 48, 48, 1)\n",
            "iteration: 31911 train shape: (99164, 48, 48, 1)\n",
            "iteration: 31912 train shape: (99166, 48, 48, 1)\n",
            "iteration: 31913 train shape: (99168, 48, 48, 1)\n",
            "iteration: 31914 train shape: (99170, 48, 48, 1)\n",
            "iteration: 31915 train shape: (99172, 48, 48, 1)\n",
            "iteration: 31916 train shape: (99174, 48, 48, 1)\n",
            "iteration: 31917 train shape: (99176, 48, 48, 1)\n",
            "iteration: 31918 train shape: (99178, 48, 48, 1)\n",
            "iteration: 31919 train shape: (99180, 48, 48, 1)\n",
            "iteration: 31920 train shape: (99182, 48, 48, 1)\n",
            "iteration: 31921 train shape: (99184, 48, 48, 1)\n",
            "iteration: 31922 train shape: (99186, 48, 48, 1)\n",
            "iteration: 31923 train shape: (99188, 48, 48, 1)\n",
            "iteration: 31924 train shape: (99190, 48, 48, 1)\n",
            "iteration: 31925 train shape: (99192, 48, 48, 1)\n",
            "iteration: 31926 train shape: (99194, 48, 48, 1)\n",
            "iteration: 31927 train shape: (99196, 48, 48, 1)\n",
            "iteration: 31928 train shape: (99198, 48, 48, 1)\n",
            "iteration: 31929 train shape: (99200, 48, 48, 1)\n",
            "iteration: 31930 train shape: (99202, 48, 48, 1)\n",
            "iteration: 31931 train shape: (99204, 48, 48, 1)\n",
            "iteration: 31932 train shape: (99206, 48, 48, 1)\n",
            "iteration: 31933 train shape: (99208, 48, 48, 1)\n",
            "iteration: 31934 train shape: (99210, 48, 48, 1)\n",
            "iteration: 31935 train shape: (99212, 48, 48, 1)\n",
            "iteration: 31936 train shape: (99214, 48, 48, 1)\n",
            "iteration: 31937 train shape: (99216, 48, 48, 1)\n",
            "iteration: 31938 train shape: (99218, 48, 48, 1)\n",
            "iteration: 31939 train shape: (99220, 48, 48, 1)\n",
            "iteration: 31940 train shape: (99222, 48, 48, 1)\n",
            "iteration: 31941 train shape: (99224, 48, 48, 1)\n",
            "iteration: 31942 train shape: (99226, 48, 48, 1)\n",
            "iteration: 31943 train shape: (99228, 48, 48, 1)\n",
            "iteration: 31944 train shape: (99230, 48, 48, 1)\n",
            "iteration: 31945 train shape: (99232, 48, 48, 1)\n",
            "iteration: 31946 train shape: (99234, 48, 48, 1)\n",
            "iteration: 31947 train shape: (99236, 48, 48, 1)\n",
            "iteration: 31948 train shape: (99238, 48, 48, 1)\n",
            "iteration: 31949 train shape: (99240, 48, 48, 1)\n",
            "iteration: 31950 train shape: (99242, 48, 48, 1)\n",
            "iteration: 31951 train shape: (99244, 48, 48, 1)\n",
            "iteration: 31952 train shape: (99246, 48, 48, 1)\n",
            "iteration: 31953 train shape: (99248, 48, 48, 1)\n",
            "iteration: 31954 train shape: (99250, 48, 48, 1)\n",
            "iteration: 31955 train shape: (99252, 48, 48, 1)\n",
            "iteration: 31956 train shape: (99254, 48, 48, 1)\n",
            "iteration: 31957 train shape: (99256, 48, 48, 1)\n",
            "iteration: 31958 train shape: (99258, 48, 48, 1)\n",
            "iteration: 31959 train shape: (99260, 48, 48, 1)\n",
            "iteration: 31960 train shape: (99262, 48, 48, 1)\n",
            "iteration: 31961 train shape: (99264, 48, 48, 1)\n",
            "iteration: 31962 train shape: (99266, 48, 48, 1)\n",
            "iteration: 31963 train shape: (99268, 48, 48, 1)\n",
            "iteration: 31964 train shape: (99270, 48, 48, 1)\n",
            "iteration: 31965 train shape: (99272, 48, 48, 1)\n",
            "iteration: 31966 train shape: (99274, 48, 48, 1)\n",
            "iteration: 31967 train shape: (99276, 48, 48, 1)\n",
            "iteration: 31968 train shape: (99278, 48, 48, 1)\n",
            "iteration: 31969 train shape: (99280, 48, 48, 1)\n",
            "iteration: 31970 train shape: (99282, 48, 48, 1)\n",
            "iteration: 31971 train shape: (99284, 48, 48, 1)\n",
            "iteration: 31972 train shape: (99286, 48, 48, 1)\n",
            "iteration: 31973 train shape: (99288, 48, 48, 1)\n",
            "iteration: 31974 train shape: (99290, 48, 48, 1)\n",
            "iteration: 31975 train shape: (99292, 48, 48, 1)\n",
            "iteration: 31976 train shape: (99294, 48, 48, 1)\n",
            "iteration: 31977 train shape: (99296, 48, 48, 1)\n",
            "iteration: 31978 train shape: (99298, 48, 48, 1)\n",
            "iteration: 31979 train shape: (99300, 48, 48, 1)\n",
            "iteration: 31980 train shape: (99302, 48, 48, 1)\n",
            "iteration: 31981 train shape: (99304, 48, 48, 1)\n",
            "iteration: 31982 train shape: (99306, 48, 48, 1)\n",
            "iteration: 31983 train shape: (99308, 48, 48, 1)\n",
            "iteration: 31984 train shape: (99310, 48, 48, 1)\n",
            "iteration: 31985 train shape: (99312, 48, 48, 1)\n",
            "iteration: 31986 train shape: (99314, 48, 48, 1)\n",
            "iteration: 31987 train shape: (99316, 48, 48, 1)\n",
            "iteration: 31988 train shape: (99318, 48, 48, 1)\n",
            "iteration: 31989 train shape: (99320, 48, 48, 1)\n",
            "iteration: 31990 train shape: (99322, 48, 48, 1)\n",
            "iteration: 31991 train shape: (99324, 48, 48, 1)\n",
            "iteration: 31992 train shape: (99326, 48, 48, 1)\n",
            "iteration: 31993 train shape: (99328, 48, 48, 1)\n",
            "iteration: 31994 train shape: (99330, 48, 48, 1)\n",
            "iteration: 31995 train shape: (99332, 48, 48, 1)\n",
            "iteration: 31996 train shape: (99334, 48, 48, 1)\n",
            "iteration: 31997 train shape: (99336, 48, 48, 1)\n",
            "iteration: 31998 train shape: (99338, 48, 48, 1)\n",
            "iteration: 31999 train shape: (99340, 48, 48, 1)\n",
            "iteration: 32000 train shape: (99342, 48, 48, 1)\n",
            "iteration: 32001 train shape: (99344, 48, 48, 1)\n",
            "iteration: 32002 train shape: (99346, 48, 48, 1)\n",
            "iteration: 32003 train shape: (99348, 48, 48, 1)\n",
            "iteration: 32004 train shape: (99350, 48, 48, 1)\n",
            "iteration: 32005 train shape: (99352, 48, 48, 1)\n",
            "iteration: 32006 train shape: (99354, 48, 48, 1)\n",
            "iteration: 32007 train shape: (99356, 48, 48, 1)\n",
            "iteration: 32008 train shape: (99358, 48, 48, 1)\n",
            "iteration: 32009 train shape: (99360, 48, 48, 1)\n",
            "iteration: 32010 train shape: (99362, 48, 48, 1)\n",
            "iteration: 32011 train shape: (99364, 48, 48, 1)\n",
            "iteration: 32012 train shape: (99366, 48, 48, 1)\n",
            "iteration: 32013 train shape: (99368, 48, 48, 1)\n",
            "iteration: 32014 train shape: (99370, 48, 48, 1)\n",
            "iteration: 32015 train shape: (99372, 48, 48, 1)\n",
            "iteration: 32016 train shape: (99374, 48, 48, 1)\n",
            "iteration: 32017 train shape: (99376, 48, 48, 1)\n",
            "iteration: 32018 train shape: (99378, 48, 48, 1)\n",
            "iteration: 32019 train shape: (99380, 48, 48, 1)\n",
            "iteration: 32020 train shape: (99382, 48, 48, 1)\n",
            "iteration: 32021 train shape: (99384, 48, 48, 1)\n",
            "iteration: 32022 train shape: (99386, 48, 48, 1)\n",
            "iteration: 32023 train shape: (99388, 48, 48, 1)\n",
            "iteration: 32024 train shape: (99390, 48, 48, 1)\n",
            "iteration: 32025 train shape: (99392, 48, 48, 1)\n",
            "iteration: 32026 train shape: (99394, 48, 48, 1)\n",
            "iteration: 32027 train shape: (99396, 48, 48, 1)\n",
            "iteration: 32028 train shape: (99398, 48, 48, 1)\n",
            "iteration: 32029 train shape: (99400, 48, 48, 1)\n",
            "iteration: 32030 train shape: (99402, 48, 48, 1)\n",
            "iteration: 32031 train shape: (99404, 48, 48, 1)\n",
            "iteration: 32032 train shape: (99406, 48, 48, 1)\n",
            "iteration: 32033 train shape: (99408, 48, 48, 1)\n",
            "iteration: 32034 train shape: (99410, 48, 48, 1)\n",
            "iteration: 32035 train shape: (99412, 48, 48, 1)\n",
            "iteration: 32036 train shape: (99414, 48, 48, 1)\n",
            "iteration: 32037 train shape: (99416, 48, 48, 1)\n",
            "iteration: 32038 train shape: (99418, 48, 48, 1)\n",
            "iteration: 32039 train shape: (99420, 48, 48, 1)\n",
            "iteration: 32040 train shape: (99422, 48, 48, 1)\n",
            "iteration: 32041 train shape: (99424, 48, 48, 1)\n",
            "iteration: 32042 train shape: (99426, 48, 48, 1)\n",
            "iteration: 32043 train shape: (99428, 48, 48, 1)\n",
            "iteration: 32044 train shape: (99430, 48, 48, 1)\n",
            "iteration: 32045 train shape: (99432, 48, 48, 1)\n",
            "iteration: 32046 train shape: (99434, 48, 48, 1)\n",
            "iteration: 32047 train shape: (99436, 48, 48, 1)\n",
            "iteration: 32048 train shape: (99438, 48, 48, 1)\n",
            "iteration: 32049 train shape: (99440, 48, 48, 1)\n",
            "iteration: 32050 train shape: (99442, 48, 48, 1)\n",
            "iteration: 32051 train shape: (99444, 48, 48, 1)\n",
            "iteration: 32052 train shape: (99446, 48, 48, 1)\n",
            "iteration: 32053 train shape: (99448, 48, 48, 1)\n",
            "iteration: 32054 train shape: (99450, 48, 48, 1)\n",
            "iteration: 32055 train shape: (99452, 48, 48, 1)\n",
            "iteration: 32056 train shape: (99454, 48, 48, 1)\n",
            "iteration: 32057 train shape: (99456, 48, 48, 1)\n",
            "iteration: 32058 train shape: (99458, 48, 48, 1)\n",
            "iteration: 32059 train shape: (99460, 48, 48, 1)\n",
            "iteration: 32060 train shape: (99462, 48, 48, 1)\n",
            "iteration: 32061 train shape: (99464, 48, 48, 1)\n",
            "iteration: 32062 train shape: (99466, 48, 48, 1)\n",
            "iteration: 32063 train shape: (99468, 48, 48, 1)\n",
            "iteration: 32064 train shape: (99470, 48, 48, 1)\n",
            "iteration: 32065 train shape: (99472, 48, 48, 1)\n",
            "iteration: 32066 train shape: (99474, 48, 48, 1)\n",
            "iteration: 32067 train shape: (99476, 48, 48, 1)\n",
            "iteration: 32068 train shape: (99478, 48, 48, 1)\n",
            "iteration: 32069 train shape: (99480, 48, 48, 1)\n",
            "iteration: 32070 train shape: (99482, 48, 48, 1)\n",
            "iteration: 32071 train shape: (99484, 48, 48, 1)\n",
            "iteration: 32072 train shape: (99486, 48, 48, 1)\n",
            "iteration: 32073 train shape: (99488, 48, 48, 1)\n",
            "iteration: 32074 train shape: (99490, 48, 48, 1)\n",
            "iteration: 32075 train shape: (99492, 48, 48, 1)\n",
            "iteration: 32076 train shape: (99494, 48, 48, 1)\n",
            "iteration: 32077 train shape: (99496, 48, 48, 1)\n",
            "iteration: 32078 train shape: (99498, 48, 48, 1)\n",
            "iteration: 32079 train shape: (99500, 48, 48, 1)\n",
            "iteration: 32080 train shape: (99502, 48, 48, 1)\n",
            "iteration: 32081 train shape: (99504, 48, 48, 1)\n",
            "iteration: 32082 train shape: (99506, 48, 48, 1)\n",
            "iteration: 32083 train shape: (99508, 48, 48, 1)\n",
            "iteration: 32084 train shape: (99510, 48, 48, 1)\n",
            "iteration: 32085 train shape: (99512, 48, 48, 1)\n",
            "iteration: 32086 train shape: (99514, 48, 48, 1)\n",
            "iteration: 32087 train shape: (99516, 48, 48, 1)\n",
            "iteration: 32088 train shape: (99518, 48, 48, 1)\n",
            "iteration: 32089 train shape: (99520, 48, 48, 1)\n",
            "iteration: 32090 train shape: (99522, 48, 48, 1)\n",
            "iteration: 32091 train shape: (99524, 48, 48, 1)\n",
            "iteration: 32092 train shape: (99526, 48, 48, 1)\n",
            "iteration: 32093 train shape: (99528, 48, 48, 1)\n",
            "iteration: 32094 train shape: (99530, 48, 48, 1)\n",
            "iteration: 32095 train shape: (99532, 48, 48, 1)\n",
            "iteration: 32096 train shape: (99534, 48, 48, 1)\n",
            "iteration: 32097 train shape: (99536, 48, 48, 1)\n",
            "iteration: 32098 train shape: (99538, 48, 48, 1)\n",
            "iteration: 32099 train shape: (99540, 48, 48, 1)\n",
            "iteration: 32100 train shape: (99542, 48, 48, 1)\n",
            "iteration: 32101 train shape: (99544, 48, 48, 1)\n",
            "iteration: 32102 train shape: (99546, 48, 48, 1)\n",
            "iteration: 32103 train shape: (99548, 48, 48, 1)\n",
            "iteration: 32104 train shape: (99550, 48, 48, 1)\n",
            "iteration: 32105 train shape: (99552, 48, 48, 1)\n",
            "iteration: 32106 train shape: (99554, 48, 48, 1)\n",
            "iteration: 32107 train shape: (99556, 48, 48, 1)\n",
            "iteration: 32108 train shape: (99558, 48, 48, 1)\n",
            "iteration: 32109 train shape: (99560, 48, 48, 1)\n",
            "iteration: 32110 train shape: (99562, 48, 48, 1)\n",
            "iteration: 32111 train shape: (99564, 48, 48, 1)\n",
            "iteration: 32112 train shape: (99566, 48, 48, 1)\n",
            "iteration: 32113 train shape: (99568, 48, 48, 1)\n",
            "iteration: 32114 train shape: (99570, 48, 48, 1)\n",
            "iteration: 32115 train shape: (99572, 48, 48, 1)\n",
            "iteration: 32116 train shape: (99574, 48, 48, 1)\n",
            "iteration: 32117 train shape: (99576, 48, 48, 1)\n",
            "iteration: 32118 train shape: (99578, 48, 48, 1)\n",
            "iteration: 32119 train shape: (99580, 48, 48, 1)\n",
            "iteration: 32120 train shape: (99582, 48, 48, 1)\n",
            "iteration: 32121 train shape: (99584, 48, 48, 1)\n",
            "iteration: 32122 train shape: (99586, 48, 48, 1)\n",
            "iteration: 32123 train shape: (99588, 48, 48, 1)\n",
            "iteration: 32124 train shape: (99590, 48, 48, 1)\n",
            "iteration: 32125 train shape: (99592, 48, 48, 1)\n",
            "iteration: 32126 train shape: (99594, 48, 48, 1)\n",
            "iteration: 32127 train shape: (99596, 48, 48, 1)\n",
            "iteration: 32128 train shape: (99598, 48, 48, 1)\n",
            "iteration: 32129 train shape: (99600, 48, 48, 1)\n",
            "iteration: 32130 train shape: (99602, 48, 48, 1)\n",
            "iteration: 32131 train shape: (99604, 48, 48, 1)\n",
            "iteration: 32132 train shape: (99606, 48, 48, 1)\n",
            "iteration: 32133 train shape: (99608, 48, 48, 1)\n",
            "iteration: 32134 train shape: (99610, 48, 48, 1)\n",
            "iteration: 32135 train shape: (99612, 48, 48, 1)\n",
            "iteration: 32136 train shape: (99614, 48, 48, 1)\n",
            "iteration: 32137 train shape: (99616, 48, 48, 1)\n",
            "iteration: 32138 train shape: (99618, 48, 48, 1)\n",
            "iteration: 32139 train shape: (99620, 48, 48, 1)\n",
            "iteration: 32140 train shape: (99622, 48, 48, 1)\n",
            "iteration: 32141 train shape: (99624, 48, 48, 1)\n",
            "iteration: 32142 train shape: (99626, 48, 48, 1)\n",
            "iteration: 32143 train shape: (99628, 48, 48, 1)\n",
            "iteration: 32144 train shape: (99630, 48, 48, 1)\n",
            "iteration: 32145 train shape: (99632, 48, 48, 1)\n",
            "iteration: 32146 train shape: (99634, 48, 48, 1)\n",
            "iteration: 32147 train shape: (99636, 48, 48, 1)\n",
            "iteration: 32148 train shape: (99638, 48, 48, 1)\n",
            "iteration: 32149 train shape: (99640, 48, 48, 1)\n",
            "iteration: 32150 train shape: (99642, 48, 48, 1)\n",
            "iteration: 32151 train shape: (99644, 48, 48, 1)\n",
            "iteration: 32152 train shape: (99646, 48, 48, 1)\n",
            "iteration: 32153 train shape: (99648, 48, 48, 1)\n",
            "iteration: 32154 train shape: (99650, 48, 48, 1)\n",
            "iteration: 32155 train shape: (99652, 48, 48, 1)\n",
            "iteration: 32156 train shape: (99654, 48, 48, 1)\n",
            "iteration: 32157 train shape: (99656, 48, 48, 1)\n",
            "iteration: 32158 train shape: (99658, 48, 48, 1)\n",
            "iteration: 32159 train shape: (99660, 48, 48, 1)\n",
            "iteration: 32160 train shape: (99662, 48, 48, 1)\n",
            "iteration: 32161 train shape: (99664, 48, 48, 1)\n",
            "iteration: 32162 train shape: (99666, 48, 48, 1)\n",
            "iteration: 32163 train shape: (99668, 48, 48, 1)\n",
            "iteration: 32164 train shape: (99670, 48, 48, 1)\n",
            "iteration: 32165 train shape: (99672, 48, 48, 1)\n",
            "iteration: 32166 train shape: (99674, 48, 48, 1)\n",
            "iteration: 32167 train shape: (99676, 48, 48, 1)\n",
            "iteration: 32168 train shape: (99678, 48, 48, 1)\n",
            "iteration: 32169 train shape: (99680, 48, 48, 1)\n",
            "iteration: 32170 train shape: (99682, 48, 48, 1)\n",
            "iteration: 32171 train shape: (99684, 48, 48, 1)\n",
            "iteration: 32172 train shape: (99686, 48, 48, 1)\n",
            "iteration: 32173 train shape: (99688, 48, 48, 1)\n",
            "iteration: 32174 train shape: (99690, 48, 48, 1)\n",
            "iteration: 32175 train shape: (99692, 48, 48, 1)\n",
            "iteration: 32176 train shape: (99694, 48, 48, 1)\n",
            "iteration: 32177 train shape: (99696, 48, 48, 1)\n",
            "iteration: 32178 train shape: (99698, 48, 48, 1)\n",
            "iteration: 32179 train shape: (99700, 48, 48, 1)\n",
            "iteration: 32180 train shape: (99702, 48, 48, 1)\n",
            "iteration: 32181 train shape: (99704, 48, 48, 1)\n",
            "iteration: 32182 train shape: (99706, 48, 48, 1)\n",
            "iteration: 32183 train shape: (99708, 48, 48, 1)\n",
            "iteration: 32184 train shape: (99710, 48, 48, 1)\n",
            "iteration: 32185 train shape: (99712, 48, 48, 1)\n",
            "iteration: 32186 train shape: (99714, 48, 48, 1)\n",
            "iteration: 32187 train shape: (99716, 48, 48, 1)\n",
            "iteration: 32188 train shape: (99718, 48, 48, 1)\n",
            "iteration: 32189 train shape: (99720, 48, 48, 1)\n",
            "iteration: 32190 train shape: (99722, 48, 48, 1)\n",
            "iteration: 32191 train shape: (99724, 48, 48, 1)\n",
            "iteration: 32192 train shape: (99726, 48, 48, 1)\n",
            "iteration: 32193 train shape: (99728, 48, 48, 1)\n",
            "iteration: 32194 train shape: (99730, 48, 48, 1)\n",
            "iteration: 32195 train shape: (99732, 48, 48, 1)\n",
            "iteration: 32196 train shape: (99734, 48, 48, 1)\n",
            "iteration: 32197 train shape: (99736, 48, 48, 1)\n",
            "iteration: 32198 train shape: (99738, 48, 48, 1)\n",
            "iteration: 32199 train shape: (99740, 48, 48, 1)\n",
            "iteration: 32200 train shape: (99742, 48, 48, 1)\n",
            "iteration: 32201 train shape: (99744, 48, 48, 1)\n",
            "iteration: 32202 train shape: (99746, 48, 48, 1)\n",
            "iteration: 32203 train shape: (99748, 48, 48, 1)\n",
            "iteration: 32204 train shape: (99750, 48, 48, 1)\n",
            "iteration: 32205 train shape: (99752, 48, 48, 1)\n",
            "iteration: 32206 train shape: (99754, 48, 48, 1)\n",
            "iteration: 32207 train shape: (99756, 48, 48, 1)\n",
            "iteration: 32208 train shape: (99758, 48, 48, 1)\n",
            "iteration: 32209 train shape: (99760, 48, 48, 1)\n",
            "iteration: 32210 train shape: (99762, 48, 48, 1)\n",
            "iteration: 32211 train shape: (99764, 48, 48, 1)\n",
            "iteration: 32212 train shape: (99766, 48, 48, 1)\n",
            "iteration: 32213 train shape: (99768, 48, 48, 1)\n",
            "iteration: 32214 train shape: (99770, 48, 48, 1)\n",
            "iteration: 32215 train shape: (99772, 48, 48, 1)\n",
            "iteration: 32216 train shape: (99774, 48, 48, 1)\n",
            "iteration: 32217 train shape: (99776, 48, 48, 1)\n",
            "iteration: 32218 train shape: (99778, 48, 48, 1)\n",
            "iteration: 32219 train shape: (99780, 48, 48, 1)\n",
            "iteration: 32220 train shape: (99782, 48, 48, 1)\n",
            "iteration: 32221 train shape: (99784, 48, 48, 1)\n",
            "iteration: 32222 train shape: (99786, 48, 48, 1)\n",
            "iteration: 32223 train shape: (99788, 48, 48, 1)\n",
            "iteration: 32224 train shape: (99790, 48, 48, 1)\n",
            "iteration: 32225 train shape: (99792, 48, 48, 1)\n",
            "iteration: 32226 train shape: (99794, 48, 48, 1)\n",
            "iteration: 32227 train shape: (99796, 48, 48, 1)\n",
            "iteration: 32228 train shape: (99798, 48, 48, 1)\n",
            "iteration: 32229 train shape: (99800, 48, 48, 1)\n",
            "iteration: 32230 train shape: (99802, 48, 48, 1)\n",
            "iteration: 32231 train shape: (99804, 48, 48, 1)\n",
            "iteration: 32232 train shape: (99806, 48, 48, 1)\n",
            "iteration: 32233 train shape: (99808, 48, 48, 1)\n",
            "iteration: 32234 train shape: (99810, 48, 48, 1)\n",
            "iteration: 32235 train shape: (99812, 48, 48, 1)\n",
            "iteration: 32236 train shape: (99814, 48, 48, 1)\n",
            "iteration: 32237 train shape: (99816, 48, 48, 1)\n",
            "iteration: 32238 train shape: (99818, 48, 48, 1)\n",
            "iteration: 32239 train shape: (99820, 48, 48, 1)\n",
            "iteration: 32240 train shape: (99822, 48, 48, 1)\n",
            "iteration: 32241 train shape: (99824, 48, 48, 1)\n",
            "iteration: 32242 train shape: (99826, 48, 48, 1)\n",
            "iteration: 32243 train shape: (99828, 48, 48, 1)\n",
            "iteration: 32244 train shape: (99830, 48, 48, 1)\n",
            "iteration: 32245 train shape: (99832, 48, 48, 1)\n",
            "iteration: 32246 train shape: (99834, 48, 48, 1)\n",
            "iteration: 32247 train shape: (99836, 48, 48, 1)\n",
            "iteration: 32248 train shape: (99838, 48, 48, 1)\n",
            "iteration: 32249 train shape: (99840, 48, 48, 1)\n",
            "iteration: 32250 train shape: (99842, 48, 48, 1)\n",
            "iteration: 32251 train shape: (99844, 48, 48, 1)\n",
            "iteration: 32252 train shape: (99846, 48, 48, 1)\n",
            "iteration: 32253 train shape: (99848, 48, 48, 1)\n",
            "iteration: 32254 train shape: (99850, 48, 48, 1)\n",
            "iteration: 32255 train shape: (99852, 48, 48, 1)\n",
            "iteration: 32256 train shape: (99854, 48, 48, 1)\n",
            "iteration: 32257 train shape: (99856, 48, 48, 1)\n",
            "iteration: 32258 train shape: (99858, 48, 48, 1)\n",
            "iteration: 32259 train shape: (99860, 48, 48, 1)\n",
            "iteration: 32260 train shape: (99862, 48, 48, 1)\n",
            "iteration: 32261 train shape: (99864, 48, 48, 1)\n",
            "iteration: 32262 train shape: (99866, 48, 48, 1)\n",
            "iteration: 32263 train shape: (99868, 48, 48, 1)\n",
            "iteration: 32264 train shape: (99870, 48, 48, 1)\n",
            "iteration: 32265 train shape: (99872, 48, 48, 1)\n",
            "iteration: 32266 train shape: (99874, 48, 48, 1)\n",
            "iteration: 32267 train shape: (99876, 48, 48, 1)\n",
            "iteration: 32268 train shape: (99878, 48, 48, 1)\n",
            "iteration: 32269 train shape: (99880, 48, 48, 1)\n",
            "iteration: 32270 train shape: (99882, 48, 48, 1)\n",
            "iteration: 32271 train shape: (99884, 48, 48, 1)\n",
            "iteration: 32272 train shape: (99886, 48, 48, 1)\n",
            "iteration: 32273 train shape: (99888, 48, 48, 1)\n",
            "iteration: 32274 train shape: (99890, 48, 48, 1)\n",
            "iteration: 32275 train shape: (99892, 48, 48, 1)\n",
            "iteration: 32276 train shape: (99894, 48, 48, 1)\n",
            "iteration: 32277 train shape: (99896, 48, 48, 1)\n",
            "iteration: 32278 train shape: (99898, 48, 48, 1)\n",
            "iteration: 32279 train shape: (99900, 48, 48, 1)\n",
            "iteration: 32280 train shape: (99902, 48, 48, 1)\n",
            "iteration: 32281 train shape: (99904, 48, 48, 1)\n",
            "iteration: 32282 train shape: (99906, 48, 48, 1)\n",
            "iteration: 32283 train shape: (99908, 48, 48, 1)\n",
            "iteration: 32284 train shape: (99910, 48, 48, 1)\n",
            "iteration: 32285 train shape: (99912, 48, 48, 1)\n",
            "iteration: 32286 train shape: (99914, 48, 48, 1)\n",
            "iteration: 32287 train shape: (99916, 48, 48, 1)\n",
            "iteration: 32288 train shape: (99918, 48, 48, 1)\n",
            "iteration: 32289 train shape: (99920, 48, 48, 1)\n",
            "iteration: 32290 train shape: (99922, 48, 48, 1)\n",
            "iteration: 32291 train shape: (99924, 48, 48, 1)\n",
            "iteration: 32292 train shape: (99926, 48, 48, 1)\n",
            "iteration: 32293 train shape: (99928, 48, 48, 1)\n",
            "iteration: 32294 train shape: (99930, 48, 48, 1)\n",
            "iteration: 32295 train shape: (99932, 48, 48, 1)\n",
            "iteration: 32296 train shape: (99934, 48, 48, 1)\n",
            "iteration: 32297 train shape: (99936, 48, 48, 1)\n",
            "iteration: 32298 train shape: (99938, 48, 48, 1)\n",
            "iteration: 32299 train shape: (99940, 48, 48, 1)\n",
            "iteration: 32300 train shape: (99942, 48, 48, 1)\n",
            "iteration: 32301 train shape: (99944, 48, 48, 1)\n",
            "iteration: 32302 train shape: (99946, 48, 48, 1)\n",
            "iteration: 32303 train shape: (99948, 48, 48, 1)\n",
            "iteration: 32304 train shape: (99950, 48, 48, 1)\n",
            "iteration: 32305 train shape: (99952, 48, 48, 1)\n",
            "iteration: 32306 train shape: (99954, 48, 48, 1)\n",
            "iteration: 32307 train shape: (99956, 48, 48, 1)\n",
            "iteration: 32308 train shape: (99958, 48, 48, 1)\n",
            "iteration: 32309 train shape: (99960, 48, 48, 1)\n",
            "iteration: 32310 train shape: (99962, 48, 48, 1)\n",
            "iteration: 32311 train shape: (99964, 48, 48, 1)\n",
            "iteration: 32312 train shape: (99966, 48, 48, 1)\n",
            "iteration: 32313 train shape: (99968, 48, 48, 1)\n",
            "iteration: 32314 train shape: (99970, 48, 48, 1)\n",
            "iteration: 32315 train shape: (99972, 48, 48, 1)\n",
            "iteration: 32316 train shape: (99974, 48, 48, 1)\n",
            "iteration: 32317 train shape: (99976, 48, 48, 1)\n",
            "iteration: 32318 train shape: (99978, 48, 48, 1)\n",
            "iteration: 32319 train shape: (99980, 48, 48, 1)\n",
            "iteration: 32320 train shape: (99982, 48, 48, 1)\n",
            "iteration: 32321 train shape: (99984, 48, 48, 1)\n",
            "iteration: 32322 train shape: (99986, 48, 48, 1)\n",
            "iteration: 32323 train shape: (99988, 48, 48, 1)\n",
            "iteration: 32324 train shape: (99990, 48, 48, 1)\n",
            "iteration: 32325 train shape: (99992, 48, 48, 1)\n",
            "iteration: 32326 train shape: (99994, 48, 48, 1)\n",
            "iteration: 32327 train shape: (99996, 48, 48, 1)\n",
            "iteration: 32328 train shape: (99998, 48, 48, 1)\n",
            "iteration: 32329 train shape: (100000, 48, 48, 1)\n",
            "iteration: 32330 train shape: (100002, 48, 48, 1)\n",
            "iteration: 32331 train shape: (100004, 48, 48, 1)\n",
            "iteration: 32332 train shape: (100006, 48, 48, 1)\n",
            "iteration: 32333 train shape: (100008, 48, 48, 1)\n",
            "iteration: 32334 train shape: (100010, 48, 48, 1)\n",
            "iteration: 32335 train shape: (100012, 48, 48, 1)\n",
            "iteration: 32336 train shape: (100014, 48, 48, 1)\n",
            "iteration: 32337 train shape: (100016, 48, 48, 1)\n",
            "iteration: 32338 train shape: (100018, 48, 48, 1)\n",
            "iteration: 32339 train shape: (100020, 48, 48, 1)\n",
            "iteration: 32340 train shape: (100022, 48, 48, 1)\n",
            "iteration: 32341 train shape: (100024, 48, 48, 1)\n",
            "iteration: 32342 train shape: (100026, 48, 48, 1)\n",
            "iteration: 32343 train shape: (100028, 48, 48, 1)\n",
            "iteration: 32344 train shape: (100030, 48, 48, 1)\n",
            "iteration: 32345 train shape: (100032, 48, 48, 1)\n",
            "iteration: 32346 train shape: (100034, 48, 48, 1)\n",
            "iteration: 32347 train shape: (100036, 48, 48, 1)\n",
            "iteration: 32348 train shape: (100038, 48, 48, 1)\n",
            "iteration: 32349 train shape: (100040, 48, 48, 1)\n",
            "iteration: 32350 train shape: (100042, 48, 48, 1)\n",
            "iteration: 32351 train shape: (100044, 48, 48, 1)\n",
            "iteration: 32352 train shape: (100046, 48, 48, 1)\n",
            "iteration: 32353 train shape: (100048, 48, 48, 1)\n",
            "iteration: 32354 train shape: (100050, 48, 48, 1)\n",
            "iteration: 32355 train shape: (100052, 48, 48, 1)\n",
            "iteration: 32356 train shape: (100054, 48, 48, 1)\n",
            "iteration: 32357 train shape: (100056, 48, 48, 1)\n",
            "iteration: 32358 train shape: (100058, 48, 48, 1)\n",
            "iteration: 32359 train shape: (100060, 48, 48, 1)\n",
            "iteration: 32360 train shape: (100062, 48, 48, 1)\n",
            "iteration: 32361 train shape: (100064, 48, 48, 1)\n",
            "iteration: 32362 train shape: (100066, 48, 48, 1)\n",
            "iteration: 32363 train shape: (100068, 48, 48, 1)\n",
            "iteration: 32364 train shape: (100070, 48, 48, 1)\n",
            "iteration: 32365 train shape: (100072, 48, 48, 1)\n",
            "iteration: 32366 train shape: (100074, 48, 48, 1)\n",
            "iteration: 32367 train shape: (100076, 48, 48, 1)\n",
            "iteration: 32368 train shape: (100078, 48, 48, 1)\n",
            "iteration: 32369 train shape: (100080, 48, 48, 1)\n",
            "iteration: 32370 train shape: (100082, 48, 48, 1)\n",
            "iteration: 32371 train shape: (100084, 48, 48, 1)\n",
            "iteration: 32372 train shape: (100086, 48, 48, 1)\n",
            "iteration: 32373 train shape: (100088, 48, 48, 1)\n",
            "iteration: 32374 train shape: (100090, 48, 48, 1)\n",
            "iteration: 32375 train shape: (100092, 48, 48, 1)\n",
            "iteration: 32376 train shape: (100094, 48, 48, 1)\n",
            "iteration: 32377 train shape: (100096, 48, 48, 1)\n",
            "iteration: 32378 train shape: (100098, 48, 48, 1)\n",
            "iteration: 32379 train shape: (100100, 48, 48, 1)\n",
            "iteration: 32380 train shape: (100102, 48, 48, 1)\n",
            "iteration: 32381 train shape: (100104, 48, 48, 1)\n",
            "iteration: 32382 train shape: (100106, 48, 48, 1)\n",
            "iteration: 32383 train shape: (100108, 48, 48, 1)\n",
            "iteration: 32384 train shape: (100110, 48, 48, 1)\n",
            "iteration: 32385 train shape: (100112, 48, 48, 1)\n",
            "iteration: 32386 train shape: (100114, 48, 48, 1)\n",
            "iteration: 32387 train shape: (100116, 48, 48, 1)\n",
            "iteration: 32388 train shape: (100118, 48, 48, 1)\n",
            "iteration: 32389 train shape: (100120, 48, 48, 1)\n",
            "iteration: 32390 train shape: (100122, 48, 48, 1)\n",
            "iteration: 32391 train shape: (100124, 48, 48, 1)\n",
            "iteration: 32392 train shape: (100126, 48, 48, 1)\n",
            "iteration: 32393 train shape: (100128, 48, 48, 1)\n",
            "iteration: 32394 train shape: (100130, 48, 48, 1)\n",
            "iteration: 32395 train shape: (100132, 48, 48, 1)\n",
            "iteration: 32396 train shape: (100134, 48, 48, 1)\n",
            "iteration: 32397 train shape: (100136, 48, 48, 1)\n",
            "iteration: 32398 train shape: (100138, 48, 48, 1)\n",
            "iteration: 32399 train shape: (100140, 48, 48, 1)\n",
            "iteration: 32400 train shape: (100142, 48, 48, 1)\n",
            "iteration: 32401 train shape: (100144, 48, 48, 1)\n",
            "iteration: 32402 train shape: (100146, 48, 48, 1)\n",
            "iteration: 32403 train shape: (100148, 48, 48, 1)\n",
            "iteration: 32404 train shape: (100150, 48, 48, 1)\n",
            "iteration: 32405 train shape: (100152, 48, 48, 1)\n",
            "iteration: 32406 train shape: (100154, 48, 48, 1)\n",
            "iteration: 32407 train shape: (100156, 48, 48, 1)\n",
            "iteration: 32408 train shape: (100158, 48, 48, 1)\n",
            "iteration: 32409 train shape: (100160, 48, 48, 1)\n",
            "iteration: 32410 train shape: (100162, 48, 48, 1)\n",
            "iteration: 32411 train shape: (100164, 48, 48, 1)\n",
            "iteration: 32412 train shape: (100166, 48, 48, 1)\n",
            "iteration: 32413 train shape: (100168, 48, 48, 1)\n",
            "iteration: 32414 train shape: (100170, 48, 48, 1)\n",
            "iteration: 32415 train shape: (100172, 48, 48, 1)\n",
            "iteration: 32416 train shape: (100174, 48, 48, 1)\n",
            "iteration: 32417 train shape: (100176, 48, 48, 1)\n",
            "iteration: 32418 train shape: (100178, 48, 48, 1)\n",
            "iteration: 32419 train shape: (100180, 48, 48, 1)\n",
            "iteration: 32420 train shape: (100182, 48, 48, 1)\n",
            "iteration: 32421 train shape: (100184, 48, 48, 1)\n",
            "iteration: 32422 train shape: (100186, 48, 48, 1)\n",
            "iteration: 32423 train shape: (100188, 48, 48, 1)\n",
            "iteration: 32424 train shape: (100190, 48, 48, 1)\n",
            "iteration: 32425 train shape: (100192, 48, 48, 1)\n",
            "iteration: 32426 train shape: (100194, 48, 48, 1)\n",
            "iteration: 32427 train shape: (100196, 48, 48, 1)\n",
            "iteration: 32428 train shape: (100198, 48, 48, 1)\n",
            "iteration: 32429 train shape: (100200, 48, 48, 1)\n",
            "iteration: 32430 train shape: (100202, 48, 48, 1)\n",
            "iteration: 32431 train shape: (100204, 48, 48, 1)\n",
            "iteration: 32432 train shape: (100206, 48, 48, 1)\n",
            "iteration: 32433 train shape: (100208, 48, 48, 1)\n",
            "iteration: 32434 train shape: (100210, 48, 48, 1)\n",
            "iteration: 32435 train shape: (100212, 48, 48, 1)\n",
            "iteration: 32436 train shape: (100214, 48, 48, 1)\n",
            "iteration: 32437 train shape: (100216, 48, 48, 1)\n",
            "iteration: 32438 train shape: (100218, 48, 48, 1)\n",
            "iteration: 32439 train shape: (100220, 48, 48, 1)\n",
            "iteration: 32440 train shape: (100222, 48, 48, 1)\n",
            "iteration: 32441 train shape: (100224, 48, 48, 1)\n",
            "iteration: 32442 train shape: (100226, 48, 48, 1)\n",
            "iteration: 32443 train shape: (100228, 48, 48, 1)\n",
            "iteration: 32444 train shape: (100230, 48, 48, 1)\n",
            "iteration: 32445 train shape: (100232, 48, 48, 1)\n",
            "iteration: 32446 train shape: (100234, 48, 48, 1)\n",
            "iteration: 32447 train shape: (100236, 48, 48, 1)\n",
            "iteration: 32448 train shape: (100238, 48, 48, 1)\n",
            "iteration: 32449 train shape: (100240, 48, 48, 1)\n",
            "iteration: 32450 train shape: (100242, 48, 48, 1)\n",
            "iteration: 32451 train shape: (100244, 48, 48, 1)\n",
            "iteration: 32452 train shape: (100246, 48, 48, 1)\n",
            "iteration: 32453 train shape: (100248, 48, 48, 1)\n",
            "iteration: 32454 train shape: (100250, 48, 48, 1)\n",
            "iteration: 32455 train shape: (100252, 48, 48, 1)\n",
            "iteration: 32456 train shape: (100254, 48, 48, 1)\n",
            "iteration: 32457 train shape: (100256, 48, 48, 1)\n",
            "iteration: 32458 train shape: (100258, 48, 48, 1)\n",
            "iteration: 32459 train shape: (100260, 48, 48, 1)\n",
            "iteration: 32460 train shape: (100262, 48, 48, 1)\n",
            "iteration: 32461 train shape: (100264, 48, 48, 1)\n",
            "iteration: 32462 train shape: (100266, 48, 48, 1)\n",
            "iteration: 32463 train shape: (100268, 48, 48, 1)\n",
            "iteration: 32464 train shape: (100270, 48, 48, 1)\n",
            "iteration: 32465 train shape: (100272, 48, 48, 1)\n",
            "iteration: 32466 train shape: (100274, 48, 48, 1)\n",
            "iteration: 32467 train shape: (100276, 48, 48, 1)\n",
            "iteration: 32468 train shape: (100278, 48, 48, 1)\n",
            "iteration: 32469 train shape: (100280, 48, 48, 1)\n",
            "iteration: 32470 train shape: (100282, 48, 48, 1)\n",
            "iteration: 32471 train shape: (100284, 48, 48, 1)\n",
            "iteration: 32472 train shape: (100286, 48, 48, 1)\n",
            "iteration: 32473 train shape: (100288, 48, 48, 1)\n",
            "iteration: 32474 train shape: (100290, 48, 48, 1)\n",
            "iteration: 32475 train shape: (100292, 48, 48, 1)\n",
            "iteration: 32476 train shape: (100294, 48, 48, 1)\n",
            "iteration: 32477 train shape: (100296, 48, 48, 1)\n",
            "iteration: 32478 train shape: (100298, 48, 48, 1)\n",
            "iteration: 32479 train shape: (100300, 48, 48, 1)\n",
            "iteration: 32480 train shape: (100302, 48, 48, 1)\n",
            "iteration: 32481 train shape: (100304, 48, 48, 1)\n",
            "iteration: 32482 train shape: (100306, 48, 48, 1)\n",
            "iteration: 32483 train shape: (100308, 48, 48, 1)\n",
            "iteration: 32484 train shape: (100310, 48, 48, 1)\n",
            "iteration: 32485 train shape: (100312, 48, 48, 1)\n",
            "iteration: 32486 train shape: (100314, 48, 48, 1)\n",
            "iteration: 32487 train shape: (100316, 48, 48, 1)\n",
            "iteration: 32488 train shape: (100318, 48, 48, 1)\n",
            "iteration: 32489 train shape: (100320, 48, 48, 1)\n",
            "iteration: 32490 train shape: (100322, 48, 48, 1)\n",
            "iteration: 32491 train shape: (100324, 48, 48, 1)\n",
            "iteration: 32492 train shape: (100326, 48, 48, 1)\n",
            "iteration: 32493 train shape: (100328, 48, 48, 1)\n",
            "iteration: 32494 train shape: (100330, 48, 48, 1)\n",
            "iteration: 32495 train shape: (100332, 48, 48, 1)\n",
            "iteration: 32496 train shape: (100334, 48, 48, 1)\n",
            "iteration: 32497 train shape: (100336, 48, 48, 1)\n",
            "iteration: 32498 train shape: (100338, 48, 48, 1)\n",
            "iteration: 32499 train shape: (100340, 48, 48, 1)\n",
            "iteration: 32500 train shape: (100342, 48, 48, 1)\n",
            "iteration: 32501 train shape: (100344, 48, 48, 1)\n",
            "iteration: 32502 train shape: (100346, 48, 48, 1)\n",
            "iteration: 32503 train shape: (100348, 48, 48, 1)\n",
            "iteration: 32504 train shape: (100350, 48, 48, 1)\n",
            "iteration: 32505 train shape: (100352, 48, 48, 1)\n",
            "iteration: 32506 train shape: (100354, 48, 48, 1)\n",
            "iteration: 32507 train shape: (100356, 48, 48, 1)\n",
            "iteration: 32508 train shape: (100358, 48, 48, 1)\n",
            "iteration: 32509 train shape: (100360, 48, 48, 1)\n",
            "iteration: 32510 train shape: (100362, 48, 48, 1)\n",
            "iteration: 32511 train shape: (100364, 48, 48, 1)\n",
            "iteration: 32512 train shape: (100366, 48, 48, 1)\n",
            "iteration: 32513 train shape: (100368, 48, 48, 1)\n",
            "iteration: 32514 train shape: (100370, 48, 48, 1)\n",
            "iteration: 32515 train shape: (100372, 48, 48, 1)\n",
            "iteration: 32516 train shape: (100374, 48, 48, 1)\n",
            "iteration: 32517 train shape: (100376, 48, 48, 1)\n",
            "iteration: 32518 train shape: (100378, 48, 48, 1)\n",
            "iteration: 32519 train shape: (100380, 48, 48, 1)\n",
            "iteration: 32520 train shape: (100382, 48, 48, 1)\n",
            "iteration: 32521 train shape: (100384, 48, 48, 1)\n",
            "iteration: 32522 train shape: (100386, 48, 48, 1)\n",
            "iteration: 32523 train shape: (100388, 48, 48, 1)\n",
            "iteration: 32524 train shape: (100390, 48, 48, 1)\n",
            "iteration: 32525 train shape: (100392, 48, 48, 1)\n",
            "iteration: 32526 train shape: (100394, 48, 48, 1)\n",
            "iteration: 32527 train shape: (100396, 48, 48, 1)\n",
            "iteration: 32528 train shape: (100398, 48, 48, 1)\n",
            "iteration: 32529 train shape: (100400, 48, 48, 1)\n",
            "iteration: 32530 train shape: (100402, 48, 48, 1)\n",
            "iteration: 32531 train shape: (100404, 48, 48, 1)\n",
            "iteration: 32532 train shape: (100406, 48, 48, 1)\n",
            "iteration: 32533 train shape: (100408, 48, 48, 1)\n",
            "iteration: 32534 train shape: (100410, 48, 48, 1)\n",
            "iteration: 32535 train shape: (100412, 48, 48, 1)\n",
            "iteration: 32536 train shape: (100414, 48, 48, 1)\n",
            "iteration: 32537 train shape: (100416, 48, 48, 1)\n",
            "iteration: 32538 train shape: (100418, 48, 48, 1)\n",
            "iteration: 32539 train shape: (100420, 48, 48, 1)\n",
            "iteration: 32540 train shape: (100422, 48, 48, 1)\n",
            "iteration: 32541 train shape: (100424, 48, 48, 1)\n",
            "iteration: 32542 train shape: (100426, 48, 48, 1)\n",
            "iteration: 32543 train shape: (100428, 48, 48, 1)\n",
            "iteration: 32544 train shape: (100430, 48, 48, 1)\n",
            "iteration: 32545 train shape: (100432, 48, 48, 1)\n",
            "iteration: 32546 train shape: (100434, 48, 48, 1)\n",
            "iteration: 32547 train shape: (100436, 48, 48, 1)\n",
            "iteration: 32548 train shape: (100438, 48, 48, 1)\n",
            "iteration: 32549 train shape: (100440, 48, 48, 1)\n",
            "iteration: 32550 train shape: (100442, 48, 48, 1)\n",
            "iteration: 32551 train shape: (100444, 48, 48, 1)\n",
            "iteration: 32552 train shape: (100446, 48, 48, 1)\n",
            "iteration: 32553 train shape: (100448, 48, 48, 1)\n",
            "iteration: 32554 train shape: (100450, 48, 48, 1)\n",
            "iteration: 32555 train shape: (100452, 48, 48, 1)\n",
            "iteration: 32556 train shape: (100454, 48, 48, 1)\n",
            "iteration: 32557 train shape: (100456, 48, 48, 1)\n",
            "iteration: 32558 train shape: (100458, 48, 48, 1)\n",
            "iteration: 32559 train shape: (100460, 48, 48, 1)\n",
            "iteration: 32560 train shape: (100462, 48, 48, 1)\n",
            "iteration: 32561 train shape: (100464, 48, 48, 1)\n",
            "iteration: 32562 train shape: (100466, 48, 48, 1)\n",
            "iteration: 32563 train shape: (100468, 48, 48, 1)\n",
            "iteration: 32564 train shape: (100470, 48, 48, 1)\n",
            "iteration: 32565 train shape: (100472, 48, 48, 1)\n",
            "iteration: 32566 train shape: (100474, 48, 48, 1)\n",
            "iteration: 32567 train shape: (100476, 48, 48, 1)\n",
            "iteration: 32568 train shape: (100478, 48, 48, 1)\n",
            "iteration: 32569 train shape: (100480, 48, 48, 1)\n",
            "iteration: 32570 train shape: (100482, 48, 48, 1)\n",
            "iteration: 32571 train shape: (100484, 48, 48, 1)\n",
            "iteration: 32572 train shape: (100486, 48, 48, 1)\n",
            "iteration: 32573 train shape: (100488, 48, 48, 1)\n",
            "iteration: 32574 train shape: (100490, 48, 48, 1)\n",
            "iteration: 32575 train shape: (100492, 48, 48, 1)\n",
            "iteration: 32576 train shape: (100494, 48, 48, 1)\n",
            "iteration: 32577 train shape: (100496, 48, 48, 1)\n",
            "iteration: 32578 train shape: (100498, 48, 48, 1)\n",
            "iteration: 32579 train shape: (100500, 48, 48, 1)\n",
            "iteration: 32580 train shape: (100502, 48, 48, 1)\n",
            "iteration: 32581 train shape: (100504, 48, 48, 1)\n",
            "iteration: 32582 train shape: (100506, 48, 48, 1)\n",
            "iteration: 32583 train shape: (100508, 48, 48, 1)\n",
            "iteration: 32584 train shape: (100510, 48, 48, 1)\n",
            "iteration: 32585 train shape: (100512, 48, 48, 1)\n",
            "iteration: 32586 train shape: (100514, 48, 48, 1)\n",
            "iteration: 32587 train shape: (100516, 48, 48, 1)\n",
            "iteration: 32588 train shape: (100518, 48, 48, 1)\n",
            "iteration: 32589 train shape: (100520, 48, 48, 1)\n",
            "iteration: 32590 train shape: (100522, 48, 48, 1)\n",
            "iteration: 32591 train shape: (100524, 48, 48, 1)\n",
            "iteration: 32592 train shape: (100526, 48, 48, 1)\n",
            "iteration: 32593 train shape: (100528, 48, 48, 1)\n",
            "iteration: 32594 train shape: (100530, 48, 48, 1)\n",
            "iteration: 32595 train shape: (100532, 48, 48, 1)\n",
            "iteration: 32596 train shape: (100534, 48, 48, 1)\n",
            "iteration: 32597 train shape: (100536, 48, 48, 1)\n",
            "iteration: 32598 train shape: (100538, 48, 48, 1)\n",
            "iteration: 32599 train shape: (100540, 48, 48, 1)\n",
            "iteration: 32600 train shape: (100542, 48, 48, 1)\n",
            "iteration: 32601 train shape: (100544, 48, 48, 1)\n",
            "iteration: 32602 train shape: (100546, 48, 48, 1)\n",
            "iteration: 32603 train shape: (100548, 48, 48, 1)\n",
            "iteration: 32604 train shape: (100550, 48, 48, 1)\n",
            "iteration: 32605 train shape: (100552, 48, 48, 1)\n",
            "iteration: 32606 train shape: (100554, 48, 48, 1)\n",
            "iteration: 32607 train shape: (100556, 48, 48, 1)\n",
            "iteration: 32608 train shape: (100558, 48, 48, 1)\n",
            "iteration: 32609 train shape: (100560, 48, 48, 1)\n",
            "iteration: 32610 train shape: (100562, 48, 48, 1)\n",
            "iteration: 32611 train shape: (100564, 48, 48, 1)\n",
            "iteration: 32612 train shape: (100566, 48, 48, 1)\n",
            "iteration: 32613 train shape: (100568, 48, 48, 1)\n",
            "iteration: 32614 train shape: (100570, 48, 48, 1)\n",
            "iteration: 32615 train shape: (100572, 48, 48, 1)\n",
            "iteration: 32616 train shape: (100574, 48, 48, 1)\n",
            "iteration: 32617 train shape: (100576, 48, 48, 1)\n",
            "iteration: 32618 train shape: (100578, 48, 48, 1)\n",
            "iteration: 32619 train shape: (100580, 48, 48, 1)\n",
            "iteration: 32620 train shape: (100582, 48, 48, 1)\n",
            "iteration: 32621 train shape: (100584, 48, 48, 1)\n",
            "iteration: 32622 train shape: (100586, 48, 48, 1)\n",
            "iteration: 32623 train shape: (100588, 48, 48, 1)\n",
            "iteration: 32624 train shape: (100590, 48, 48, 1)\n",
            "iteration: 32625 train shape: (100592, 48, 48, 1)\n",
            "iteration: 32626 train shape: (100594, 48, 48, 1)\n",
            "iteration: 32627 train shape: (100596, 48, 48, 1)\n",
            "iteration: 32628 train shape: (100598, 48, 48, 1)\n",
            "iteration: 32629 train shape: (100600, 48, 48, 1)\n",
            "iteration: 32630 train shape: (100602, 48, 48, 1)\n",
            "iteration: 32631 train shape: (100604, 48, 48, 1)\n",
            "iteration: 32632 train shape: (100606, 48, 48, 1)\n",
            "iteration: 32633 train shape: (100608, 48, 48, 1)\n",
            "iteration: 32634 train shape: (100610, 48, 48, 1)\n",
            "iteration: 32635 train shape: (100612, 48, 48, 1)\n",
            "iteration: 32636 train shape: (100614, 48, 48, 1)\n",
            "iteration: 32637 train shape: (100616, 48, 48, 1)\n",
            "iteration: 32638 train shape: (100618, 48, 48, 1)\n",
            "iteration: 32639 train shape: (100620, 48, 48, 1)\n",
            "iteration: 32640 train shape: (100622, 48, 48, 1)\n",
            "iteration: 32641 train shape: (100624, 48, 48, 1)\n",
            "iteration: 32642 train shape: (100626, 48, 48, 1)\n",
            "iteration: 32643 train shape: (100628, 48, 48, 1)\n",
            "iteration: 32644 train shape: (100630, 48, 48, 1)\n",
            "iteration: 32645 train shape: (100632, 48, 48, 1)\n",
            "iteration: 32646 train shape: (100634, 48, 48, 1)\n",
            "iteration: 32647 train shape: (100636, 48, 48, 1)\n",
            "iteration: 32648 train shape: (100638, 48, 48, 1)\n",
            "iteration: 32649 train shape: (100640, 48, 48, 1)\n",
            "iteration: 32650 train shape: (100642, 48, 48, 1)\n",
            "iteration: 32651 train shape: (100644, 48, 48, 1)\n",
            "iteration: 32652 train shape: (100646, 48, 48, 1)\n",
            "iteration: 32653 train shape: (100648, 48, 48, 1)\n",
            "iteration: 32654 train shape: (100650, 48, 48, 1)\n",
            "iteration: 32655 train shape: (100652, 48, 48, 1)\n",
            "iteration: 32656 train shape: (100654, 48, 48, 1)\n",
            "iteration: 32657 train shape: (100656, 48, 48, 1)\n",
            "iteration: 32658 train shape: (100658, 48, 48, 1)\n",
            "iteration: 32659 train shape: (100660, 48, 48, 1)\n",
            "iteration: 32660 train shape: (100662, 48, 48, 1)\n",
            "iteration: 32661 train shape: (100664, 48, 48, 1)\n",
            "iteration: 32662 train shape: (100666, 48, 48, 1)\n",
            "iteration: 32663 train shape: (100668, 48, 48, 1)\n",
            "iteration: 32664 train shape: (100670, 48, 48, 1)\n",
            "iteration: 32665 train shape: (100672, 48, 48, 1)\n",
            "iteration: 32666 train shape: (100674, 48, 48, 1)\n",
            "iteration: 32667 train shape: (100676, 48, 48, 1)\n",
            "iteration: 32668 train shape: (100678, 48, 48, 1)\n",
            "iteration: 32669 train shape: (100680, 48, 48, 1)\n",
            "iteration: 32670 train shape: (100682, 48, 48, 1)\n",
            "iteration: 32671 train shape: (100684, 48, 48, 1)\n",
            "iteration: 32672 train shape: (100686, 48, 48, 1)\n",
            "iteration: 32673 train shape: (100688, 48, 48, 1)\n",
            "iteration: 32674 train shape: (100690, 48, 48, 1)\n",
            "iteration: 32675 train shape: (100692, 48, 48, 1)\n",
            "iteration: 32676 train shape: (100694, 48, 48, 1)\n",
            "iteration: 32677 train shape: (100696, 48, 48, 1)\n",
            "iteration: 32678 train shape: (100698, 48, 48, 1)\n",
            "iteration: 32679 train shape: (100700, 48, 48, 1)\n",
            "iteration: 32680 train shape: (100702, 48, 48, 1)\n",
            "iteration: 32681 train shape: (100704, 48, 48, 1)\n",
            "iteration: 32682 train shape: (100706, 48, 48, 1)\n",
            "iteration: 32683 train shape: (100708, 48, 48, 1)\n",
            "iteration: 32684 train shape: (100710, 48, 48, 1)\n",
            "iteration: 32685 train shape: (100712, 48, 48, 1)\n",
            "iteration: 32686 train shape: (100714, 48, 48, 1)\n",
            "iteration: 32687 train shape: (100716, 48, 48, 1)\n",
            "iteration: 32688 train shape: (100718, 48, 48, 1)\n",
            "iteration: 32689 train shape: (100720, 48, 48, 1)\n",
            "iteration: 32690 train shape: (100722, 48, 48, 1)\n",
            "iteration: 32691 train shape: (100724, 48, 48, 1)\n",
            "iteration: 32692 train shape: (100726, 48, 48, 1)\n",
            "iteration: 32693 train shape: (100728, 48, 48, 1)\n",
            "iteration: 32694 train shape: (100730, 48, 48, 1)\n",
            "iteration: 32695 train shape: (100732, 48, 48, 1)\n",
            "iteration: 32696 train shape: (100734, 48, 48, 1)\n",
            "iteration: 32697 train shape: (100736, 48, 48, 1)\n",
            "iteration: 32698 train shape: (100738, 48, 48, 1)\n",
            "iteration: 32699 train shape: (100740, 48, 48, 1)\n",
            "iteration: 32700 train shape: (100742, 48, 48, 1)\n",
            "iteration: 32701 train shape: (100744, 48, 48, 1)\n",
            "iteration: 32702 train shape: (100746, 48, 48, 1)\n",
            "iteration: 32703 train shape: (100748, 48, 48, 1)\n",
            "iteration: 32704 train shape: (100750, 48, 48, 1)\n",
            "iteration: 32705 train shape: (100752, 48, 48, 1)\n",
            "iteration: 32706 train shape: (100754, 48, 48, 1)\n",
            "iteration: 32707 train shape: (100756, 48, 48, 1)\n",
            "iteration: 32708 train shape: (100758, 48, 48, 1)\n",
            "iteration: 32709 train shape: (100760, 48, 48, 1)\n",
            "iteration: 32710 train shape: (100762, 48, 48, 1)\n",
            "iteration: 32711 train shape: (100764, 48, 48, 1)\n",
            "iteration: 32712 train shape: (100766, 48, 48, 1)\n",
            "iteration: 32713 train shape: (100768, 48, 48, 1)\n",
            "iteration: 32714 train shape: (100770, 48, 48, 1)\n",
            "iteration: 32715 train shape: (100772, 48, 48, 1)\n",
            "iteration: 32716 train shape: (100774, 48, 48, 1)\n",
            "iteration: 32717 train shape: (100776, 48, 48, 1)\n",
            "iteration: 32718 train shape: (100778, 48, 48, 1)\n",
            "iteration: 32719 train shape: (100780, 48, 48, 1)\n",
            "iteration: 32720 train shape: (100782, 48, 48, 1)\n",
            "iteration: 32721 train shape: (100784, 48, 48, 1)\n",
            "iteration: 32722 train shape: (100786, 48, 48, 1)\n",
            "iteration: 32723 train shape: (100788, 48, 48, 1)\n",
            "iteration: 32724 train shape: (100790, 48, 48, 1)\n",
            "iteration: 32725 train shape: (100792, 48, 48, 1)\n",
            "iteration: 32726 train shape: (100794, 48, 48, 1)\n",
            "iteration: 32727 train shape: (100796, 48, 48, 1)\n",
            "iteration: 32728 train shape: (100798, 48, 48, 1)\n",
            "iteration: 32729 train shape: (100800, 48, 48, 1)\n",
            "iteration: 32730 train shape: (100802, 48, 48, 1)\n",
            "iteration: 32731 train shape: (100804, 48, 48, 1)\n",
            "iteration: 32732 train shape: (100806, 48, 48, 1)\n",
            "iteration: 32733 train shape: (100808, 48, 48, 1)\n",
            "iteration: 32734 train shape: (100810, 48, 48, 1)\n",
            "iteration: 32735 train shape: (100812, 48, 48, 1)\n",
            "iteration: 32736 train shape: (100814, 48, 48, 1)\n",
            "iteration: 32737 train shape: (100816, 48, 48, 1)\n",
            "iteration: 32738 train shape: (100818, 48, 48, 1)\n",
            "iteration: 32739 train shape: (100820, 48, 48, 1)\n",
            "iteration: 32740 train shape: (100822, 48, 48, 1)\n",
            "iteration: 32741 train shape: (100824, 48, 48, 1)\n",
            "iteration: 32742 train shape: (100826, 48, 48, 1)\n",
            "iteration: 32743 train shape: (100828, 48, 48, 1)\n",
            "iteration: 32744 train shape: (100830, 48, 48, 1)\n",
            "iteration: 32745 train shape: (100832, 48, 48, 1)\n",
            "iteration: 32746 train shape: (100834, 48, 48, 1)\n",
            "iteration: 32747 train shape: (100836, 48, 48, 1)\n",
            "iteration: 32748 train shape: (100838, 48, 48, 1)\n",
            "iteration: 32749 train shape: (100840, 48, 48, 1)\n",
            "iteration: 32750 train shape: (100842, 48, 48, 1)\n",
            "iteration: 32751 train shape: (100844, 48, 48, 1)\n",
            "iteration: 32752 train shape: (100846, 48, 48, 1)\n",
            "iteration: 32753 train shape: (100848, 48, 48, 1)\n",
            "iteration: 32754 train shape: (100850, 48, 48, 1)\n",
            "iteration: 32755 train shape: (100852, 48, 48, 1)\n",
            "iteration: 32756 train shape: (100854, 48, 48, 1)\n",
            "iteration: 32757 train shape: (100856, 48, 48, 1)\n",
            "iteration: 32758 train shape: (100858, 48, 48, 1)\n",
            "iteration: 32759 train shape: (100860, 48, 48, 1)\n",
            "iteration: 32760 train shape: (100862, 48, 48, 1)\n",
            "iteration: 32761 train shape: (100864, 48, 48, 1)\n",
            "iteration: 32762 train shape: (100866, 48, 48, 1)\n",
            "iteration: 32763 train shape: (100868, 48, 48, 1)\n",
            "iteration: 32764 train shape: (100870, 48, 48, 1)\n",
            "iteration: 32765 train shape: (100872, 48, 48, 1)\n",
            "iteration: 32766 train shape: (100874, 48, 48, 1)\n",
            "iteration: 32767 train shape: (100876, 48, 48, 1)\n",
            "iteration: 32768 train shape: (100878, 48, 48, 1)\n",
            "iteration: 32769 train shape: (100880, 48, 48, 1)\n",
            "iteration: 32770 train shape: (100882, 48, 48, 1)\n",
            "iteration: 32771 train shape: (100884, 48, 48, 1)\n",
            "iteration: 32772 train shape: (100886, 48, 48, 1)\n",
            "iteration: 32773 train shape: (100888, 48, 48, 1)\n",
            "iteration: 32774 train shape: (100890, 48, 48, 1)\n",
            "iteration: 32775 train shape: (100892, 48, 48, 1)\n",
            "iteration: 32776 train shape: (100894, 48, 48, 1)\n",
            "iteration: 32777 train shape: (100896, 48, 48, 1)\n",
            "iteration: 32778 train shape: (100898, 48, 48, 1)\n",
            "iteration: 32779 train shape: (100900, 48, 48, 1)\n",
            "iteration: 32780 train shape: (100902, 48, 48, 1)\n",
            "iteration: 32781 train shape: (100904, 48, 48, 1)\n",
            "iteration: 32782 train shape: (100906, 48, 48, 1)\n",
            "iteration: 32783 train shape: (100908, 48, 48, 1)\n",
            "iteration: 32784 train shape: (100910, 48, 48, 1)\n",
            "iteration: 32785 train shape: (100912, 48, 48, 1)\n",
            "iteration: 32786 train shape: (100914, 48, 48, 1)\n",
            "iteration: 32787 train shape: (100916, 48, 48, 1)\n",
            "iteration: 32788 train shape: (100918, 48, 48, 1)\n",
            "iteration: 32789 train shape: (100920, 48, 48, 1)\n",
            "iteration: 32790 train shape: (100922, 48, 48, 1)\n",
            "iteration: 32791 train shape: (100924, 48, 48, 1)\n",
            "iteration: 32792 train shape: (100926, 48, 48, 1)\n",
            "iteration: 32793 train shape: (100928, 48, 48, 1)\n",
            "iteration: 32794 train shape: (100930, 48, 48, 1)\n",
            "iteration: 32795 train shape: (100932, 48, 48, 1)\n",
            "iteration: 32796 train shape: (100934, 48, 48, 1)\n",
            "iteration: 32797 train shape: (100936, 48, 48, 1)\n",
            "iteration: 32798 train shape: (100938, 48, 48, 1)\n",
            "iteration: 32799 train shape: (100940, 48, 48, 1)\n",
            "iteration: 32800 train shape: (100942, 48, 48, 1)\n",
            "iteration: 32801 train shape: (100944, 48, 48, 1)\n",
            "iteration: 32802 train shape: (100946, 48, 48, 1)\n",
            "iteration: 32803 train shape: (100948, 48, 48, 1)\n",
            "iteration: 32804 train shape: (100950, 48, 48, 1)\n",
            "iteration: 32805 train shape: (100952, 48, 48, 1)\n",
            "iteration: 32806 train shape: (100954, 48, 48, 1)\n",
            "iteration: 32807 train shape: (100956, 48, 48, 1)\n",
            "iteration: 32808 train shape: (100958, 48, 48, 1)\n",
            "iteration: 32809 train shape: (100960, 48, 48, 1)\n",
            "iteration: 32810 train shape: (100962, 48, 48, 1)\n",
            "iteration: 32811 train shape: (100964, 48, 48, 1)\n",
            "iteration: 32812 train shape: (100966, 48, 48, 1)\n",
            "iteration: 32813 train shape: (100968, 48, 48, 1)\n",
            "iteration: 32814 train shape: (100970, 48, 48, 1)\n",
            "iteration: 32815 train shape: (100972, 48, 48, 1)\n",
            "iteration: 32816 train shape: (100974, 48, 48, 1)\n",
            "iteration: 32817 train shape: (100976, 48, 48, 1)\n",
            "iteration: 32818 train shape: (100978, 48, 48, 1)\n",
            "iteration: 32819 train shape: (100980, 48, 48, 1)\n",
            "iteration: 32820 train shape: (100982, 48, 48, 1)\n",
            "iteration: 32821 train shape: (100984, 48, 48, 1)\n",
            "iteration: 32822 train shape: (100986, 48, 48, 1)\n",
            "iteration: 32823 train shape: (100988, 48, 48, 1)\n",
            "iteration: 32824 train shape: (100990, 48, 48, 1)\n",
            "iteration: 32825 train shape: (100992, 48, 48, 1)\n",
            "iteration: 32826 train shape: (100994, 48, 48, 1)\n",
            "iteration: 32827 train shape: (100996, 48, 48, 1)\n",
            "iteration: 32828 train shape: (100998, 48, 48, 1)\n",
            "iteration: 32829 train shape: (101000, 48, 48, 1)\n",
            "iteration: 32830 train shape: (101002, 48, 48, 1)\n",
            "iteration: 32831 train shape: (101004, 48, 48, 1)\n",
            "iteration: 32832 train shape: (101006, 48, 48, 1)\n",
            "iteration: 32833 train shape: (101008, 48, 48, 1)\n",
            "iteration: 32834 train shape: (101010, 48, 48, 1)\n",
            "iteration: 32835 train shape: (101012, 48, 48, 1)\n",
            "iteration: 32836 train shape: (101014, 48, 48, 1)\n",
            "iteration: 32837 train shape: (101016, 48, 48, 1)\n",
            "iteration: 32838 train shape: (101018, 48, 48, 1)\n",
            "iteration: 32839 train shape: (101020, 48, 48, 1)\n",
            "iteration: 32840 train shape: (101022, 48, 48, 1)\n",
            "iteration: 32841 train shape: (101024, 48, 48, 1)\n",
            "iteration: 32842 train shape: (101026, 48, 48, 1)\n",
            "iteration: 32843 train shape: (101028, 48, 48, 1)\n",
            "iteration: 32844 train shape: (101030, 48, 48, 1)\n",
            "iteration: 32845 train shape: (101032, 48, 48, 1)\n",
            "iteration: 32846 train shape: (101034, 48, 48, 1)\n",
            "iteration: 32847 train shape: (101036, 48, 48, 1)\n",
            "iteration: 32848 train shape: (101038, 48, 48, 1)\n",
            "iteration: 32849 train shape: (101040, 48, 48, 1)\n",
            "iteration: 32850 train shape: (101042, 48, 48, 1)\n",
            "iteration: 32851 train shape: (101044, 48, 48, 1)\n",
            "iteration: 32852 train shape: (101046, 48, 48, 1)\n",
            "iteration: 32853 train shape: (101048, 48, 48, 1)\n",
            "iteration: 32854 train shape: (101050, 48, 48, 1)\n",
            "iteration: 32855 train shape: (101052, 48, 48, 1)\n",
            "iteration: 32856 train shape: (101054, 48, 48, 1)\n",
            "iteration: 32857 train shape: (101056, 48, 48, 1)\n",
            "iteration: 32858 train shape: (101058, 48, 48, 1)\n",
            "iteration: 32859 train shape: (101060, 48, 48, 1)\n",
            "iteration: 32860 train shape: (101062, 48, 48, 1)\n",
            "iteration: 32861 train shape: (101064, 48, 48, 1)\n",
            "iteration: 32862 train shape: (101066, 48, 48, 1)\n",
            "iteration: 32863 train shape: (101068, 48, 48, 1)\n",
            "iteration: 32864 train shape: (101070, 48, 48, 1)\n",
            "iteration: 32865 train shape: (101072, 48, 48, 1)\n",
            "iteration: 32866 train shape: (101074, 48, 48, 1)\n",
            "iteration: 32867 train shape: (101076, 48, 48, 1)\n",
            "iteration: 32868 train shape: (101078, 48, 48, 1)\n",
            "iteration: 32869 train shape: (101080, 48, 48, 1)\n",
            "iteration: 32870 train shape: (101082, 48, 48, 1)\n",
            "iteration: 32871 train shape: (101084, 48, 48, 1)\n",
            "iteration: 32872 train shape: (101086, 48, 48, 1)\n",
            "iteration: 32873 train shape: (101088, 48, 48, 1)\n",
            "iteration: 32874 train shape: (101090, 48, 48, 1)\n",
            "iteration: 32875 train shape: (101092, 48, 48, 1)\n",
            "iteration: 32876 train shape: (101094, 48, 48, 1)\n",
            "iteration: 32877 train shape: (101096, 48, 48, 1)\n",
            "iteration: 32878 train shape: (101098, 48, 48, 1)\n",
            "iteration: 32879 train shape: (101100, 48, 48, 1)\n",
            "iteration: 32880 train shape: (101102, 48, 48, 1)\n",
            "iteration: 32881 train shape: (101104, 48, 48, 1)\n",
            "iteration: 32882 train shape: (101106, 48, 48, 1)\n",
            "iteration: 32883 train shape: (101108, 48, 48, 1)\n",
            "iteration: 32884 train shape: (101110, 48, 48, 1)\n",
            "iteration: 32885 train shape: (101112, 48, 48, 1)\n",
            "iteration: 32886 train shape: (101114, 48, 48, 1)\n",
            "iteration: 32887 train shape: (101116, 48, 48, 1)\n",
            "iteration: 32888 train shape: (101118, 48, 48, 1)\n",
            "iteration: 32889 train shape: (101120, 48, 48, 1)\n",
            "iteration: 32890 train shape: (101122, 48, 48, 1)\n",
            "iteration: 32891 train shape: (101124, 48, 48, 1)\n",
            "iteration: 32892 train shape: (101126, 48, 48, 1)\n",
            "iteration: 32893 train shape: (101128, 48, 48, 1)\n",
            "iteration: 32894 train shape: (101130, 48, 48, 1)\n",
            "iteration: 32895 train shape: (101132, 48, 48, 1)\n",
            "iteration: 32896 train shape: (101134, 48, 48, 1)\n",
            "iteration: 32897 train shape: (101136, 48, 48, 1)\n",
            "iteration: 32898 train shape: (101138, 48, 48, 1)\n",
            "iteration: 32899 train shape: (101140, 48, 48, 1)\n",
            "iteration: 32900 train shape: (101142, 48, 48, 1)\n",
            "iteration: 32901 train shape: (101144, 48, 48, 1)\n",
            "iteration: 32902 train shape: (101146, 48, 48, 1)\n",
            "iteration: 32903 train shape: (101148, 48, 48, 1)\n",
            "iteration: 32904 train shape: (101150, 48, 48, 1)\n",
            "iteration: 32905 train shape: (101152, 48, 48, 1)\n",
            "iteration: 32906 train shape: (101154, 48, 48, 1)\n",
            "iteration: 32907 train shape: (101156, 48, 48, 1)\n",
            "iteration: 32908 train shape: (101158, 48, 48, 1)\n",
            "iteration: 32909 train shape: (101160, 48, 48, 1)\n",
            "iteration: 32910 train shape: (101162, 48, 48, 1)\n",
            "iteration: 32911 train shape: (101164, 48, 48, 1)\n",
            "iteration: 32912 train shape: (101166, 48, 48, 1)\n",
            "iteration: 32913 train shape: (101168, 48, 48, 1)\n",
            "iteration: 32914 train shape: (101170, 48, 48, 1)\n",
            "iteration: 32915 train shape: (101172, 48, 48, 1)\n",
            "iteration: 32916 train shape: (101174, 48, 48, 1)\n",
            "iteration: 32917 train shape: (101176, 48, 48, 1)\n",
            "iteration: 32918 train shape: (101178, 48, 48, 1)\n",
            "iteration: 32919 train shape: (101180, 48, 48, 1)\n",
            "iteration: 32920 train shape: (101182, 48, 48, 1)\n",
            "iteration: 32921 train shape: (101184, 48, 48, 1)\n",
            "iteration: 32922 train shape: (101186, 48, 48, 1)\n",
            "iteration: 32923 train shape: (101188, 48, 48, 1)\n",
            "iteration: 32924 train shape: (101190, 48, 48, 1)\n",
            "iteration: 32925 train shape: (101192, 48, 48, 1)\n",
            "iteration: 32926 train shape: (101194, 48, 48, 1)\n",
            "iteration: 32927 train shape: (101196, 48, 48, 1)\n",
            "iteration: 32928 train shape: (101198, 48, 48, 1)\n",
            "iteration: 32929 train shape: (101200, 48, 48, 1)\n",
            "iteration: 32930 train shape: (101202, 48, 48, 1)\n",
            "iteration: 32931 train shape: (101204, 48, 48, 1)\n",
            "iteration: 32932 train shape: (101206, 48, 48, 1)\n",
            "iteration: 32933 train shape: (101208, 48, 48, 1)\n",
            "iteration: 32934 train shape: (101210, 48, 48, 1)\n",
            "iteration: 32935 train shape: (101212, 48, 48, 1)\n",
            "iteration: 32936 train shape: (101214, 48, 48, 1)\n",
            "iteration: 32937 train shape: (101216, 48, 48, 1)\n",
            "iteration: 32938 train shape: (101218, 48, 48, 1)\n",
            "iteration: 32939 train shape: (101220, 48, 48, 1)\n",
            "iteration: 32940 train shape: (101222, 48, 48, 1)\n",
            "iteration: 32941 train shape: (101224, 48, 48, 1)\n",
            "iteration: 32942 train shape: (101226, 48, 48, 1)\n",
            "iteration: 32943 train shape: (101228, 48, 48, 1)\n",
            "iteration: 32944 train shape: (101230, 48, 48, 1)\n",
            "iteration: 32945 train shape: (101232, 48, 48, 1)\n",
            "iteration: 32946 train shape: (101234, 48, 48, 1)\n",
            "iteration: 32947 train shape: (101236, 48, 48, 1)\n",
            "iteration: 32948 train shape: (101238, 48, 48, 1)\n",
            "iteration: 32949 train shape: (101240, 48, 48, 1)\n",
            "iteration: 32950 train shape: (101242, 48, 48, 1)\n",
            "iteration: 32951 train shape: (101244, 48, 48, 1)\n",
            "iteration: 32952 train shape: (101246, 48, 48, 1)\n",
            "iteration: 32953 train shape: (101248, 48, 48, 1)\n",
            "iteration: 32954 train shape: (101250, 48, 48, 1)\n",
            "iteration: 32955 train shape: (101252, 48, 48, 1)\n",
            "iteration: 32956 train shape: (101254, 48, 48, 1)\n",
            "iteration: 32957 train shape: (101256, 48, 48, 1)\n",
            "iteration: 32958 train shape: (101258, 48, 48, 1)\n",
            "iteration: 32959 train shape: (101260, 48, 48, 1)\n",
            "iteration: 32960 train shape: (101262, 48, 48, 1)\n",
            "iteration: 32961 train shape: (101264, 48, 48, 1)\n",
            "iteration: 32962 train shape: (101266, 48, 48, 1)\n",
            "iteration: 32963 train shape: (101268, 48, 48, 1)\n",
            "iteration: 32964 train shape: (101270, 48, 48, 1)\n",
            "iteration: 32965 train shape: (101272, 48, 48, 1)\n",
            "iteration: 32966 train shape: (101274, 48, 48, 1)\n",
            "iteration: 32967 train shape: (101276, 48, 48, 1)\n",
            "iteration: 32968 train shape: (101278, 48, 48, 1)\n",
            "iteration: 32969 train shape: (101280, 48, 48, 1)\n",
            "iteration: 32970 train shape: (101282, 48, 48, 1)\n",
            "iteration: 32971 train shape: (101284, 48, 48, 1)\n",
            "iteration: 32972 train shape: (101286, 48, 48, 1)\n",
            "iteration: 32973 train shape: (101288, 48, 48, 1)\n",
            "iteration: 32974 train shape: (101290, 48, 48, 1)\n",
            "iteration: 32975 train shape: (101292, 48, 48, 1)\n",
            "iteration: 32976 train shape: (101294, 48, 48, 1)\n",
            "iteration: 32977 train shape: (101296, 48, 48, 1)\n",
            "iteration: 32978 train shape: (101298, 48, 48, 1)\n",
            "iteration: 32979 train shape: (101300, 48, 48, 1)\n",
            "iteration: 32980 train shape: (101302, 48, 48, 1)\n",
            "iteration: 32981 train shape: (101304, 48, 48, 1)\n",
            "iteration: 32982 train shape: (101306, 48, 48, 1)\n",
            "iteration: 32983 train shape: (101308, 48, 48, 1)\n",
            "iteration: 32984 train shape: (101310, 48, 48, 1)\n",
            "iteration: 32985 train shape: (101312, 48, 48, 1)\n",
            "iteration: 32986 train shape: (101314, 48, 48, 1)\n",
            "iteration: 32987 train shape: (101316, 48, 48, 1)\n",
            "iteration: 32988 train shape: (101318, 48, 48, 1)\n",
            "iteration: 32989 train shape: (101320, 48, 48, 1)\n",
            "iteration: 32990 train shape: (101322, 48, 48, 1)\n",
            "iteration: 32991 train shape: (101324, 48, 48, 1)\n",
            "iteration: 32992 train shape: (101326, 48, 48, 1)\n",
            "iteration: 32993 train shape: (101328, 48, 48, 1)\n",
            "iteration: 32994 train shape: (101330, 48, 48, 1)\n",
            "iteration: 32995 train shape: (101332, 48, 48, 1)\n",
            "iteration: 32996 train shape: (101334, 48, 48, 1)\n",
            "iteration: 32997 train shape: (101336, 48, 48, 1)\n",
            "iteration: 32998 train shape: (101338, 48, 48, 1)\n",
            "iteration: 32999 train shape: (101340, 48, 48, 1)\n",
            "iteration: 33000 train shape: (101342, 48, 48, 1)\n",
            "iteration: 33001 train shape: (101344, 48, 48, 1)\n",
            "iteration: 33002 train shape: (101346, 48, 48, 1)\n",
            "iteration: 33003 train shape: (101348, 48, 48, 1)\n",
            "iteration: 33004 train shape: (101350, 48, 48, 1)\n",
            "iteration: 33005 train shape: (101352, 48, 48, 1)\n",
            "iteration: 33006 train shape: (101354, 48, 48, 1)\n",
            "iteration: 33007 train shape: (101356, 48, 48, 1)\n",
            "iteration: 33008 train shape: (101358, 48, 48, 1)\n",
            "iteration: 33009 train shape: (101360, 48, 48, 1)\n",
            "iteration: 33010 train shape: (101362, 48, 48, 1)\n",
            "iteration: 33011 train shape: (101364, 48, 48, 1)\n",
            "iteration: 33012 train shape: (101366, 48, 48, 1)\n",
            "iteration: 33013 train shape: (101368, 48, 48, 1)\n",
            "iteration: 33014 train shape: (101370, 48, 48, 1)\n",
            "iteration: 33015 train shape: (101372, 48, 48, 1)\n",
            "iteration: 33016 train shape: (101374, 48, 48, 1)\n",
            "iteration: 33017 train shape: (101376, 48, 48, 1)\n",
            "iteration: 33018 train shape: (101378, 48, 48, 1)\n",
            "iteration: 33019 train shape: (101380, 48, 48, 1)\n",
            "iteration: 33020 train shape: (101382, 48, 48, 1)\n",
            "iteration: 33021 train shape: (101384, 48, 48, 1)\n",
            "iteration: 33022 train shape: (101386, 48, 48, 1)\n",
            "iteration: 33023 train shape: (101388, 48, 48, 1)\n",
            "iteration: 33024 train shape: (101390, 48, 48, 1)\n",
            "iteration: 33025 train shape: (101392, 48, 48, 1)\n",
            "iteration: 33026 train shape: (101394, 48, 48, 1)\n",
            "iteration: 33027 train shape: (101396, 48, 48, 1)\n",
            "iteration: 33028 train shape: (101398, 48, 48, 1)\n",
            "iteration: 33029 train shape: (101400, 48, 48, 1)\n",
            "iteration: 33030 train shape: (101402, 48, 48, 1)\n",
            "iteration: 33031 train shape: (101404, 48, 48, 1)\n",
            "iteration: 33032 train shape: (101406, 48, 48, 1)\n",
            "iteration: 33033 train shape: (101408, 48, 48, 1)\n",
            "iteration: 33034 train shape: (101410, 48, 48, 1)\n",
            "iteration: 33035 train shape: (101412, 48, 48, 1)\n",
            "iteration: 33036 train shape: (101414, 48, 48, 1)\n",
            "iteration: 33037 train shape: (101416, 48, 48, 1)\n",
            "iteration: 33038 train shape: (101418, 48, 48, 1)\n",
            "iteration: 33039 train shape: (101420, 48, 48, 1)\n",
            "iteration: 33040 train shape: (101422, 48, 48, 1)\n",
            "iteration: 33041 train shape: (101424, 48, 48, 1)\n",
            "iteration: 33042 train shape: (101426, 48, 48, 1)\n",
            "iteration: 33043 train shape: (101428, 48, 48, 1)\n",
            "iteration: 33044 train shape: (101430, 48, 48, 1)\n",
            "iteration: 33045 train shape: (101432, 48, 48, 1)\n",
            "iteration: 33046 train shape: (101434, 48, 48, 1)\n",
            "iteration: 33047 train shape: (101436, 48, 48, 1)\n",
            "iteration: 33048 train shape: (101438, 48, 48, 1)\n",
            "iteration: 33049 train shape: (101440, 48, 48, 1)\n",
            "iteration: 33050 train shape: (101442, 48, 48, 1)\n",
            "iteration: 33051 train shape: (101444, 48, 48, 1)\n",
            "iteration: 33052 train shape: (101446, 48, 48, 1)\n",
            "iteration: 33053 train shape: (101448, 48, 48, 1)\n",
            "iteration: 33054 train shape: (101450, 48, 48, 1)\n",
            "iteration: 33055 train shape: (101452, 48, 48, 1)\n",
            "iteration: 33056 train shape: (101454, 48, 48, 1)\n",
            "iteration: 33057 train shape: (101456, 48, 48, 1)\n",
            "iteration: 33058 train shape: (101458, 48, 48, 1)\n",
            "iteration: 33059 train shape: (101460, 48, 48, 1)\n",
            "iteration: 33060 train shape: (101462, 48, 48, 1)\n",
            "iteration: 33061 train shape: (101464, 48, 48, 1)\n",
            "iteration: 33062 train shape: (101466, 48, 48, 1)\n",
            "iteration: 33063 train shape: (101468, 48, 48, 1)\n",
            "iteration: 33064 train shape: (101470, 48, 48, 1)\n",
            "iteration: 33065 train shape: (101472, 48, 48, 1)\n",
            "iteration: 33066 train shape: (101474, 48, 48, 1)\n",
            "iteration: 33067 train shape: (101476, 48, 48, 1)\n",
            "iteration: 33068 train shape: (101478, 48, 48, 1)\n",
            "iteration: 33069 train shape: (101480, 48, 48, 1)\n",
            "iteration: 33070 train shape: (101482, 48, 48, 1)\n",
            "iteration: 33071 train shape: (101484, 48, 48, 1)\n",
            "iteration: 33072 train shape: (101486, 48, 48, 1)\n",
            "iteration: 33073 train shape: (101488, 48, 48, 1)\n",
            "iteration: 33074 train shape: (101490, 48, 48, 1)\n",
            "iteration: 33075 train shape: (101492, 48, 48, 1)\n",
            "iteration: 33076 train shape: (101494, 48, 48, 1)\n",
            "iteration: 33077 train shape: (101496, 48, 48, 1)\n",
            "iteration: 33078 train shape: (101498, 48, 48, 1)\n",
            "iteration: 33079 train shape: (101500, 48, 48, 1)\n",
            "iteration: 33080 train shape: (101502, 48, 48, 1)\n",
            "iteration: 33081 train shape: (101504, 48, 48, 1)\n",
            "iteration: 33082 train shape: (101506, 48, 48, 1)\n",
            "iteration: 33083 train shape: (101508, 48, 48, 1)\n",
            "iteration: 33084 train shape: (101510, 48, 48, 1)\n",
            "iteration: 33085 train shape: (101512, 48, 48, 1)\n",
            "iteration: 33086 train shape: (101514, 48, 48, 1)\n",
            "iteration: 33087 train shape: (101516, 48, 48, 1)\n",
            "iteration: 33088 train shape: (101518, 48, 48, 1)\n",
            "iteration: 33089 train shape: (101520, 48, 48, 1)\n",
            "iteration: 33090 train shape: (101522, 48, 48, 1)\n",
            "iteration: 33091 train shape: (101524, 48, 48, 1)\n",
            "iteration: 33092 train shape: (101526, 48, 48, 1)\n",
            "iteration: 33093 train shape: (101528, 48, 48, 1)\n",
            "iteration: 33094 train shape: (101530, 48, 48, 1)\n",
            "iteration: 33095 train shape: (101532, 48, 48, 1)\n",
            "iteration: 33096 train shape: (101534, 48, 48, 1)\n",
            "iteration: 33097 train shape: (101536, 48, 48, 1)\n",
            "iteration: 33098 train shape: (101538, 48, 48, 1)\n",
            "iteration: 33099 train shape: (101540, 48, 48, 1)\n",
            "iteration: 33100 train shape: (101542, 48, 48, 1)\n",
            "iteration: 33101 train shape: (101544, 48, 48, 1)\n",
            "iteration: 33102 train shape: (101546, 48, 48, 1)\n",
            "iteration: 33103 train shape: (101548, 48, 48, 1)\n",
            "iteration: 33104 train shape: (101550, 48, 48, 1)\n",
            "iteration: 33105 train shape: (101552, 48, 48, 1)\n",
            "iteration: 33106 train shape: (101554, 48, 48, 1)\n",
            "iteration: 33107 train shape: (101556, 48, 48, 1)\n",
            "iteration: 33108 train shape: (101558, 48, 48, 1)\n",
            "iteration: 33109 train shape: (101560, 48, 48, 1)\n",
            "iteration: 33110 train shape: (101562, 48, 48, 1)\n",
            "iteration: 33111 train shape: (101564, 48, 48, 1)\n",
            "iteration: 33112 train shape: (101566, 48, 48, 1)\n",
            "iteration: 33113 train shape: (101568, 48, 48, 1)\n",
            "iteration: 33114 train shape: (101570, 48, 48, 1)\n",
            "iteration: 33115 train shape: (101572, 48, 48, 1)\n",
            "iteration: 33116 train shape: (101574, 48, 48, 1)\n",
            "iteration: 33117 train shape: (101576, 48, 48, 1)\n",
            "iteration: 33118 train shape: (101578, 48, 48, 1)\n",
            "iteration: 33119 train shape: (101580, 48, 48, 1)\n",
            "iteration: 33120 train shape: (101582, 48, 48, 1)\n",
            "iteration: 33121 train shape: (101584, 48, 48, 1)\n",
            "iteration: 33122 train shape: (101586, 48, 48, 1)\n",
            "iteration: 33123 train shape: (101588, 48, 48, 1)\n",
            "iteration: 33124 train shape: (101590, 48, 48, 1)\n",
            "iteration: 33125 train shape: (101592, 48, 48, 1)\n",
            "iteration: 33126 train shape: (101594, 48, 48, 1)\n",
            "iteration: 33127 train shape: (101596, 48, 48, 1)\n",
            "iteration: 33128 train shape: (101598, 48, 48, 1)\n",
            "iteration: 33129 train shape: (101600, 48, 48, 1)\n",
            "iteration: 33130 train shape: (101602, 48, 48, 1)\n",
            "iteration: 33131 train shape: (101604, 48, 48, 1)\n",
            "iteration: 33132 train shape: (101606, 48, 48, 1)\n",
            "iteration: 33133 train shape: (101608, 48, 48, 1)\n",
            "iteration: 33134 train shape: (101610, 48, 48, 1)\n",
            "iteration: 33135 train shape: (101612, 48, 48, 1)\n",
            "iteration: 33136 train shape: (101614, 48, 48, 1)\n",
            "iteration: 33137 train shape: (101616, 48, 48, 1)\n",
            "iteration: 33138 train shape: (101618, 48, 48, 1)\n",
            "iteration: 33139 train shape: (101620, 48, 48, 1)\n",
            "iteration: 33140 train shape: (101622, 48, 48, 1)\n",
            "iteration: 33141 train shape: (101624, 48, 48, 1)\n",
            "iteration: 33142 train shape: (101626, 48, 48, 1)\n",
            "iteration: 33143 train shape: (101628, 48, 48, 1)\n",
            "iteration: 33144 train shape: (101630, 48, 48, 1)\n",
            "iteration: 33145 train shape: (101632, 48, 48, 1)\n",
            "iteration: 33146 train shape: (101634, 48, 48, 1)\n",
            "iteration: 33147 train shape: (101636, 48, 48, 1)\n",
            "iteration: 33148 train shape: (101638, 48, 48, 1)\n",
            "iteration: 33149 train shape: (101640, 48, 48, 1)\n",
            "iteration: 33150 train shape: (101642, 48, 48, 1)\n",
            "iteration: 33151 train shape: (101644, 48, 48, 1)\n",
            "iteration: 33152 train shape: (101646, 48, 48, 1)\n",
            "iteration: 33153 train shape: (101648, 48, 48, 1)\n",
            "iteration: 33154 train shape: (101650, 48, 48, 1)\n",
            "iteration: 33155 train shape: (101652, 48, 48, 1)\n",
            "iteration: 33156 train shape: (101654, 48, 48, 1)\n",
            "iteration: 33157 train shape: (101656, 48, 48, 1)\n",
            "iteration: 33158 train shape: (101658, 48, 48, 1)\n",
            "iteration: 33159 train shape: (101660, 48, 48, 1)\n",
            "iteration: 33160 train shape: (101662, 48, 48, 1)\n",
            "iteration: 33161 train shape: (101664, 48, 48, 1)\n",
            "iteration: 33162 train shape: (101666, 48, 48, 1)\n",
            "iteration: 33163 train shape: (101668, 48, 48, 1)\n",
            "iteration: 33164 train shape: (101670, 48, 48, 1)\n",
            "iteration: 33165 train shape: (101672, 48, 48, 1)\n",
            "iteration: 33166 train shape: (101674, 48, 48, 1)\n",
            "iteration: 33167 train shape: (101676, 48, 48, 1)\n",
            "iteration: 33168 train shape: (101678, 48, 48, 1)\n",
            "iteration: 33169 train shape: (101680, 48, 48, 1)\n",
            "iteration: 33170 train shape: (101682, 48, 48, 1)\n",
            "iteration: 33171 train shape: (101684, 48, 48, 1)\n",
            "iteration: 33172 train shape: (101686, 48, 48, 1)\n",
            "iteration: 33173 train shape: (101688, 48, 48, 1)\n",
            "iteration: 33174 train shape: (101690, 48, 48, 1)\n",
            "iteration: 33175 train shape: (101692, 48, 48, 1)\n",
            "iteration: 33176 train shape: (101694, 48, 48, 1)\n",
            "iteration: 33177 train shape: (101696, 48, 48, 1)\n",
            "iteration: 33178 train shape: (101698, 48, 48, 1)\n",
            "iteration: 33179 train shape: (101700, 48, 48, 1)\n",
            "iteration: 33180 train shape: (101702, 48, 48, 1)\n",
            "iteration: 33181 train shape: (101704, 48, 48, 1)\n",
            "iteration: 33182 train shape: (101706, 48, 48, 1)\n",
            "iteration: 33183 train shape: (101708, 48, 48, 1)\n",
            "iteration: 33184 train shape: (101710, 48, 48, 1)\n",
            "iteration: 33185 train shape: (101712, 48, 48, 1)\n",
            "iteration: 33186 train shape: (101714, 48, 48, 1)\n",
            "iteration: 33187 train shape: (101716, 48, 48, 1)\n",
            "iteration: 33188 train shape: (101718, 48, 48, 1)\n",
            "iteration: 33189 train shape: (101720, 48, 48, 1)\n",
            "iteration: 33190 train shape: (101722, 48, 48, 1)\n",
            "iteration: 33191 train shape: (101724, 48, 48, 1)\n",
            "iteration: 33192 train shape: (101726, 48, 48, 1)\n",
            "iteration: 33193 train shape: (101728, 48, 48, 1)\n",
            "iteration: 33194 train shape: (101730, 48, 48, 1)\n",
            "iteration: 33195 train shape: (101732, 48, 48, 1)\n",
            "iteration: 33196 train shape: (101734, 48, 48, 1)\n",
            "iteration: 33197 train shape: (101736, 48, 48, 1)\n",
            "iteration: 33198 train shape: (101738, 48, 48, 1)\n",
            "iteration: 33199 train shape: (101740, 48, 48, 1)\n",
            "iteration: 33200 train shape: (101742, 48, 48, 1)\n",
            "iteration: 33201 train shape: (101744, 48, 48, 1)\n",
            "iteration: 33202 train shape: (101746, 48, 48, 1)\n",
            "iteration: 33203 train shape: (101748, 48, 48, 1)\n",
            "iteration: 33204 train shape: (101750, 48, 48, 1)\n",
            "iteration: 33208 train shape: (101758, 48, 48, 1)\n",
            "iteration: 33209 train shape: (101760, 48, 48, 1)\n",
            "iteration: 33210 train shape: (101762, 48, 48, 1)\n",
            "iteration: 33211 train shape: (101764, 48, 48, 1)\n",
            "iteration: 33212 train shape: (101766, 48, 48, 1)\n",
            "iteration: 33213 train shape: (101768, 48, 48, 1)\n",
            "iteration: 33214 train shape: (101770, 48, 48, 1)\n",
            "iteration: 33215 train shape: (101772, 48, 48, 1)\n",
            "iteration: 33216 train shape: (101774, 48, 48, 1)\n",
            "iteration: 33217 train shape: (101776, 48, 48, 1)\n",
            "iteration: 33218 train shape: (101778, 48, 48, 1)\n",
            "iteration: 33219 train shape: (101780, 48, 48, 1)\n",
            "iteration: 33220 train shape: (101782, 48, 48, 1)\n",
            "iteration: 33221 train shape: (101784, 48, 48, 1)\n",
            "iteration: 33222 train shape: (101786, 48, 48, 1)\n",
            "iteration: 33223 train shape: (101788, 48, 48, 1)\n",
            "iteration: 33224 train shape: (101790, 48, 48, 1)\n",
            "iteration: 33225 train shape: (101792, 48, 48, 1)\n",
            "iteration: 33226 train shape: (101794, 48, 48, 1)\n",
            "iteration: 33227 train shape: (101796, 48, 48, 1)\n",
            "iteration: 33228 train shape: (101798, 48, 48, 1)\n",
            "iteration: 33229 train shape: (101800, 48, 48, 1)\n",
            "iteration: 33230 train shape: (101802, 48, 48, 1)\n",
            "iteration: 33231 train shape: (101804, 48, 48, 1)\n",
            "iteration: 33232 train shape: (101806, 48, 48, 1)\n",
            "iteration: 33233 train shape: (101808, 48, 48, 1)\n",
            "iteration: 33234 train shape: (101810, 48, 48, 1)\n",
            "iteration: 33235 train shape: (101812, 48, 48, 1)\n",
            "iteration: 33236 train shape: (101814, 48, 48, 1)\n",
            "iteration: 33237 train shape: (101816, 48, 48, 1)\n",
            "iteration: 33238 train shape: (101818, 48, 48, 1)\n",
            "iteration: 33239 train shape: (101820, 48, 48, 1)\n",
            "iteration: 33240 train shape: (101822, 48, 48, 1)\n",
            "iteration: 33241 train shape: (101824, 48, 48, 1)\n",
            "iteration: 33242 train shape: (101826, 48, 48, 1)\n",
            "iteration: 33243 train shape: (101828, 48, 48, 1)\n",
            "iteration: 33244 train shape: (101830, 48, 48, 1)\n",
            "iteration: 33245 train shape: (101832, 48, 48, 1)\n",
            "iteration: 33246 train shape: (101834, 48, 48, 1)\n",
            "iteration: 33247 train shape: (101836, 48, 48, 1)\n",
            "iteration: 33248 train shape: (101838, 48, 48, 1)\n",
            "iteration: 33249 train shape: (101840, 48, 48, 1)\n",
            "iteration: 33250 train shape: (101842, 48, 48, 1)\n",
            "iteration: 33251 train shape: (101844, 48, 48, 1)\n",
            "iteration: 33252 train shape: (101846, 48, 48, 1)\n",
            "iteration: 33253 train shape: (101848, 48, 48, 1)\n",
            "iteration: 33254 train shape: (101850, 48, 48, 1)\n",
            "iteration: 33255 train shape: (101852, 48, 48, 1)\n",
            "iteration: 33256 train shape: (101854, 48, 48, 1)\n",
            "iteration: 33257 train shape: (101856, 48, 48, 1)\n",
            "iteration: 33258 train shape: (101858, 48, 48, 1)\n",
            "iteration: 33259 train shape: (101860, 48, 48, 1)\n",
            "iteration: 33260 train shape: (101862, 48, 48, 1)\n",
            "iteration: 33261 train shape: (101864, 48, 48, 1)\n",
            "iteration: 33262 train shape: (101866, 48, 48, 1)\n",
            "iteration: 33263 train shape: (101868, 48, 48, 1)\n",
            "iteration: 33264 train shape: (101870, 48, 48, 1)\n",
            "iteration: 33265 train shape: (101872, 48, 48, 1)\n",
            "iteration: 33266 train shape: (101874, 48, 48, 1)\n",
            "iteration: 33267 train shape: (101876, 48, 48, 1)\n",
            "iteration: 33268 train shape: (101878, 48, 48, 1)\n",
            "iteration: 33269 train shape: (101880, 48, 48, 1)\n",
            "iteration: 33270 train shape: (101882, 48, 48, 1)\n",
            "iteration: 33271 train shape: (101884, 48, 48, 1)\n",
            "iteration: 33272 train shape: (101886, 48, 48, 1)\n",
            "iteration: 33273 train shape: (101888, 48, 48, 1)\n",
            "iteration: 33274 train shape: (101890, 48, 48, 1)\n",
            "iteration: 33275 train shape: (101892, 48, 48, 1)\n",
            "iteration: 33276 train shape: (101894, 48, 48, 1)\n",
            "iteration: 33277 train shape: (101896, 48, 48, 1)\n",
            "iteration: 33278 train shape: (101898, 48, 48, 1)\n",
            "iteration: 33279 train shape: (101900, 48, 48, 1)\n",
            "iteration: 33280 train shape: (101902, 48, 48, 1)\n",
            "iteration: 33281 train shape: (101904, 48, 48, 1)\n",
            "iteration: 33282 train shape: (101906, 48, 48, 1)\n",
            "iteration: 33283 train shape: (101908, 48, 48, 1)\n",
            "iteration: 33284 train shape: (101910, 48, 48, 1)\n",
            "iteration: 33285 train shape: (101912, 48, 48, 1)\n",
            "iteration: 33286 train shape: (101914, 48, 48, 1)\n",
            "iteration: 33287 train shape: (101916, 48, 48, 1)\n",
            "iteration: 33288 train shape: (101918, 48, 48, 1)\n",
            "iteration: 33289 train shape: (101920, 48, 48, 1)\n",
            "iteration: 33290 train shape: (101922, 48, 48, 1)\n",
            "iteration: 33291 train shape: (101924, 48, 48, 1)\n",
            "iteration: 33292 train shape: (101926, 48, 48, 1)\n",
            "iteration: 33293 train shape: (101928, 48, 48, 1)\n",
            "iteration: 33294 train shape: (101930, 48, 48, 1)\n",
            "iteration: 33295 train shape: (101932, 48, 48, 1)\n",
            "iteration: 33296 train shape: (101934, 48, 48, 1)\n",
            "iteration: 33297 train shape: (101936, 48, 48, 1)\n",
            "iteration: 33298 train shape: (101938, 48, 48, 1)\n",
            "iteration: 33299 train shape: (101940, 48, 48, 1)\n",
            "iteration: 33300 train shape: (101942, 48, 48, 1)\n",
            "iteration: 33301 train shape: (101944, 48, 48, 1)\n",
            "iteration: 33302 train shape: (101946, 48, 48, 1)\n",
            "iteration: 33303 train shape: (101948, 48, 48, 1)\n",
            "iteration: 33304 train shape: (101950, 48, 48, 1)\n",
            "iteration: 33305 train shape: (101952, 48, 48, 1)\n",
            "iteration: 33306 train shape: (101954, 48, 48, 1)\n",
            "iteration: 33307 train shape: (101956, 48, 48, 1)\n",
            "iteration: 33308 train shape: (101958, 48, 48, 1)\n",
            "iteration: 33309 train shape: (101960, 48, 48, 1)\n",
            "iteration: 33310 train shape: (101962, 48, 48, 1)\n",
            "iteration: 33311 train shape: (101964, 48, 48, 1)\n",
            "iteration: 33312 train shape: (101966, 48, 48, 1)\n",
            "iteration: 33313 train shape: (101968, 48, 48, 1)\n",
            "iteration: 33314 train shape: (101970, 48, 48, 1)\n",
            "iteration: 33315 train shape: (101972, 48, 48, 1)\n",
            "iteration: 33316 train shape: (101974, 48, 48, 1)\n",
            "iteration: 33317 train shape: (101976, 48, 48, 1)\n",
            "iteration: 33318 train shape: (101978, 48, 48, 1)\n",
            "iteration: 33319 train shape: (101980, 48, 48, 1)\n",
            "iteration: 33320 train shape: (101982, 48, 48, 1)\n",
            "iteration: 33321 train shape: (101984, 48, 48, 1)\n",
            "iteration: 33322 train shape: (101986, 48, 48, 1)\n",
            "iteration: 33323 train shape: (101988, 48, 48, 1)\n",
            "iteration: 33324 train shape: (101990, 48, 48, 1)\n",
            "iteration: 33325 train shape: (101992, 48, 48, 1)\n",
            "iteration: 33326 train shape: (101994, 48, 48, 1)\n",
            "iteration: 33327 train shape: (101996, 48, 48, 1)\n",
            "iteration: 33328 train shape: (101998, 48, 48, 1)\n",
            "iteration: 33329 train shape: (102000, 48, 48, 1)\n",
            "iteration: 33330 train shape: (102002, 48, 48, 1)\n",
            "iteration: 33331 train shape: (102004, 48, 48, 1)\n",
            "iteration: 33332 train shape: (102006, 48, 48, 1)\n",
            "iteration: 33333 train shape: (102008, 48, 48, 1)\n",
            "iteration: 33334 train shape: (102010, 48, 48, 1)\n",
            "iteration: 33335 train shape: (102012, 48, 48, 1)\n",
            "iteration: 33336 train shape: (102014, 48, 48, 1)\n",
            "iteration: 33337 train shape: (102016, 48, 48, 1)\n",
            "iteration: 33338 train shape: (102018, 48, 48, 1)\n",
            "iteration: 33339 train shape: (102020, 48, 48, 1)\n",
            "iteration: 33340 train shape: (102022, 48, 48, 1)\n",
            "iteration: 33341 train shape: (102024, 48, 48, 1)\n",
            "iteration: 33342 train shape: (102026, 48, 48, 1)\n",
            "iteration: 33343 train shape: (102028, 48, 48, 1)\n",
            "iteration: 33344 train shape: (102030, 48, 48, 1)\n",
            "iteration: 33345 train shape: (102032, 48, 48, 1)\n",
            "iteration: 33346 train shape: (102034, 48, 48, 1)\n",
            "iteration: 33347 train shape: (102036, 48, 48, 1)\n",
            "iteration: 33348 train shape: (102038, 48, 48, 1)\n",
            "iteration: 33349 train shape: (102040, 48, 48, 1)\n",
            "iteration: 33350 train shape: (102042, 48, 48, 1)\n",
            "iteration: 33351 train shape: (102044, 48, 48, 1)\n",
            "iteration: 33352 train shape: (102046, 48, 48, 1)\n",
            "iteration: 33353 train shape: (102048, 48, 48, 1)\n",
            "iteration: 33354 train shape: (102050, 48, 48, 1)\n",
            "iteration: 33355 train shape: (102052, 48, 48, 1)\n",
            "iteration: 33356 train shape: (102054, 48, 48, 1)\n",
            "iteration: 33357 train shape: (102056, 48, 48, 1)\n",
            "iteration: 33358 train shape: (102058, 48, 48, 1)\n",
            "iteration: 33359 train shape: (102060, 48, 48, 1)\n",
            "iteration: 33360 train shape: (102062, 48, 48, 1)\n",
            "iteration: 33361 train shape: (102064, 48, 48, 1)\n",
            "iteration: 33362 train shape: (102066, 48, 48, 1)\n",
            "iteration: 33363 train shape: (102068, 48, 48, 1)\n",
            "iteration: 33364 train shape: (102070, 48, 48, 1)\n",
            "iteration: 33365 train shape: (102072, 48, 48, 1)\n",
            "iteration: 33366 train shape: (102074, 48, 48, 1)\n",
            "iteration: 33367 train shape: (102076, 48, 48, 1)\n",
            "iteration: 33368 train shape: (102078, 48, 48, 1)\n",
            "iteration: 33369 train shape: (102080, 48, 48, 1)\n",
            "iteration: 33370 train shape: (102082, 48, 48, 1)\n",
            "iteration: 33371 train shape: (102084, 48, 48, 1)\n",
            "iteration: 33372 train shape: (102086, 48, 48, 1)\n",
            "iteration: 33373 train shape: (102088, 48, 48, 1)\n",
            "iteration: 33374 train shape: (102090, 48, 48, 1)\n",
            "iteration: 33375 train shape: (102092, 48, 48, 1)\n",
            "iteration: 33376 train shape: (102094, 48, 48, 1)\n",
            "iteration: 33377 train shape: (102096, 48, 48, 1)\n",
            "iteration: 33378 train shape: (102098, 48, 48, 1)\n",
            "iteration: 33379 train shape: (102100, 48, 48, 1)\n",
            "iteration: 33380 train shape: (102102, 48, 48, 1)\n",
            "iteration: 33381 train shape: (102104, 48, 48, 1)\n",
            "iteration: 33382 train shape: (102106, 48, 48, 1)\n",
            "iteration: 33383 train shape: (102108, 48, 48, 1)\n",
            "iteration: 33384 train shape: (102110, 48, 48, 1)\n",
            "iteration: 33385 train shape: (102112, 48, 48, 1)\n",
            "iteration: 33386 train shape: (102114, 48, 48, 1)\n",
            "iteration: 33387 train shape: (102116, 48, 48, 1)\n",
            "iteration: 33388 train shape: (102118, 48, 48, 1)\n",
            "iteration: 33389 train shape: (102120, 48, 48, 1)\n",
            "iteration: 33390 train shape: (102122, 48, 48, 1)\n",
            "iteration: 33391 train shape: (102124, 48, 48, 1)\n",
            "iteration: 33392 train shape: (102126, 48, 48, 1)\n",
            "iteration: 33393 train shape: (102128, 48, 48, 1)\n",
            "iteration: 33394 train shape: (102130, 48, 48, 1)\n",
            "iteration: 33395 train shape: (102132, 48, 48, 1)\n",
            "iteration: 33396 train shape: (102134, 48, 48, 1)\n",
            "iteration: 33397 train shape: (102136, 48, 48, 1)\n",
            "iteration: 33398 train shape: (102138, 48, 48, 1)\n",
            "iteration: 33399 train shape: (102140, 48, 48, 1)\n",
            "iteration: 33400 train shape: (102142, 48, 48, 1)\n",
            "iteration: 33401 train shape: (102144, 48, 48, 1)\n",
            "iteration: 33402 train shape: (102146, 48, 48, 1)\n",
            "iteration: 33403 train shape: (102148, 48, 48, 1)\n",
            "iteration: 33404 train shape: (102150, 48, 48, 1)\n",
            "iteration: 33405 train shape: (102152, 48, 48, 1)\n",
            "iteration: 33406 train shape: (102154, 48, 48, 1)\n",
            "iteration: 33407 train shape: (102156, 48, 48, 1)\n",
            "iteration: 33408 train shape: (102158, 48, 48, 1)\n",
            "iteration: 33409 train shape: (102160, 48, 48, 1)\n",
            "iteration: 33410 train shape: (102162, 48, 48, 1)\n",
            "iteration: 33411 train shape: (102164, 48, 48, 1)\n",
            "iteration: 33412 train shape: (102166, 48, 48, 1)\n",
            "iteration: 33413 train shape: (102168, 48, 48, 1)\n",
            "iteration: 33414 train shape: (102170, 48, 48, 1)\n",
            "iteration: 33415 train shape: (102172, 48, 48, 1)\n",
            "iteration: 33416 train shape: (102174, 48, 48, 1)\n",
            "iteration: 33417 train shape: (102176, 48, 48, 1)\n",
            "iteration: 33418 train shape: (102178, 48, 48, 1)\n",
            "iteration: 33419 train shape: (102180, 48, 48, 1)\n",
            "iteration: 33420 train shape: (102182, 48, 48, 1)\n",
            "iteration: 33421 train shape: (102184, 48, 48, 1)\n",
            "iteration: 33422 train shape: (102186, 48, 48, 1)\n",
            "iteration: 33423 train shape: (102188, 48, 48, 1)\n",
            "iteration: 33424 train shape: (102190, 48, 48, 1)\n",
            "iteration: 33425 train shape: (102192, 48, 48, 1)\n",
            "iteration: 33426 train shape: (102194, 48, 48, 1)\n",
            "iteration: 33427 train shape: (102196, 48, 48, 1)\n",
            "iteration: 33428 train shape: (102198, 48, 48, 1)\n",
            "iteration: 33429 train shape: (102200, 48, 48, 1)\n",
            "iteration: 33430 train shape: (102202, 48, 48, 1)\n",
            "iteration: 33431 train shape: (102204, 48, 48, 1)\n",
            "iteration: 33432 train shape: (102206, 48, 48, 1)\n",
            "iteration: 33433 train shape: (102208, 48, 48, 1)\n",
            "iteration: 33434 train shape: (102210, 48, 48, 1)\n",
            "iteration: 33435 train shape: (102212, 48, 48, 1)\n",
            "iteration: 33436 train shape: (102214, 48, 48, 1)\n",
            "iteration: 33437 train shape: (102216, 48, 48, 1)\n",
            "iteration: 33438 train shape: (102218, 48, 48, 1)\n",
            "iteration: 33439 train shape: (102220, 48, 48, 1)\n",
            "iteration: 33440 train shape: (102222, 48, 48, 1)\n",
            "iteration: 33441 train shape: (102224, 48, 48, 1)\n",
            "iteration: 33442 train shape: (102226, 48, 48, 1)\n",
            "iteration: 33443 train shape: (102228, 48, 48, 1)\n",
            "iteration: 33444 train shape: (102230, 48, 48, 1)\n",
            "iteration: 33445 train shape: (102232, 48, 48, 1)\n",
            "iteration: 33446 train shape: (102234, 48, 48, 1)\n",
            "iteration: 33447 train shape: (102236, 48, 48, 1)\n",
            "iteration: 33448 train shape: (102238, 48, 48, 1)\n",
            "iteration: 33449 train shape: (102240, 48, 48, 1)\n",
            "iteration: 33450 train shape: (102242, 48, 48, 1)\n",
            "iteration: 33451 train shape: (102244, 48, 48, 1)\n",
            "iteration: 33452 train shape: (102246, 48, 48, 1)\n",
            "iteration: 33453 train shape: (102248, 48, 48, 1)\n",
            "iteration: 33454 train shape: (102250, 48, 48, 1)\n",
            "iteration: 33455 train shape: (102252, 48, 48, 1)\n",
            "iteration: 33456 train shape: (102254, 48, 48, 1)\n",
            "iteration: 33457 train shape: (102256, 48, 48, 1)\n",
            "iteration: 33458 train shape: (102258, 48, 48, 1)\n",
            "iteration: 33459 train shape: (102260, 48, 48, 1)\n",
            "iteration: 33460 train shape: (102262, 48, 48, 1)\n",
            "iteration: 33461 train shape: (102264, 48, 48, 1)\n",
            "iteration: 33462 train shape: (102266, 48, 48, 1)\n",
            "iteration: 33463 train shape: (102268, 48, 48, 1)\n",
            "iteration: 33464 train shape: (102270, 48, 48, 1)\n",
            "iteration: 33465 train shape: (102272, 48, 48, 1)\n",
            "iteration: 33466 train shape: (102274, 48, 48, 1)\n",
            "iteration: 33467 train shape: (102276, 48, 48, 1)\n",
            "iteration: 33468 train shape: (102278, 48, 48, 1)\n",
            "iteration: 33469 train shape: (102280, 48, 48, 1)\n",
            "iteration: 33470 train shape: (102282, 48, 48, 1)\n",
            "iteration: 33471 train shape: (102284, 48, 48, 1)\n",
            "iteration: 33472 train shape: (102286, 48, 48, 1)\n",
            "iteration: 33473 train shape: (102288, 48, 48, 1)\n",
            "iteration: 33474 train shape: (102290, 48, 48, 1)\n",
            "iteration: 33475 train shape: (102292, 48, 48, 1)\n",
            "iteration: 33476 train shape: (102294, 48, 48, 1)\n",
            "iteration: 33477 train shape: (102296, 48, 48, 1)\n",
            "iteration: 33478 train shape: (102298, 48, 48, 1)\n",
            "iteration: 33479 train shape: (102300, 48, 48, 1)\n",
            "iteration: 33480 train shape: (102302, 48, 48, 1)\n",
            "iteration: 33481 train shape: (102304, 48, 48, 1)\n",
            "iteration: 33482 train shape: (102306, 48, 48, 1)\n",
            "iteration: 33483 train shape: (102308, 48, 48, 1)\n",
            "iteration: 33484 train shape: (102310, 48, 48, 1)\n",
            "iteration: 33485 train shape: (102312, 48, 48, 1)\n",
            "iteration: 33486 train shape: (102314, 48, 48, 1)\n",
            "iteration: 33487 train shape: (102316, 48, 48, 1)\n",
            "iteration: 33488 train shape: (102318, 48, 48, 1)\n",
            "iteration: 33489 train shape: (102320, 48, 48, 1)\n",
            "iteration: 33490 train shape: (102322, 48, 48, 1)\n",
            "iteration: 33491 train shape: (102324, 48, 48, 1)\n",
            "iteration: 33492 train shape: (102326, 48, 48, 1)\n",
            "iteration: 33493 train shape: (102328, 48, 48, 1)\n",
            "iteration: 33494 train shape: (102330, 48, 48, 1)\n",
            "iteration: 33495 train shape: (102332, 48, 48, 1)\n",
            "iteration: 33496 train shape: (102334, 48, 48, 1)\n",
            "iteration: 33497 train shape: (102336, 48, 48, 1)\n",
            "iteration: 33498 train shape: (102338, 48, 48, 1)\n",
            "iteration: 33499 train shape: (102340, 48, 48, 1)\n",
            "iteration: 33500 train shape: (102342, 48, 48, 1)\n",
            "iteration: 33501 train shape: (102344, 48, 48, 1)\n",
            "iteration: 33502 train shape: (102346, 48, 48, 1)\n",
            "iteration: 33503 train shape: (102348, 48, 48, 1)\n",
            "iteration: 33504 train shape: (102350, 48, 48, 1)\n",
            "iteration: 33505 train shape: (102352, 48, 48, 1)\n",
            "iteration: 33506 train shape: (102354, 48, 48, 1)\n",
            "iteration: 33507 train shape: (102356, 48, 48, 1)\n",
            "iteration: 33508 train shape: (102358, 48, 48, 1)\n",
            "iteration: 33509 train shape: (102360, 48, 48, 1)\n",
            "iteration: 33510 train shape: (102362, 48, 48, 1)\n",
            "iteration: 33511 train shape: (102364, 48, 48, 1)\n",
            "iteration: 33512 train shape: (102366, 48, 48, 1)\n",
            "iteration: 33513 train shape: (102368, 48, 48, 1)\n",
            "iteration: 33514 train shape: (102370, 48, 48, 1)\n",
            "iteration: 33515 train shape: (102372, 48, 48, 1)\n",
            "iteration: 33516 train shape: (102374, 48, 48, 1)\n",
            "iteration: 33517 train shape: (102376, 48, 48, 1)\n",
            "iteration: 33518 train shape: (102378, 48, 48, 1)\n",
            "iteration: 33519 train shape: (102380, 48, 48, 1)\n",
            "iteration: 33520 train shape: (102382, 48, 48, 1)\n",
            "iteration: 33521 train shape: (102384, 48, 48, 1)\n",
            "iteration: 33522 train shape: (102386, 48, 48, 1)\n",
            "iteration: 33523 train shape: (102388, 48, 48, 1)\n",
            "iteration: 33524 train shape: (102390, 48, 48, 1)\n",
            "iteration: 33525 train shape: (102392, 48, 48, 1)\n",
            "iteration: 33526 train shape: (102394, 48, 48, 1)\n",
            "iteration: 33527 train shape: (102396, 48, 48, 1)\n",
            "iteration: 33528 train shape: (102398, 48, 48, 1)\n",
            "iteration: 33529 train shape: (102400, 48, 48, 1)\n",
            "iteration: 33530 train shape: (102402, 48, 48, 1)\n",
            "iteration: 33531 train shape: (102404, 48, 48, 1)\n",
            "iteration: 33532 train shape: (102406, 48, 48, 1)\n",
            "iteration: 33533 train shape: (102408, 48, 48, 1)\n",
            "iteration: 33534 train shape: (102410, 48, 48, 1)\n",
            "iteration: 33535 train shape: (102412, 48, 48, 1)\n",
            "iteration: 33536 train shape: (102414, 48, 48, 1)\n",
            "iteration: 33537 train shape: (102416, 48, 48, 1)\n",
            "iteration: 33538 train shape: (102418, 48, 48, 1)\n",
            "iteration: 33539 train shape: (102420, 48, 48, 1)\n",
            "iteration: 33540 train shape: (102422, 48, 48, 1)\n",
            "iteration: 33541 train shape: (102424, 48, 48, 1)\n",
            "iteration: 33542 train shape: (102426, 48, 48, 1)\n",
            "iteration: 33543 train shape: (102428, 48, 48, 1)\n",
            "iteration: 33544 train shape: (102430, 48, 48, 1)\n",
            "iteration: 33545 train shape: (102432, 48, 48, 1)\n",
            "iteration: 33546 train shape: (102434, 48, 48, 1)\n",
            "iteration: 33547 train shape: (102436, 48, 48, 1)\n",
            "iteration: 33548 train shape: (102438, 48, 48, 1)\n",
            "iteration: 33549 train shape: (102440, 48, 48, 1)\n",
            "iteration: 33550 train shape: (102442, 48, 48, 1)\n",
            "iteration: 33551 train shape: (102444, 48, 48, 1)\n",
            "iteration: 33552 train shape: (102446, 48, 48, 1)\n",
            "iteration: 33553 train shape: (102448, 48, 48, 1)\n",
            "iteration: 33554 train shape: (102450, 48, 48, 1)\n",
            "iteration: 33555 train shape: (102452, 48, 48, 1)\n",
            "iteration: 33556 train shape: (102454, 48, 48, 1)\n",
            "iteration: 33557 train shape: (102456, 48, 48, 1)\n",
            "iteration: 33558 train shape: (102458, 48, 48, 1)\n",
            "iteration: 33559 train shape: (102460, 48, 48, 1)\n",
            "iteration: 33560 train shape: (102462, 48, 48, 1)\n",
            "iteration: 33561 train shape: (102464, 48, 48, 1)\n",
            "iteration: 33562 train shape: (102466, 48, 48, 1)\n",
            "iteration: 33563 train shape: (102468, 48, 48, 1)\n",
            "iteration: 33564 train shape: (102470, 48, 48, 1)\n",
            "iteration: 33565 train shape: (102472, 48, 48, 1)\n",
            "iteration: 33566 train shape: (102474, 48, 48, 1)\n",
            "iteration: 33567 train shape: (102476, 48, 48, 1)\n",
            "iteration: 33568 train shape: (102478, 48, 48, 1)\n",
            "iteration: 33569 train shape: (102480, 48, 48, 1)\n",
            "iteration: 33570 train shape: (102482, 48, 48, 1)\n",
            "iteration: 33571 train shape: (102484, 48, 48, 1)\n",
            "iteration: 33572 train shape: (102486, 48, 48, 1)\n",
            "iteration: 33573 train shape: (102488, 48, 48, 1)\n",
            "iteration: 33574 train shape: (102490, 48, 48, 1)\n",
            "iteration: 33575 train shape: (102492, 48, 48, 1)\n",
            "iteration: 33576 train shape: (102494, 48, 48, 1)\n",
            "iteration: 33577 train shape: (102496, 48, 48, 1)\n",
            "iteration: 33578 train shape: (102498, 48, 48, 1)\n",
            "iteration: 33579 train shape: (102500, 48, 48, 1)\n",
            "iteration: 33580 train shape: (102502, 48, 48, 1)\n",
            "iteration: 33581 train shape: (102504, 48, 48, 1)\n",
            "iteration: 33582 train shape: (102506, 48, 48, 1)\n",
            "iteration: 33583 train shape: (102508, 48, 48, 1)\n",
            "iteration: 33584 train shape: (102510, 48, 48, 1)\n",
            "iteration: 33585 train shape: (102512, 48, 48, 1)\n",
            "iteration: 33586 train shape: (102514, 48, 48, 1)\n",
            "iteration: 33587 train shape: (102516, 48, 48, 1)\n",
            "iteration: 33588 train shape: (102518, 48, 48, 1)\n",
            "iteration: 33589 train shape: (102520, 48, 48, 1)\n",
            "iteration: 33590 train shape: (102522, 48, 48, 1)\n",
            "iteration: 33591 train shape: (102524, 48, 48, 1)\n",
            "iteration: 33592 train shape: (102526, 48, 48, 1)\n",
            "iteration: 33593 train shape: (102528, 48, 48, 1)\n",
            "iteration: 33594 train shape: (102530, 48, 48, 1)\n",
            "iteration: 33595 train shape: (102532, 48, 48, 1)\n",
            "iteration: 33596 train shape: (102534, 48, 48, 1)\n",
            "iteration: 33597 train shape: (102536, 48, 48, 1)\n",
            "iteration: 33598 train shape: (102538, 48, 48, 1)\n",
            "iteration: 33599 train shape: (102540, 48, 48, 1)\n",
            "iteration: 33600 train shape: (102542, 48, 48, 1)\n",
            "iteration: 33601 train shape: (102544, 48, 48, 1)\n",
            "iteration: 33602 train shape: (102546, 48, 48, 1)\n",
            "iteration: 33603 train shape: (102548, 48, 48, 1)\n",
            "iteration: 33604 train shape: (102550, 48, 48, 1)\n",
            "iteration: 33605 train shape: (102552, 48, 48, 1)\n",
            "iteration: 33606 train shape: (102554, 48, 48, 1)\n",
            "iteration: 33607 train shape: (102556, 48, 48, 1)\n",
            "iteration: 33608 train shape: (102558, 48, 48, 1)\n",
            "iteration: 33609 train shape: (102560, 48, 48, 1)\n",
            "iteration: 33610 train shape: (102562, 48, 48, 1)\n",
            "iteration: 33611 train shape: (102564, 48, 48, 1)\n",
            "iteration: 33612 train shape: (102566, 48, 48, 1)\n",
            "iteration: 33613 train shape: (102568, 48, 48, 1)\n",
            "iteration: 33614 train shape: (102570, 48, 48, 1)\n",
            "iteration: 33615 train shape: (102572, 48, 48, 1)\n",
            "iteration: 33616 train shape: (102574, 48, 48, 1)\n",
            "iteration: 33617 train shape: (102576, 48, 48, 1)\n",
            "iteration: 33618 train shape: (102578, 48, 48, 1)\n",
            "iteration: 33619 train shape: (102580, 48, 48, 1)\n",
            "iteration: 33620 train shape: (102582, 48, 48, 1)\n",
            "iteration: 33621 train shape: (102584, 48, 48, 1)\n",
            "iteration: 33622 train shape: (102586, 48, 48, 1)\n",
            "iteration: 33623 train shape: (102588, 48, 48, 1)\n",
            "iteration: 33624 train shape: (102590, 48, 48, 1)\n",
            "iteration: 33625 train shape: (102592, 48, 48, 1)\n",
            "iteration: 33626 train shape: (102594, 48, 48, 1)\n",
            "iteration: 33627 train shape: (102596, 48, 48, 1)\n",
            "iteration: 33628 train shape: (102598, 48, 48, 1)\n",
            "iteration: 33629 train shape: (102600, 48, 48, 1)\n",
            "iteration: 33630 train shape: (102602, 48, 48, 1)\n",
            "iteration: 33631 train shape: (102604, 48, 48, 1)\n",
            "iteration: 33632 train shape: (102606, 48, 48, 1)\n",
            "iteration: 33633 train shape: (102608, 48, 48, 1)\n",
            "iteration: 33634 train shape: (102610, 48, 48, 1)\n",
            "iteration: 33635 train shape: (102612, 48, 48, 1)\n",
            "iteration: 33636 train shape: (102614, 48, 48, 1)\n",
            "iteration: 33637 train shape: (102616, 48, 48, 1)\n",
            "iteration: 33638 train shape: (102618, 48, 48, 1)\n",
            "iteration: 33639 train shape: (102620, 48, 48, 1)\n",
            "iteration: 33640 train shape: (102622, 48, 48, 1)\n",
            "iteration: 33641 train shape: (102624, 48, 48, 1)\n",
            "iteration: 33642 train shape: (102626, 48, 48, 1)\n",
            "iteration: 33643 train shape: (102628, 48, 48, 1)\n",
            "iteration: 33644 train shape: (102630, 48, 48, 1)\n",
            "iteration: 33645 train shape: (102632, 48, 48, 1)\n",
            "iteration: 33646 train shape: (102634, 48, 48, 1)\n",
            "iteration: 33647 train shape: (102636, 48, 48, 1)\n",
            "iteration: 33648 train shape: (102638, 48, 48, 1)\n",
            "iteration: 33649 train shape: (102640, 48, 48, 1)\n",
            "iteration: 33650 train shape: (102642, 48, 48, 1)\n",
            "iteration: 33651 train shape: (102644, 48, 48, 1)\n",
            "iteration: 33652 train shape: (102646, 48, 48, 1)\n",
            "iteration: 33653 train shape: (102648, 48, 48, 1)\n",
            "iteration: 33654 train shape: (102650, 48, 48, 1)\n",
            "iteration: 33655 train shape: (102652, 48, 48, 1)\n",
            "iteration: 33656 train shape: (102654, 48, 48, 1)\n",
            "iteration: 33657 train shape: (102656, 48, 48, 1)\n",
            "iteration: 33658 train shape: (102658, 48, 48, 1)\n",
            "iteration: 33659 train shape: (102660, 48, 48, 1)\n",
            "iteration: 33660 train shape: (102662, 48, 48, 1)\n",
            "iteration: 33661 train shape: (102664, 48, 48, 1)\n",
            "iteration: 33662 train shape: (102666, 48, 48, 1)\n",
            "iteration: 33663 train shape: (102668, 48, 48, 1)\n",
            "iteration: 33664 train shape: (102670, 48, 48, 1)\n",
            "iteration: 33665 train shape: (102672, 48, 48, 1)\n",
            "iteration: 33666 train shape: (102674, 48, 48, 1)\n",
            "iteration: 33667 train shape: (102676, 48, 48, 1)\n",
            "iteration: 33668 train shape: (102678, 48, 48, 1)\n",
            "iteration: 33669 train shape: (102680, 48, 48, 1)\n",
            "iteration: 33670 train shape: (102682, 48, 48, 1)\n",
            "iteration: 33671 train shape: (102684, 48, 48, 1)\n",
            "iteration: 33672 train shape: (102686, 48, 48, 1)\n",
            "iteration: 33673 train shape: (102688, 48, 48, 1)\n",
            "iteration: 33674 train shape: (102690, 48, 48, 1)\n",
            "iteration: 33675 train shape: (102692, 48, 48, 1)\n",
            "iteration: 33676 train shape: (102694, 48, 48, 1)\n",
            "iteration: 33677 train shape: (102696, 48, 48, 1)\n",
            "iteration: 33678 train shape: (102698, 48, 48, 1)\n",
            "iteration: 33679 train shape: (102700, 48, 48, 1)\n",
            "iteration: 33680 train shape: (102702, 48, 48, 1)\n",
            "iteration: 33681 train shape: (102704, 48, 48, 1)\n",
            "iteration: 33682 train shape: (102706, 48, 48, 1)\n",
            "iteration: 33683 train shape: (102708, 48, 48, 1)\n",
            "iteration: 33684 train shape: (102710, 48, 48, 1)\n",
            "iteration: 33685 train shape: (102712, 48, 48, 1)\n",
            "iteration: 33686 train shape: (102714, 48, 48, 1)\n",
            "iteration: 33687 train shape: (102716, 48, 48, 1)\n",
            "iteration: 33688 train shape: (102718, 48, 48, 1)\n",
            "iteration: 33689 train shape: (102720, 48, 48, 1)\n",
            "iteration: 33690 train shape: (102722, 48, 48, 1)\n",
            "iteration: 33691 train shape: (102724, 48, 48, 1)\n",
            "iteration: 33692 train shape: (102726, 48, 48, 1)\n",
            "iteration: 33693 train shape: (102728, 48, 48, 1)\n",
            "iteration: 33694 train shape: (102730, 48, 48, 1)\n",
            "iteration: 33695 train shape: (102732, 48, 48, 1)\n",
            "iteration: 33696 train shape: (102734, 48, 48, 1)\n",
            "iteration: 33697 train shape: (102736, 48, 48, 1)\n",
            "iteration: 33698 train shape: (102738, 48, 48, 1)\n",
            "iteration: 33699 train shape: (102740, 48, 48, 1)\n",
            "iteration: 33700 train shape: (102742, 48, 48, 1)\n",
            "iteration: 33701 train shape: (102744, 48, 48, 1)\n",
            "iteration: 33702 train shape: (102746, 48, 48, 1)\n",
            "iteration: 33703 train shape: (102748, 48, 48, 1)\n",
            "iteration: 33704 train shape: (102750, 48, 48, 1)\n",
            "iteration: 33705 train shape: (102752, 48, 48, 1)\n",
            "iteration: 33706 train shape: (102754, 48, 48, 1)\n",
            "iteration: 33707 train shape: (102756, 48, 48, 1)\n",
            "iteration: 33708 train shape: (102758, 48, 48, 1)\n",
            "iteration: 33709 train shape: (102760, 48, 48, 1)\n",
            "iteration: 33710 train shape: (102762, 48, 48, 1)\n",
            "iteration: 33711 train shape: (102764, 48, 48, 1)\n",
            "iteration: 33712 train shape: (102766, 48, 48, 1)\n",
            "iteration: 33713 train shape: (102768, 48, 48, 1)\n",
            "iteration: 33714 train shape: (102770, 48, 48, 1)\n",
            "iteration: 33715 train shape: (102772, 48, 48, 1)\n",
            "iteration: 33716 train shape: (102774, 48, 48, 1)\n",
            "iteration: 33717 train shape: (102776, 48, 48, 1)\n",
            "iteration: 33718 train shape: (102778, 48, 48, 1)\n",
            "iteration: 33719 train shape: (102780, 48, 48, 1)\n",
            "iteration: 33720 train shape: (102782, 48, 48, 1)\n",
            "iteration: 33721 train shape: (102784, 48, 48, 1)\n",
            "iteration: 33722 train shape: (102786, 48, 48, 1)\n",
            "iteration: 33723 train shape: (102788, 48, 48, 1)\n",
            "iteration: 33724 train shape: (102790, 48, 48, 1)\n",
            "iteration: 33725 train shape: (102792, 48, 48, 1)\n",
            "iteration: 33726 train shape: (102794, 48, 48, 1)\n",
            "iteration: 33727 train shape: (102796, 48, 48, 1)\n",
            "iteration: 33728 train shape: (102798, 48, 48, 1)\n",
            "iteration: 33729 train shape: (102800, 48, 48, 1)\n",
            "iteration: 33730 train shape: (102802, 48, 48, 1)\n",
            "iteration: 33731 train shape: (102804, 48, 48, 1)\n",
            "iteration: 33732 train shape: (102806, 48, 48, 1)\n",
            "iteration: 33733 train shape: (102808, 48, 48, 1)\n",
            "iteration: 33734 train shape: (102810, 48, 48, 1)\n",
            "iteration: 33735 train shape: (102812, 48, 48, 1)\n",
            "iteration: 33736 train shape: (102814, 48, 48, 1)\n",
            "iteration: 33737 train shape: (102816, 48, 48, 1)\n",
            "iteration: 33738 train shape: (102818, 48, 48, 1)\n",
            "iteration: 33739 train shape: (102820, 48, 48, 1)\n",
            "iteration: 33740 train shape: (102822, 48, 48, 1)\n",
            "iteration: 33741 train shape: (102824, 48, 48, 1)\n",
            "iteration: 33742 train shape: (102826, 48, 48, 1)\n",
            "iteration: 33743 train shape: (102828, 48, 48, 1)\n",
            "iteration: 33744 train shape: (102830, 48, 48, 1)\n",
            "iteration: 33745 train shape: (102832, 48, 48, 1)\n",
            "iteration: 33746 train shape: (102834, 48, 48, 1)\n",
            "iteration: 33747 train shape: (102836, 48, 48, 1)\n",
            "iteration: 33748 train shape: (102838, 48, 48, 1)\n",
            "iteration: 33749 train shape: (102840, 48, 48, 1)\n",
            "iteration: 33750 train shape: (102842, 48, 48, 1)\n",
            "iteration: 33751 train shape: (102844, 48, 48, 1)\n",
            "iteration: 33752 train shape: (102846, 48, 48, 1)\n",
            "iteration: 33753 train shape: (102848, 48, 48, 1)\n",
            "iteration: 33754 train shape: (102850, 48, 48, 1)\n",
            "iteration: 33755 train shape: (102852, 48, 48, 1)\n",
            "iteration: 33756 train shape: (102854, 48, 48, 1)\n",
            "iteration: 33757 train shape: (102856, 48, 48, 1)\n",
            "iteration: 33758 train shape: (102858, 48, 48, 1)\n",
            "iteration: 33759 train shape: (102860, 48, 48, 1)\n",
            "iteration: 33760 train shape: (102862, 48, 48, 1)\n",
            "iteration: 33761 train shape: (102864, 48, 48, 1)\n",
            "iteration: 33762 train shape: (102866, 48, 48, 1)\n",
            "iteration: 33763 train shape: (102868, 48, 48, 1)\n",
            "iteration: 33764 train shape: (102870, 48, 48, 1)\n",
            "iteration: 33765 train shape: (102872, 48, 48, 1)\n",
            "iteration: 33766 train shape: (102874, 48, 48, 1)\n",
            "iteration: 33767 train shape: (102876, 48, 48, 1)\n",
            "iteration: 33768 train shape: (102878, 48, 48, 1)\n",
            "iteration: 33769 train shape: (102880, 48, 48, 1)\n",
            "iteration: 33770 train shape: (102882, 48, 48, 1)\n",
            "iteration: 33771 train shape: (102884, 48, 48, 1)\n",
            "iteration: 33772 train shape: (102886, 48, 48, 1)\n",
            "iteration: 33773 train shape: (102888, 48, 48, 1)\n",
            "iteration: 33774 train shape: (102890, 48, 48, 1)\n",
            "iteration: 33775 train shape: (102892, 48, 48, 1)\n",
            "iteration: 33776 train shape: (102894, 48, 48, 1)\n",
            "iteration: 33777 train shape: (102896, 48, 48, 1)\n",
            "iteration: 33778 train shape: (102898, 48, 48, 1)\n",
            "iteration: 33779 train shape: (102900, 48, 48, 1)\n",
            "iteration: 33780 train shape: (102902, 48, 48, 1)\n",
            "iteration: 33781 train shape: (102904, 48, 48, 1)\n",
            "iteration: 33782 train shape: (102906, 48, 48, 1)\n",
            "iteration: 33783 train shape: (102908, 48, 48, 1)\n",
            "iteration: 33784 train shape: (102910, 48, 48, 1)\n",
            "iteration: 33785 train shape: (102912, 48, 48, 1)\n",
            "iteration: 33786 train shape: (102914, 48, 48, 1)\n",
            "iteration: 33787 train shape: (102916, 48, 48, 1)\n",
            "iteration: 33788 train shape: (102918, 48, 48, 1)\n",
            "iteration: 33789 train shape: (102920, 48, 48, 1)\n",
            "iteration: 33790 train shape: (102922, 48, 48, 1)\n",
            "iteration: 33791 train shape: (102924, 48, 48, 1)\n",
            "iteration: 33792 train shape: (102926, 48, 48, 1)\n",
            "iteration: 33793 train shape: (102928, 48, 48, 1)\n",
            "iteration: 33794 train shape: (102930, 48, 48, 1)\n",
            "iteration: 33795 train shape: (102932, 48, 48, 1)\n",
            "iteration: 33796 train shape: (102934, 48, 48, 1)\n",
            "iteration: 33797 train shape: (102936, 48, 48, 1)\n",
            "iteration: 33798 train shape: (102938, 48, 48, 1)\n",
            "iteration: 33799 train shape: (102940, 48, 48, 1)\n",
            "iteration: 33800 train shape: (102942, 48, 48, 1)\n",
            "iteration: 33801 train shape: (102944, 48, 48, 1)\n",
            "iteration: 33802 train shape: (102946, 48, 48, 1)\n",
            "iteration: 33803 train shape: (102948, 48, 48, 1)\n",
            "iteration: 33804 train shape: (102950, 48, 48, 1)\n",
            "iteration: 33805 train shape: (102952, 48, 48, 1)\n",
            "iteration: 33806 train shape: (102954, 48, 48, 1)\n",
            "iteration: 33807 train shape: (102956, 48, 48, 1)\n",
            "iteration: 33808 train shape: (102958, 48, 48, 1)\n",
            "iteration: 33809 train shape: (102960, 48, 48, 1)\n",
            "iteration: 33810 train shape: (102962, 48, 48, 1)\n",
            "iteration: 33811 train shape: (102964, 48, 48, 1)\n",
            "iteration: 33812 train shape: (102966, 48, 48, 1)\n",
            "iteration: 33813 train shape: (102968, 48, 48, 1)\n",
            "iteration: 33814 train shape: (102970, 48, 48, 1)\n",
            "iteration: 33815 train shape: (102972, 48, 48, 1)\n",
            "iteration: 33816 train shape: (102974, 48, 48, 1)\n",
            "iteration: 33817 train shape: (102976, 48, 48, 1)\n",
            "iteration: 33818 train shape: (102978, 48, 48, 1)\n",
            "iteration: 33819 train shape: (102980, 48, 48, 1)\n",
            "iteration: 33820 train shape: (102982, 48, 48, 1)\n",
            "iteration: 33821 train shape: (102984, 48, 48, 1)\n",
            "iteration: 33822 train shape: (102986, 48, 48, 1)\n",
            "iteration: 33823 train shape: (102988, 48, 48, 1)\n",
            "iteration: 33824 train shape: (102990, 48, 48, 1)\n",
            "iteration: 33825 train shape: (102992, 48, 48, 1)\n",
            "iteration: 33826 train shape: (102994, 48, 48, 1)\n",
            "iteration: 33827 train shape: (102996, 48, 48, 1)\n",
            "iteration: 33828 train shape: (102998, 48, 48, 1)\n",
            "iteration: 33829 train shape: (103000, 48, 48, 1)\n",
            "iteration: 33830 train shape: (103002, 48, 48, 1)\n",
            "iteration: 33831 train shape: (103004, 48, 48, 1)\n",
            "iteration: 33832 train shape: (103006, 48, 48, 1)\n",
            "iteration: 33833 train shape: (103008, 48, 48, 1)\n",
            "iteration: 33834 train shape: (103010, 48, 48, 1)\n",
            "iteration: 33835 train shape: (103012, 48, 48, 1)\n",
            "iteration: 33836 train shape: (103014, 48, 48, 1)\n",
            "iteration: 33837 train shape: (103016, 48, 48, 1)\n",
            "iteration: 33838 train shape: (103018, 48, 48, 1)\n",
            "iteration: 33839 train shape: (103020, 48, 48, 1)\n",
            "iteration: 33840 train shape: (103022, 48, 48, 1)\n",
            "iteration: 33841 train shape: (103024, 48, 48, 1)\n",
            "iteration: 33842 train shape: (103026, 48, 48, 1)\n",
            "iteration: 33843 train shape: (103028, 48, 48, 1)\n",
            "iteration: 33844 train shape: (103030, 48, 48, 1)\n",
            "iteration: 33845 train shape: (103032, 48, 48, 1)\n",
            "iteration: 33846 train shape: (103034, 48, 48, 1)\n",
            "iteration: 33847 train shape: (103036, 48, 48, 1)\n",
            "iteration: 33848 train shape: (103038, 48, 48, 1)\n",
            "iteration: 33849 train shape: (103040, 48, 48, 1)\n",
            "iteration: 33850 train shape: (103042, 48, 48, 1)\n",
            "iteration: 33851 train shape: (103044, 48, 48, 1)\n",
            "iteration: 33852 train shape: (103046, 48, 48, 1)\n",
            "iteration: 33853 train shape: (103048, 48, 48, 1)\n",
            "iteration: 33854 train shape: (103050, 48, 48, 1)\n",
            "iteration: 33855 train shape: (103052, 48, 48, 1)\n",
            "iteration: 33856 train shape: (103054, 48, 48, 1)\n",
            "iteration: 33857 train shape: (103056, 48, 48, 1)\n",
            "iteration: 33858 train shape: (103058, 48, 48, 1)\n",
            "iteration: 33859 train shape: (103060, 48, 48, 1)\n",
            "iteration: 33860 train shape: (103062, 48, 48, 1)\n",
            "iteration: 33861 train shape: (103064, 48, 48, 1)\n",
            "iteration: 33862 train shape: (103066, 48, 48, 1)\n",
            "iteration: 33863 train shape: (103068, 48, 48, 1)\n",
            "iteration: 33864 train shape: (103070, 48, 48, 1)\n",
            "iteration: 33865 train shape: (103072, 48, 48, 1)\n",
            "iteration: 33866 train shape: (103074, 48, 48, 1)\n",
            "iteration: 33867 train shape: (103076, 48, 48, 1)\n",
            "iteration: 33868 train shape: (103078, 48, 48, 1)\n",
            "iteration: 33869 train shape: (103080, 48, 48, 1)\n",
            "iteration: 33870 train shape: (103082, 48, 48, 1)\n",
            "iteration: 33871 train shape: (103084, 48, 48, 1)\n",
            "iteration: 33872 train shape: (103086, 48, 48, 1)\n",
            "iteration: 33873 train shape: (103088, 48, 48, 1)\n",
            "iteration: 33874 train shape: (103090, 48, 48, 1)\n",
            "iteration: 33875 train shape: (103092, 48, 48, 1)\n",
            "iteration: 33876 train shape: (103094, 48, 48, 1)\n",
            "iteration: 33877 train shape: (103096, 48, 48, 1)\n",
            "iteration: 33878 train shape: (103098, 48, 48, 1)\n",
            "iteration: 33879 train shape: (103100, 48, 48, 1)\n",
            "iteration: 33880 train shape: (103102, 48, 48, 1)\n",
            "iteration: 33881 train shape: (103104, 48, 48, 1)\n",
            "iteration: 33882 train shape: (103106, 48, 48, 1)\n",
            "iteration: 33883 train shape: (103108, 48, 48, 1)\n",
            "iteration: 33884 train shape: (103110, 48, 48, 1)\n",
            "iteration: 33885 train shape: (103112, 48, 48, 1)\n",
            "iteration: 33886 train shape: (103114, 48, 48, 1)\n",
            "iteration: 33887 train shape: (103116, 48, 48, 1)\n",
            "iteration: 33888 train shape: (103118, 48, 48, 1)\n",
            "iteration: 33889 train shape: (103120, 48, 48, 1)\n",
            "iteration: 33890 train shape: (103122, 48, 48, 1)\n",
            "iteration: 33891 train shape: (103124, 48, 48, 1)\n",
            "iteration: 33892 train shape: (103126, 48, 48, 1)\n",
            "iteration: 33893 train shape: (103128, 48, 48, 1)\n",
            "iteration: 33894 train shape: (103130, 48, 48, 1)\n",
            "iteration: 33895 train shape: (103132, 48, 48, 1)\n",
            "iteration: 33896 train shape: (103134, 48, 48, 1)\n",
            "iteration: 33897 train shape: (103136, 48, 48, 1)\n",
            "iteration: 33898 train shape: (103138, 48, 48, 1)\n",
            "iteration: 33899 train shape: (103140, 48, 48, 1)\n",
            "iteration: 33900 train shape: (103142, 48, 48, 1)\n",
            "iteration: 33901 train shape: (103144, 48, 48, 1)\n",
            "iteration: 33902 train shape: (103146, 48, 48, 1)\n",
            "iteration: 33903 train shape: (103148, 48, 48, 1)\n",
            "iteration: 33904 train shape: (103150, 48, 48, 1)\n",
            "iteration: 33905 train shape: (103152, 48, 48, 1)\n",
            "iteration: 33906 train shape: (103154, 48, 48, 1)\n",
            "iteration: 33907 train shape: (103156, 48, 48, 1)\n",
            "iteration: 33908 train shape: (103158, 48, 48, 1)\n",
            "iteration: 33909 train shape: (103160, 48, 48, 1)\n",
            "iteration: 33910 train shape: (103162, 48, 48, 1)\n",
            "iteration: 33911 train shape: (103164, 48, 48, 1)\n",
            "iteration: 33912 train shape: (103166, 48, 48, 1)\n",
            "iteration: 33913 train shape: (103168, 48, 48, 1)\n",
            "iteration: 33914 train shape: (103170, 48, 48, 1)\n",
            "iteration: 33915 train shape: (103172, 48, 48, 1)\n",
            "iteration: 33916 train shape: (103174, 48, 48, 1)\n",
            "iteration: 33917 train shape: (103176, 48, 48, 1)\n",
            "iteration: 33918 train shape: (103178, 48, 48, 1)\n",
            "iteration: 33919 train shape: (103180, 48, 48, 1)\n",
            "iteration: 33920 train shape: (103182, 48, 48, 1)\n",
            "iteration: 33921 train shape: (103184, 48, 48, 1)\n",
            "iteration: 33922 train shape: (103186, 48, 48, 1)\n",
            "iteration: 33923 train shape: (103188, 48, 48, 1)\n",
            "iteration: 33924 train shape: (103190, 48, 48, 1)\n",
            "iteration: 33925 train shape: (103192, 48, 48, 1)\n",
            "iteration: 33926 train shape: (103194, 48, 48, 1)\n",
            "iteration: 33927 train shape: (103196, 48, 48, 1)\n",
            "iteration: 33928 train shape: (103198, 48, 48, 1)\n",
            "iteration: 33929 train shape: (103200, 48, 48, 1)\n",
            "iteration: 33930 train shape: (103202, 48, 48, 1)\n",
            "iteration: 33931 train shape: (103204, 48, 48, 1)\n",
            "iteration: 33932 train shape: (103206, 48, 48, 1)\n",
            "iteration: 33933 train shape: (103208, 48, 48, 1)\n",
            "iteration: 33934 train shape: (103210, 48, 48, 1)\n",
            "iteration: 33935 train shape: (103212, 48, 48, 1)\n",
            "iteration: 33936 train shape: (103214, 48, 48, 1)\n",
            "iteration: 33937 train shape: (103216, 48, 48, 1)\n",
            "iteration: 33938 train shape: (103218, 48, 48, 1)\n",
            "iteration: 33939 train shape: (103220, 48, 48, 1)\n",
            "iteration: 33940 train shape: (103222, 48, 48, 1)\n",
            "iteration: 33941 train shape: (103224, 48, 48, 1)\n",
            "iteration: 33942 train shape: (103226, 48, 48, 1)\n",
            "iteration: 33943 train shape: (103228, 48, 48, 1)\n",
            "iteration: 33944 train shape: (103230, 48, 48, 1)\n",
            "iteration: 33945 train shape: (103232, 48, 48, 1)\n",
            "iteration: 33946 train shape: (103234, 48, 48, 1)\n",
            "iteration: 33947 train shape: (103236, 48, 48, 1)\n",
            "iteration: 33948 train shape: (103238, 48, 48, 1)\n",
            "iteration: 33949 train shape: (103240, 48, 48, 1)\n",
            "iteration: 33950 train shape: (103242, 48, 48, 1)\n",
            "iteration: 33951 train shape: (103244, 48, 48, 1)\n",
            "iteration: 33952 train shape: (103246, 48, 48, 1)\n",
            "iteration: 33953 train shape: (103248, 48, 48, 1)\n",
            "iteration: 33954 train shape: (103250, 48, 48, 1)\n",
            "iteration: 33955 train shape: (103252, 48, 48, 1)\n",
            "iteration: 33956 train shape: (103254, 48, 48, 1)\n",
            "iteration: 33957 train shape: (103256, 48, 48, 1)\n",
            "iteration: 33958 train shape: (103258, 48, 48, 1)\n",
            "iteration: 33959 train shape: (103260, 48, 48, 1)\n",
            "iteration: 33960 train shape: (103262, 48, 48, 1)\n",
            "iteration: 33961 train shape: (103264, 48, 48, 1)\n",
            "iteration: 33962 train shape: (103266, 48, 48, 1)\n",
            "iteration: 33963 train shape: (103268, 48, 48, 1)\n",
            "iteration: 33964 train shape: (103270, 48, 48, 1)\n",
            "iteration: 33965 train shape: (103272, 48, 48, 1)\n",
            "iteration: 33966 train shape: (103274, 48, 48, 1)\n",
            "iteration: 33967 train shape: (103276, 48, 48, 1)\n",
            "iteration: 33968 train shape: (103278, 48, 48, 1)\n",
            "iteration: 33969 train shape: (103280, 48, 48, 1)\n",
            "iteration: 33970 train shape: (103282, 48, 48, 1)\n",
            "iteration: 33971 train shape: (103284, 48, 48, 1)\n",
            "iteration: 33972 train shape: (103286, 48, 48, 1)\n",
            "iteration: 33973 train shape: (103288, 48, 48, 1)\n",
            "iteration: 33974 train shape: (103290, 48, 48, 1)\n",
            "iteration: 33975 train shape: (103292, 48, 48, 1)\n",
            "iteration: 33976 train shape: (103294, 48, 48, 1)\n",
            "iteration: 33977 train shape: (103296, 48, 48, 1)\n",
            "iteration: 33978 train shape: (103298, 48, 48, 1)\n",
            "iteration: 33979 train shape: (103300, 48, 48, 1)\n",
            "iteration: 33980 train shape: (103302, 48, 48, 1)\n",
            "iteration: 33981 train shape: (103304, 48, 48, 1)\n",
            "iteration: 33982 train shape: (103306, 48, 48, 1)\n",
            "iteration: 33983 train shape: (103308, 48, 48, 1)\n",
            "iteration: 33984 train shape: (103310, 48, 48, 1)\n",
            "iteration: 33985 train shape: (103312, 48, 48, 1)\n",
            "iteration: 33986 train shape: (103314, 48, 48, 1)\n",
            "iteration: 33987 train shape: (103316, 48, 48, 1)\n",
            "iteration: 33988 train shape: (103318, 48, 48, 1)\n",
            "iteration: 33989 train shape: (103320, 48, 48, 1)\n",
            "iteration: 33990 train shape: (103322, 48, 48, 1)\n",
            "iteration: 33991 train shape: (103324, 48, 48, 1)\n",
            "iteration: 33992 train shape: (103326, 48, 48, 1)\n",
            "iteration: 33993 train shape: (103328, 48, 48, 1)\n",
            "iteration: 33994 train shape: (103330, 48, 48, 1)\n",
            "iteration: 33995 train shape: (103332, 48, 48, 1)\n",
            "iteration: 33996 train shape: (103334, 48, 48, 1)\n",
            "iteration: 33997 train shape: (103336, 48, 48, 1)\n",
            "iteration: 33998 train shape: (103338, 48, 48, 1)\n",
            "iteration: 33999 train shape: (103340, 48, 48, 1)\n",
            "iteration: 34000 train shape: (103342, 48, 48, 1)\n",
            "iteration: 34001 train shape: (103344, 48, 48, 1)\n",
            "iteration: 34002 train shape: (103346, 48, 48, 1)\n",
            "iteration: 34003 train shape: (103348, 48, 48, 1)\n",
            "iteration: 34004 train shape: (103350, 48, 48, 1)\n",
            "iteration: 34005 train shape: (103352, 48, 48, 1)\n",
            "iteration: 34006 train shape: (103354, 48, 48, 1)\n",
            "iteration: 34007 train shape: (103356, 48, 48, 1)\n",
            "iteration: 34008 train shape: (103358, 48, 48, 1)\n",
            "iteration: 34009 train shape: (103360, 48, 48, 1)\n",
            "iteration: 34010 train shape: (103362, 48, 48, 1)\n",
            "iteration: 34011 train shape: (103364, 48, 48, 1)\n",
            "iteration: 34012 train shape: (103366, 48, 48, 1)\n",
            "iteration: 34013 train shape: (103368, 48, 48, 1)\n",
            "iteration: 34014 train shape: (103370, 48, 48, 1)\n",
            "iteration: 34015 train shape: (103372, 48, 48, 1)\n",
            "iteration: 34016 train shape: (103374, 48, 48, 1)\n",
            "iteration: 34017 train shape: (103376, 48, 48, 1)\n",
            "iteration: 34018 train shape: (103378, 48, 48, 1)\n",
            "iteration: 34019 train shape: (103380, 48, 48, 1)\n",
            "iteration: 34020 train shape: (103382, 48, 48, 1)\n",
            "iteration: 34021 train shape: (103384, 48, 48, 1)\n",
            "iteration: 34022 train shape: (103386, 48, 48, 1)\n",
            "iteration: 34023 train shape: (103388, 48, 48, 1)\n",
            "iteration: 34024 train shape: (103390, 48, 48, 1)\n",
            "iteration: 34025 train shape: (103392, 48, 48, 1)\n",
            "iteration: 34026 train shape: (103394, 48, 48, 1)\n",
            "iteration: 34027 train shape: (103396, 48, 48, 1)\n",
            "iteration: 34028 train shape: (103398, 48, 48, 1)\n",
            "iteration: 34029 train shape: (103400, 48, 48, 1)\n",
            "iteration: 34030 train shape: (103402, 48, 48, 1)\n",
            "iteration: 34031 train shape: (103404, 48, 48, 1)\n",
            "iteration: 34032 train shape: (103406, 48, 48, 1)\n",
            "iteration: 34033 train shape: (103408, 48, 48, 1)\n",
            "iteration: 34034 train shape: (103410, 48, 48, 1)\n",
            "iteration: 34035 train shape: (103412, 48, 48, 1)\n",
            "iteration: 34036 train shape: (103414, 48, 48, 1)\n",
            "iteration: 34037 train shape: (103416, 48, 48, 1)\n",
            "iteration: 34038 train shape: (103418, 48, 48, 1)\n",
            "iteration: 34039 train shape: (103420, 48, 48, 1)\n",
            "iteration: 34040 train shape: (103422, 48, 48, 1)\n",
            "iteration: 34041 train shape: (103424, 48, 48, 1)\n",
            "iteration: 34042 train shape: (103426, 48, 48, 1)\n",
            "iteration: 34043 train shape: (103428, 48, 48, 1)\n",
            "iteration: 34044 train shape: (103430, 48, 48, 1)\n",
            "iteration: 34045 train shape: (103432, 48, 48, 1)\n",
            "iteration: 34046 train shape: (103434, 48, 48, 1)\n",
            "iteration: 34047 train shape: (103436, 48, 48, 1)\n",
            "iteration: 34048 train shape: (103438, 48, 48, 1)\n",
            "iteration: 34049 train shape: (103440, 48, 48, 1)\n",
            "iteration: 34050 train shape: (103442, 48, 48, 1)\n",
            "iteration: 34051 train shape: (103444, 48, 48, 1)\n",
            "iteration: 34052 train shape: (103446, 48, 48, 1)\n",
            "iteration: 34053 train shape: (103448, 48, 48, 1)\n",
            "iteration: 34054 train shape: (103450, 48, 48, 1)\n",
            "iteration: 34055 train shape: (103452, 48, 48, 1)\n",
            "iteration: 34056 train shape: (103454, 48, 48, 1)\n",
            "iteration: 34057 train shape: (103456, 48, 48, 1)\n",
            "iteration: 34058 train shape: (103458, 48, 48, 1)\n",
            "iteration: 34059 train shape: (103460, 48, 48, 1)\n",
            "iteration: 34060 train shape: (103462, 48, 48, 1)\n",
            "iteration: 34061 train shape: (103464, 48, 48, 1)\n",
            "iteration: 34062 train shape: (103466, 48, 48, 1)\n",
            "iteration: 34063 train shape: (103468, 48, 48, 1)\n",
            "iteration: 34064 train shape: (103470, 48, 48, 1)\n",
            "iteration: 34065 train shape: (103472, 48, 48, 1)\n",
            "iteration: 34066 train shape: (103474, 48, 48, 1)\n",
            "iteration: 34067 train shape: (103476, 48, 48, 1)\n",
            "iteration: 34068 train shape: (103478, 48, 48, 1)\n",
            "iteration: 34069 train shape: (103480, 48, 48, 1)\n",
            "iteration: 34070 train shape: (103482, 48, 48, 1)\n",
            "iteration: 34071 train shape: (103484, 48, 48, 1)\n",
            "iteration: 34072 train shape: (103486, 48, 48, 1)\n",
            "iteration: 34073 train shape: (103488, 48, 48, 1)\n",
            "iteration: 34074 train shape: (103490, 48, 48, 1)\n",
            "iteration: 34075 train shape: (103492, 48, 48, 1)\n",
            "iteration: 34076 train shape: (103494, 48, 48, 1)\n",
            "iteration: 34077 train shape: (103496, 48, 48, 1)\n",
            "iteration: 34078 train shape: (103498, 48, 48, 1)\n",
            "iteration: 34079 train shape: (103500, 48, 48, 1)\n",
            "iteration: 34080 train shape: (103502, 48, 48, 1)\n",
            "iteration: 34081 train shape: (103504, 48, 48, 1)\n",
            "iteration: 34082 train shape: (103506, 48, 48, 1)\n",
            "iteration: 34083 train shape: (103508, 48, 48, 1)\n",
            "iteration: 34084 train shape: (103510, 48, 48, 1)\n",
            "iteration: 34085 train shape: (103512, 48, 48, 1)\n",
            "iteration: 34086 train shape: (103514, 48, 48, 1)\n",
            "iteration: 34087 train shape: (103516, 48, 48, 1)\n",
            "iteration: 34088 train shape: (103518, 48, 48, 1)\n",
            "iteration: 34089 train shape: (103520, 48, 48, 1)\n",
            "iteration: 34090 train shape: (103522, 48, 48, 1)\n",
            "iteration: 34091 train shape: (103524, 48, 48, 1)\n",
            "iteration: 34092 train shape: (103526, 48, 48, 1)\n",
            "iteration: 34093 train shape: (103528, 48, 48, 1)\n",
            "iteration: 34094 train shape: (103530, 48, 48, 1)\n",
            "iteration: 34095 train shape: (103532, 48, 48, 1)\n",
            "iteration: 34096 train shape: (103534, 48, 48, 1)\n",
            "iteration: 34097 train shape: (103536, 48, 48, 1)\n",
            "iteration: 34098 train shape: (103538, 48, 48, 1)\n",
            "iteration: 34099 train shape: (103540, 48, 48, 1)\n",
            "iteration: 34100 train shape: (103542, 48, 48, 1)\n",
            "iteration: 34101 train shape: (103544, 48, 48, 1)\n",
            "iteration: 34102 train shape: (103546, 48, 48, 1)\n",
            "iteration: 34103 train shape: (103548, 48, 48, 1)\n",
            "iteration: 34104 train shape: (103550, 48, 48, 1)\n",
            "iteration: 34105 train shape: (103552, 48, 48, 1)\n",
            "iteration: 34106 train shape: (103554, 48, 48, 1)\n",
            "iteration: 34107 train shape: (103556, 48, 48, 1)\n",
            "iteration: 34108 train shape: (103558, 48, 48, 1)\n",
            "iteration: 34109 train shape: (103560, 48, 48, 1)\n",
            "iteration: 34110 train shape: (103562, 48, 48, 1)\n",
            "iteration: 34111 train shape: (103564, 48, 48, 1)\n",
            "iteration: 34112 train shape: (103566, 48, 48, 1)\n",
            "iteration: 34113 train shape: (103568, 48, 48, 1)\n",
            "iteration: 34114 train shape: (103570, 48, 48, 1)\n",
            "iteration: 34115 train shape: (103572, 48, 48, 1)\n",
            "iteration: 34116 train shape: (103574, 48, 48, 1)\n",
            "iteration: 34117 train shape: (103576, 48, 48, 1)\n",
            "iteration: 34118 train shape: (103578, 48, 48, 1)\n",
            "iteration: 34119 train shape: (103580, 48, 48, 1)\n",
            "iteration: 34120 train shape: (103582, 48, 48, 1)\n",
            "iteration: 34121 train shape: (103584, 48, 48, 1)\n",
            "iteration: 34122 train shape: (103586, 48, 48, 1)\n",
            "iteration: 34123 train shape: (103588, 48, 48, 1)\n",
            "iteration: 34124 train shape: (103590, 48, 48, 1)\n",
            "iteration: 34125 train shape: (103592, 48, 48, 1)\n",
            "iteration: 34126 train shape: (103594, 48, 48, 1)\n",
            "iteration: 34127 train shape: (103596, 48, 48, 1)\n",
            "iteration: 34128 train shape: (103598, 48, 48, 1)\n",
            "iteration: 34129 train shape: (103600, 48, 48, 1)\n",
            "iteration: 34130 train shape: (103602, 48, 48, 1)\n",
            "iteration: 34131 train shape: (103604, 48, 48, 1)\n",
            "iteration: 34132 train shape: (103606, 48, 48, 1)\n",
            "iteration: 34133 train shape: (103608, 48, 48, 1)\n",
            "iteration: 34134 train shape: (103610, 48, 48, 1)\n",
            "iteration: 34135 train shape: (103612, 48, 48, 1)\n",
            "iteration: 34136 train shape: (103614, 48, 48, 1)\n",
            "iteration: 34137 train shape: (103616, 48, 48, 1)\n",
            "iteration: 34138 train shape: (103618, 48, 48, 1)\n",
            "iteration: 34139 train shape: (103620, 48, 48, 1)\n",
            "iteration: 34140 train shape: (103622, 48, 48, 1)\n",
            "iteration: 34141 train shape: (103624, 48, 48, 1)\n",
            "iteration: 34142 train shape: (103626, 48, 48, 1)\n",
            "iteration: 34143 train shape: (103628, 48, 48, 1)\n",
            "iteration: 34144 train shape: (103630, 48, 48, 1)\n",
            "iteration: 34145 train shape: (103632, 48, 48, 1)\n",
            "iteration: 34146 train shape: (103634, 48, 48, 1)\n",
            "iteration: 34147 train shape: (103636, 48, 48, 1)\n",
            "iteration: 34148 train shape: (103638, 48, 48, 1)\n",
            "iteration: 34149 train shape: (103640, 48, 48, 1)\n",
            "iteration: 34150 train shape: (103642, 48, 48, 1)\n",
            "iteration: 34151 train shape: (103644, 48, 48, 1)\n",
            "iteration: 34152 train shape: (103646, 48, 48, 1)\n",
            "iteration: 34153 train shape: (103648, 48, 48, 1)\n",
            "iteration: 34154 train shape: (103650, 48, 48, 1)\n",
            "iteration: 34155 train shape: (103652, 48, 48, 1)\n",
            "iteration: 34156 train shape: (103654, 48, 48, 1)\n",
            "iteration: 34157 train shape: (103656, 48, 48, 1)\n",
            "iteration: 34158 train shape: (103658, 48, 48, 1)\n",
            "iteration: 34159 train shape: (103660, 48, 48, 1)\n",
            "iteration: 34160 train shape: (103662, 48, 48, 1)\n",
            "iteration: 34161 train shape: (103664, 48, 48, 1)\n",
            "iteration: 34162 train shape: (103666, 48, 48, 1)\n",
            "iteration: 34163 train shape: (103668, 48, 48, 1)\n",
            "iteration: 34164 train shape: (103670, 48, 48, 1)\n",
            "iteration: 34165 train shape: (103672, 48, 48, 1)\n",
            "iteration: 34166 train shape: (103674, 48, 48, 1)\n",
            "iteration: 34167 train shape: (103676, 48, 48, 1)\n",
            "iteration: 34168 train shape: (103678, 48, 48, 1)\n",
            "iteration: 34169 train shape: (103680, 48, 48, 1)\n",
            "iteration: 34170 train shape: (103682, 48, 48, 1)\n",
            "iteration: 34171 train shape: (103684, 48, 48, 1)\n",
            "iteration: 34172 train shape: (103686, 48, 48, 1)\n",
            "iteration: 34173 train shape: (103688, 48, 48, 1)\n",
            "iteration: 34174 train shape: (103690, 48, 48, 1)\n",
            "iteration: 34175 train shape: (103692, 48, 48, 1)\n",
            "iteration: 34176 train shape: (103694, 48, 48, 1)\n",
            "iteration: 34177 train shape: (103696, 48, 48, 1)\n",
            "iteration: 34178 train shape: (103698, 48, 48, 1)\n",
            "iteration: 34179 train shape: (103700, 48, 48, 1)\n",
            "iteration: 34180 train shape: (103702, 48, 48, 1)\n",
            "iteration: 34181 train shape: (103704, 48, 48, 1)\n",
            "iteration: 34182 train shape: (103706, 48, 48, 1)\n",
            "iteration: 34183 train shape: (103708, 48, 48, 1)\n",
            "iteration: 34184 train shape: (103710, 48, 48, 1)\n",
            "iteration: 34185 train shape: (103712, 48, 48, 1)\n",
            "iteration: 34186 train shape: (103714, 48, 48, 1)\n",
            "iteration: 34187 train shape: (103716, 48, 48, 1)\n",
            "iteration: 34188 train shape: (103718, 48, 48, 1)\n",
            "iteration: 34189 train shape: (103720, 48, 48, 1)\n",
            "iteration: 34190 train shape: (103722, 48, 48, 1)\n",
            "iteration: 34191 train shape: (103724, 48, 48, 1)\n",
            "iteration: 34192 train shape: (103726, 48, 48, 1)\n",
            "iteration: 34193 train shape: (103728, 48, 48, 1)\n",
            "iteration: 34194 train shape: (103730, 48, 48, 1)\n",
            "iteration: 34195 train shape: (103732, 48, 48, 1)\n",
            "iteration: 34196 train shape: (103734, 48, 48, 1)\n",
            "iteration: 34197 train shape: (103736, 48, 48, 1)\n",
            "iteration: 34198 train shape: (103738, 48, 48, 1)\n",
            "iteration: 34199 train shape: (103740, 48, 48, 1)\n",
            "iteration: 34200 train shape: (103742, 48, 48, 1)\n",
            "iteration: 34201 train shape: (103744, 48, 48, 1)\n",
            "iteration: 34202 train shape: (103746, 48, 48, 1)\n",
            "iteration: 34203 train shape: (103748, 48, 48, 1)\n",
            "iteration: 34204 train shape: (103750, 48, 48, 1)\n",
            "iteration: 34205 train shape: (103752, 48, 48, 1)\n",
            "iteration: 34206 train shape: (103754, 48, 48, 1)\n",
            "iteration: 34207 train shape: (103756, 48, 48, 1)\n",
            "iteration: 34208 train shape: (103758, 48, 48, 1)\n",
            "iteration: 34209 train shape: (103760, 48, 48, 1)\n",
            "iteration: 34210 train shape: (103762, 48, 48, 1)\n",
            "iteration: 34211 train shape: (103764, 48, 48, 1)\n",
            "iteration: 34212 train shape: (103766, 48, 48, 1)\n",
            "iteration: 34213 train shape: (103768, 48, 48, 1)\n",
            "iteration: 34214 train shape: (103770, 48, 48, 1)\n",
            "iteration: 34215 train shape: (103772, 48, 48, 1)\n",
            "iteration: 34216 train shape: (103774, 48, 48, 1)\n",
            "iteration: 34217 train shape: (103776, 48, 48, 1)\n",
            "iteration: 34218 train shape: (103778, 48, 48, 1)\n",
            "iteration: 34219 train shape: (103780, 48, 48, 1)\n",
            "iteration: 34220 train shape: (103782, 48, 48, 1)\n",
            "iteration: 34221 train shape: (103784, 48, 48, 1)\n",
            "iteration: 34222 train shape: (103786, 48, 48, 1)\n",
            "iteration: 34223 train shape: (103788, 48, 48, 1)\n",
            "iteration: 34224 train shape: (103790, 48, 48, 1)\n",
            "iteration: 34225 train shape: (103792, 48, 48, 1)\n",
            "iteration: 34226 train shape: (103794, 48, 48, 1)\n",
            "iteration: 34227 train shape: (103796, 48, 48, 1)\n",
            "iteration: 34228 train shape: (103798, 48, 48, 1)\n",
            "iteration: 34229 train shape: (103800, 48, 48, 1)\n",
            "iteration: 34230 train shape: (103802, 48, 48, 1)\n",
            "iteration: 34231 train shape: (103804, 48, 48, 1)\n",
            "iteration: 34232 train shape: (103806, 48, 48, 1)\n",
            "iteration: 34233 train shape: (103808, 48, 48, 1)\n",
            "iteration: 34234 train shape: (103810, 48, 48, 1)\n",
            "iteration: 34235 train shape: (103812, 48, 48, 1)\n",
            "iteration: 34236 train shape: (103814, 48, 48, 1)\n",
            "iteration: 34237 train shape: (103816, 48, 48, 1)\n",
            "iteration: 34238 train shape: (103818, 48, 48, 1)\n",
            "iteration: 34239 train shape: (103820, 48, 48, 1)\n",
            "iteration: 34240 train shape: (103822, 48, 48, 1)\n",
            "iteration: 34241 train shape: (103824, 48, 48, 1)\n",
            "iteration: 34242 train shape: (103826, 48, 48, 1)\n",
            "iteration: 34243 train shape: (103828, 48, 48, 1)\n",
            "iteration: 34244 train shape: (103830, 48, 48, 1)\n",
            "iteration: 34245 train shape: (103832, 48, 48, 1)\n",
            "iteration: 34246 train shape: (103834, 48, 48, 1)\n",
            "iteration: 34247 train shape: (103836, 48, 48, 1)\n",
            "iteration: 34248 train shape: (103838, 48, 48, 1)\n",
            "iteration: 34249 train shape: (103840, 48, 48, 1)\n",
            "iteration: 34250 train shape: (103842, 48, 48, 1)\n",
            "iteration: 34251 train shape: (103844, 48, 48, 1)\n",
            "iteration: 34252 train shape: (103846, 48, 48, 1)\n",
            "iteration: 34253 train shape: (103848, 48, 48, 1)\n",
            "iteration: 34254 train shape: (103850, 48, 48, 1)\n",
            "iteration: 34255 train shape: (103852, 48, 48, 1)\n",
            "iteration: 34256 train shape: (103854, 48, 48, 1)\n",
            "iteration: 34257 train shape: (103856, 48, 48, 1)\n",
            "iteration: 34258 train shape: (103858, 48, 48, 1)\n",
            "iteration: 34259 train shape: (103860, 48, 48, 1)\n",
            "iteration: 34260 train shape: (103862, 48, 48, 1)\n",
            "iteration: 34261 train shape: (103864, 48, 48, 1)\n",
            "iteration: 34262 train shape: (103866, 48, 48, 1)\n",
            "iteration: 34263 train shape: (103868, 48, 48, 1)\n",
            "iteration: 34264 train shape: (103870, 48, 48, 1)\n",
            "iteration: 34265 train shape: (103872, 48, 48, 1)\n",
            "iteration: 34266 train shape: (103874, 48, 48, 1)\n",
            "iteration: 34267 train shape: (103876, 48, 48, 1)\n",
            "iteration: 34268 train shape: (103878, 48, 48, 1)\n",
            "iteration: 34269 train shape: (103880, 48, 48, 1)\n",
            "iteration: 34270 train shape: (103882, 48, 48, 1)\n",
            "iteration: 34271 train shape: (103884, 48, 48, 1)\n",
            "iteration: 34272 train shape: (103886, 48, 48, 1)\n",
            "iteration: 34273 train shape: (103888, 48, 48, 1)\n",
            "iteration: 34274 train shape: (103890, 48, 48, 1)\n",
            "iteration: 34275 train shape: (103892, 48, 48, 1)\n",
            "iteration: 34276 train shape: (103894, 48, 48, 1)\n",
            "iteration: 34277 train shape: (103896, 48, 48, 1)\n",
            "iteration: 34278 train shape: (103898, 48, 48, 1)\n",
            "iteration: 34279 train shape: (103900, 48, 48, 1)\n",
            "iteration: 34280 train shape: (103902, 48, 48, 1)\n",
            "iteration: 34281 train shape: (103904, 48, 48, 1)\n",
            "iteration: 34282 train shape: (103906, 48, 48, 1)\n",
            "iteration: 34283 train shape: (103908, 48, 48, 1)\n",
            "iteration: 34284 train shape: (103910, 48, 48, 1)\n",
            "iteration: 34285 train shape: (103912, 48, 48, 1)\n",
            "iteration: 34286 train shape: (103914, 48, 48, 1)\n",
            "iteration: 34287 train shape: (103916, 48, 48, 1)\n",
            "iteration: 34288 train shape: (103918, 48, 48, 1)\n",
            "iteration: 34289 train shape: (103920, 48, 48, 1)\n",
            "iteration: 34290 train shape: (103922, 48, 48, 1)\n",
            "iteration: 34291 train shape: (103924, 48, 48, 1)\n",
            "iteration: 34292 train shape: (103926, 48, 48, 1)\n",
            "iteration: 34293 train shape: (103928, 48, 48, 1)\n",
            "iteration: 34294 train shape: (103930, 48, 48, 1)\n",
            "iteration: 34295 train shape: (103932, 48, 48, 1)\n",
            "iteration: 34296 train shape: (103934, 48, 48, 1)\n",
            "iteration: 34297 train shape: (103936, 48, 48, 1)\n",
            "iteration: 34298 train shape: (103938, 48, 48, 1)\n",
            "iteration: 34299 train shape: (103940, 48, 48, 1)\n",
            "iteration: 34300 train shape: (103942, 48, 48, 1)\n",
            "iteration: 34301 train shape: (103944, 48, 48, 1)\n",
            "iteration: 34302 train shape: (103946, 48, 48, 1)\n",
            "iteration: 34303 train shape: (103948, 48, 48, 1)\n",
            "iteration: 34304 train shape: (103950, 48, 48, 1)\n",
            "iteration: 34305 train shape: (103952, 48, 48, 1)\n",
            "iteration: 34306 train shape: (103954, 48, 48, 1)\n",
            "iteration: 34307 train shape: (103956, 48, 48, 1)\n",
            "iteration: 34308 train shape: (103958, 48, 48, 1)\n",
            "iteration: 34309 train shape: (103960, 48, 48, 1)\n",
            "iteration: 34310 train shape: (103962, 48, 48, 1)\n",
            "iteration: 34311 train shape: (103964, 48, 48, 1)\n",
            "iteration: 34312 train shape: (103966, 48, 48, 1)\n",
            "iteration: 34313 train shape: (103968, 48, 48, 1)\n",
            "iteration: 34314 train shape: (103970, 48, 48, 1)\n",
            "iteration: 34315 train shape: (103972, 48, 48, 1)\n",
            "iteration: 34316 train shape: (103974, 48, 48, 1)\n",
            "iteration: 34317 train shape: (103976, 48, 48, 1)\n",
            "iteration: 34318 train shape: (103978, 48, 48, 1)\n",
            "iteration: 34319 train shape: (103980, 48, 48, 1)\n",
            "iteration: 34320 train shape: (103982, 48, 48, 1)\n",
            "iteration: 34321 train shape: (103984, 48, 48, 1)\n",
            "iteration: 34322 train shape: (103986, 48, 48, 1)\n",
            "iteration: 34323 train shape: (103988, 48, 48, 1)\n",
            "iteration: 34324 train shape: (103990, 48, 48, 1)\n",
            "iteration: 34325 train shape: (103992, 48, 48, 1)\n",
            "iteration: 34326 train shape: (103994, 48, 48, 1)\n",
            "iteration: 34327 train shape: (103996, 48, 48, 1)\n",
            "iteration: 34328 train shape: (103998, 48, 48, 1)\n",
            "iteration: 34329 train shape: (104000, 48, 48, 1)\n",
            "iteration: 34330 train shape: (104002, 48, 48, 1)\n",
            "iteration: 34331 train shape: (104004, 48, 48, 1)\n",
            "iteration: 34332 train shape: (104006, 48, 48, 1)\n",
            "iteration: 34333 train shape: (104008, 48, 48, 1)\n",
            "iteration: 34334 train shape: (104010, 48, 48, 1)\n",
            "iteration: 34335 train shape: (104012, 48, 48, 1)\n",
            "iteration: 34336 train shape: (104014, 48, 48, 1)\n",
            "iteration: 34337 train shape: (104016, 48, 48, 1)\n",
            "iteration: 34338 train shape: (104018, 48, 48, 1)\n",
            "iteration: 34339 train shape: (104020, 48, 48, 1)\n",
            "iteration: 34340 train shape: (104022, 48, 48, 1)\n",
            "iteration: 34341 train shape: (104024, 48, 48, 1)\n",
            "iteration: 34342 train shape: (104026, 48, 48, 1)\n",
            "iteration: 34343 train shape: (104028, 48, 48, 1)\n",
            "iteration: 34344 train shape: (104030, 48, 48, 1)\n",
            "iteration: 34345 train shape: (104032, 48, 48, 1)\n",
            "iteration: 34346 train shape: (104034, 48, 48, 1)\n",
            "iteration: 34347 train shape: (104036, 48, 48, 1)\n",
            "iteration: 34348 train shape: (104038, 48, 48, 1)\n",
            "iteration: 34349 train shape: (104040, 48, 48, 1)\n",
            "iteration: 34350 train shape: (104042, 48, 48, 1)\n",
            "iteration: 34351 train shape: (104044, 48, 48, 1)\n",
            "iteration: 34352 train shape: (104046, 48, 48, 1)\n",
            "iteration: 34353 train shape: (104048, 48, 48, 1)\n",
            "iteration: 34354 train shape: (104050, 48, 48, 1)\n",
            "iteration: 34355 train shape: (104052, 48, 48, 1)\n",
            "iteration: 34356 train shape: (104054, 48, 48, 1)\n",
            "iteration: 34357 train shape: (104056, 48, 48, 1)\n",
            "iteration: 34358 train shape: (104058, 48, 48, 1)\n",
            "iteration: 34359 train shape: (104060, 48, 48, 1)\n",
            "iteration: 34360 train shape: (104062, 48, 48, 1)\n",
            "iteration: 34361 train shape: (104064, 48, 48, 1)\n",
            "iteration: 34362 train shape: (104066, 48, 48, 1)\n",
            "iteration: 34363 train shape: (104068, 48, 48, 1)\n",
            "iteration: 34364 train shape: (104070, 48, 48, 1)\n",
            "iteration: 34365 train shape: (104072, 48, 48, 1)\n",
            "iteration: 34366 train shape: (104074, 48, 48, 1)\n",
            "iteration: 34367 train shape: (104076, 48, 48, 1)\n",
            "iteration: 34368 train shape: (104078, 48, 48, 1)\n",
            "iteration: 34369 train shape: (104080, 48, 48, 1)\n",
            "iteration: 34370 train shape: (104082, 48, 48, 1)\n",
            "iteration: 34371 train shape: (104084, 48, 48, 1)\n",
            "iteration: 34372 train shape: (104086, 48, 48, 1)\n",
            "iteration: 34373 train shape: (104088, 48, 48, 1)\n",
            "iteration: 34374 train shape: (104090, 48, 48, 1)\n",
            "iteration: 34375 train shape: (104092, 48, 48, 1)\n",
            "iteration: 34376 train shape: (104094, 48, 48, 1)\n",
            "iteration: 34377 train shape: (104096, 48, 48, 1)\n",
            "iteration: 34378 train shape: (104098, 48, 48, 1)\n",
            "iteration: 34379 train shape: (104100, 48, 48, 1)\n",
            "iteration: 34380 train shape: (104102, 48, 48, 1)\n",
            "iteration: 34381 train shape: (104104, 48, 48, 1)\n",
            "iteration: 34382 train shape: (104106, 48, 48, 1)\n",
            "iteration: 34383 train shape: (104108, 48, 48, 1)\n",
            "iteration: 34384 train shape: (104110, 48, 48, 1)\n",
            "iteration: 34385 train shape: (104112, 48, 48, 1)\n",
            "iteration: 34386 train shape: (104114, 48, 48, 1)\n",
            "iteration: 34387 train shape: (104116, 48, 48, 1)\n",
            "iteration: 34388 train shape: (104118, 48, 48, 1)\n",
            "iteration: 34389 train shape: (104120, 48, 48, 1)\n",
            "iteration: 34390 train shape: (104122, 48, 48, 1)\n",
            "iteration: 34391 train shape: (104124, 48, 48, 1)\n",
            "iteration: 34392 train shape: (104126, 48, 48, 1)\n",
            "iteration: 34393 train shape: (104128, 48, 48, 1)\n",
            "iteration: 34394 train shape: (104130, 48, 48, 1)\n",
            "iteration: 34395 train shape: (104132, 48, 48, 1)\n",
            "iteration: 34396 train shape: (104134, 48, 48, 1)\n",
            "iteration: 34397 train shape: (104136, 48, 48, 1)\n",
            "iteration: 34398 train shape: (104138, 48, 48, 1)\n",
            "iteration: 34399 train shape: (104140, 48, 48, 1)\n",
            "iteration: 34400 train shape: (104142, 48, 48, 1)\n",
            "iteration: 34401 train shape: (104144, 48, 48, 1)\n",
            "iteration: 34402 train shape: (104146, 48, 48, 1)\n",
            "iteration: 34403 train shape: (104148, 48, 48, 1)\n",
            "iteration: 34404 train shape: (104150, 48, 48, 1)\n",
            "iteration: 34405 train shape: (104152, 48, 48, 1)\n",
            "iteration: 34406 train shape: (104154, 48, 48, 1)\n",
            "iteration: 34407 train shape: (104156, 48, 48, 1)\n",
            "iteration: 34408 train shape: (104158, 48, 48, 1)\n",
            "iteration: 34409 train shape: (104160, 48, 48, 1)\n",
            "iteration: 34410 train shape: (104162, 48, 48, 1)\n",
            "iteration: 34411 train shape: (104164, 48, 48, 1)\n",
            "iteration: 34412 train shape: (104166, 48, 48, 1)\n",
            "iteration: 34413 train shape: (104168, 48, 48, 1)\n",
            "iteration: 34414 train shape: (104170, 48, 48, 1)\n",
            "iteration: 34415 train shape: (104172, 48, 48, 1)\n",
            "iteration: 34416 train shape: (104174, 48, 48, 1)\n",
            "iteration: 34417 train shape: (104176, 48, 48, 1)\n",
            "iteration: 34418 train shape: (104178, 48, 48, 1)\n",
            "iteration: 34419 train shape: (104180, 48, 48, 1)\n",
            "iteration: 34420 train shape: (104182, 48, 48, 1)\n",
            "iteration: 34421 train shape: (104184, 48, 48, 1)\n",
            "iteration: 34422 train shape: (104186, 48, 48, 1)\n",
            "iteration: 34423 train shape: (104188, 48, 48, 1)\n",
            "iteration: 34424 train shape: (104190, 48, 48, 1)\n",
            "iteration: 34425 train shape: (104192, 48, 48, 1)\n",
            "iteration: 34426 train shape: (104194, 48, 48, 1)\n",
            "iteration: 34427 train shape: (104196, 48, 48, 1)\n",
            "iteration: 34428 train shape: (104198, 48, 48, 1)\n",
            "iteration: 34429 train shape: (104200, 48, 48, 1)\n",
            "iteration: 34430 train shape: (104202, 48, 48, 1)\n",
            "iteration: 34431 train shape: (104204, 48, 48, 1)\n",
            "iteration: 34432 train shape: (104206, 48, 48, 1)\n",
            "iteration: 34433 train shape: (104208, 48, 48, 1)\n",
            "iteration: 34434 train shape: (104210, 48, 48, 1)\n",
            "iteration: 34435 train shape: (104212, 48, 48, 1)\n",
            "iteration: 34436 train shape: (104214, 48, 48, 1)\n",
            "iteration: 34437 train shape: (104216, 48, 48, 1)\n",
            "iteration: 34438 train shape: (104218, 48, 48, 1)\n",
            "iteration: 34439 train shape: (104220, 48, 48, 1)\n",
            "iteration: 34440 train shape: (104222, 48, 48, 1)\n",
            "iteration: 34441 train shape: (104224, 48, 48, 1)\n",
            "iteration: 34442 train shape: (104226, 48, 48, 1)\n",
            "iteration: 34443 train shape: (104228, 48, 48, 1)\n",
            "iteration: 34444 train shape: (104230, 48, 48, 1)\n",
            "iteration: 34445 train shape: (104232, 48, 48, 1)\n",
            "iteration: 34446 train shape: (104234, 48, 48, 1)\n",
            "iteration: 34447 train shape: (104236, 48, 48, 1)\n",
            "iteration: 34448 train shape: (104238, 48, 48, 1)\n",
            "iteration: 34449 train shape: (104240, 48, 48, 1)\n",
            "iteration: 34450 train shape: (104242, 48, 48, 1)\n",
            "iteration: 34451 train shape: (104244, 48, 48, 1)\n",
            "iteration: 34452 train shape: (104246, 48, 48, 1)\n",
            "iteration: 34453 train shape: (104248, 48, 48, 1)\n",
            "iteration: 34454 train shape: (104250, 48, 48, 1)\n",
            "iteration: 34455 train shape: (104252, 48, 48, 1)\n",
            "iteration: 34456 train shape: (104254, 48, 48, 1)\n",
            "iteration: 34457 train shape: (104256, 48, 48, 1)\n",
            "iteration: 34458 train shape: (104258, 48, 48, 1)\n",
            "iteration: 34459 train shape: (104260, 48, 48, 1)\n",
            "iteration: 34460 train shape: (104262, 48, 48, 1)\n",
            "iteration: 34461 train shape: (104264, 48, 48, 1)\n",
            "iteration: 34462 train shape: (104266, 48, 48, 1)\n",
            "iteration: 34463 train shape: (104268, 48, 48, 1)\n",
            "iteration: 34464 train shape: (104270, 48, 48, 1)\n",
            "iteration: 34465 train shape: (104272, 48, 48, 1)\n",
            "iteration: 34466 train shape: (104274, 48, 48, 1)\n",
            "iteration: 34467 train shape: (104276, 48, 48, 1)\n",
            "iteration: 34468 train shape: (104278, 48, 48, 1)\n",
            "iteration: 34469 train shape: (104280, 48, 48, 1)\n",
            "iteration: 34470 train shape: (104282, 48, 48, 1)\n",
            "iteration: 34471 train shape: (104284, 48, 48, 1)\n",
            "iteration: 34472 train shape: (104286, 48, 48, 1)\n",
            "iteration: 34473 train shape: (104288, 48, 48, 1)\n",
            "iteration: 34474 train shape: (104290, 48, 48, 1)\n",
            "iteration: 34475 train shape: (104292, 48, 48, 1)\n",
            "iteration: 34476 train shape: (104294, 48, 48, 1)\n",
            "iteration: 34477 train shape: (104296, 48, 48, 1)\n",
            "iteration: 34478 train shape: (104298, 48, 48, 1)\n",
            "iteration: 34479 train shape: (104300, 48, 48, 1)\n",
            "iteration: 34480 train shape: (104302, 48, 48, 1)\n",
            "iteration: 34481 train shape: (104304, 48, 48, 1)\n",
            "iteration: 34482 train shape: (104306, 48, 48, 1)\n",
            "iteration: 34483 train shape: (104308, 48, 48, 1)\n",
            "iteration: 34484 train shape: (104310, 48, 48, 1)\n",
            "iteration: 34485 train shape: (104312, 48, 48, 1)\n",
            "iteration: 34486 train shape: (104314, 48, 48, 1)\n",
            "iteration: 34487 train shape: (104316, 48, 48, 1)\n",
            "iteration: 34488 train shape: (104318, 48, 48, 1)\n",
            "iteration: 34489 train shape: (104320, 48, 48, 1)\n",
            "iteration: 34490 train shape: (104322, 48, 48, 1)\n",
            "iteration: 34491 train shape: (104324, 48, 48, 1)\n",
            "iteration: 34492 train shape: (104326, 48, 48, 1)\n",
            "iteration: 34493 train shape: (104328, 48, 48, 1)\n",
            "iteration: 34494 train shape: (104330, 48, 48, 1)\n",
            "iteration: 34495 train shape: (104332, 48, 48, 1)\n",
            "iteration: 34496 train shape: (104334, 48, 48, 1)\n",
            "iteration: 34497 train shape: (104336, 48, 48, 1)\n",
            "iteration: 34498 train shape: (104338, 48, 48, 1)\n",
            "iteration: 34499 train shape: (104340, 48, 48, 1)\n",
            "iteration: 34500 train shape: (104342, 48, 48, 1)\n",
            "iteration: 34501 train shape: (104344, 48, 48, 1)\n",
            "iteration: 34502 train shape: (104346, 48, 48, 1)\n",
            "iteration: 34503 train shape: (104348, 48, 48, 1)\n",
            "iteration: 34504 train shape: (104350, 48, 48, 1)\n",
            "iteration: 34505 train shape: (104352, 48, 48, 1)\n",
            "iteration: 34506 train shape: (104354, 48, 48, 1)\n",
            "iteration: 34507 train shape: (104356, 48, 48, 1)\n",
            "iteration: 34508 train shape: (104358, 48, 48, 1)\n",
            "iteration: 34509 train shape: (104360, 48, 48, 1)\n",
            "iteration: 34510 train shape: (104362, 48, 48, 1)\n",
            "iteration: 34511 train shape: (104364, 48, 48, 1)\n",
            "iteration: 34512 train shape: (104366, 48, 48, 1)\n",
            "iteration: 34513 train shape: (104368, 48, 48, 1)\n",
            "iteration: 34514 train shape: (104370, 48, 48, 1)\n",
            "iteration: 34515 train shape: (104372, 48, 48, 1)\n",
            "iteration: 34516 train shape: (104374, 48, 48, 1)\n",
            "iteration: 34517 train shape: (104376, 48, 48, 1)\n",
            "iteration: 34518 train shape: (104378, 48, 48, 1)\n",
            "iteration: 34519 train shape: (104380, 48, 48, 1)\n",
            "iteration: 34520 train shape: (104382, 48, 48, 1)\n",
            "iteration: 34521 train shape: (104384, 48, 48, 1)\n",
            "iteration: 34522 train shape: (104386, 48, 48, 1)\n",
            "iteration: 34523 train shape: (104388, 48, 48, 1)\n",
            "iteration: 34524 train shape: (104390, 48, 48, 1)\n",
            "iteration: 34525 train shape: (104392, 48, 48, 1)\n",
            "iteration: 34526 train shape: (104394, 48, 48, 1)\n",
            "iteration: 34527 train shape: (104396, 48, 48, 1)\n",
            "iteration: 34528 train shape: (104398, 48, 48, 1)\n",
            "iteration: 34529 train shape: (104400, 48, 48, 1)\n",
            "iteration: 34530 train shape: (104402, 48, 48, 1)\n",
            "iteration: 34531 train shape: (104404, 48, 48, 1)\n",
            "iteration: 34532 train shape: (104406, 48, 48, 1)\n",
            "iteration: 34533 train shape: (104408, 48, 48, 1)\n",
            "iteration: 34534 train shape: (104410, 48, 48, 1)\n",
            "iteration: 34535 train shape: (104412, 48, 48, 1)\n",
            "iteration: 34536 train shape: (104414, 48, 48, 1)\n",
            "iteration: 34537 train shape: (104416, 48, 48, 1)\n",
            "iteration: 34538 train shape: (104418, 48, 48, 1)\n",
            "iteration: 34539 train shape: (104420, 48, 48, 1)\n",
            "iteration: 34540 train shape: (104422, 48, 48, 1)\n",
            "iteration: 34541 train shape: (104424, 48, 48, 1)\n",
            "iteration: 34542 train shape: (104426, 48, 48, 1)\n",
            "iteration: 34543 train shape: (104428, 48, 48, 1)\n",
            "iteration: 34544 train shape: (104430, 48, 48, 1)\n",
            "iteration: 34545 train shape: (104432, 48, 48, 1)\n",
            "iteration: 34546 train shape: (104434, 48, 48, 1)\n",
            "iteration: 34547 train shape: (104436, 48, 48, 1)\n",
            "iteration: 34548 train shape: (104438, 48, 48, 1)\n",
            "iteration: 34549 train shape: (104440, 48, 48, 1)\n",
            "iteration: 34550 train shape: (104442, 48, 48, 1)\n",
            "iteration: 34551 train shape: (104444, 48, 48, 1)\n",
            "iteration: 34552 train shape: (104446, 48, 48, 1)\n",
            "iteration: 34553 train shape: (104448, 48, 48, 1)\n",
            "iteration: 34554 train shape: (104450, 48, 48, 1)\n",
            "iteration: 34555 train shape: (104452, 48, 48, 1)\n",
            "iteration: 34556 train shape: (104454, 48, 48, 1)\n",
            "iteration: 34557 train shape: (104456, 48, 48, 1)\n",
            "iteration: 34558 train shape: (104458, 48, 48, 1)\n",
            "iteration: 34559 train shape: (104460, 48, 48, 1)\n",
            "iteration: 34560 train shape: (104462, 48, 48, 1)\n",
            "iteration: 34561 train shape: (104464, 48, 48, 1)\n",
            "iteration: 34562 train shape: (104466, 48, 48, 1)\n",
            "iteration: 34563 train shape: (104468, 48, 48, 1)\n",
            "iteration: 34564 train shape: (104470, 48, 48, 1)\n",
            "iteration: 34565 train shape: (104472, 48, 48, 1)\n",
            "iteration: 34566 train shape: (104474, 48, 48, 1)\n",
            "iteration: 34567 train shape: (104476, 48, 48, 1)\n",
            "iteration: 34568 train shape: (104478, 48, 48, 1)\n",
            "iteration: 34569 train shape: (104480, 48, 48, 1)\n",
            "iteration: 34570 train shape: (104482, 48, 48, 1)\n",
            "iteration: 34571 train shape: (104484, 48, 48, 1)\n",
            "iteration: 34572 train shape: (104486, 48, 48, 1)\n",
            "iteration: 34573 train shape: (104488, 48, 48, 1)\n",
            "iteration: 34574 train shape: (104490, 48, 48, 1)\n",
            "iteration: 34575 train shape: (104492, 48, 48, 1)\n",
            "iteration: 34576 train shape: (104494, 48, 48, 1)\n",
            "iteration: 34577 train shape: (104496, 48, 48, 1)\n",
            "iteration: 34578 train shape: (104498, 48, 48, 1)\n",
            "iteration: 34579 train shape: (104500, 48, 48, 1)\n",
            "iteration: 34580 train shape: (104502, 48, 48, 1)\n",
            "iteration: 34581 train shape: (104504, 48, 48, 1)\n",
            "iteration: 34582 train shape: (104506, 48, 48, 1)\n",
            "iteration: 34583 train shape: (104508, 48, 48, 1)\n",
            "iteration: 34584 train shape: (104510, 48, 48, 1)\n",
            "iteration: 34585 train shape: (104512, 48, 48, 1)\n",
            "iteration: 34586 train shape: (104514, 48, 48, 1)\n",
            "iteration: 34587 train shape: (104516, 48, 48, 1)\n",
            "iteration: 34588 train shape: (104518, 48, 48, 1)\n",
            "iteration: 34589 train shape: (104520, 48, 48, 1)\n",
            "iteration: 34590 train shape: (104522, 48, 48, 1)\n",
            "iteration: 34591 train shape: (104524, 48, 48, 1)\n",
            "iteration: 34592 train shape: (104526, 48, 48, 1)\n",
            "iteration: 34593 train shape: (104528, 48, 48, 1)\n",
            "iteration: 34594 train shape: (104530, 48, 48, 1)\n",
            "iteration: 34595 train shape: (104532, 48, 48, 1)\n",
            "iteration: 34596 train shape: (104534, 48, 48, 1)\n",
            "iteration: 34597 train shape: (104536, 48, 48, 1)\n",
            "iteration: 34598 train shape: (104538, 48, 48, 1)\n",
            "iteration: 34599 train shape: (104540, 48, 48, 1)\n",
            "iteration: 34600 train shape: (104542, 48, 48, 1)\n",
            "iteration: 34601 train shape: (104544, 48, 48, 1)\n",
            "iteration: 34602 train shape: (104546, 48, 48, 1)\n",
            "iteration: 34603 train shape: (104548, 48, 48, 1)\n",
            "iteration: 34604 train shape: (104550, 48, 48, 1)\n",
            "iteration: 34605 train shape: (104552, 48, 48, 1)\n",
            "iteration: 34606 train shape: (104554, 48, 48, 1)\n",
            "iteration: 34607 train shape: (104556, 48, 48, 1)\n",
            "iteration: 34608 train shape: (104558, 48, 48, 1)\n",
            "iteration: 34609 train shape: (104560, 48, 48, 1)\n",
            "iteration: 34610 train shape: (104562, 48, 48, 1)\n",
            "iteration: 34611 train shape: (104564, 48, 48, 1)\n",
            "iteration: 34612 train shape: (104566, 48, 48, 1)\n",
            "iteration: 34613 train shape: (104568, 48, 48, 1)\n",
            "iteration: 34614 train shape: (104570, 48, 48, 1)\n",
            "iteration: 34615 train shape: (104572, 48, 48, 1)\n",
            "iteration: 34616 train shape: (104574, 48, 48, 1)\n",
            "iteration: 34617 train shape: (104576, 48, 48, 1)\n",
            "iteration: 34618 train shape: (104578, 48, 48, 1)\n",
            "iteration: 34619 train shape: (104580, 48, 48, 1)\n",
            "iteration: 34620 train shape: (104582, 48, 48, 1)\n",
            "iteration: 34621 train shape: (104584, 48, 48, 1)\n",
            "iteration: 34622 train shape: (104586, 48, 48, 1)\n",
            "iteration: 34623 train shape: (104588, 48, 48, 1)\n",
            "iteration: 34624 train shape: (104590, 48, 48, 1)\n",
            "iteration: 34625 train shape: (104592, 48, 48, 1)\n",
            "iteration: 34626 train shape: (104594, 48, 48, 1)\n",
            "iteration: 34627 train shape: (104596, 48, 48, 1)\n",
            "iteration: 34628 train shape: (104598, 48, 48, 1)\n",
            "iteration: 34629 train shape: (104600, 48, 48, 1)\n",
            "iteration: 34630 train shape: (104602, 48, 48, 1)\n",
            "iteration: 34631 train shape: (104604, 48, 48, 1)\n",
            "iteration: 34632 train shape: (104606, 48, 48, 1)\n",
            "iteration: 34633 train shape: (104608, 48, 48, 1)\n",
            "iteration: 34634 train shape: (104610, 48, 48, 1)\n",
            "iteration: 34635 train shape: (104612, 48, 48, 1)\n",
            "iteration: 34636 train shape: (104614, 48, 48, 1)\n",
            "iteration: 34637 train shape: (104616, 48, 48, 1)\n",
            "iteration: 34638 train shape: (104618, 48, 48, 1)\n",
            "iteration: 34639 train shape: (104620, 48, 48, 1)\n",
            "iteration: 34640 train shape: (104622, 48, 48, 1)\n",
            "iteration: 34641 train shape: (104624, 48, 48, 1)\n",
            "iteration: 34642 train shape: (104626, 48, 48, 1)\n",
            "iteration: 34643 train shape: (104628, 48, 48, 1)\n",
            "iteration: 34644 train shape: (104630, 48, 48, 1)\n",
            "iteration: 34645 train shape: (104632, 48, 48, 1)\n",
            "iteration: 34646 train shape: (104634, 48, 48, 1)\n",
            "iteration: 34647 train shape: (104636, 48, 48, 1)\n",
            "iteration: 34648 train shape: (104638, 48, 48, 1)\n",
            "iteration: 34649 train shape: (104640, 48, 48, 1)\n",
            "iteration: 34650 train shape: (104642, 48, 48, 1)\n",
            "iteration: 34651 train shape: (104644, 48, 48, 1)\n",
            "iteration: 34652 train shape: (104646, 48, 48, 1)\n",
            "iteration: 34653 train shape: (104648, 48, 48, 1)\n",
            "iteration: 34654 train shape: (104650, 48, 48, 1)\n",
            "iteration: 34655 train shape: (104652, 48, 48, 1)\n",
            "iteration: 34656 train shape: (104654, 48, 48, 1)\n",
            "iteration: 34657 train shape: (104656, 48, 48, 1)\n",
            "iteration: 34658 train shape: (104658, 48, 48, 1)\n",
            "iteration: 34659 train shape: (104660, 48, 48, 1)\n",
            "iteration: 34660 train shape: (104662, 48, 48, 1)\n",
            "iteration: 34661 train shape: (104664, 48, 48, 1)\n",
            "iteration: 34662 train shape: (104666, 48, 48, 1)\n",
            "iteration: 34663 train shape: (104668, 48, 48, 1)\n",
            "iteration: 34664 train shape: (104670, 48, 48, 1)\n",
            "iteration: 34665 train shape: (104672, 48, 48, 1)\n",
            "iteration: 34666 train shape: (104674, 48, 48, 1)\n",
            "iteration: 34667 train shape: (104676, 48, 48, 1)\n",
            "iteration: 34668 train shape: (104678, 48, 48, 1)\n",
            "iteration: 34669 train shape: (104680, 48, 48, 1)\n",
            "iteration: 34670 train shape: (104682, 48, 48, 1)\n",
            "iteration: 34671 train shape: (104684, 48, 48, 1)\n",
            "iteration: 34672 train shape: (104686, 48, 48, 1)\n",
            "iteration: 34673 train shape: (104688, 48, 48, 1)\n",
            "iteration: 34674 train shape: (104690, 48, 48, 1)\n",
            "iteration: 34675 train shape: (104692, 48, 48, 1)\n",
            "iteration: 34676 train shape: (104694, 48, 48, 1)\n",
            "iteration: 34677 train shape: (104696, 48, 48, 1)\n",
            "iteration: 34678 train shape: (104698, 48, 48, 1)\n",
            "iteration: 34679 train shape: (104700, 48, 48, 1)\n",
            "iteration: 34680 train shape: (104702, 48, 48, 1)\n",
            "iteration: 34681 train shape: (104704, 48, 48, 1)\n",
            "iteration: 34682 train shape: (104706, 48, 48, 1)\n",
            "iteration: 34683 train shape: (104708, 48, 48, 1)\n",
            "iteration: 34684 train shape: (104710, 48, 48, 1)\n",
            "iteration: 34685 train shape: (104712, 48, 48, 1)\n",
            "iteration: 34686 train shape: (104714, 48, 48, 1)\n",
            "iteration: 34687 train shape: (104716, 48, 48, 1)\n",
            "iteration: 34688 train shape: (104718, 48, 48, 1)\n",
            "iteration: 34689 train shape: (104720, 48, 48, 1)\n",
            "iteration: 34690 train shape: (104722, 48, 48, 1)\n",
            "iteration: 34691 train shape: (104724, 48, 48, 1)\n",
            "iteration: 34692 train shape: (104726, 48, 48, 1)\n",
            "iteration: 34693 train shape: (104728, 48, 48, 1)\n",
            "iteration: 34694 train shape: (104730, 48, 48, 1)\n",
            "iteration: 34695 train shape: (104732, 48, 48, 1)\n",
            "iteration: 34696 train shape: (104734, 48, 48, 1)\n",
            "iteration: 34697 train shape: (104736, 48, 48, 1)\n",
            "iteration: 34698 train shape: (104738, 48, 48, 1)\n",
            "iteration: 34699 train shape: (104740, 48, 48, 1)\n",
            "iteration: 34700 train shape: (104742, 48, 48, 1)\n",
            "iteration: 34701 train shape: (104744, 48, 48, 1)\n",
            "iteration: 34702 train shape: (104746, 48, 48, 1)\n",
            "iteration: 34703 train shape: (104748, 48, 48, 1)\n",
            "iteration: 34704 train shape: (104750, 48, 48, 1)\n",
            "iteration: 34705 train shape: (104752, 48, 48, 1)\n",
            "iteration: 34706 train shape: (104754, 48, 48, 1)\n",
            "iteration: 34707 train shape: (104756, 48, 48, 1)\n",
            "iteration: 34708 train shape: (104758, 48, 48, 1)\n",
            "iteration: 34709 train shape: (104760, 48, 48, 1)\n",
            "iteration: 34710 train shape: (104762, 48, 48, 1)\n",
            "iteration: 34711 train shape: (104764, 48, 48, 1)\n",
            "iteration: 34712 train shape: (104766, 48, 48, 1)\n",
            "iteration: 34713 train shape: (104768, 48, 48, 1)\n",
            "iteration: 34714 train shape: (104770, 48, 48, 1)\n",
            "iteration: 34715 train shape: (104772, 48, 48, 1)\n",
            "iteration: 34716 train shape: (104774, 48, 48, 1)\n",
            "iteration: 34717 train shape: (104776, 48, 48, 1)\n",
            "iteration: 34718 train shape: (104778, 48, 48, 1)\n",
            "iteration: 34719 train shape: (104780, 48, 48, 1)\n",
            "iteration: 34720 train shape: (104782, 48, 48, 1)\n",
            "iteration: 34721 train shape: (104784, 48, 48, 1)\n",
            "iteration: 34722 train shape: (104786, 48, 48, 1)\n",
            "iteration: 34723 train shape: (104788, 48, 48, 1)\n",
            "iteration: 34724 train shape: (104790, 48, 48, 1)\n",
            "iteration: 34725 train shape: (104792, 48, 48, 1)\n",
            "iteration: 34726 train shape: (104794, 48, 48, 1)\n",
            "iteration: 34727 train shape: (104796, 48, 48, 1)\n",
            "iteration: 34728 train shape: (104798, 48, 48, 1)\n",
            "iteration: 34729 train shape: (104800, 48, 48, 1)\n",
            "iteration: 34730 train shape: (104802, 48, 48, 1)\n",
            "iteration: 34731 train shape: (104804, 48, 48, 1)\n",
            "iteration: 34732 train shape: (104806, 48, 48, 1)\n",
            "iteration: 34733 train shape: (104808, 48, 48, 1)\n",
            "iteration: 34734 train shape: (104810, 48, 48, 1)\n",
            "iteration: 34735 train shape: (104812, 48, 48, 1)\n",
            "iteration: 34736 train shape: (104814, 48, 48, 1)\n",
            "iteration: 34737 train shape: (104816, 48, 48, 1)\n",
            "iteration: 34738 train shape: (104818, 48, 48, 1)\n",
            "iteration: 34739 train shape: (104820, 48, 48, 1)\n",
            "iteration: 34740 train shape: (104822, 48, 48, 1)\n",
            "iteration: 34741 train shape: (104824, 48, 48, 1)\n",
            "iteration: 34742 train shape: (104826, 48, 48, 1)\n",
            "iteration: 34743 train shape: (104828, 48, 48, 1)\n",
            "iteration: 34744 train shape: (104830, 48, 48, 1)\n",
            "iteration: 34745 train shape: (104832, 48, 48, 1)\n",
            "iteration: 34746 train shape: (104834, 48, 48, 1)\n",
            "iteration: 34747 train shape: (104836, 48, 48, 1)\n",
            "iteration: 34748 train shape: (104838, 48, 48, 1)\n",
            "iteration: 34749 train shape: (104840, 48, 48, 1)\n",
            "iteration: 34750 train shape: (104842, 48, 48, 1)\n",
            "iteration: 34751 train shape: (104844, 48, 48, 1)\n",
            "iteration: 34752 train shape: (104846, 48, 48, 1)\n",
            "iteration: 34753 train shape: (104848, 48, 48, 1)\n",
            "iteration: 34754 train shape: (104850, 48, 48, 1)\n",
            "iteration: 34755 train shape: (104852, 48, 48, 1)\n",
            "iteration: 34756 train shape: (104854, 48, 48, 1)\n",
            "iteration: 34757 train shape: (104856, 48, 48, 1)\n",
            "iteration: 34758 train shape: (104858, 48, 48, 1)\n",
            "iteration: 34759 train shape: (104860, 48, 48, 1)\n",
            "iteration: 34760 train shape: (104862, 48, 48, 1)\n",
            "iteration: 34761 train shape: (104864, 48, 48, 1)\n",
            "iteration: 34762 train shape: (104866, 48, 48, 1)\n",
            "iteration: 34763 train shape: (104868, 48, 48, 1)\n",
            "iteration: 34764 train shape: (104870, 48, 48, 1)\n",
            "iteration: 34765 train shape: (104872, 48, 48, 1)\n",
            "iteration: 34766 train shape: (104874, 48, 48, 1)\n",
            "iteration: 34767 train shape: (104876, 48, 48, 1)\n",
            "iteration: 34768 train shape: (104878, 48, 48, 1)\n",
            "iteration: 34769 train shape: (104880, 48, 48, 1)\n",
            "iteration: 34770 train shape: (104882, 48, 48, 1)\n",
            "iteration: 34771 train shape: (104884, 48, 48, 1)\n",
            "iteration: 34772 train shape: (104886, 48, 48, 1)\n",
            "iteration: 34773 train shape: (104888, 48, 48, 1)\n",
            "iteration: 34774 train shape: (104890, 48, 48, 1)\n",
            "iteration: 34775 train shape: (104892, 48, 48, 1)\n",
            "iteration: 34776 train shape: (104894, 48, 48, 1)\n",
            "iteration: 34777 train shape: (104896, 48, 48, 1)\n",
            "iteration: 34778 train shape: (104898, 48, 48, 1)\n",
            "iteration: 34779 train shape: (104900, 48, 48, 1)\n",
            "iteration: 34780 train shape: (104902, 48, 48, 1)\n",
            "iteration: 34781 train shape: (104904, 48, 48, 1)\n",
            "iteration: 34782 train shape: (104906, 48, 48, 1)\n",
            "iteration: 34783 train shape: (104908, 48, 48, 1)\n",
            "iteration: 34784 train shape: (104910, 48, 48, 1)\n",
            "iteration: 34785 train shape: (104912, 48, 48, 1)\n",
            "iteration: 34786 train shape: (104914, 48, 48, 1)\n",
            "iteration: 34787 train shape: (104916, 48, 48, 1)\n",
            "iteration: 34788 train shape: (104918, 48, 48, 1)\n",
            "iteration: 34789 train shape: (104920, 48, 48, 1)\n",
            "iteration: 34790 train shape: (104922, 48, 48, 1)\n",
            "iteration: 34791 train shape: (104924, 48, 48, 1)\n",
            "iteration: 34792 train shape: (104926, 48, 48, 1)\n",
            "iteration: 34793 train shape: (104928, 48, 48, 1)\n",
            "iteration: 34794 train shape: (104930, 48, 48, 1)\n",
            "iteration: 34795 train shape: (104932, 48, 48, 1)\n",
            "iteration: 34796 train shape: (104934, 48, 48, 1)\n",
            "iteration: 34797 train shape: (104936, 48, 48, 1)\n",
            "iteration: 34798 train shape: (104938, 48, 48, 1)\n",
            "iteration: 34799 train shape: (104940, 48, 48, 1)\n",
            "iteration: 34800 train shape: (104942, 48, 48, 1)\n",
            "iteration: 34801 train shape: (104944, 48, 48, 1)\n",
            "iteration: 34802 train shape: (104946, 48, 48, 1)\n",
            "iteration: 34803 train shape: (104948, 48, 48, 1)\n",
            "iteration: 34804 train shape: (104950, 48, 48, 1)\n",
            "iteration: 34805 train shape: (104952, 48, 48, 1)\n",
            "iteration: 34806 train shape: (104954, 48, 48, 1)\n",
            "iteration: 34807 train shape: (104956, 48, 48, 1)\n",
            "iteration: 34808 train shape: (104958, 48, 48, 1)\n",
            "iteration: 34809 train shape: (104960, 48, 48, 1)\n",
            "iteration: 34810 train shape: (104962, 48, 48, 1)\n",
            "iteration: 34811 train shape: (104964, 48, 48, 1)\n",
            "iteration: 34812 train shape: (104966, 48, 48, 1)\n",
            "iteration: 34813 train shape: (104968, 48, 48, 1)\n",
            "iteration: 34814 train shape: (104970, 48, 48, 1)\n",
            "iteration: 34815 train shape: (104972, 48, 48, 1)\n",
            "iteration: 34816 train shape: (104974, 48, 48, 1)\n",
            "iteration: 34817 train shape: (104976, 48, 48, 1)\n",
            "iteration: 34818 train shape: (104978, 48, 48, 1)\n",
            "iteration: 34819 train shape: (104980, 48, 48, 1)\n",
            "iteration: 34820 train shape: (104982, 48, 48, 1)\n",
            "iteration: 34821 train shape: (104984, 48, 48, 1)\n",
            "iteration: 34822 train shape: (104986, 48, 48, 1)\n",
            "iteration: 34823 train shape: (104988, 48, 48, 1)\n",
            "iteration: 34824 train shape: (104990, 48, 48, 1)\n",
            "iteration: 34825 train shape: (104992, 48, 48, 1)\n",
            "iteration: 34826 train shape: (104994, 48, 48, 1)\n",
            "iteration: 34827 train shape: (104996, 48, 48, 1)\n",
            "iteration: 34828 train shape: (104998, 48, 48, 1)\n",
            "iteration: 34829 train shape: (105000, 48, 48, 1)\n",
            "iteration: 34830 train shape: (105002, 48, 48, 1)\n",
            "iteration: 34831 train shape: (105004, 48, 48, 1)\n",
            "iteration: 34832 train shape: (105006, 48, 48, 1)\n",
            "iteration: 34833 train shape: (105008, 48, 48, 1)\n",
            "iteration: 34834 train shape: (105010, 48, 48, 1)\n",
            "iteration: 34835 train shape: (105012, 48, 48, 1)\n",
            "iteration: 34836 train shape: (105014, 48, 48, 1)\n",
            "iteration: 34837 train shape: (105016, 48, 48, 1)\n",
            "iteration: 34838 train shape: (105018, 48, 48, 1)\n",
            "iteration: 34839 train shape: (105020, 48, 48, 1)\n",
            "iteration: 34840 train shape: (105022, 48, 48, 1)\n",
            "iteration: 34841 train shape: (105024, 48, 48, 1)\n",
            "iteration: 34842 train shape: (105026, 48, 48, 1)\n",
            "iteration: 34843 train shape: (105028, 48, 48, 1)\n",
            "iteration: 34844 train shape: (105030, 48, 48, 1)\n",
            "iteration: 34845 train shape: (105032, 48, 48, 1)\n",
            "iteration: 34846 train shape: (105034, 48, 48, 1)\n",
            "iteration: 34847 train shape: (105036, 48, 48, 1)\n",
            "iteration: 34848 train shape: (105038, 48, 48, 1)\n",
            "iteration: 34849 train shape: (105040, 48, 48, 1)\n",
            "iteration: 34850 train shape: (105042, 48, 48, 1)\n",
            "iteration: 34851 train shape: (105044, 48, 48, 1)\n",
            "iteration: 34852 train shape: (105046, 48, 48, 1)\n",
            "iteration: 34853 train shape: (105048, 48, 48, 1)\n",
            "iteration: 34854 train shape: (105050, 48, 48, 1)\n",
            "iteration: 34855 train shape: (105052, 48, 48, 1)\n",
            "iteration: 34856 train shape: (105054, 48, 48, 1)\n",
            "iteration: 34857 train shape: (105056, 48, 48, 1)\n",
            "iteration: 34858 train shape: (105058, 48, 48, 1)\n",
            "iteration: 34859 train shape: (105060, 48, 48, 1)\n",
            "iteration: 34860 train shape: (105062, 48, 48, 1)\n",
            "iteration: 34861 train shape: (105064, 48, 48, 1)\n",
            "iteration: 34862 train shape: (105066, 48, 48, 1)\n",
            "iteration: 34863 train shape: (105068, 48, 48, 1)\n",
            "iteration: 34864 train shape: (105070, 48, 48, 1)\n",
            "iteration: 34865 train shape: (105072, 48, 48, 1)\n",
            "iteration: 34866 train shape: (105074, 48, 48, 1)\n",
            "iteration: 34867 train shape: (105076, 48, 48, 1)\n",
            "iteration: 34868 train shape: (105078, 48, 48, 1)\n",
            "iteration: 34869 train shape: (105080, 48, 48, 1)\n",
            "iteration: 34870 train shape: (105082, 48, 48, 1)\n",
            "iteration: 34871 train shape: (105084, 48, 48, 1)\n",
            "iteration: 34872 train shape: (105086, 48, 48, 1)\n",
            "iteration: 34873 train shape: (105088, 48, 48, 1)\n",
            "iteration: 34874 train shape: (105090, 48, 48, 1)\n",
            "iteration: 34875 train shape: (105092, 48, 48, 1)\n",
            "iteration: 34876 train shape: (105094, 48, 48, 1)\n",
            "iteration: 34877 train shape: (105096, 48, 48, 1)\n",
            "iteration: 34878 train shape: (105098, 48, 48, 1)\n",
            "iteration: 34879 train shape: (105100, 48, 48, 1)\n",
            "iteration: 34880 train shape: (105102, 48, 48, 1)\n",
            "iteration: 34881 train shape: (105104, 48, 48, 1)\n",
            "iteration: 34882 train shape: (105106, 48, 48, 1)\n",
            "iteration: 34883 train shape: (105108, 48, 48, 1)\n",
            "iteration: 34884 train shape: (105110, 48, 48, 1)\n",
            "iteration: 34885 train shape: (105112, 48, 48, 1)\n",
            "iteration: 34886 train shape: (105114, 48, 48, 1)\n",
            "iteration: 34887 train shape: (105116, 48, 48, 1)\n",
            "iteration: 34888 train shape: (105118, 48, 48, 1)\n",
            "iteration: 34889 train shape: (105120, 48, 48, 1)\n",
            "iteration: 34890 train shape: (105122, 48, 48, 1)\n",
            "iteration: 34891 train shape: (105124, 48, 48, 1)\n",
            "iteration: 34892 train shape: (105126, 48, 48, 1)\n",
            "iteration: 34893 train shape: (105128, 48, 48, 1)\n",
            "iteration: 34894 train shape: (105130, 48, 48, 1)\n",
            "iteration: 34895 train shape: (105132, 48, 48, 1)\n",
            "iteration: 34896 train shape: (105134, 48, 48, 1)\n",
            "iteration: 34897 train shape: (105136, 48, 48, 1)\n",
            "iteration: 34898 train shape: (105138, 48, 48, 1)\n",
            "iteration: 34899 train shape: (105140, 48, 48, 1)\n",
            "iteration: 34900 train shape: (105142, 48, 48, 1)\n",
            "iteration: 34901 train shape: (105144, 48, 48, 1)\n",
            "iteration: 34902 train shape: (105146, 48, 48, 1)\n",
            "iteration: 34903 train shape: (105148, 48, 48, 1)\n",
            "iteration: 34904 train shape: (105150, 48, 48, 1)\n",
            "iteration: 34905 train shape: (105152, 48, 48, 1)\n",
            "iteration: 34906 train shape: (105154, 48, 48, 1)\n",
            "iteration: 34907 train shape: (105156, 48, 48, 1)\n",
            "iteration: 34908 train shape: (105158, 48, 48, 1)\n",
            "iteration: 34909 train shape: (105160, 48, 48, 1)\n",
            "iteration: 34910 train shape: (105162, 48, 48, 1)\n",
            "iteration: 34911 train shape: (105164, 48, 48, 1)\n",
            "iteration: 34912 train shape: (105166, 48, 48, 1)\n",
            "iteration: 34913 train shape: (105168, 48, 48, 1)\n",
            "iteration: 34914 train shape: (105170, 48, 48, 1)\n",
            "iteration: 34915 train shape: (105172, 48, 48, 1)\n",
            "iteration: 34916 train shape: (105174, 48, 48, 1)\n",
            "iteration: 34917 train shape: (105176, 48, 48, 1)\n",
            "iteration: 34918 train shape: (105178, 48, 48, 1)\n",
            "iteration: 34919 train shape: (105180, 48, 48, 1)\n",
            "iteration: 34920 train shape: (105182, 48, 48, 1)\n",
            "iteration: 34921 train shape: (105184, 48, 48, 1)\n",
            "iteration: 34922 train shape: (105186, 48, 48, 1)\n",
            "iteration: 34923 train shape: (105188, 48, 48, 1)\n",
            "iteration: 34924 train shape: (105190, 48, 48, 1)\n",
            "iteration: 34925 train shape: (105192, 48, 48, 1)\n",
            "iteration: 34926 train shape: (105194, 48, 48, 1)\n",
            "iteration: 34927 train shape: (105196, 48, 48, 1)\n",
            "iteration: 34928 train shape: (105198, 48, 48, 1)\n",
            "iteration: 34929 train shape: (105200, 48, 48, 1)\n",
            "iteration: 34930 train shape: (105202, 48, 48, 1)\n",
            "iteration: 34931 train shape: (105204, 48, 48, 1)\n",
            "iteration: 34932 train shape: (105206, 48, 48, 1)\n",
            "iteration: 34933 train shape: (105208, 48, 48, 1)\n",
            "iteration: 34934 train shape: (105210, 48, 48, 1)\n",
            "iteration: 34935 train shape: (105212, 48, 48, 1)\n",
            "iteration: 34936 train shape: (105214, 48, 48, 1)\n",
            "iteration: 34937 train shape: (105216, 48, 48, 1)\n",
            "iteration: 34938 train shape: (105218, 48, 48, 1)\n",
            "iteration: 34939 train shape: (105220, 48, 48, 1)\n",
            "iteration: 34940 train shape: (105222, 48, 48, 1)\n",
            "iteration: 34941 train shape: (105224, 48, 48, 1)\n",
            "iteration: 34942 train shape: (105226, 48, 48, 1)\n",
            "iteration: 34943 train shape: (105228, 48, 48, 1)\n",
            "iteration: 34944 train shape: (105230, 48, 48, 1)\n",
            "iteration: 34945 train shape: (105232, 48, 48, 1)\n",
            "iteration: 34946 train shape: (105234, 48, 48, 1)\n",
            "iteration: 34947 train shape: (105236, 48, 48, 1)\n",
            "iteration: 34948 train shape: (105238, 48, 48, 1)\n",
            "iteration: 34949 train shape: (105240, 48, 48, 1)\n",
            "iteration: 34950 train shape: (105242, 48, 48, 1)\n",
            "iteration: 34951 train shape: (105244, 48, 48, 1)\n",
            "iteration: 34952 train shape: (105246, 48, 48, 1)\n",
            "iteration: 34953 train shape: (105248, 48, 48, 1)\n",
            "iteration: 34954 train shape: (105250, 48, 48, 1)\n",
            "iteration: 34955 train shape: (105252, 48, 48, 1)\n",
            "iteration: 34956 train shape: (105254, 48, 48, 1)\n",
            "iteration: 34957 train shape: (105256, 48, 48, 1)\n",
            "iteration: 34958 train shape: (105258, 48, 48, 1)\n",
            "iteration: 34959 train shape: (105260, 48, 48, 1)\n",
            "iteration: 34960 train shape: (105262, 48, 48, 1)\n",
            "iteration: 34961 train shape: (105264, 48, 48, 1)\n",
            "iteration: 34962 train shape: (105266, 48, 48, 1)\n",
            "iteration: 34963 train shape: (105268, 48, 48, 1)\n",
            "iteration: 34964 train shape: (105270, 48, 48, 1)\n",
            "iteration: 34965 train shape: (105272, 48, 48, 1)\n",
            "iteration: 34966 train shape: (105274, 48, 48, 1)\n",
            "iteration: 34967 train shape: (105276, 48, 48, 1)\n",
            "iteration: 34968 train shape: (105278, 48, 48, 1)\n",
            "iteration: 34969 train shape: (105280, 48, 48, 1)\n",
            "iteration: 34970 train shape: (105282, 48, 48, 1)\n",
            "iteration: 34971 train shape: (105284, 48, 48, 1)\n",
            "iteration: 34972 train shape: (105286, 48, 48, 1)\n",
            "iteration: 34973 train shape: (105288, 48, 48, 1)\n",
            "iteration: 34974 train shape: (105290, 48, 48, 1)\n",
            "iteration: 34975 train shape: (105292, 48, 48, 1)\n",
            "iteration: 34976 train shape: (105294, 48, 48, 1)\n",
            "iteration: 34977 train shape: (105296, 48, 48, 1)\n",
            "iteration: 34978 train shape: (105298, 48, 48, 1)\n",
            "iteration: 34979 train shape: (105300, 48, 48, 1)\n",
            "iteration: 34980 train shape: (105302, 48, 48, 1)\n",
            "iteration: 34981 train shape: (105304, 48, 48, 1)\n",
            "iteration: 34982 train shape: (105306, 48, 48, 1)\n",
            "iteration: 34983 train shape: (105308, 48, 48, 1)\n",
            "iteration: 34984 train shape: (105310, 48, 48, 1)\n",
            "iteration: 34985 train shape: (105312, 48, 48, 1)\n",
            "iteration: 34986 train shape: (105314, 48, 48, 1)\n",
            "iteration: 34987 train shape: (105316, 48, 48, 1)\n",
            "iteration: 34988 train shape: (105318, 48, 48, 1)\n",
            "iteration: 34989 train shape: (105320, 48, 48, 1)\n",
            "iteration: 34990 train shape: (105322, 48, 48, 1)\n",
            "iteration: 34991 train shape: (105324, 48, 48, 1)\n",
            "iteration: 34992 train shape: (105326, 48, 48, 1)\n",
            "iteration: 34993 train shape: (105328, 48, 48, 1)\n",
            "iteration: 34994 train shape: (105330, 48, 48, 1)\n",
            "iteration: 34995 train shape: (105332, 48, 48, 1)\n",
            "iteration: 34996 train shape: (105334, 48, 48, 1)\n",
            "iteration: 34997 train shape: (105336, 48, 48, 1)\n",
            "iteration: 34998 train shape: (105338, 48, 48, 1)\n",
            "iteration: 34999 train shape: (105340, 48, 48, 1)\n",
            "iteration: 35000 train shape: (105342, 48, 48, 1)\n",
            "iteration: 35001 train shape: (105344, 48, 48, 1)\n",
            "iteration: 35002 train shape: (105346, 48, 48, 1)\n",
            "iteration: 35003 train shape: (105348, 48, 48, 1)\n",
            "iteration: 35004 train shape: (105350, 48, 48, 1)\n",
            "iteration: 35005 train shape: (105352, 48, 48, 1)\n",
            "iteration: 35006 train shape: (105354, 48, 48, 1)\n",
            "iteration: 35007 train shape: (105356, 48, 48, 1)\n",
            "iteration: 35008 train shape: (105358, 48, 48, 1)\n",
            "iteration: 35009 train shape: (105360, 48, 48, 1)\n",
            "iteration: 35010 train shape: (105362, 48, 48, 1)\n",
            "iteration: 35011 train shape: (105364, 48, 48, 1)\n",
            "iteration: 35012 train shape: (105366, 48, 48, 1)\n",
            "iteration: 35013 train shape: (105368, 48, 48, 1)\n",
            "iteration: 35014 train shape: (105370, 48, 48, 1)\n",
            "iteration: 35015 train shape: (105372, 48, 48, 1)\n",
            "iteration: 35016 train shape: (105374, 48, 48, 1)\n",
            "iteration: 35017 train shape: (105376, 48, 48, 1)\n",
            "iteration: 35018 train shape: (105378, 48, 48, 1)\n",
            "iteration: 35019 train shape: (105380, 48, 48, 1)\n",
            "iteration: 35020 train shape: (105382, 48, 48, 1)\n",
            "iteration: 35021 train shape: (105384, 48, 48, 1)\n",
            "iteration: 35022 train shape: (105386, 48, 48, 1)\n",
            "iteration: 35023 train shape: (105388, 48, 48, 1)\n",
            "iteration: 35024 train shape: (105390, 48, 48, 1)\n",
            "iteration: 35025 train shape: (105392, 48, 48, 1)\n",
            "iteration: 35026 train shape: (105394, 48, 48, 1)\n",
            "iteration: 35027 train shape: (105396, 48, 48, 1)\n",
            "iteration: 35028 train shape: (105398, 48, 48, 1)\n",
            "iteration: 35029 train shape: (105400, 48, 48, 1)\n",
            "iteration: 35030 train shape: (105402, 48, 48, 1)\n",
            "iteration: 35031 train shape: (105404, 48, 48, 1)\n",
            "iteration: 35032 train shape: (105406, 48, 48, 1)\n",
            "iteration: 35033 train shape: (105408, 48, 48, 1)\n",
            "iteration: 35034 train shape: (105410, 48, 48, 1)\n",
            "iteration: 35035 train shape: (105412, 48, 48, 1)\n",
            "iteration: 35036 train shape: (105414, 48, 48, 1)\n",
            "iteration: 35037 train shape: (105416, 48, 48, 1)\n",
            "iteration: 35038 train shape: (105418, 48, 48, 1)\n",
            "iteration: 35039 train shape: (105420, 48, 48, 1)\n",
            "iteration: 35040 train shape: (105422, 48, 48, 1)\n",
            "iteration: 35041 train shape: (105424, 48, 48, 1)\n",
            "iteration: 35042 train shape: (105426, 48, 48, 1)\n",
            "iteration: 35043 train shape: (105428, 48, 48, 1)\n",
            "iteration: 35044 train shape: (105430, 48, 48, 1)\n",
            "iteration: 35045 train shape: (105432, 48, 48, 1)\n",
            "iteration: 35046 train shape: (105434, 48, 48, 1)\n",
            "iteration: 35047 train shape: (105436, 48, 48, 1)\n",
            "iteration: 35048 train shape: (105438, 48, 48, 1)\n",
            "iteration: 35049 train shape: (105440, 48, 48, 1)\n",
            "iteration: 35050 train shape: (105442, 48, 48, 1)\n",
            "iteration: 35051 train shape: (105444, 48, 48, 1)\n",
            "iteration: 35052 train shape: (105446, 48, 48, 1)\n",
            "iteration: 35053 train shape: (105448, 48, 48, 1)\n",
            "iteration: 35054 train shape: (105450, 48, 48, 1)\n",
            "iteration: 35055 train shape: (105452, 48, 48, 1)\n",
            "iteration: 35056 train shape: (105454, 48, 48, 1)\n",
            "iteration: 35057 train shape: (105456, 48, 48, 1)\n",
            "iteration: 35058 train shape: (105458, 48, 48, 1)\n",
            "iteration: 35059 train shape: (105460, 48, 48, 1)\n",
            "iteration: 35060 train shape: (105462, 48, 48, 1)\n",
            "iteration: 35061 train shape: (105464, 48, 48, 1)\n",
            "iteration: 35062 train shape: (105466, 48, 48, 1)\n",
            "iteration: 35063 train shape: (105468, 48, 48, 1)\n",
            "iteration: 35064 train shape: (105470, 48, 48, 1)\n",
            "iteration: 35065 train shape: (105472, 48, 48, 1)\n",
            "iteration: 35066 train shape: (105474, 48, 48, 1)\n",
            "iteration: 35067 train shape: (105476, 48, 48, 1)\n",
            "iteration: 35068 train shape: (105478, 48, 48, 1)\n",
            "iteration: 35069 train shape: (105480, 48, 48, 1)\n",
            "iteration: 35070 train shape: (105482, 48, 48, 1)\n",
            "iteration: 35071 train shape: (105484, 48, 48, 1)\n",
            "iteration: 35072 train shape: (105486, 48, 48, 1)\n",
            "iteration: 35073 train shape: (105488, 48, 48, 1)\n",
            "iteration: 35074 train shape: (105490, 48, 48, 1)\n",
            "iteration: 35075 train shape: (105492, 48, 48, 1)\n",
            "iteration: 35076 train shape: (105494, 48, 48, 1)\n",
            "iteration: 35077 train shape: (105496, 48, 48, 1)\n",
            "iteration: 35078 train shape: (105498, 48, 48, 1)\n",
            "iteration: 35079 train shape: (105500, 48, 48, 1)\n",
            "iteration: 35080 train shape: (105502, 48, 48, 1)\n",
            "iteration: 35081 train shape: (105504, 48, 48, 1)\n",
            "iteration: 35082 train shape: (105506, 48, 48, 1)\n",
            "iteration: 35083 train shape: (105508, 48, 48, 1)\n",
            "iteration: 35084 train shape: (105510, 48, 48, 1)\n",
            "iteration: 35085 train shape: (105512, 48, 48, 1)\n",
            "iteration: 35086 train shape: (105514, 48, 48, 1)\n",
            "iteration: 35087 train shape: (105516, 48, 48, 1)\n",
            "iteration: 35088 train shape: (105518, 48, 48, 1)\n",
            "iteration: 35089 train shape: (105520, 48, 48, 1)\n",
            "iteration: 35090 train shape: (105522, 48, 48, 1)\n",
            "iteration: 35091 train shape: (105524, 48, 48, 1)\n",
            "iteration: 35092 train shape: (105526, 48, 48, 1)\n",
            "iteration: 35093 train shape: (105528, 48, 48, 1)\n",
            "iteration: 35094 train shape: (105530, 48, 48, 1)\n",
            "iteration: 35095 train shape: (105532, 48, 48, 1)\n",
            "iteration: 35096 train shape: (105534, 48, 48, 1)\n",
            "iteration: 35097 train shape: (105536, 48, 48, 1)\n",
            "iteration: 35098 train shape: (105538, 48, 48, 1)\n",
            "iteration: 35099 train shape: (105540, 48, 48, 1)\n",
            "iteration: 35100 train shape: (105542, 48, 48, 1)\n",
            "iteration: 35101 train shape: (105544, 48, 48, 1)\n",
            "iteration: 35102 train shape: (105546, 48, 48, 1)\n",
            "iteration: 35103 train shape: (105548, 48, 48, 1)\n",
            "iteration: 35104 train shape: (105550, 48, 48, 1)\n",
            "iteration: 35105 train shape: (105552, 48, 48, 1)\n",
            "iteration: 35106 train shape: (105554, 48, 48, 1)\n",
            "iteration: 35107 train shape: (105556, 48, 48, 1)\n",
            "iteration: 35108 train shape: (105558, 48, 48, 1)\n",
            "iteration: 35109 train shape: (105560, 48, 48, 1)\n",
            "iteration: 35110 train shape: (105562, 48, 48, 1)\n",
            "iteration: 35111 train shape: (105564, 48, 48, 1)\n",
            "iteration: 35112 train shape: (105566, 48, 48, 1)\n",
            "iteration: 35113 train shape: (105568, 48, 48, 1)\n",
            "iteration: 35114 train shape: (105570, 48, 48, 1)\n",
            "iteration: 35115 train shape: (105572, 48, 48, 1)\n",
            "iteration: 35116 train shape: (105574, 48, 48, 1)\n",
            "iteration: 35117 train shape: (105576, 48, 48, 1)\n",
            "iteration: 35118 train shape: (105578, 48, 48, 1)\n",
            "iteration: 35119 train shape: (105580, 48, 48, 1)\n",
            "iteration: 35120 train shape: (105582, 48, 48, 1)\n",
            "iteration: 35121 train shape: (105584, 48, 48, 1)\n",
            "iteration: 35122 train shape: (105586, 48, 48, 1)\n",
            "iteration: 35123 train shape: (105588, 48, 48, 1)\n",
            "iteration: 35124 train shape: (105590, 48, 48, 1)\n",
            "iteration: 35125 train shape: (105592, 48, 48, 1)\n",
            "iteration: 35126 train shape: (105594, 48, 48, 1)\n",
            "iteration: 35127 train shape: (105596, 48, 48, 1)\n",
            "iteration: 35128 train shape: (105598, 48, 48, 1)\n",
            "iteration: 35129 train shape: (105600, 48, 48, 1)\n",
            "iteration: 35130 train shape: (105602, 48, 48, 1)\n",
            "iteration: 35131 train shape: (105604, 48, 48, 1)\n",
            "iteration: 35132 train shape: (105606, 48, 48, 1)\n",
            "iteration: 35133 train shape: (105608, 48, 48, 1)\n",
            "iteration: 35134 train shape: (105610, 48, 48, 1)\n",
            "iteration: 35135 train shape: (105612, 48, 48, 1)\n",
            "iteration: 35136 train shape: (105614, 48, 48, 1)\n",
            "iteration: 35137 train shape: (105616, 48, 48, 1)\n",
            "iteration: 35138 train shape: (105618, 48, 48, 1)\n",
            "iteration: 35139 train shape: (105620, 48, 48, 1)\n",
            "iteration: 35140 train shape: (105622, 48, 48, 1)\n",
            "iteration: 35141 train shape: (105624, 48, 48, 1)\n",
            "iteration: 35142 train shape: (105626, 48, 48, 1)\n",
            "iteration: 35143 train shape: (105628, 48, 48, 1)\n",
            "iteration: 35144 train shape: (105630, 48, 48, 1)\n",
            "iteration: 35145 train shape: (105632, 48, 48, 1)\n",
            "iteration: 35146 train shape: (105634, 48, 48, 1)\n",
            "iteration: 35147 train shape: (105636, 48, 48, 1)\n",
            "iteration: 35148 train shape: (105638, 48, 48, 1)\n",
            "iteration: 35149 train shape: (105640, 48, 48, 1)\n",
            "iteration: 35150 train shape: (105642, 48, 48, 1)\n",
            "iteration: 35151 train shape: (105644, 48, 48, 1)\n",
            "iteration: 35152 train shape: (105646, 48, 48, 1)\n",
            "iteration: 35153 train shape: (105648, 48, 48, 1)\n",
            "iteration: 35154 train shape: (105650, 48, 48, 1)\n",
            "iteration: 35155 train shape: (105652, 48, 48, 1)\n",
            "iteration: 35156 train shape: (105654, 48, 48, 1)\n",
            "iteration: 35157 train shape: (105656, 48, 48, 1)\n",
            "iteration: 35158 train shape: (105658, 48, 48, 1)\n",
            "iteration: 35159 train shape: (105660, 48, 48, 1)\n",
            "iteration: 35160 train shape: (105662, 48, 48, 1)\n",
            "iteration: 35161 train shape: (105664, 48, 48, 1)\n",
            "iteration: 35162 train shape: (105666, 48, 48, 1)\n",
            "iteration: 35163 train shape: (105668, 48, 48, 1)\n",
            "iteration: 35164 train shape: (105670, 48, 48, 1)\n",
            "iteration: 35165 train shape: (105672, 48, 48, 1)\n",
            "iteration: 35166 train shape: (105674, 48, 48, 1)\n",
            "iteration: 35167 train shape: (105676, 48, 48, 1)\n",
            "iteration: 35168 train shape: (105678, 48, 48, 1)\n",
            "iteration: 35169 train shape: (105680, 48, 48, 1)\n",
            "iteration: 35170 train shape: (105682, 48, 48, 1)\n",
            "iteration: 35171 train shape: (105684, 48, 48, 1)\n",
            "iteration: 35172 train shape: (105686, 48, 48, 1)\n",
            "iteration: 35173 train shape: (105688, 48, 48, 1)\n",
            "iteration: 35174 train shape: (105690, 48, 48, 1)\n",
            "iteration: 35175 train shape: (105692, 48, 48, 1)\n",
            "iteration: 35176 train shape: (105694, 48, 48, 1)\n",
            "iteration: 35177 train shape: (105696, 48, 48, 1)\n",
            "iteration: 35178 train shape: (105698, 48, 48, 1)\n",
            "iteration: 35179 train shape: (105700, 48, 48, 1)\n",
            "iteration: 35180 train shape: (105702, 48, 48, 1)\n",
            "iteration: 35181 train shape: (105704, 48, 48, 1)\n",
            "iteration: 35182 train shape: (105706, 48, 48, 1)\n",
            "iteration: 35183 train shape: (105708, 48, 48, 1)\n",
            "iteration: 35184 train shape: (105710, 48, 48, 1)\n",
            "iteration: 35185 train shape: (105712, 48, 48, 1)\n",
            "iteration: 35186 train shape: (105714, 48, 48, 1)\n",
            "iteration: 35187 train shape: (105716, 48, 48, 1)\n",
            "iteration: 35188 train shape: (105718, 48, 48, 1)\n",
            "iteration: 35189 train shape: (105720, 48, 48, 1)\n",
            "iteration: 35190 train shape: (105722, 48, 48, 1)\n",
            "iteration: 35191 train shape: (105724, 48, 48, 1)\n",
            "iteration: 35192 train shape: (105726, 48, 48, 1)\n",
            "iteration: 35193 train shape: (105728, 48, 48, 1)\n",
            "iteration: 35194 train shape: (105730, 48, 48, 1)\n",
            "iteration: 35195 train shape: (105732, 48, 48, 1)\n",
            "iteration: 35196 train shape: (105734, 48, 48, 1)\n",
            "iteration: 35197 train shape: (105736, 48, 48, 1)\n",
            "iteration: 35198 train shape: (105738, 48, 48, 1)\n",
            "iteration: 35199 train shape: (105740, 48, 48, 1)\n",
            "iteration: 35200 train shape: (105742, 48, 48, 1)\n",
            "iteration: 35201 train shape: (105744, 48, 48, 1)\n",
            "iteration: 35202 train shape: (105746, 48, 48, 1)\n",
            "iteration: 35203 train shape: (105748, 48, 48, 1)\n",
            "iteration: 35204 train shape: (105750, 48, 48, 1)\n",
            "iteration: 35205 train shape: (105752, 48, 48, 1)\n",
            "iteration: 35206 train shape: (105754, 48, 48, 1)\n",
            "iteration: 35207 train shape: (105756, 48, 48, 1)\n",
            "iteration: 35208 train shape: (105758, 48, 48, 1)\n",
            "iteration: 35209 train shape: (105760, 48, 48, 1)\n",
            "iteration: 35210 train shape: (105762, 48, 48, 1)\n",
            "iteration: 35211 train shape: (105764, 48, 48, 1)\n",
            "iteration: 35212 train shape: (105766, 48, 48, 1)\n",
            "iteration: 35213 train shape: (105768, 48, 48, 1)\n",
            "iteration: 35214 train shape: (105770, 48, 48, 1)\n",
            "iteration: 35215 train shape: (105772, 48, 48, 1)\n",
            "iteration: 35216 train shape: (105774, 48, 48, 1)\n",
            "iteration: 35217 train shape: (105776, 48, 48, 1)\n",
            "iteration: 35218 train shape: (105778, 48, 48, 1)\n",
            "iteration: 35219 train shape: (105780, 48, 48, 1)\n",
            "iteration: 35220 train shape: (105782, 48, 48, 1)\n",
            "iteration: 35221 train shape: (105784, 48, 48, 1)\n",
            "iteration: 35222 train shape: (105786, 48, 48, 1)\n",
            "iteration: 35223 train shape: (105788, 48, 48, 1)\n",
            "iteration: 35224 train shape: (105790, 48, 48, 1)\n",
            "iteration: 35225 train shape: (105792, 48, 48, 1)\n",
            "iteration: 35226 train shape: (105794, 48, 48, 1)\n",
            "iteration: 35227 train shape: (105796, 48, 48, 1)\n",
            "iteration: 35228 train shape: (105798, 48, 48, 1)\n",
            "iteration: 35229 train shape: (105800, 48, 48, 1)\n",
            "iteration: 35230 train shape: (105802, 48, 48, 1)\n",
            "iteration: 35231 train shape: (105804, 48, 48, 1)\n",
            "iteration: 35232 train shape: (105806, 48, 48, 1)\n",
            "iteration: 35233 train shape: (105808, 48, 48, 1)\n",
            "iteration: 35234 train shape: (105810, 48, 48, 1)\n",
            "iteration: 35235 train shape: (105812, 48, 48, 1)\n",
            "iteration: 35236 train shape: (105814, 48, 48, 1)\n",
            "iteration: 35237 train shape: (105816, 48, 48, 1)\n",
            "iteration: 35238 train shape: (105818, 48, 48, 1)\n",
            "iteration: 35239 train shape: (105820, 48, 48, 1)\n",
            "iteration: 35240 train shape: (105822, 48, 48, 1)\n",
            "iteration: 35241 train shape: (105824, 48, 48, 1)\n",
            "iteration: 35242 train shape: (105826, 48, 48, 1)\n",
            "iteration: 35243 train shape: (105828, 48, 48, 1)\n",
            "iteration: 35244 train shape: (105830, 48, 48, 1)\n",
            "iteration: 35245 train shape: (105832, 48, 48, 1)\n",
            "iteration: 35246 train shape: (105834, 48, 48, 1)\n",
            "iteration: 35247 train shape: (105836, 48, 48, 1)\n",
            "iteration: 35248 train shape: (105838, 48, 48, 1)\n",
            "iteration: 35249 train shape: (105840, 48, 48, 1)\n",
            "iteration: 35250 train shape: (105842, 48, 48, 1)\n",
            "iteration: 35251 train shape: (105844, 48, 48, 1)\n",
            "iteration: 35252 train shape: (105846, 48, 48, 1)\n",
            "iteration: 35253 train shape: (105848, 48, 48, 1)\n",
            "iteration: 35254 train shape: (105850, 48, 48, 1)\n",
            "iteration: 35255 train shape: (105852, 48, 48, 1)\n",
            "iteration: 35256 train shape: (105854, 48, 48, 1)\n",
            "iteration: 35257 train shape: (105856, 48, 48, 1)\n",
            "iteration: 35258 train shape: (105858, 48, 48, 1)\n",
            "iteration: 35259 train shape: (105860, 48, 48, 1)\n",
            "iteration: 35260 train shape: (105862, 48, 48, 1)\n",
            "iteration: 35261 train shape: (105864, 48, 48, 1)\n",
            "iteration: 35262 train shape: (105866, 48, 48, 1)\n",
            "iteration: 35263 train shape: (105868, 48, 48, 1)\n",
            "iteration: 35264 train shape: (105870, 48, 48, 1)\n",
            "iteration: 35265 train shape: (105872, 48, 48, 1)\n",
            "iteration: 35266 train shape: (105874, 48, 48, 1)\n",
            "iteration: 35267 train shape: (105876, 48, 48, 1)\n",
            "iteration: 35268 train shape: (105878, 48, 48, 1)\n",
            "iteration: 35269 train shape: (105880, 48, 48, 1)\n",
            "iteration: 35270 train shape: (105882, 48, 48, 1)\n",
            "iteration: 35271 train shape: (105884, 48, 48, 1)\n",
            "iteration: 35272 train shape: (105886, 48, 48, 1)\n",
            "iteration: 35273 train shape: (105888, 48, 48, 1)\n",
            "iteration: 35274 train shape: (105890, 48, 48, 1)\n",
            "iteration: 35275 train shape: (105892, 48, 48, 1)\n",
            "iteration: 35276 train shape: (105894, 48, 48, 1)\n",
            "iteration: 35277 train shape: (105896, 48, 48, 1)\n",
            "iteration: 35278 train shape: (105898, 48, 48, 1)\n",
            "iteration: 35279 train shape: (105900, 48, 48, 1)\n",
            "iteration: 35280 train shape: (105902, 48, 48, 1)\n",
            "iteration: 35281 train shape: (105904, 48, 48, 1)\n",
            "iteration: 35282 train shape: (105906, 48, 48, 1)\n",
            "iteration: 35283 train shape: (105908, 48, 48, 1)\n",
            "iteration: 35284 train shape: (105910, 48, 48, 1)\n",
            "iteration: 35285 train shape: (105912, 48, 48, 1)\n",
            "iteration: 35286 train shape: (105914, 48, 48, 1)\n",
            "iteration: 35287 train shape: (105916, 48, 48, 1)\n",
            "iteration: 35288 train shape: (105918, 48, 48, 1)\n",
            "iteration: 35289 train shape: (105920, 48, 48, 1)\n",
            "iteration: 35290 train shape: (105922, 48, 48, 1)\n",
            "iteration: 35291 train shape: (105924, 48, 48, 1)\n",
            "iteration: 35292 train shape: (105926, 48, 48, 1)\n",
            "iteration: 35293 train shape: (105928, 48, 48, 1)\n",
            "iteration: 35294 train shape: (105930, 48, 48, 1)\n",
            "iteration: 35295 train shape: (105932, 48, 48, 1)\n",
            "iteration: 35296 train shape: (105934, 48, 48, 1)\n",
            "iteration: 35297 train shape: (105936, 48, 48, 1)\n",
            "iteration: 35298 train shape: (105938, 48, 48, 1)\n",
            "iteration: 35299 train shape: (105940, 48, 48, 1)\n",
            "iteration: 35300 train shape: (105942, 48, 48, 1)\n",
            "iteration: 35301 train shape: (105944, 48, 48, 1)\n",
            "iteration: 35302 train shape: (105946, 48, 48, 1)\n",
            "iteration: 35303 train shape: (105948, 48, 48, 1)\n",
            "iteration: 35304 train shape: (105950, 48, 48, 1)\n",
            "iteration: 35305 train shape: (105952, 48, 48, 1)\n",
            "iteration: 35306 train shape: (105954, 48, 48, 1)\n",
            "iteration: 35307 train shape: (105956, 48, 48, 1)\n",
            "iteration: 35308 train shape: (105958, 48, 48, 1)\n",
            "iteration: 35309 train shape: (105960, 48, 48, 1)\n",
            "iteration: 35310 train shape: (105962, 48, 48, 1)\n",
            "iteration: 35311 train shape: (105964, 48, 48, 1)\n",
            "iteration: 35312 train shape: (105966, 48, 48, 1)\n",
            "iteration: 35313 train shape: (105968, 48, 48, 1)\n",
            "iteration: 35314 train shape: (105970, 48, 48, 1)\n",
            "iteration: 35315 train shape: (105972, 48, 48, 1)\n",
            "iteration: 35316 train shape: (105974, 48, 48, 1)\n",
            "iteration: 35317 train shape: (105976, 48, 48, 1)\n",
            "iteration: 35318 train shape: (105978, 48, 48, 1)\n",
            "iteration: 35319 train shape: (105980, 48, 48, 1)\n",
            "iteration: 35320 train shape: (105982, 48, 48, 1)\n",
            "iteration: 35321 train shape: (105984, 48, 48, 1)\n",
            "iteration: 35322 train shape: (105986, 48, 48, 1)\n",
            "iteration: 35323 train shape: (105988, 48, 48, 1)\n",
            "iteration: 35324 train shape: (105990, 48, 48, 1)\n",
            "iteration: 35325 train shape: (105992, 48, 48, 1)\n",
            "iteration: 35326 train shape: (105994, 48, 48, 1)\n",
            "iteration: 35327 train shape: (105996, 48, 48, 1)\n",
            "iteration: 35328 train shape: (105998, 48, 48, 1)\n",
            "iteration: 35329 train shape: (106000, 48, 48, 1)\n",
            "iteration: 35330 train shape: (106002, 48, 48, 1)\n",
            "iteration: 35331 train shape: (106004, 48, 48, 1)\n",
            "iteration: 35332 train shape: (106006, 48, 48, 1)\n",
            "iteration: 35333 train shape: (106008, 48, 48, 1)\n",
            "iteration: 35334 train shape: (106010, 48, 48, 1)\n",
            "iteration: 35335 train shape: (106012, 48, 48, 1)\n",
            "iteration: 35336 train shape: (106014, 48, 48, 1)\n",
            "iteration: 35337 train shape: (106016, 48, 48, 1)\n",
            "iteration: 35338 train shape: (106018, 48, 48, 1)\n",
            "iteration: 35339 train shape: (106020, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEZnykiu1m9j",
        "outputId": "d64864b5-1b55-4a53-ab06-9ee48d7ebf3d"
      },
      "source": [
        "\n",
        "X_train = np.concatenate((X_train, X_train_aug), axis=0)\n",
        "y_train = np.concatenate((y_train, y_train_aug), axis=0)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(141360, 48, 48, 1)\n",
            "(141360, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q5nypb9PBqO"
      },
      "source": [
        "#brightness\n",
        "'''\n",
        "iterations_per_image = 2\n",
        "for k in range(len(X_train)):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "  for i in range(iterations_per_image):\n",
        "    contrast = iaa.GammaContrast(gamma=2.0 + i)\n",
        "    contrast_image =contrast.augment_image(img)\n",
        "    #contrast_image_reshaped = contrast_image.reshape((1, ) + contrast_image.shape) \n",
        "    contrasted_images.append(contrast_image)\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  #X_train.extend(contrasted_images)\n",
        "  X_train = np.concatenate((X_train, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]*iterations_per_image\n",
        "  #y_train.extend(emotions_list)\n",
        "  y_train = np.concatenate((y_train, emotions_list), axis=0)\n",
        "  print(\"iteration: {}train shape: {}\" ,k,X_train.shape)\n",
        "#plt.show()\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(r\"C:\\Users\\shir2\\Desktop\\Shir\\MSc\\%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sONoLUCKEOZR",
        "outputId": "2930a5ea-c24b-4092-80c0-335ffdb5832a"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18600, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 48, 48, 128)  3328        input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 2304)      13824       input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 100)          0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 2304)         0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3556)         0           flatten_6[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            3557        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-d1e73e3edebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbicogan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiCoGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbicogan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-449f3802f696>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;31m# Sample noise and generate img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mimgs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# Train the discriminator (img -> z is valid, z -> img is fake)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  indices[0,0] = 6 is not in [0, 6)\n\t [[node model_3/embedding_3/embedding_lookup (defined at <ipython-input-76-449f3802f696>:263) ]]\n\t [[model_3/embedding_3/embedding_lookup/_6]]\n  (1) Invalid argument:  indices[0,0] = 6 is not in [0, 6)\n\t [[node model_3/embedding_3/embedding_lookup (defined at <ipython-input-76-449f3802f696>:263) ]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_2655]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_3/embedding_3/embedding_lookup:\n model_3/embedding_3/embedding_lookup/2580 (defined at /usr/lib/python3.7/contextlib.py:112)\n\nInput Source operations connected to node model_3/embedding_3/embedding_lookup:\n model_3/embedding_3/embedding_lookup/2580 (defined at /usr/lib/python3.7/contextlib.py:112)\n\nFunction call stack:\npredict_function -> predict_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7OvsALJTfRi",
        "outputId": "12a355d3-9d4b-49ca-ad2b-aba5aa6f7fad"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 48, 48, 128)  3328        input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 100)          0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          51712       flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 2304)      16128       input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 1152)         0           leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 2304)         0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3968)         0           dropout_2[0][0]                  \n",
            "                                                                 flatten_10[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 1)            3969        concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 4608)              465408    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)   (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 12, 12, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 48, 48, 1)         4609      \n",
            "=================================================================\n",
            "Total params: 1,256,833\n",
            "Trainable params: 1,256,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.692794, acc.: 41.80%] [G loss: 1.363244]\n",
            "20 [D loss: 0.997228, acc.: 49.22%] [G loss: 1.079654]\n",
            "40 [D loss: 10.192451, acc.: 38.28%] [G loss: 16.920563]\n",
            "60 [D loss: 4.623682, acc.: 29.30%] [G loss: 7.144755]\n",
            "80 [D loss: 0.145405, acc.: 96.88%] [G loss: 3.568455]\n",
            "100 [D loss: 0.171819, acc.: 96.09%] [G loss: 4.876417]\n",
            "120 [D loss: 0.059815, acc.: 100.00%] [G loss: 8.702084]\n",
            "140 [D loss: 0.277375, acc.: 89.45%] [G loss: 4.059882]\n",
            "160 [D loss: 0.139765, acc.: 94.53%] [G loss: 6.267387]\n",
            "180 [D loss: 0.098693, acc.: 96.48%] [G loss: 7.450905]\n",
            "200 [D loss: 0.075423, acc.: 96.88%] [G loss: 9.491319]\n",
            "220 [D loss: 0.860310, acc.: 76.17%] [G loss: 7.120339]\n",
            "240 [D loss: 0.021870, acc.: 100.00%] [G loss: 10.763273]\n",
            "260 [D loss: 0.283057, acc.: 98.83%] [G loss: 4.523032]\n",
            "280 [D loss: 0.064968, acc.: 99.22%] [G loss: 8.937262]\n",
            "300 [D loss: 0.257237, acc.: 98.05%] [G loss: 5.262499]\n",
            "320 [D loss: 0.638951, acc.: 46.88%] [G loss: 5.962174]\n",
            "340 [D loss: 0.419628, acc.: 89.84%] [G loss: 4.417611]\n",
            "360 [D loss: 0.347575, acc.: 93.36%] [G loss: 3.552187]\n",
            "380 [D loss: 0.969137, acc.: 64.84%] [G loss: 2.598330]\n",
            "400 [D loss: 0.632267, acc.: 78.52%] [G loss: 3.372662]\n",
            "420 [D loss: 0.525285, acc.: 83.98%] [G loss: 2.757557]\n",
            "440 [D loss: 0.400338, acc.: 88.28%] [G loss: 5.184896]\n",
            "460 [D loss: 0.484532, acc.: 89.84%] [G loss: 5.033338]\n",
            "480 [D loss: 0.526991, acc.: 77.34%] [G loss: 3.353721]\n",
            "500 [D loss: 0.484876, acc.: 87.11%] [G loss: 3.852768]\n",
            "520 [D loss: 0.238678, acc.: 94.14%] [G loss: 6.053322]\n",
            "540 [D loss: 0.310701, acc.: 91.41%] [G loss: 4.480524]\n",
            "560 [D loss: 0.386147, acc.: 92.19%] [G loss: 5.068025]\n",
            "580 [D loss: 0.441447, acc.: 82.42%] [G loss: 8.714972]\n",
            "600 [D loss: 2.068088, acc.: 35.16%] [G loss: 5.693320]\n",
            "620 [D loss: 0.076214, acc.: 96.88%] [G loss: 15.504116]\n",
            "640 [D loss: 0.402508, acc.: 89.84%] [G loss: 5.015415]\n",
            "660 [D loss: 0.291974, acc.: 86.33%] [G loss: 6.800432]\n",
            "680 [D loss: 0.365668, acc.: 85.94%] [G loss: 4.809776]\n",
            "700 [D loss: 0.375825, acc.: 87.89%] [G loss: 7.323451]\n",
            "720 [D loss: 0.584250, acc.: 77.34%] [G loss: 3.898723]\n",
            "740 [D loss: 0.480237, acc.: 77.34%] [G loss: 3.939720]\n",
            "760 [D loss: 0.615871, acc.: 68.75%] [G loss: 3.039476]\n",
            "780 [D loss: 0.518439, acc.: 75.39%] [G loss: 3.012846]\n",
            "800 [D loss: 0.591483, acc.: 67.58%] [G loss: 2.936316]\n",
            "820 [D loss: 0.580957, acc.: 68.75%] [G loss: 3.117126]\n",
            "840 [D loss: 0.453159, acc.: 84.77%] [G loss: 3.259461]\n",
            "860 [D loss: 0.419408, acc.: 83.20%] [G loss: 4.634182]\n",
            "880 [D loss: 0.313598, acc.: 90.62%] [G loss: 4.173212]\n",
            "900 [D loss: 0.333613, acc.: 86.33%] [G loss: 5.602916]\n",
            "920 [D loss: 0.195415, acc.: 96.48%] [G loss: 6.447292]\n",
            "940 [D loss: 0.307183, acc.: 89.45%] [G loss: 5.055152]\n",
            "960 [D loss: 0.452245, acc.: 81.64%] [G loss: 4.614908]\n",
            "980 [D loss: 0.375906, acc.: 84.38%] [G loss: 4.065977]\n",
            "1000 [D loss: 0.322876, acc.: 87.50%] [G loss: 4.418130]\n",
            "1020 [D loss: 0.586964, acc.: 73.05%] [G loss: 3.883324]\n",
            "1040 [D loss: 0.422864, acc.: 80.08%] [G loss: 3.679060]\n",
            "1060 [D loss: 0.452724, acc.: 79.30%] [G loss: 4.292919]\n",
            "1080 [D loss: 0.270108, acc.: 92.19%] [G loss: 5.464712]\n",
            "1100 [D loss: 0.379701, acc.: 83.98%] [G loss: 5.048302]\n",
            "1120 [D loss: 0.425299, acc.: 85.55%] [G loss: 4.107570]\n",
            "1140 [D loss: 0.346484, acc.: 86.33%] [G loss: 4.969882]\n",
            "1160 [D loss: 0.475160, acc.: 74.61%] [G loss: 4.161832]\n",
            "1180 [D loss: 0.556998, acc.: 73.83%] [G loss: 3.082120]\n",
            "1200 [D loss: 0.484912, acc.: 76.56%] [G loss: 3.433202]\n",
            "1220 [D loss: 0.389804, acc.: 84.77%] [G loss: 3.793457]\n",
            "1240 [D loss: 0.383044, acc.: 83.20%] [G loss: 4.044407]\n",
            "1260 [D loss: 0.557780, acc.: 74.61%] [G loss: 3.227710]\n",
            "1280 [D loss: 0.454065, acc.: 78.52%] [G loss: 3.505677]\n",
            "1300 [D loss: 0.447838, acc.: 81.25%] [G loss: 3.583960]\n",
            "1320 [D loss: 0.614067, acc.: 71.88%] [G loss: 3.790469]\n",
            "1340 [D loss: 0.397807, acc.: 83.59%] [G loss: 3.764839]\n",
            "1360 [D loss: 0.807834, acc.: 53.91%] [G loss: 3.551185]\n",
            "1380 [D loss: 0.439475, acc.: 81.64%] [G loss: 3.677790]\n",
            "1400 [D loss: 0.483154, acc.: 77.34%] [G loss: 3.529718]\n",
            "1420 [D loss: 0.475700, acc.: 76.56%] [G loss: 3.799642]\n",
            "1440 [D loss: 0.545951, acc.: 74.61%] [G loss: 3.470526]\n",
            "1460 [D loss: 0.441273, acc.: 81.64%] [G loss: 3.969632]\n",
            "1480 [D loss: 0.387720, acc.: 82.81%] [G loss: 4.333671]\n",
            "1500 [D loss: 0.557913, acc.: 71.88%] [G loss: 3.855313]\n",
            "1520 [D loss: 0.368076, acc.: 82.81%] [G loss: 4.123768]\n",
            "1540 [D loss: 0.410299, acc.: 82.42%] [G loss: 4.277707]\n",
            "1560 [D loss: 0.400476, acc.: 82.81%] [G loss: 3.751390]\n",
            "1580 [D loss: 0.449101, acc.: 79.30%] [G loss: 3.734212]\n",
            "1600 [D loss: 0.415674, acc.: 81.25%] [G loss: 3.873059]\n",
            "1620 [D loss: 0.265750, acc.: 92.19%] [G loss: 2.964638]\n",
            "1640 [D loss: 0.489255, acc.: 76.17%] [G loss: 3.519682]\n",
            "1660 [D loss: 0.477191, acc.: 78.91%] [G loss: 4.055447]\n",
            "1680 [D loss: 0.386211, acc.: 82.03%] [G loss: 4.089319]\n",
            "1700 [D loss: 0.438694, acc.: 80.86%] [G loss: 3.858937]\n",
            "1720 [D loss: 0.324894, acc.: 87.11%] [G loss: 4.758214]\n",
            "1740 [D loss: 0.508147, acc.: 73.83%] [G loss: 3.894225]\n",
            "1760 [D loss: 0.466030, acc.: 81.25%] [G loss: 4.541659]\n",
            "1780 [D loss: 0.499243, acc.: 77.34%] [G loss: 3.977340]\n",
            "1800 [D loss: 0.441533, acc.: 81.64%] [G loss: 3.896728]\n",
            "1820 [D loss: 0.476543, acc.: 77.34%] [G loss: 4.024343]\n",
            "1840 [D loss: 0.380871, acc.: 82.81%] [G loss: 4.468583]\n",
            "1860 [D loss: 0.521969, acc.: 74.22%] [G loss: 4.363077]\n",
            "1880 [D loss: 0.398990, acc.: 85.16%] [G loss: 4.545360]\n",
            "1900 [D loss: 0.352253, acc.: 84.77%] [G loss: 4.854161]\n",
            "1920 [D loss: 0.411604, acc.: 80.86%] [G loss: 4.394051]\n",
            "1940 [D loss: 0.348949, acc.: 86.72%] [G loss: 4.684855]\n",
            "1960 [D loss: 0.408454, acc.: 83.59%] [G loss: 4.370540]\n",
            "1980 [D loss: 0.445645, acc.: 78.12%] [G loss: 4.173965]\n",
            "2000 [D loss: 0.382557, acc.: 83.59%] [G loss: 4.587084]\n",
            "2020 [D loss: 0.528059, acc.: 77.73%] [G loss: 4.250171]\n",
            "2040 [D loss: 0.434418, acc.: 81.64%] [G loss: 3.984667]\n",
            "2060 [D loss: 0.396934, acc.: 83.59%] [G loss: 4.381786]\n",
            "2080 [D loss: 0.406715, acc.: 81.64%] [G loss: 4.359241]\n",
            "2100 [D loss: 0.498469, acc.: 76.17%] [G loss: 4.309250]\n",
            "2120 [D loss: 0.392552, acc.: 81.64%] [G loss: 4.542624]\n",
            "2140 [D loss: 0.497858, acc.: 75.39%] [G loss: 4.256995]\n",
            "2160 [D loss: 0.431669, acc.: 78.52%] [G loss: 4.374342]\n",
            "2180 [D loss: 0.386353, acc.: 82.03%] [G loss: 4.030497]\n",
            "2200 [D loss: 0.393525, acc.: 84.38%] [G loss: 4.327544]\n",
            "2220 [D loss: 0.471047, acc.: 76.17%] [G loss: 4.148203]\n",
            "2240 [D loss: 0.431076, acc.: 80.86%] [G loss: 4.362601]\n",
            "2260 [D loss: 0.376126, acc.: 82.81%] [G loss: 4.311936]\n",
            "2280 [D loss: 0.435135, acc.: 82.03%] [G loss: 4.216527]\n",
            "2300 [D loss: 0.412959, acc.: 80.47%] [G loss: 4.111159]\n",
            "2320 [D loss: 0.550280, acc.: 71.88%] [G loss: 3.931458]\n",
            "2340 [D loss: 0.335698, acc.: 87.50%] [G loss: 4.733627]\n",
            "2360 [D loss: 0.425331, acc.: 80.86%] [G loss: 4.688098]\n",
            "2380 [D loss: 0.424558, acc.: 82.81%] [G loss: 4.332030]\n",
            "2400 [D loss: 0.427221, acc.: 78.91%] [G loss: 4.370893]\n",
            "2420 [D loss: 0.318654, acc.: 88.28%] [G loss: 4.803578]\n",
            "2440 [D loss: 0.413908, acc.: 83.59%] [G loss: 4.504460]\n",
            "2460 [D loss: 0.421730, acc.: 79.69%] [G loss: 4.413567]\n",
            "2480 [D loss: 0.419884, acc.: 81.64%] [G loss: 4.405695]\n",
            "2500 [D loss: 0.424117, acc.: 82.42%] [G loss: 4.691920]\n",
            "2520 [D loss: 0.453394, acc.: 80.47%] [G loss: 4.406740]\n",
            "2540 [D loss: 0.374935, acc.: 84.77%] [G loss: 4.736284]\n",
            "2560 [D loss: 0.321794, acc.: 87.50%] [G loss: 5.267558]\n",
            "2580 [D loss: 0.518656, acc.: 72.27%] [G loss: 4.482098]\n",
            "2600 [D loss: 0.388319, acc.: 83.98%] [G loss: 4.813517]\n",
            "2620 [D loss: 0.328663, acc.: 86.33%] [G loss: 4.836767]\n",
            "2640 [D loss: 0.318099, acc.: 85.55%] [G loss: 5.641196]\n",
            "2660 [D loss: 0.326650, acc.: 87.50%] [G loss: 5.069518]\n",
            "2680 [D loss: 0.447819, acc.: 77.34%] [G loss: 5.226392]\n",
            "2700 [D loss: 0.347934, acc.: 87.50%] [G loss: 4.671406]\n",
            "2720 [D loss: 0.453368, acc.: 76.56%] [G loss: 4.273150]\n",
            "2740 [D loss: 0.432023, acc.: 78.12%] [G loss: 4.159589]\n",
            "2760 [D loss: 0.420880, acc.: 80.47%] [G loss: 4.599861]\n",
            "2780 [D loss: 0.566641, acc.: 69.92%] [G loss: 4.225712]\n",
            "2800 [D loss: 0.463328, acc.: 76.95%] [G loss: 4.600976]\n",
            "2820 [D loss: 0.350038, acc.: 83.98%] [G loss: 4.546372]\n",
            "2840 [D loss: 0.403359, acc.: 81.25%] [G loss: 5.237429]\n",
            "2860 [D loss: 0.345739, acc.: 85.94%] [G loss: 4.741522]\n",
            "2880 [D loss: 0.354460, acc.: 85.16%] [G loss: 4.596492]\n",
            "2900 [D loss: 0.375729, acc.: 80.86%] [G loss: 4.914245]\n",
            "2920 [D loss: 0.322705, acc.: 86.33%] [G loss: 5.045888]\n",
            "2940 [D loss: 0.247954, acc.: 91.41%] [G loss: 5.091659]\n",
            "2960 [D loss: 0.557924, acc.: 72.27%] [G loss: 4.150468]\n",
            "2980 [D loss: 0.327608, acc.: 89.06%] [G loss: 4.811913]\n",
            "3000 [D loss: 0.412486, acc.: 79.30%] [G loss: 4.731018]\n",
            "3020 [D loss: 0.344638, acc.: 87.11%] [G loss: 5.435119]\n",
            "3040 [D loss: 0.331938, acc.: 83.59%] [G loss: 4.884459]\n",
            "3060 [D loss: 0.328882, acc.: 83.98%] [G loss: 5.597978]\n",
            "3080 [D loss: 0.380475, acc.: 81.25%] [G loss: 4.764737]\n",
            "3100 [D loss: 0.393132, acc.: 81.64%] [G loss: 4.988447]\n",
            "3120 [D loss: 0.417215, acc.: 81.64%] [G loss: 4.992435]\n",
            "3140 [D loss: 0.358083, acc.: 83.59%] [G loss: 5.373530]\n",
            "3160 [D loss: 0.343837, acc.: 85.16%] [G loss: 5.502664]\n",
            "3180 [D loss: 0.242313, acc.: 91.41%] [G loss: 5.834956]\n",
            "3200 [D loss: 0.299378, acc.: 90.23%] [G loss: 5.601436]\n",
            "3220 [D loss: 0.500149, acc.: 76.17%] [G loss: 5.214738]\n",
            "3240 [D loss: 0.355832, acc.: 85.16%] [G loss: 5.202427]\n",
            "3260 [D loss: 0.290968, acc.: 88.28%] [G loss: 6.063551]\n",
            "3280 [D loss: 0.327741, acc.: 87.89%] [G loss: 5.059925]\n",
            "3300 [D loss: 0.451675, acc.: 79.30%] [G loss: 3.925858]\n",
            "3320 [D loss: 0.313318, acc.: 86.33%] [G loss: 5.141754]\n",
            "3340 [D loss: 0.371400, acc.: 84.77%] [G loss: 5.368063]\n",
            "3360 [D loss: 0.335157, acc.: 87.89%] [G loss: 5.477335]\n",
            "3380 [D loss: 0.408369, acc.: 82.81%] [G loss: 4.978508]\n",
            "3400 [D loss: 0.377747, acc.: 82.03%] [G loss: 5.000229]\n",
            "3420 [D loss: 0.411301, acc.: 80.08%] [G loss: 4.702883]\n",
            "3440 [D loss: 0.389751, acc.: 81.25%] [G loss: 5.163847]\n",
            "3460 [D loss: 0.364691, acc.: 84.38%] [G loss: 4.934480]\n",
            "3480 [D loss: 0.313698, acc.: 87.89%] [G loss: 5.449514]\n",
            "3500 [D loss: 0.315051, acc.: 88.67%] [G loss: 5.151597]\n",
            "3520 [D loss: 0.294548, acc.: 88.28%] [G loss: 5.393147]\n",
            "3540 [D loss: 0.360445, acc.: 83.20%] [G loss: 5.641194]\n",
            "3560 [D loss: 0.415940, acc.: 80.47%] [G loss: 5.054404]\n",
            "3580 [D loss: 0.353312, acc.: 84.38%] [G loss: 5.222958]\n",
            "3600 [D loss: 0.288212, acc.: 88.28%] [G loss: 5.577487]\n",
            "3620 [D loss: 0.343285, acc.: 85.55%] [G loss: 5.528058]\n",
            "3640 [D loss: 0.379959, acc.: 82.42%] [G loss: 4.946277]\n",
            "3660 [D loss: 0.299427, acc.: 85.94%] [G loss: 5.427139]\n",
            "3680 [D loss: 0.308425, acc.: 86.72%] [G loss: 5.204938]\n",
            "3700 [D loss: 0.379670, acc.: 83.98%] [G loss: 4.750484]\n",
            "3720 [D loss: 0.323518, acc.: 89.06%] [G loss: 6.012178]\n",
            "3740 [D loss: 0.311610, acc.: 88.67%] [G loss: 5.523414]\n",
            "3760 [D loss: 0.424161, acc.: 83.98%] [G loss: 5.111194]\n",
            "3780 [D loss: 0.404188, acc.: 80.47%] [G loss: 4.824166]\n",
            "3800 [D loss: 0.348751, acc.: 83.98%] [G loss: 5.565083]\n",
            "3820 [D loss: 0.362564, acc.: 82.03%] [G loss: 5.672406]\n",
            "3840 [D loss: 0.274154, acc.: 90.23%] [G loss: 5.396473]\n",
            "3860 [D loss: 0.397751, acc.: 82.42%] [G loss: 5.311002]\n",
            "3880 [D loss: 0.281521, acc.: 89.84%] [G loss: 5.293602]\n",
            "3900 [D loss: 0.239948, acc.: 93.36%] [G loss: 4.203655]\n",
            "3920 [D loss: 0.388629, acc.: 83.20%] [G loss: 5.298659]\n",
            "3940 [D loss: 0.416746, acc.: 82.42%] [G loss: 5.064167]\n",
            "3960 [D loss: 0.326796, acc.: 89.06%] [G loss: 5.618279]\n",
            "3980 [D loss: 0.406631, acc.: 81.64%] [G loss: 5.007459]\n",
            "4000 [D loss: 0.359046, acc.: 83.98%] [G loss: 5.233293]\n",
            "4020 [D loss: 0.309346, acc.: 87.11%] [G loss: 5.397418]\n",
            "4040 [D loss: 0.397237, acc.: 83.59%] [G loss: 5.136081]\n",
            "4060 [D loss: 0.263437, acc.: 89.45%] [G loss: 5.792728]\n",
            "4080 [D loss: 0.375356, acc.: 83.59%] [G loss: 5.597792]\n",
            "4100 [D loss: 0.425198, acc.: 77.73%] [G loss: 5.603860]\n",
            "4120 [D loss: 0.355144, acc.: 85.16%] [G loss: 4.942945]\n",
            "4140 [D loss: 0.310467, acc.: 87.50%] [G loss: 5.567299]\n",
            "4160 [D loss: 0.415152, acc.: 81.64%] [G loss: 5.416082]\n",
            "4180 [D loss: 0.411281, acc.: 82.42%] [G loss: 4.888762]\n",
            "4200 [D loss: 0.355689, acc.: 84.77%] [G loss: 5.024950]\n",
            "4220 [D loss: 0.364526, acc.: 83.59%] [G loss: 5.475099]\n",
            "4240 [D loss: 0.337517, acc.: 87.89%] [G loss: 5.470329]\n",
            "4260 [D loss: 0.354351, acc.: 84.77%] [G loss: 5.294339]\n",
            "4280 [D loss: 0.390846, acc.: 82.42%] [G loss: 5.764386]\n",
            "4300 [D loss: 0.399027, acc.: 82.81%] [G loss: 4.830018]\n",
            "4320 [D loss: 0.411984, acc.: 79.69%] [G loss: 5.266260]\n",
            "4340 [D loss: 0.371327, acc.: 83.98%] [G loss: 4.919018]\n",
            "4360 [D loss: 0.377299, acc.: 86.72%] [G loss: 5.610697]\n",
            "4380 [D loss: 0.404603, acc.: 83.20%] [G loss: 4.994227]\n",
            "4400 [D loss: 0.330091, acc.: 87.89%] [G loss: 5.244478]\n",
            "4420 [D loss: 0.374008, acc.: 82.81%] [G loss: 5.226240]\n",
            "4440 [D loss: 0.343358, acc.: 86.33%] [G loss: 5.445497]\n",
            "4460 [D loss: 0.418696, acc.: 81.64%] [G loss: 4.754268]\n",
            "4480 [D loss: 0.319797, acc.: 86.72%] [G loss: 5.319033]\n",
            "4500 [D loss: 0.287284, acc.: 88.67%] [G loss: 5.353791]\n",
            "4520 [D loss: 0.341780, acc.: 87.11%] [G loss: 4.987555]\n",
            "4540 [D loss: 0.390417, acc.: 85.16%] [G loss: 5.194599]\n",
            "4560 [D loss: 0.325254, acc.: 89.06%] [G loss: 5.506325]\n",
            "4580 [D loss: 0.397132, acc.: 81.64%] [G loss: 5.209028]\n",
            "4600 [D loss: 0.335165, acc.: 86.72%] [G loss: 5.436874]\n",
            "4620 [D loss: 0.194968, acc.: 94.53%] [G loss: 3.942992]\n",
            "4640 [D loss: 0.283390, acc.: 89.06%] [G loss: 5.332844]\n",
            "4660 [D loss: 0.310230, acc.: 87.89%] [G loss: 6.026187]\n",
            "4680 [D loss: 0.429914, acc.: 79.30%] [G loss: 5.205091]\n",
            "4700 [D loss: 0.346587, acc.: 86.33%] [G loss: 5.363153]\n",
            "4720 [D loss: 0.315344, acc.: 87.11%] [G loss: 5.414298]\n",
            "4740 [D loss: 0.379832, acc.: 81.64%] [G loss: 4.749491]\n",
            "4760 [D loss: 0.429490, acc.: 81.25%] [G loss: 5.200512]\n",
            "4780 [D loss: 0.391730, acc.: 85.16%] [G loss: 5.007379]\n",
            "4800 [D loss: 0.334838, acc.: 85.16%] [G loss: 5.388161]\n",
            "4820 [D loss: 0.325584, acc.: 88.67%] [G loss: 5.573364]\n",
            "4840 [D loss: 0.339214, acc.: 83.20%] [G loss: 5.809021]\n",
            "4860 [D loss: 0.338332, acc.: 84.77%] [G loss: 5.561607]\n",
            "4880 [D loss: 0.374262, acc.: 85.16%] [G loss: 5.383989]\n",
            "4900 [D loss: 0.379931, acc.: 86.33%] [G loss: 5.251416]\n",
            "4920 [D loss: 0.331883, acc.: 87.11%] [G loss: 5.405425]\n",
            "4940 [D loss: 0.334563, acc.: 84.38%] [G loss: 5.640156]\n",
            "4960 [D loss: 0.439441, acc.: 81.25%] [G loss: 5.522693]\n",
            "4980 [D loss: 0.289872, acc.: 89.84%] [G loss: 5.592683]\n",
            "5000 [D loss: 0.394560, acc.: 84.38%] [G loss: 5.213978]\n",
            "5020 [D loss: 0.306190, acc.: 87.11%] [G loss: 5.533834]\n",
            "5040 [D loss: 0.349813, acc.: 86.72%] [G loss: 5.110530]\n",
            "5060 [D loss: 0.300289, acc.: 91.02%] [G loss: 5.372245]\n",
            "5080 [D loss: 0.360505, acc.: 83.20%] [G loss: 5.911191]\n",
            "5100 [D loss: 0.319047, acc.: 86.72%] [G loss: 5.747505]\n",
            "5120 [D loss: 0.363069, acc.: 84.77%] [G loss: 5.584892]\n",
            "5140 [D loss: 0.385717, acc.: 83.59%] [G loss: 5.609719]\n",
            "5160 [D loss: 0.362564, acc.: 83.98%] [G loss: 5.778864]\n",
            "5180 [D loss: 0.317650, acc.: 85.55%] [G loss: 6.104589]\n",
            "5200 [D loss: 0.251684, acc.: 91.02%] [G loss: 5.762933]\n",
            "5220 [D loss: 0.301229, acc.: 87.89%] [G loss: 5.769100]\n",
            "5240 [D loss: 0.306466, acc.: 88.28%] [G loss: 5.430788]\n",
            "5260 [D loss: 0.298505, acc.: 87.50%] [G loss: 6.293819]\n",
            "5280 [D loss: 0.282071, acc.: 86.72%] [G loss: 5.871499]\n",
            "5300 [D loss: 0.314756, acc.: 87.11%] [G loss: 6.444508]\n",
            "5320 [D loss: 0.384698, acc.: 82.42%] [G loss: 5.484086]\n",
            "5340 [D loss: 0.300356, acc.: 88.28%] [G loss: 5.790524]\n",
            "5360 [D loss: 0.294371, acc.: 85.94%] [G loss: 5.791577]\n",
            "5380 [D loss: 0.342749, acc.: 85.16%] [G loss: 4.439356]\n",
            "5400 [D loss: 0.235910, acc.: 91.80%] [G loss: 5.700068]\n",
            "5420 [D loss: 0.365386, acc.: 83.98%] [G loss: 5.132436]\n",
            "5440 [D loss: 0.331750, acc.: 87.11%] [G loss: 5.891356]\n",
            "5460 [D loss: 0.262088, acc.: 89.06%] [G loss: 5.571550]\n",
            "5480 [D loss: 0.314808, acc.: 87.11%] [G loss: 5.646025]\n",
            "5500 [D loss: 0.233844, acc.: 92.19%] [G loss: 6.248126]\n",
            "5520 [D loss: 0.347749, acc.: 85.55%] [G loss: 5.817762]\n",
            "5540 [D loss: 0.362213, acc.: 84.77%] [G loss: 5.883071]\n",
            "5560 [D loss: 0.292542, acc.: 88.67%] [G loss: 6.394390]\n",
            "5580 [D loss: 0.268725, acc.: 89.45%] [G loss: 6.440895]\n",
            "5600 [D loss: 0.316412, acc.: 85.55%] [G loss: 6.040981]\n",
            "5620 [D loss: 0.222469, acc.: 92.19%] [G loss: 6.744613]\n",
            "5640 [D loss: 0.263532, acc.: 87.89%] [G loss: 6.116303]\n",
            "5660 [D loss: 0.286192, acc.: 89.45%] [G loss: 6.010793]\n",
            "5680 [D loss: 0.296819, acc.: 88.28%] [G loss: 5.903666]\n",
            "5700 [D loss: 0.282366, acc.: 87.89%] [G loss: 5.365356]\n",
            "5720 [D loss: 0.395449, acc.: 80.08%] [G loss: 5.242566]\n",
            "5740 [D loss: 0.326615, acc.: 85.55%] [G loss: 5.571678]\n",
            "5760 [D loss: 0.332601, acc.: 84.38%] [G loss: 5.675895]\n",
            "5780 [D loss: 0.385176, acc.: 81.25%] [G loss: 5.950945]\n",
            "5800 [D loss: 0.382811, acc.: 83.20%] [G loss: 5.254795]\n",
            "5820 [D loss: 0.333592, acc.: 87.50%] [G loss: 5.504701]\n",
            "5840 [D loss: 0.419590, acc.: 82.42%] [G loss: 5.453451]\n",
            "5860 [D loss: 0.273522, acc.: 91.80%] [G loss: 5.945286]\n",
            "5880 [D loss: 0.345261, acc.: 86.33%] [G loss: 5.456802]\n",
            "5900 [D loss: 0.327052, acc.: 83.59%] [G loss: 5.362304]\n",
            "5920 [D loss: 0.342705, acc.: 83.59%] [G loss: 5.935745]\n",
            "5940 [D loss: 0.264386, acc.: 89.06%] [G loss: 3.793453]\n",
            "5960 [D loss: 0.301584, acc.: 87.89%] [G loss: 5.095552]\n",
            "5980 [D loss: 0.363940, acc.: 84.77%] [G loss: 5.450327]\n",
            "6000 [D loss: 0.402290, acc.: 80.47%] [G loss: 4.837795]\n",
            "6020 [D loss: 0.348144, acc.: 83.98%] [G loss: 6.261810]\n",
            "6040 [D loss: 0.287552, acc.: 88.67%] [G loss: 5.299012]\n",
            "6060 [D loss: 0.445677, acc.: 77.34%] [G loss: 5.946408]\n",
            "6080 [D loss: 0.227243, acc.: 90.62%] [G loss: 6.162244]\n",
            "6100 [D loss: 0.353216, acc.: 83.20%] [G loss: 6.010668]\n",
            "6120 [D loss: 0.375577, acc.: 82.81%] [G loss: 5.656889]\n",
            "6140 [D loss: 0.345884, acc.: 86.33%] [G loss: 5.737312]\n",
            "6160 [D loss: 0.334394, acc.: 86.72%] [G loss: 5.291653]\n",
            "6180 [D loss: 0.300782, acc.: 85.94%] [G loss: 5.146754]\n",
            "6200 [D loss: 0.281913, acc.: 90.62%] [G loss: 5.748967]\n",
            "6220 [D loss: 0.334877, acc.: 85.94%] [G loss: 5.493914]\n",
            "6240 [D loss: 0.337611, acc.: 85.55%] [G loss: 5.100847]\n",
            "6260 [D loss: 0.324709, acc.: 84.77%] [G loss: 5.309240]\n",
            "6280 [D loss: 0.383528, acc.: 83.20%] [G loss: 5.204239]\n",
            "6300 [D loss: 0.456260, acc.: 78.12%] [G loss: 4.921412]\n",
            "6320 [D loss: 0.319257, acc.: 88.28%] [G loss: 4.922379]\n",
            "6340 [D loss: 0.347562, acc.: 85.16%] [G loss: 5.244738]\n",
            "6360 [D loss: 0.268125, acc.: 89.84%] [G loss: 5.325434]\n",
            "6380 [D loss: 0.284629, acc.: 87.50%] [G loss: 5.339823]\n",
            "6400 [D loss: 0.453858, acc.: 76.56%] [G loss: 5.390445]\n",
            "6420 [D loss: 0.307804, acc.: 86.72%] [G loss: 5.089771]\n",
            "6440 [D loss: 0.370477, acc.: 85.55%] [G loss: 5.368149]\n",
            "6460 [D loss: 0.339559, acc.: 85.16%] [G loss: 5.092026]\n",
            "6480 [D loss: 0.411319, acc.: 81.25%] [G loss: 4.833236]\n",
            "6500 [D loss: 0.332855, acc.: 87.50%] [G loss: 5.338369]\n",
            "6520 [D loss: 0.360680, acc.: 84.77%] [G loss: 5.107018]\n",
            "6540 [D loss: 0.360588, acc.: 82.81%] [G loss: 5.363896]\n",
            "6560 [D loss: 0.259306, acc.: 88.67%] [G loss: 5.369337]\n",
            "6580 [D loss: 0.339890, acc.: 83.20%] [G loss: 4.808709]\n",
            "6600 [D loss: 0.140767, acc.: 94.53%] [G loss: 5.386733]\n",
            "6620 [D loss: 0.347659, acc.: 87.50%] [G loss: 5.324479]\n",
            "6640 [D loss: 0.380079, acc.: 83.98%] [G loss: 4.730723]\n",
            "6660 [D loss: 0.366442, acc.: 83.59%] [G loss: 6.020202]\n",
            "6680 [D loss: 0.331211, acc.: 86.72%] [G loss: 5.635776]\n",
            "6700 [D loss: 0.316687, acc.: 87.11%] [G loss: 5.261915]\n",
            "6720 [D loss: 0.263666, acc.: 87.50%] [G loss: 5.789947]\n",
            "6740 [D loss: 0.450168, acc.: 77.34%] [G loss: 5.162899]\n",
            "6760 [D loss: 0.289071, acc.: 90.62%] [G loss: 5.552494]\n",
            "6780 [D loss: 0.316516, acc.: 86.33%] [G loss: 5.380622]\n",
            "6800 [D loss: 0.350055, acc.: 86.72%] [G loss: 5.656824]\n",
            "6820 [D loss: 0.322616, acc.: 87.50%] [G loss: 5.498057]\n",
            "6840 [D loss: 0.322218, acc.: 86.33%] [G loss: 5.597958]\n",
            "6860 [D loss: 0.340773, acc.: 84.77%] [G loss: 5.283210]\n",
            "6880 [D loss: 0.350483, acc.: 87.89%] [G loss: 5.216579]\n",
            "6900 [D loss: 0.244983, acc.: 89.84%] [G loss: 5.237999]\n",
            "6920 [D loss: 0.318166, acc.: 87.11%] [G loss: 5.810130]\n",
            "6940 [D loss: 0.416369, acc.: 79.69%] [G loss: 5.419020]\n",
            "6960 [D loss: 0.433403, acc.: 82.81%] [G loss: 6.122477]\n",
            "6980 [D loss: 0.337892, acc.: 86.33%] [G loss: 5.497224]\n",
            "7000 [D loss: 0.373318, acc.: 83.20%] [G loss: 5.608348]\n",
            "7020 [D loss: 0.269202, acc.: 88.67%] [G loss: 5.577579]\n",
            "7040 [D loss: 0.088703, acc.: 98.44%] [G loss: 3.912928]\n",
            "7060 [D loss: 0.328243, acc.: 85.94%] [G loss: 4.819490]\n",
            "7080 [D loss: 0.420681, acc.: 78.12%] [G loss: 5.446097]\n",
            "7100 [D loss: 0.448072, acc.: 79.69%] [G loss: 4.985517]\n",
            "7120 [D loss: 0.429468, acc.: 79.30%] [G loss: 5.316717]\n",
            "7140 [D loss: 0.312819, acc.: 87.89%] [G loss: 5.414249]\n",
            "7160 [D loss: 0.332111, acc.: 86.72%] [G loss: 5.827503]\n",
            "7180 [D loss: 0.238801, acc.: 89.45%] [G loss: 5.157704]\n",
            "7200 [D loss: 0.307728, acc.: 85.55%] [G loss: 5.356025]\n",
            "7220 [D loss: 0.321008, acc.: 83.98%] [G loss: 5.497461]\n",
            "7240 [D loss: 0.357018, acc.: 84.77%] [G loss: 5.168530]\n",
            "7260 [D loss: 0.404600, acc.: 82.03%] [G loss: 5.734508]\n",
            "7280 [D loss: 0.351773, acc.: 81.64%] [G loss: 5.576539]\n",
            "7300 [D loss: 0.271966, acc.: 88.67%] [G loss: 5.642989]\n",
            "7320 [D loss: 0.271960, acc.: 87.50%] [G loss: 5.433709]\n",
            "7340 [D loss: 0.310674, acc.: 85.55%] [G loss: 5.472804]\n",
            "7360 [D loss: 0.297896, acc.: 87.89%] [G loss: 5.940438]\n",
            "7380 [D loss: 0.315322, acc.: 85.55%] [G loss: 5.497225]\n",
            "7400 [D loss: 0.260705, acc.: 90.62%] [G loss: 5.451283]\n",
            "7420 [D loss: 0.347332, acc.: 83.20%] [G loss: 5.647743]\n",
            "7440 [D loss: 0.437393, acc.: 80.86%] [G loss: 5.248051]\n",
            "7460 [D loss: 0.309130, acc.: 87.89%] [G loss: 5.564451]\n",
            "7480 [D loss: 0.258613, acc.: 90.23%] [G loss: 5.398925]\n",
            "7500 [D loss: 0.373239, acc.: 85.55%] [G loss: 5.228782]\n",
            "7520 [D loss: 0.362169, acc.: 83.59%] [G loss: 5.316082]\n",
            "7540 [D loss: 0.237251, acc.: 89.45%] [G loss: 5.718632]\n",
            "7560 [D loss: 0.389103, acc.: 81.64%] [G loss: 5.932398]\n",
            "7580 [D loss: 0.286184, acc.: 89.06%] [G loss: 5.214523]\n",
            "7600 [D loss: 0.367061, acc.: 85.55%] [G loss: 5.895893]\n",
            "7620 [D loss: 0.338533, acc.: 85.94%] [G loss: 5.206831]\n",
            "7640 [D loss: 0.390912, acc.: 82.42%] [G loss: 5.079202]\n",
            "7660 [D loss: 0.307353, acc.: 89.45%] [G loss: 4.937565]\n",
            "7680 [D loss: 0.323087, acc.: 88.28%] [G loss: 5.009768]\n",
            "7700 [D loss: 0.308532, acc.: 89.06%] [G loss: 5.077505]\n",
            "7720 [D loss: 0.298557, acc.: 89.06%] [G loss: 5.603384]\n",
            "7740 [D loss: 0.280219, acc.: 88.28%] [G loss: 5.810865]\n",
            "7760 [D loss: 0.355162, acc.: 83.98%] [G loss: 5.248317]\n",
            "7780 [D loss: 0.324720, acc.: 87.50%] [G loss: 5.954989]\n",
            "7800 [D loss: 0.315485, acc.: 87.50%] [G loss: 5.386941]\n",
            "7820 [D loss: 0.510927, acc.: 75.00%] [G loss: 5.247611]\n",
            "7840 [D loss: 0.834379, acc.: 64.45%] [G loss: 4.532631]\n",
            "7860 [D loss: 0.543879, acc.: 72.27%] [G loss: 3.511292]\n",
            "7880 [D loss: 0.712902, acc.: 58.20%] [G loss: 2.758089]\n",
            "7900 [D loss: 0.697044, acc.: 58.98%] [G loss: 2.727075]\n",
            "7920 [D loss: 0.569789, acc.: 72.66%] [G loss: 3.217299]\n",
            "7940 [D loss: 0.593531, acc.: 68.36%] [G loss: 3.060927]\n",
            "7960 [D loss: 0.450351, acc.: 81.64%] [G loss: 3.634347]\n",
            "7980 [D loss: 0.427071, acc.: 76.95%] [G loss: 4.088515]\n",
            "8000 [D loss: 0.403474, acc.: 81.64%] [G loss: 4.158008]\n",
            "8020 [D loss: 0.264738, acc.: 89.06%] [G loss: 4.936428]\n",
            "8040 [D loss: 0.573446, acc.: 75.39%] [G loss: 3.190813]\n",
            "8060 [D loss: 0.286453, acc.: 86.33%] [G loss: 5.337687]\n",
            "8080 [D loss: 0.353029, acc.: 86.33%] [G loss: 5.581841]\n",
            "8100 [D loss: 0.340519, acc.: 84.38%] [G loss: 6.170039]\n",
            "8120 [D loss: 0.395155, acc.: 82.03%] [G loss: 5.883152]\n",
            "8140 [D loss: 0.319650, acc.: 83.98%] [G loss: 5.560670]\n",
            "8160 [D loss: 0.368742, acc.: 85.16%] [G loss: 5.223540]\n",
            "8180 [D loss: 0.317306, acc.: 86.72%] [G loss: 6.455637]\n",
            "8200 [D loss: 0.360140, acc.: 87.11%] [G loss: 5.370908]\n",
            "8220 [D loss: 0.297327, acc.: 88.67%] [G loss: 6.195693]\n",
            "8240 [D loss: 0.468766, acc.: 77.73%] [G loss: 4.452073]\n",
            "8260 [D loss: 0.342802, acc.: 83.98%] [G loss: 5.458196]\n",
            "8280 [D loss: 0.330584, acc.: 85.55%] [G loss: 4.843324]\n",
            "8300 [D loss: 0.348265, acc.: 86.33%] [G loss: 4.559250]\n",
            "8320 [D loss: 0.306799, acc.: 86.72%] [G loss: 5.830043]\n",
            "8340 [D loss: 0.432454, acc.: 82.03%] [G loss: 4.482040]\n",
            "8360 [D loss: 0.376301, acc.: 83.98%] [G loss: 5.358859]\n",
            "8380 [D loss: 0.282868, acc.: 88.67%] [G loss: 5.618600]\n",
            "8400 [D loss: 0.336804, acc.: 82.81%] [G loss: 5.274965]\n",
            "8420 [D loss: 0.363905, acc.: 83.20%] [G loss: 5.295593]\n",
            "8440 [D loss: 0.443801, acc.: 77.34%] [G loss: 4.648054]\n",
            "8460 [D loss: 0.347363, acc.: 86.72%] [G loss: 4.994941]\n",
            "8480 [D loss: 0.335500, acc.: 85.94%] [G loss: 4.914373]\n",
            "8500 [D loss: 0.316940, acc.: 87.89%] [G loss: 4.927286]\n",
            "8520 [D loss: 0.316858, acc.: 85.55%] [G loss: 4.926901]\n",
            "8540 [D loss: 0.358397, acc.: 84.38%] [G loss: 4.686515]\n",
            "8560 [D loss: 0.340775, acc.: 86.33%] [G loss: 4.704080]\n",
            "8580 [D loss: 0.328553, acc.: 84.77%] [G loss: 4.910675]\n",
            "8600 [D loss: 0.311557, acc.: 89.84%] [G loss: 4.430941]\n",
            "8620 [D loss: 0.300677, acc.: 88.28%] [G loss: 5.097423]\n",
            "8640 [D loss: 0.379789, acc.: 85.16%] [G loss: 4.683819]\n",
            "8660 [D loss: 0.383746, acc.: 85.55%] [G loss: 4.533529]\n",
            "8680 [D loss: 0.366701, acc.: 83.20%] [G loss: 4.618659]\n",
            "8700 [D loss: 0.438025, acc.: 81.25%] [G loss: 4.483653]\n",
            "8720 [D loss: 0.291162, acc.: 89.84%] [G loss: 4.547482]\n",
            "8740 [D loss: 0.272211, acc.: 89.45%] [G loss: 5.259771]\n",
            "8760 [D loss: 0.300338, acc.: 89.06%] [G loss: 4.994818]\n",
            "8780 [D loss: 0.165917, acc.: 95.70%] [G loss: 4.161804]\n",
            "8800 [D loss: 0.262181, acc.: 89.06%] [G loss: 4.381589]\n",
            "8820 [D loss: 0.366845, acc.: 84.77%] [G loss: 4.678288]\n",
            "8840 [D loss: 0.356838, acc.: 84.38%] [G loss: 4.391065]\n",
            "8860 [D loss: 0.317773, acc.: 86.72%] [G loss: 5.087899]\n",
            "8880 [D loss: 0.301357, acc.: 89.06%] [G loss: 5.207426]\n",
            "8900 [D loss: 0.415138, acc.: 81.25%] [G loss: 4.783024]\n",
            "8920 [D loss: 0.261798, acc.: 89.45%] [G loss: 4.700616]\n",
            "8940 [D loss: 0.290867, acc.: 87.89%] [G loss: 5.362225]\n",
            "8960 [D loss: 0.302861, acc.: 89.45%] [G loss: 5.130871]\n",
            "8980 [D loss: 0.405977, acc.: 82.42%] [G loss: 4.672053]\n",
            "9000 [D loss: 0.301594, acc.: 86.33%] [G loss: 4.924905]\n",
            "9020 [D loss: 0.374966, acc.: 85.16%] [G loss: 4.598363]\n",
            "9040 [D loss: 0.356573, acc.: 83.98%] [G loss: 4.320249]\n",
            "9060 [D loss: 0.291078, acc.: 89.84%] [G loss: 5.025998]\n",
            "9080 [D loss: 0.284955, acc.: 89.06%] [G loss: 4.873401]\n",
            "9100 [D loss: 0.340587, acc.: 84.38%] [G loss: 4.727383]\n",
            "9120 [D loss: 0.409597, acc.: 82.42%] [G loss: 4.274794]\n",
            "9140 [D loss: 0.340841, acc.: 85.94%] [G loss: 4.420088]\n",
            "9160 [D loss: 0.367653, acc.: 87.11%] [G loss: 4.632758]\n",
            "9180 [D loss: 0.343595, acc.: 85.55%] [G loss: 4.362648]\n",
            "9200 [D loss: 0.402040, acc.: 83.98%] [G loss: 4.331769]\n",
            "9220 [D loss: 0.338936, acc.: 85.94%] [G loss: 4.614621]\n",
            "9240 [D loss: 0.314944, acc.: 85.94%] [G loss: 4.928287]\n",
            "9260 [D loss: 0.313025, acc.: 87.11%] [G loss: 4.394022]\n",
            "9280 [D loss: 0.340353, acc.: 87.11%] [G loss: 4.729609]\n",
            "9300 [D loss: 0.388792, acc.: 82.81%] [G loss: 4.618552]\n",
            "9320 [D loss: 0.323104, acc.: 84.38%] [G loss: 4.993255]\n",
            "9340 [D loss: 0.388894, acc.: 85.94%] [G loss: 4.203055]\n",
            "9360 [D loss: 0.432078, acc.: 81.64%] [G loss: 4.226387]\n",
            "9380 [D loss: 0.400364, acc.: 81.25%] [G loss: 4.625824]\n",
            "9400 [D loss: 0.405556, acc.: 83.59%] [G loss: 4.488070]\n",
            "9420 [D loss: 0.323865, acc.: 85.94%] [G loss: 4.650316]\n",
            "9440 [D loss: 0.354810, acc.: 85.55%] [G loss: 4.565337]\n",
            "9460 [D loss: 0.413705, acc.: 81.64%] [G loss: 4.067022]\n",
            "9480 [D loss: 0.342281, acc.: 84.77%] [G loss: 4.250812]\n",
            "9500 [D loss: 0.310325, acc.: 87.50%] [G loss: 4.318416]\n",
            "9520 [D loss: 0.323131, acc.: 88.67%] [G loss: 4.726884]\n",
            "9540 [D loss: 0.360582, acc.: 84.38%] [G loss: 4.409189]\n",
            "9560 [D loss: 0.355458, acc.: 82.81%] [G loss: 4.455582]\n",
            "9580 [D loss: 0.338105, acc.: 85.55%] [G loss: 4.128982]\n",
            "9600 [D loss: 0.347263, acc.: 87.89%] [G loss: 4.534519]\n",
            "9620 [D loss: 0.298550, acc.: 87.89%] [G loss: 4.690908]\n",
            "9640 [D loss: 0.349257, acc.: 84.77%] [G loss: 4.295678]\n",
            "9660 [D loss: 0.279677, acc.: 88.67%] [G loss: 4.817104]\n",
            "9680 [D loss: 0.357021, acc.: 84.77%] [G loss: 4.612180]\n",
            "9700 [D loss: 0.225340, acc.: 91.02%] [G loss: 3.468170]\n",
            "9720 [D loss: 0.408100, acc.: 80.86%] [G loss: 4.110803]\n",
            "9740 [D loss: 0.354475, acc.: 87.89%] [G loss: 4.343664]\n",
            "9760 [D loss: 0.454871, acc.: 81.64%] [G loss: 4.114688]\n",
            "9780 [D loss: 0.265703, acc.: 89.84%] [G loss: 4.728211]\n",
            "9800 [D loss: 0.389367, acc.: 84.38%] [G loss: 4.563790]\n",
            "9820 [D loss: 0.375536, acc.: 83.20%] [G loss: 4.234245]\n",
            "9840 [D loss: 0.290670, acc.: 88.28%] [G loss: 4.791508]\n",
            "9860 [D loss: 0.300152, acc.: 87.89%] [G loss: 4.726147]\n",
            "9880 [D loss: 0.333102, acc.: 87.89%] [G loss: 4.696382]\n",
            "9900 [D loss: 0.356743, acc.: 85.55%] [G loss: 4.687780]\n",
            "9920 [D loss: 0.362884, acc.: 84.38%] [G loss: 4.338201]\n",
            "9940 [D loss: 0.310876, acc.: 86.72%] [G loss: 4.624803]\n",
            "9960 [D loss: 0.325089, acc.: 87.11%] [G loss: 5.075215]\n",
            "9980 [D loss: 0.354865, acc.: 85.55%] [G loss: 4.868052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKyOoPom5Zz",
        "outputId": "c8d3af2f-3729-4904-f9a0-9cb8c6d9a8f8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 48, 48, 128)  3328        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 100)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          51712       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 2304)      16128       input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1152)         0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2304)         0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_15[0][0]                 \n",
            "                                                                 flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            3969        concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.775621, acc.: 20.31%] [G loss: 1.527493]\n",
            "20 [D loss: 1.136945, acc.: 31.64%] [G loss: 1.106470]\n",
            "40 [D loss: 0.436015, acc.: 49.61%] [G loss: 17.784233]\n",
            "60 [D loss: 0.537491, acc.: 73.83%] [G loss: 4.473651]\n",
            "80 [D loss: 0.834621, acc.: 22.66%] [G loss: 1.869293]\n",
            "100 [D loss: 0.741704, acc.: 39.45%] [G loss: 1.638891]\n",
            "120 [D loss: 0.575083, acc.: 82.81%] [G loss: 2.042169]\n",
            "140 [D loss: 0.455398, acc.: 78.12%] [G loss: 3.064530]\n",
            "160 [D loss: 1.021226, acc.: 2.73%] [G loss: 1.166114]\n",
            "180 [D loss: 0.558926, acc.: 76.17%] [G loss: 2.472870]\n",
            "200 [D loss: 0.770760, acc.: 41.80%] [G loss: 1.451331]\n",
            "220 [D loss: 0.876846, acc.: 49.22%] [G loss: 3.595408]\n",
            "240 [D loss: 0.186598, acc.: 100.00%] [G loss: 7.332498]\n",
            "260 [D loss: 0.682056, acc.: 68.75%] [G loss: 1.890666]\n",
            "280 [D loss: 0.209264, acc.: 95.31%] [G loss: 10.532845]\n",
            "300 [D loss: 0.691736, acc.: 60.55%] [G loss: 1.672787]\n",
            "320 [D loss: 0.672510, acc.: 71.48%] [G loss: 1.888943]\n",
            "340 [D loss: 0.385103, acc.: 89.06%] [G loss: 4.332753]\n",
            "360 [D loss: 0.380074, acc.: 84.77%] [G loss: 4.375402]\n",
            "380 [D loss: 0.762896, acc.: 57.81%] [G loss: 1.722982]\n",
            "400 [D loss: 0.730825, acc.: 46.09%] [G loss: 2.234101]\n",
            "420 [D loss: 0.644904, acc.: 57.81%] [G loss: 2.380552]\n",
            "440 [D loss: 0.686306, acc.: 59.77%] [G loss: 2.222342]\n",
            "460 [D loss: 0.746045, acc.: 54.69%] [G loss: 1.957719]\n",
            "480 [D loss: 0.601959, acc.: 67.58%] [G loss: 2.125723]\n",
            "500 [D loss: 0.650282, acc.: 62.11%] [G loss: 2.268615]\n",
            "520 [D loss: 0.609233, acc.: 67.58%] [G loss: 2.602067]\n",
            "540 [D loss: 0.572285, acc.: 69.53%] [G loss: 2.928072]\n",
            "560 [D loss: 0.618016, acc.: 66.41%] [G loss: 2.138952]\n",
            "580 [D loss: 0.671801, acc.: 58.20%] [G loss: 1.966664]\n",
            "600 [D loss: 0.712011, acc.: 48.83%] [G loss: 1.836228]\n",
            "620 [D loss: 0.744234, acc.: 47.27%] [G loss: 1.852621]\n",
            "640 [D loss: 0.625981, acc.: 65.23%] [G loss: 2.130571]\n",
            "660 [D loss: 0.605573, acc.: 68.75%] [G loss: 2.238631]\n",
            "680 [D loss: 0.745590, acc.: 51.17%] [G loss: 1.863794]\n",
            "700 [D loss: 0.541753, acc.: 73.05%] [G loss: 2.539672]\n",
            "720 [D loss: 0.546152, acc.: 75.78%] [G loss: 2.414199]\n",
            "740 [D loss: 0.617598, acc.: 65.62%] [G loss: 2.347521]\n",
            "760 [D loss: 0.445802, acc.: 83.98%] [G loss: 3.243727]\n",
            "780 [D loss: 0.495237, acc.: 78.12%] [G loss: 2.732445]\n",
            "800 [D loss: 0.560501, acc.: 75.39%] [G loss: 2.522155]\n",
            "820 [D loss: 0.555277, acc.: 67.97%] [G loss: 2.861588]\n",
            "840 [D loss: 0.536461, acc.: 76.95%] [G loss: 2.788723]\n",
            "860 [D loss: 0.578135, acc.: 70.70%] [G loss: 3.652322]\n",
            "880 [D loss: 0.623166, acc.: 65.23%] [G loss: 2.995444]\n",
            "900 [D loss: 0.660175, acc.: 62.89%] [G loss: 2.713311]\n",
            "920 [D loss: 0.448838, acc.: 81.25%] [G loss: 3.552639]\n",
            "940 [D loss: 0.507613, acc.: 76.56%] [G loss: 3.297152]\n",
            "960 [D loss: 0.538144, acc.: 73.05%] [G loss: 3.129484]\n",
            "980 [D loss: 0.657594, acc.: 61.72%] [G loss: 3.054131]\n",
            "1000 [D loss: 0.435557, acc.: 84.77%] [G loss: 3.602971]\n",
            "1020 [D loss: 0.389269, acc.: 85.16%] [G loss: 4.215716]\n",
            "1040 [D loss: 0.441645, acc.: 80.47%] [G loss: 3.759854]\n",
            "1060 [D loss: 0.417450, acc.: 81.64%] [G loss: 3.896385]\n",
            "1080 [D loss: 0.648562, acc.: 64.84%] [G loss: 3.626264]\n",
            "1100 [D loss: 0.402189, acc.: 83.98%] [G loss: 3.678422]\n",
            "1120 [D loss: 0.418486, acc.: 82.81%] [G loss: 3.632628]\n",
            "1140 [D loss: 0.467013, acc.: 76.95%] [G loss: 3.824034]\n",
            "1160 [D loss: 0.570863, acc.: 70.31%] [G loss: 3.044311]\n",
            "1180 [D loss: 0.511243, acc.: 76.95%] [G loss: 3.353204]\n",
            "1200 [D loss: 0.478253, acc.: 78.12%] [G loss: 3.580424]\n",
            "1220 [D loss: 0.470860, acc.: 77.73%] [G loss: 3.401241]\n",
            "1240 [D loss: 0.509738, acc.: 75.00%] [G loss: 3.294276]\n",
            "1260 [D loss: 0.510494, acc.: 78.91%] [G loss: 3.284930]\n",
            "1280 [D loss: 0.589470, acc.: 67.19%] [G loss: 3.138620]\n",
            "1300 [D loss: 0.555125, acc.: 73.05%] [G loss: 2.883266]\n",
            "1320 [D loss: 0.479775, acc.: 79.30%] [G loss: 3.127534]\n",
            "1340 [D loss: 0.498140, acc.: 75.78%] [G loss: 3.167541]\n",
            "1360 [D loss: 0.530504, acc.: 76.56%] [G loss: 2.911742]\n",
            "1380 [D loss: 0.482003, acc.: 77.34%] [G loss: 3.266805]\n",
            "1400 [D loss: 0.470014, acc.: 80.47%] [G loss: 3.325715]\n",
            "1420 [D loss: 0.546466, acc.: 73.44%] [G loss: 3.093244]\n",
            "1440 [D loss: 0.548517, acc.: 73.83%] [G loss: 3.152225]\n",
            "1460 [D loss: 0.533497, acc.: 71.88%] [G loss: 3.297659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1-QlEvelsO",
        "outputId": "d5e6db9b-1b83-4118-fa9f-1173864d140b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      16128       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_73 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_33 (Flatten)            (None, 2304)         0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_32 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4708)         0           input_73[0][0]                   \n",
            "                                                                 flatten_33[0][0]                 \n",
            "                                                                 flatten_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 1024)         4822016     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, 1024)         0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 1024)         0           leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1024)         1049600     dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1024)         0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 1024)         0           leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 1024)         1049600     dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1024)         0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 1024)         0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 1)            1025        dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.849750, acc.: 30.47%] [G loss: 5.367685]\n",
            "20 [D loss: 0.341891, acc.: 91.41%] [G loss: 8.554846]\n",
            "40 [D loss: 0.710111, acc.: 44.14%] [G loss: 2.083147]\n",
            "60 [D loss: 0.715512, acc.: 46.48%] [G loss: 1.738583]\n",
            "80 [D loss: 0.648917, acc.: 56.25%] [G loss: 1.863081]\n",
            "100 [D loss: 0.701115, acc.: 44.92%] [G loss: 1.767049]\n",
            "120 [D loss: 0.732758, acc.: 40.23%] [G loss: 1.580268]\n",
            "140 [D loss: 0.706723, acc.: 48.05%] [G loss: 1.537512]\n",
            "160 [D loss: 0.717456, acc.: 40.23%] [G loss: 1.437199]\n",
            "180 [D loss: 0.710523, acc.: 42.97%] [G loss: 1.439812]\n",
            "200 [D loss: 0.711228, acc.: 45.70%] [G loss: 1.451416]\n",
            "220 [D loss: 0.721137, acc.: 35.55%] [G loss: 1.414149]\n",
            "240 [D loss: 0.714687, acc.: 42.58%] [G loss: 1.432792]\n",
            "260 [D loss: 0.703062, acc.: 46.09%] [G loss: 1.423483]\n",
            "280 [D loss: 0.724828, acc.: 36.72%] [G loss: 1.404998]\n",
            "300 [D loss: 0.716795, acc.: 42.58%] [G loss: 1.396966]\n",
            "320 [D loss: 0.720376, acc.: 42.97%] [G loss: 1.444355]\n",
            "340 [D loss: 0.706069, acc.: 46.88%] [G loss: 1.423882]\n",
            "360 [D loss: 0.709865, acc.: 39.06%] [G loss: 1.404603]\n",
            "380 [D loss: 0.714929, acc.: 43.75%] [G loss: 1.404289]\n",
            "400 [D loss: 0.711042, acc.: 42.97%] [G loss: 1.414505]\n",
            "420 [D loss: 0.717568, acc.: 40.23%] [G loss: 1.403865]\n",
            "440 [D loss: 0.711510, acc.: 40.23%] [G loss: 1.410104]\n",
            "460 [D loss: 0.714272, acc.: 41.41%] [G loss: 1.419285]\n",
            "480 [D loss: 0.708251, acc.: 41.02%] [G loss: 1.413074]\n",
            "500 [D loss: 0.706564, acc.: 40.62%] [G loss: 1.414759]\n",
            "520 [D loss: 0.711570, acc.: 39.84%] [G loss: 1.416901]\n",
            "540 [D loss: 0.707966, acc.: 44.14%] [G loss: 1.396562]\n",
            "560 [D loss: 0.714514, acc.: 40.62%] [G loss: 1.400588]\n",
            "580 [D loss: 0.708786, acc.: 39.06%] [G loss: 1.401240]\n",
            "600 [D loss: 0.715925, acc.: 37.50%] [G loss: 1.401176]\n",
            "620 [D loss: 0.700743, acc.: 42.19%] [G loss: 1.417309]\n",
            "640 [D loss: 0.706829, acc.: 40.23%] [G loss: 1.403668]\n",
            "660 [D loss: 0.707380, acc.: 39.84%] [G loss: 1.389831]\n",
            "680 [D loss: 0.707225, acc.: 39.84%] [G loss: 1.398783]\n",
            "700 [D loss: 0.703080, acc.: 40.62%] [G loss: 1.396986]\n",
            "720 [D loss: 0.718166, acc.: 34.38%] [G loss: 1.389264]\n",
            "740 [D loss: 0.713579, acc.: 40.23%] [G loss: 1.403226]\n",
            "760 [D loss: 0.705897, acc.: 45.31%] [G loss: 1.437489]\n",
            "780 [D loss: 0.707634, acc.: 40.62%] [G loss: 1.401700]\n",
            "800 [D loss: 0.701959, acc.: 39.06%] [G loss: 1.398977]\n",
            "820 [D loss: 0.706529, acc.: 42.19%] [G loss: 1.400484]\n",
            "840 [D loss: 0.700337, acc.: 41.02%] [G loss: 1.400969]\n",
            "860 [D loss: 0.706927, acc.: 38.67%] [G loss: 1.405506]\n",
            "880 [D loss: 0.708806, acc.: 38.28%] [G loss: 1.387069]\n",
            "900 [D loss: 0.706434, acc.: 41.41%] [G loss: 1.398114]\n",
            "920 [D loss: 0.704907, acc.: 35.94%] [G loss: 1.406425]\n",
            "940 [D loss: 0.696471, acc.: 47.66%] [G loss: 1.392628]\n",
            "960 [D loss: 0.705585, acc.: 41.02%] [G loss: 1.396953]\n",
            "980 [D loss: 0.705313, acc.: 39.84%] [G loss: 1.393413]\n",
            "1000 [D loss: 0.693701, acc.: 48.44%] [G loss: 1.410988]\n",
            "1020 [D loss: 0.707825, acc.: 38.28%] [G loss: 1.393575]\n",
            "1040 [D loss: 0.700687, acc.: 44.53%] [G loss: 1.400779]\n",
            "1060 [D loss: 0.703935, acc.: 39.84%] [G loss: 1.387593]\n",
            "1080 [D loss: 0.706629, acc.: 39.84%] [G loss: 1.389481]\n",
            "1100 [D loss: 0.707315, acc.: 37.89%] [G loss: 1.378371]\n",
            "1120 [D loss: 0.707380, acc.: 42.97%] [G loss: 1.383276]\n",
            "1140 [D loss: 0.701792, acc.: 45.31%] [G loss: 1.392983]\n",
            "1160 [D loss: 0.698018, acc.: 41.41%] [G loss: 1.396971]\n",
            "1180 [D loss: 0.710089, acc.: 35.94%] [G loss: 1.389826]\n",
            "1200 [D loss: 0.704007, acc.: 40.23%] [G loss: 1.392146]\n",
            "1220 [D loss: 0.704774, acc.: 42.97%] [G loss: 1.406959]\n",
            "1240 [D loss: 0.699959, acc.: 41.41%] [G loss: 1.412235]\n",
            "1260 [D loss: 0.702670, acc.: 44.53%] [G loss: 1.408520]\n",
            "1280 [D loss: 0.704078, acc.: 41.41%] [G loss: 1.384988]\n",
            "1300 [D loss: 0.707281, acc.: 40.62%] [G loss: 1.399330]\n",
            "1320 [D loss: 0.709640, acc.: 39.06%] [G loss: 1.400736]\n",
            "1340 [D loss: 0.697176, acc.: 46.88%] [G loss: 1.408559]\n",
            "1360 [D loss: 0.690890, acc.: 46.09%] [G loss: 1.407980]\n",
            "1380 [D loss: 0.695711, acc.: 48.44%] [G loss: 1.384939]\n",
            "1400 [D loss: 0.696284, acc.: 41.41%] [G loss: 1.391898]\n",
            "1420 [D loss: 0.702413, acc.: 41.02%] [G loss: 1.391516]\n",
            "1440 [D loss: 0.702305, acc.: 43.75%] [G loss: 1.404577]\n",
            "1460 [D loss: 0.702467, acc.: 40.62%] [G loss: 1.401769]\n",
            "1480 [D loss: 0.700135, acc.: 39.84%] [G loss: 1.391395]\n",
            "1500 [D loss: 0.701792, acc.: 41.80%] [G loss: 1.390833]\n",
            "1520 [D loss: 0.700585, acc.: 42.19%] [G loss: 1.394488]\n",
            "1540 [D loss: 0.704487, acc.: 41.41%] [G loss: 1.397046]\n",
            "1560 [D loss: 0.695279, acc.: 46.48%] [G loss: 1.408600]\n",
            "1580 [D loss: 0.695438, acc.: 48.83%] [G loss: 1.392757]\n",
            "1600 [D loss: 0.704662, acc.: 38.67%] [G loss: 1.390574]\n",
            "1620 [D loss: 0.699533, acc.: 46.09%] [G loss: 1.401004]\n",
            "1640 [D loss: 0.702735, acc.: 43.75%] [G loss: 1.398700]\n",
            "1660 [D loss: 0.697723, acc.: 43.75%] [G loss: 1.401226]\n",
            "1680 [D loss: 0.697773, acc.: 41.80%] [G loss: 1.410201]\n",
            "1700 [D loss: 0.696773, acc.: 43.36%] [G loss: 1.407255]\n",
            "1720 [D loss: 0.701552, acc.: 39.45%] [G loss: 1.395131]\n",
            "1740 [D loss: 0.696195, acc.: 48.44%] [G loss: 1.396674]\n",
            "1760 [D loss: 0.702511, acc.: 43.36%] [G loss: 1.383245]\n",
            "1780 [D loss: 0.704975, acc.: 40.62%] [G loss: 1.399527]\n",
            "1800 [D loss: 0.698895, acc.: 41.41%] [G loss: 1.391627]\n",
            "1820 [D loss: 0.702525, acc.: 37.50%] [G loss: 1.391825]\n",
            "1840 [D loss: 0.698295, acc.: 46.48%] [G loss: 1.401908]\n",
            "1860 [D loss: 0.699764, acc.: 42.97%] [G loss: 1.399934]\n",
            "1880 [D loss: 0.702305, acc.: 41.80%] [G loss: 1.396282]\n",
            "1900 [D loss: 0.705898, acc.: 42.19%] [G loss: 1.384895]\n",
            "1920 [D loss: 0.700531, acc.: 44.92%] [G loss: 1.391573]\n",
            "1940 [D loss: 0.707957, acc.: 41.02%] [G loss: 1.386891]\n",
            "1960 [D loss: 0.700286, acc.: 41.41%] [G loss: 1.396966]\n",
            "1980 [D loss: 0.697184, acc.: 42.19%] [G loss: 1.398432]\n",
            "2000 [D loss: 0.701151, acc.: 46.09%] [G loss: 1.408123]\n",
            "2020 [D loss: 0.707549, acc.: 41.80%] [G loss: 1.400710]\n",
            "2040 [D loss: 0.706370, acc.: 41.80%] [G loss: 1.382454]\n",
            "2060 [D loss: 0.696820, acc.: 44.14%] [G loss: 1.416820]\n",
            "2080 [D loss: 0.704532, acc.: 39.84%] [G loss: 1.388172]\n",
            "2100 [D loss: 0.698797, acc.: 41.80%] [G loss: 1.401481]\n",
            "2120 [D loss: 0.701321, acc.: 42.58%] [G loss: 1.405037]\n",
            "2140 [D loss: 0.700277, acc.: 42.58%] [G loss: 1.388465]\n",
            "2160 [D loss: 0.706183, acc.: 34.77%] [G loss: 1.395349]\n",
            "2180 [D loss: 0.700379, acc.: 44.53%] [G loss: 1.399946]\n",
            "2200 [D loss: 0.706776, acc.: 35.55%] [G loss: 1.397158]\n",
            "2220 [D loss: 0.690010, acc.: 49.61%] [G loss: 1.415059]\n",
            "2240 [D loss: 0.700388, acc.: 45.31%] [G loss: 1.396531]\n",
            "2260 [D loss: 0.698710, acc.: 41.41%] [G loss: 1.387498]\n",
            "2280 [D loss: 0.699097, acc.: 44.53%] [G loss: 1.400677]\n",
            "2300 [D loss: 0.698183, acc.: 41.41%] [G loss: 1.393741]\n",
            "2320 [D loss: 0.700072, acc.: 44.92%] [G loss: 1.387109]\n",
            "2340 [D loss: 0.702045, acc.: 43.75%] [G loss: 1.396456]\n",
            "2360 [D loss: 0.698545, acc.: 42.97%] [G loss: 1.386749]\n",
            "2380 [D loss: 0.701191, acc.: 44.53%] [G loss: 1.390651]\n",
            "2400 [D loss: 0.702897, acc.: 41.02%] [G loss: 1.385156]\n",
            "2420 [D loss: 0.705316, acc.: 41.02%] [G loss: 1.401502]\n",
            "2440 [D loss: 0.706111, acc.: 37.11%] [G loss: 1.388617]\n",
            "2460 [D loss: 0.702543, acc.: 41.02%] [G loss: 1.407373]\n",
            "2480 [D loss: 0.705135, acc.: 44.92%] [G loss: 1.386979]\n",
            "2500 [D loss: 0.699529, acc.: 43.75%] [G loss: 1.433604]\n",
            "2520 [D loss: 0.701719, acc.: 43.36%] [G loss: 1.395393]\n",
            "2540 [D loss: 0.699508, acc.: 42.58%] [G loss: 1.396002]\n",
            "2560 [D loss: 0.703406, acc.: 38.67%] [G loss: 1.390184]\n",
            "2580 [D loss: 0.702974, acc.: 38.67%] [G loss: 1.384311]\n",
            "2600 [D loss: 0.697291, acc.: 44.92%] [G loss: 1.391137]\n",
            "2620 [D loss: 0.698857, acc.: 46.48%] [G loss: 1.397241]\n",
            "2640 [D loss: 0.702192, acc.: 41.02%] [G loss: 1.391943]\n",
            "2660 [D loss: 0.703083, acc.: 35.94%] [G loss: 1.392532]\n",
            "2680 [D loss: 0.704199, acc.: 41.80%] [G loss: 1.383410]\n",
            "2700 [D loss: 0.705399, acc.: 40.62%] [G loss: 1.386579]\n",
            "2720 [D loss: 0.702500, acc.: 42.19%] [G loss: 1.398633]\n",
            "2740 [D loss: 0.701238, acc.: 41.41%] [G loss: 1.387710]\n",
            "2760 [D loss: 0.695434, acc.: 47.66%] [G loss: 1.398785]\n",
            "2780 [D loss: 0.698381, acc.: 46.48%] [G loss: 1.409770]\n",
            "2800 [D loss: 0.703224, acc.: 41.02%] [G loss: 1.398382]\n",
            "2820 [D loss: 0.704637, acc.: 42.97%] [G loss: 1.391324]\n",
            "2840 [D loss: 0.695273, acc.: 46.48%] [G loss: 1.403218]\n",
            "2860 [D loss: 0.703234, acc.: 45.31%] [G loss: 1.395308]\n",
            "2880 [D loss: 0.699889, acc.: 41.80%] [G loss: 1.389093]\n",
            "2900 [D loss: 0.704404, acc.: 42.19%] [G loss: 1.391987]\n",
            "2920 [D loss: 0.700413, acc.: 43.36%] [G loss: 1.394257]\n",
            "2940 [D loss: 0.701555, acc.: 45.31%] [G loss: 1.386916]\n",
            "2960 [D loss: 0.704574, acc.: 41.02%] [G loss: 1.397935]\n",
            "2980 [D loss: 0.701216, acc.: 38.28%] [G loss: 1.397118]\n",
            "3000 [D loss: 0.703859, acc.: 43.75%] [G loss: 1.386564]\n",
            "3020 [D loss: 0.697224, acc.: 47.27%] [G loss: 1.404107]\n",
            "3040 [D loss: 0.700284, acc.: 44.14%] [G loss: 1.408972]\n",
            "3060 [D loss: 0.710935, acc.: 39.45%] [G loss: 1.398036]\n",
            "3080 [D loss: 0.701705, acc.: 43.75%] [G loss: 1.385907]\n",
            "3100 [D loss: 0.703351, acc.: 42.19%] [G loss: 1.400969]\n",
            "3120 [D loss: 0.705543, acc.: 35.55%] [G loss: 1.395355]\n",
            "3140 [D loss: 0.705566, acc.: 41.02%] [G loss: 1.389833]\n",
            "3160 [D loss: 0.694014, acc.: 50.78%] [G loss: 1.399073]\n",
            "3180 [D loss: 0.696958, acc.: 47.27%] [G loss: 1.398360]\n",
            "3200 [D loss: 0.697163, acc.: 45.31%] [G loss: 1.396299]\n",
            "3220 [D loss: 0.704172, acc.: 39.45%] [G loss: 1.401600]\n",
            "3240 [D loss: 0.706146, acc.: 35.55%] [G loss: 1.386641]\n",
            "3260 [D loss: 0.693955, acc.: 46.88%] [G loss: 1.392742]\n",
            "3280 [D loss: 0.694101, acc.: 48.83%] [G loss: 1.400743]\n",
            "3300 [D loss: 0.699886, acc.: 42.58%] [G loss: 1.380824]\n",
            "3320 [D loss: 0.709387, acc.: 47.27%] [G loss: 1.398697]\n",
            "3340 [D loss: 0.699752, acc.: 48.44%] [G loss: 1.394596]\n",
            "3360 [D loss: 0.706852, acc.: 40.62%] [G loss: 1.405188]\n",
            "3380 [D loss: 0.699275, acc.: 44.53%] [G loss: 1.398795]\n",
            "3400 [D loss: 0.696006, acc.: 44.92%] [G loss: 1.421854]\n",
            "3420 [D loss: 0.696535, acc.: 48.05%] [G loss: 1.411503]\n",
            "3440 [D loss: 0.702331, acc.: 43.36%] [G loss: 1.399069]\n",
            "3460 [D loss: 0.705177, acc.: 42.19%] [G loss: 1.388886]\n",
            "3480 [D loss: 0.703387, acc.: 36.33%] [G loss: 1.393602]\n",
            "3500 [D loss: 0.701317, acc.: 38.67%] [G loss: 1.405229]\n",
            "3520 [D loss: 0.700779, acc.: 36.33%] [G loss: 1.394291]\n",
            "3540 [D loss: 0.704368, acc.: 43.75%] [G loss: 1.388223]\n",
            "3560 [D loss: 0.698320, acc.: 44.92%] [G loss: 1.408260]\n",
            "3580 [D loss: 0.699920, acc.: 41.80%] [G loss: 1.396066]\n",
            "3600 [D loss: 0.703215, acc.: 42.97%] [G loss: 1.393963]\n",
            "3620 [D loss: 0.704095, acc.: 42.58%] [G loss: 1.389001]\n",
            "3640 [D loss: 0.696503, acc.: 46.48%] [G loss: 1.391987]\n",
            "3660 [D loss: 0.706459, acc.: 39.45%] [G loss: 1.403823]\n",
            "3680 [D loss: 0.701097, acc.: 45.70%] [G loss: 1.391359]\n",
            "3700 [D loss: 0.700797, acc.: 40.62%] [G loss: 1.401428]\n",
            "3720 [D loss: 0.706842, acc.: 39.45%] [G loss: 1.401338]\n",
            "3740 [D loss: 0.695790, acc.: 48.44%] [G loss: 1.410640]\n",
            "3760 [D loss: 0.700871, acc.: 41.02%] [G loss: 1.389013]\n",
            "3780 [D loss: 0.698214, acc.: 45.31%] [G loss: 1.391579]\n",
            "3800 [D loss: 0.702306, acc.: 41.02%] [G loss: 1.390742]\n",
            "3820 [D loss: 0.699549, acc.: 44.92%] [G loss: 1.388456]\n",
            "3840 [D loss: 0.726881, acc.: 41.02%] [G loss: 1.427503]\n",
            "3860 [D loss: 0.706836, acc.: 40.23%] [G loss: 1.408166]\n",
            "3880 [D loss: 0.708650, acc.: 37.89%] [G loss: 1.388743]\n",
            "3900 [D loss: 0.703004, acc.: 42.19%] [G loss: 1.413317]\n",
            "3920 [D loss: 0.701894, acc.: 43.75%] [G loss: 1.391886]\n",
            "3940 [D loss: 0.695446, acc.: 49.22%] [G loss: 1.405801]\n",
            "3960 [D loss: 0.704857, acc.: 41.02%] [G loss: 1.401155]\n",
            "3980 [D loss: 0.699231, acc.: 42.58%] [G loss: 1.404975]\n",
            "4000 [D loss: 0.699956, acc.: 39.84%] [G loss: 1.397395]\n",
            "4020 [D loss: 0.693868, acc.: 48.83%] [G loss: 1.414370]\n",
            "4040 [D loss: 0.699592, acc.: 46.48%] [G loss: 1.402936]\n",
            "4060 [D loss: 0.697715, acc.: 45.31%] [G loss: 1.424391]\n",
            "4080 [D loss: 0.708574, acc.: 40.23%] [G loss: 1.381411]\n",
            "4100 [D loss: 1.072704, acc.: 84.38%] [G loss: 26.749468]\n",
            "4120 [D loss: 0.717382, acc.: 67.97%] [G loss: 6.265586]\n",
            "4140 [D loss: 1.001233, acc.: 50.00%] [G loss: 3.090046]\n",
            "4160 [D loss: 0.768635, acc.: 51.56%] [G loss: 2.125261]\n",
            "4180 [D loss: 0.740759, acc.: 46.48%] [G loss: 1.783273]\n",
            "4200 [D loss: 0.722959, acc.: 47.66%] [G loss: 1.520224]\n",
            "4220 [D loss: 0.734547, acc.: 45.31%] [G loss: 1.466105]\n",
            "4240 [D loss: 0.688544, acc.: 57.03%] [G loss: 1.615793]\n",
            "4260 [D loss: 0.738717, acc.: 43.75%] [G loss: 1.405132]\n",
            "4280 [D loss: 0.722575, acc.: 42.19%] [G loss: 1.447261]\n",
            "4300 [D loss: 0.690146, acc.: 49.61%] [G loss: 1.476141]\n",
            "4320 [D loss: 0.719582, acc.: 43.75%] [G loss: 1.429009]\n",
            "4340 [D loss: 0.720578, acc.: 45.70%] [G loss: 1.434604]\n",
            "4360 [D loss: 0.706739, acc.: 45.70%] [G loss: 1.414366]\n",
            "4380 [D loss: 0.702524, acc.: 48.44%] [G loss: 1.432420]\n",
            "4400 [D loss: 0.710964, acc.: 48.83%] [G loss: 1.441298]\n",
            "4420 [D loss: 0.711855, acc.: 46.48%] [G loss: 1.427701]\n",
            "4440 [D loss: 0.710840, acc.: 47.66%] [G loss: 1.458087]\n",
            "4460 [D loss: 0.705373, acc.: 44.53%] [G loss: 1.425750]\n",
            "4480 [D loss: 0.704472, acc.: 48.83%] [G loss: 1.433243]\n",
            "4500 [D loss: 0.698071, acc.: 51.17%] [G loss: 1.431295]\n",
            "4520 [D loss: 0.716092, acc.: 41.80%] [G loss: 1.414632]\n",
            "4540 [D loss: 0.709937, acc.: 44.53%] [G loss: 1.402517]\n",
            "4560 [D loss: 0.706339, acc.: 43.75%] [G loss: 1.417696]\n",
            "4580 [D loss: 0.701099, acc.: 49.61%] [G loss: 1.398619]\n",
            "4600 [D loss: 0.705118, acc.: 43.75%] [G loss: 1.420572]\n",
            "4620 [D loss: 0.706447, acc.: 43.75%] [G loss: 1.411010]\n",
            "4640 [D loss: 0.701780, acc.: 48.44%] [G loss: 1.398860]\n",
            "4660 [D loss: 0.705210, acc.: 48.05%] [G loss: 1.383090]\n",
            "4680 [D loss: 0.708879, acc.: 46.09%] [G loss: 1.397547]\n",
            "4700 [D loss: 0.710725, acc.: 42.97%] [G loss: 1.417038]\n",
            "4720 [D loss: 0.704634, acc.: 45.70%] [G loss: 1.393745]\n",
            "4740 [D loss: 0.696728, acc.: 51.56%] [G loss: 1.395010]\n",
            "4760 [D loss: 0.702551, acc.: 48.83%] [G loss: 1.404335]\n",
            "4780 [D loss: 0.699379, acc.: 46.48%] [G loss: 1.415344]\n",
            "4800 [D loss: 0.700585, acc.: 49.22%] [G loss: 1.406814]\n",
            "4820 [D loss: 0.698388, acc.: 46.88%] [G loss: 1.390556]\n",
            "4840 [D loss: 0.694551, acc.: 51.56%] [G loss: 1.385368]\n",
            "4860 [D loss: 0.694786, acc.: 53.12%] [G loss: 1.396678]\n",
            "4880 [D loss: 0.703839, acc.: 44.53%] [G loss: 1.405728]\n",
            "4900 [D loss: 0.705474, acc.: 44.53%] [G loss: 1.391244]\n",
            "4920 [D loss: 0.698867, acc.: 46.09%] [G loss: 1.391430]\n",
            "4940 [D loss: 0.705874, acc.: 39.45%] [G loss: 1.395215]\n",
            "4960 [D loss: 0.697893, acc.: 49.61%] [G loss: 1.396267]\n",
            "4980 [D loss: 0.709537, acc.: 39.84%] [G loss: 1.394031]\n",
            "5000 [D loss: 0.704992, acc.: 42.19%] [G loss: 1.393940]\n",
            "5020 [D loss: 0.702913, acc.: 48.05%] [G loss: 1.390844]\n",
            "5040 [D loss: 0.697565, acc.: 48.83%] [G loss: 1.388696]\n",
            "5060 [D loss: 0.700234, acc.: 42.58%] [G loss: 1.404058]\n",
            "5080 [D loss: 0.703479, acc.: 44.92%] [G loss: 1.406952]\n",
            "5100 [D loss: 0.693664, acc.: 49.22%] [G loss: 1.407932]\n",
            "5120 [D loss: 0.706131, acc.: 42.97%] [G loss: 1.404334]\n",
            "5140 [D loss: 0.700269, acc.: 48.44%] [G loss: 1.391555]\n",
            "5160 [D loss: 0.696768, acc.: 45.31%] [G loss: 1.396595]\n",
            "5180 [D loss: 0.699592, acc.: 46.09%] [G loss: 1.397417]\n",
            "5200 [D loss: 0.695383, acc.: 47.27%] [G loss: 1.387535]\n",
            "5220 [D loss: 0.693621, acc.: 47.27%] [G loss: 1.408859]\n",
            "5240 [D loss: 0.695037, acc.: 47.66%] [G loss: 1.403044]\n",
            "5260 [D loss: 0.700985, acc.: 41.41%] [G loss: 1.395171]\n",
            "5280 [D loss: 0.695701, acc.: 47.66%] [G loss: 1.403005]\n",
            "5300 [D loss: 0.700270, acc.: 42.19%] [G loss: 1.394384]\n",
            "5320 [D loss: 0.698988, acc.: 44.92%] [G loss: 1.401334]\n",
            "5340 [D loss: 0.694417, acc.: 53.12%] [G loss: 1.388224]\n",
            "5360 [D loss: 0.696847, acc.: 50.78%] [G loss: 1.402042]\n",
            "5380 [D loss: 0.699440, acc.: 46.48%] [G loss: 1.400354]\n",
            "5400 [D loss: 0.695333, acc.: 46.88%] [G loss: 1.389589]\n",
            "5420 [D loss: 0.693278, acc.: 52.34%] [G loss: 1.395435]\n",
            "5440 [D loss: 0.697515, acc.: 52.73%] [G loss: 1.403365]\n",
            "5460 [D loss: 0.700039, acc.: 44.92%] [G loss: 1.385313]\n",
            "5480 [D loss: 0.701342, acc.: 41.80%] [G loss: 1.400771]\n",
            "5500 [D loss: 0.691950, acc.: 52.73%] [G loss: 1.405167]\n",
            "5520 [D loss: 0.697928, acc.: 47.66%] [G loss: 1.407117]\n",
            "5540 [D loss: 0.694162, acc.: 50.39%] [G loss: 1.412841]\n",
            "5560 [D loss: 0.696665, acc.: 49.22%] [G loss: 1.398718]\n",
            "5580 [D loss: 0.700371, acc.: 45.31%] [G loss: 1.393484]\n",
            "5600 [D loss: 0.702304, acc.: 42.19%] [G loss: 1.393507]\n",
            "5620 [D loss: 0.694285, acc.: 50.39%] [G loss: 1.394578]\n",
            "5640 [D loss: 0.697163, acc.: 46.88%] [G loss: 1.395253]\n",
            "5660 [D loss: 0.693129, acc.: 50.78%] [G loss: 1.392592]\n",
            "5680 [D loss: 0.696335, acc.: 45.31%] [G loss: 1.386523]\n",
            "5700 [D loss: 0.697125, acc.: 50.00%] [G loss: 1.412799]\n",
            "5720 [D loss: 0.697893, acc.: 47.27%] [G loss: 1.399107]\n",
            "5740 [D loss: 0.695130, acc.: 49.22%] [G loss: 1.400753]\n",
            "5760 [D loss: 0.698657, acc.: 44.14%] [G loss: 1.395851]\n",
            "5780 [D loss: 0.697089, acc.: 47.66%] [G loss: 1.392132]\n",
            "5800 [D loss: 0.700125, acc.: 46.09%] [G loss: 1.401990]\n",
            "5820 [D loss: 0.697709, acc.: 47.27%] [G loss: 1.394505]\n",
            "5840 [D loss: 0.693451, acc.: 50.39%] [G loss: 1.400309]\n",
            "5860 [D loss: 0.697274, acc.: 45.31%] [G loss: 1.386672]\n",
            "5880 [D loss: 0.700695, acc.: 44.53%] [G loss: 1.398656]\n",
            "5900 [D loss: 0.697150, acc.: 47.27%] [G loss: 1.390164]\n",
            "5920 [D loss: 0.696779, acc.: 44.14%] [G loss: 1.396117]\n",
            "5940 [D loss: 0.693782, acc.: 50.00%] [G loss: 1.390036]\n",
            "5960 [D loss: 0.700256, acc.: 51.17%] [G loss: 1.398869]\n",
            "5980 [D loss: 0.696114, acc.: 49.22%] [G loss: 1.395630]\n",
            "6000 [D loss: 0.697736, acc.: 45.70%] [G loss: 1.393245]\n",
            "6020 [D loss: 0.699556, acc.: 43.75%] [G loss: 1.396108]\n",
            "6040 [D loss: 0.693481, acc.: 50.39%] [G loss: 1.397763]\n",
            "6060 [D loss: 0.703812, acc.: 44.14%] [G loss: 1.388477]\n",
            "6080 [D loss: 0.697451, acc.: 43.36%] [G loss: 1.389992]\n",
            "6100 [D loss: 0.692298, acc.: 46.88%] [G loss: 1.387400]\n",
            "6120 [D loss: 0.699595, acc.: 45.70%] [G loss: 1.405494]\n",
            "6140 [D loss: 0.697563, acc.: 45.70%] [G loss: 1.396657]\n",
            "6160 [D loss: 0.695493, acc.: 48.83%] [G loss: 1.383792]\n",
            "6180 [D loss: 0.697858, acc.: 43.75%] [G loss: 1.381215]\n",
            "6200 [D loss: 0.695829, acc.: 48.83%] [G loss: 1.407797]\n",
            "6220 [D loss: 0.717301, acc.: 37.11%] [G loss: 1.387314]\n",
            "6240 [D loss: 0.697440, acc.: 46.88%] [G loss: 1.404925]\n",
            "6260 [D loss: 0.693353, acc.: 48.05%] [G loss: 1.386393]\n",
            "6280 [D loss: 0.691351, acc.: 53.52%] [G loss: 1.411113]\n",
            "6300 [D loss: 0.697193, acc.: 48.44%] [G loss: 1.393546]\n",
            "6320 [D loss: 0.693712, acc.: 48.05%] [G loss: 1.386686]\n",
            "6340 [D loss: 0.701024, acc.: 43.75%] [G loss: 1.396646]\n",
            "6360 [D loss: 0.701343, acc.: 46.09%] [G loss: 1.392262]\n",
            "6380 [D loss: 0.696558, acc.: 48.05%] [G loss: 1.396049]\n",
            "6400 [D loss: 0.699096, acc.: 45.31%] [G loss: 1.396192]\n",
            "6420 [D loss: 0.695062, acc.: 48.44%] [G loss: 1.393358]\n",
            "6440 [D loss: 0.699274, acc.: 41.80%] [G loss: 1.398998]\n",
            "6460 [D loss: 0.698745, acc.: 42.97%] [G loss: 1.413985]\n",
            "6480 [D loss: 0.696595, acc.: 49.22%] [G loss: 1.407619]\n",
            "6500 [D loss: 0.699104, acc.: 44.53%] [G loss: 1.392327]\n",
            "6520 [D loss: 0.696238, acc.: 47.66%] [G loss: 1.400739]\n",
            "6540 [D loss: 0.694688, acc.: 49.61%] [G loss: 1.409534]\n",
            "6560 [D loss: 0.700760, acc.: 41.80%] [G loss: 1.394047]\n",
            "6580 [D loss: 0.690933, acc.: 51.56%] [G loss: 1.394747]\n",
            "6600 [D loss: 0.698814, acc.: 42.58%] [G loss: 1.389279]\n",
            "6620 [D loss: 0.700194, acc.: 44.53%] [G loss: 1.398422]\n",
            "6640 [D loss: 0.698942, acc.: 47.27%] [G loss: 1.386726]\n",
            "6660 [D loss: 0.700201, acc.: 48.05%] [G loss: 1.389932]\n",
            "6680 [D loss: 0.697296, acc.: 46.88%] [G loss: 1.393817]\n",
            "6700 [D loss: 0.700214, acc.: 42.97%] [G loss: 1.383901]\n",
            "6720 [D loss: 0.697019, acc.: 46.88%] [G loss: 1.392673]\n",
            "6740 [D loss: 0.701964, acc.: 48.05%] [G loss: 1.392654]\n",
            "6760 [D loss: 0.696323, acc.: 52.34%] [G loss: 1.397271]\n",
            "6780 [D loss: 0.699229, acc.: 43.75%] [G loss: 1.387532]\n",
            "6800 [D loss: 0.693560, acc.: 50.78%] [G loss: 1.384406]\n",
            "6820 [D loss: 0.699900, acc.: 46.48%] [G loss: 1.396095]\n",
            "6840 [D loss: 0.700351, acc.: 46.09%] [G loss: 1.400817]\n",
            "6860 [D loss: 0.696565, acc.: 50.39%] [G loss: 1.393807]\n",
            "6880 [D loss: 0.694871, acc.: 49.22%] [G loss: 1.389832]\n",
            "6900 [D loss: 0.700529, acc.: 46.88%] [G loss: 1.386945]\n",
            "6920 [D loss: 0.695679, acc.: 47.27%] [G loss: 1.394230]\n",
            "6940 [D loss: 0.697947, acc.: 43.75%] [G loss: 1.383168]\n",
            "6960 [D loss: 0.697765, acc.: 47.27%] [G loss: 1.401471]\n",
            "6980 [D loss: 0.699596, acc.: 42.97%] [G loss: 1.397855]\n",
            "7000 [D loss: 0.701152, acc.: 42.19%] [G loss: 1.395025]\n",
            "7020 [D loss: 0.697597, acc.: 44.53%] [G loss: 1.392945]\n",
            "7040 [D loss: 0.696670, acc.: 44.14%] [G loss: 1.395268]\n",
            "7060 [D loss: 0.701981, acc.: 41.80%] [G loss: 1.394562]\n",
            "7080 [D loss: 0.698433, acc.: 48.83%] [G loss: 1.398969]\n",
            "7100 [D loss: 0.699508, acc.: 44.92%] [G loss: 1.388082]\n",
            "7120 [D loss: 0.698022, acc.: 46.88%] [G loss: 1.393650]\n",
            "7140 [D loss: 0.694677, acc.: 44.53%] [G loss: 1.384647]\n",
            "7160 [D loss: 0.698689, acc.: 47.27%] [G loss: 1.392577]\n",
            "7180 [D loss: 0.698242, acc.: 42.58%] [G loss: 1.391645]\n",
            "7200 [D loss: 0.700687, acc.: 43.75%] [G loss: 1.396249]\n",
            "7220 [D loss: 0.696790, acc.: 44.14%] [G loss: 1.388211]\n",
            "7240 [D loss: 0.692794, acc.: 46.48%] [G loss: 1.397785]\n",
            "7260 [D loss: 0.698412, acc.: 43.36%] [G loss: 1.383541]\n",
            "7280 [D loss: 0.696848, acc.: 45.70%] [G loss: 1.394438]\n",
            "7300 [D loss: 0.694678, acc.: 45.70%] [G loss: 1.395237]\n",
            "7320 [D loss: 0.693550, acc.: 47.66%] [G loss: 1.395023]\n",
            "7340 [D loss: 0.697784, acc.: 46.09%] [G loss: 1.393231]\n",
            "7360 [D loss: 0.699125, acc.: 43.36%] [G loss: 1.386775]\n",
            "7380 [D loss: 0.698780, acc.: 42.97%] [G loss: 1.399514]\n",
            "7400 [D loss: 0.693878, acc.: 49.61%] [G loss: 1.394288]\n",
            "7420 [D loss: 0.694396, acc.: 50.39%] [G loss: 1.387681]\n",
            "7440 [D loss: 0.699487, acc.: 42.58%] [G loss: 1.391767]\n",
            "7460 [D loss: 0.695520, acc.: 47.27%] [G loss: 1.400790]\n",
            "7480 [D loss: 0.696784, acc.: 43.75%] [G loss: 1.389204]\n",
            "7500 [D loss: 0.705090, acc.: 40.23%] [G loss: 1.392731]\n",
            "7520 [D loss: 0.698871, acc.: 41.02%] [G loss: 1.388768]\n",
            "7540 [D loss: 0.697078, acc.: 46.09%] [G loss: 1.396813]\n",
            "7560 [D loss: 0.696970, acc.: 43.36%] [G loss: 1.395193]\n",
            "7580 [D loss: 0.696497, acc.: 46.09%] [G loss: 1.390957]\n",
            "7600 [D loss: 0.702041, acc.: 41.02%] [G loss: 1.395289]\n",
            "7620 [D loss: 0.697056, acc.: 43.75%] [G loss: 1.393222]\n",
            "7640 [D loss: 0.696136, acc.: 47.27%] [G loss: 1.394576]\n",
            "7660 [D loss: 0.694996, acc.: 47.27%] [G loss: 1.393033]\n",
            "7680 [D loss: 0.695782, acc.: 46.48%] [G loss: 1.385630]\n",
            "7700 [D loss: 0.693979, acc.: 50.39%] [G loss: 1.398277]\n",
            "7720 [D loss: 0.702364, acc.: 45.70%] [G loss: 1.391881]\n",
            "7740 [D loss: 0.699747, acc.: 38.67%] [G loss: 1.395062]\n",
            "7760 [D loss: 0.697299, acc.: 44.53%] [G loss: 1.393300]\n",
            "7780 [D loss: 0.697575, acc.: 44.14%] [G loss: 1.388515]\n",
            "7800 [D loss: 0.696327, acc.: 47.27%] [G loss: 1.399309]\n",
            "7820 [D loss: 0.699796, acc.: 39.45%] [G loss: 1.394260]\n",
            "7840 [D loss: 0.697553, acc.: 44.92%] [G loss: 1.389583]\n",
            "7860 [D loss: 0.698274, acc.: 40.62%] [G loss: 1.385870]\n",
            "7880 [D loss: 0.695591, acc.: 50.39%] [G loss: 1.394403]\n",
            "7900 [D loss: 0.697667, acc.: 42.97%] [G loss: 1.399807]\n",
            "7920 [D loss: 0.699197, acc.: 43.36%] [G loss: 1.389332]\n",
            "7940 [D loss: 0.695461, acc.: 43.75%] [G loss: 1.395153]\n",
            "7960 [D loss: 0.693985, acc.: 50.78%] [G loss: 1.396758]\n",
            "7980 [D loss: 0.694616, acc.: 49.61%] [G loss: 1.394727]\n",
            "8000 [D loss: 0.698273, acc.: 47.66%] [G loss: 1.394009]\n",
            "8020 [D loss: 0.697546, acc.: 46.09%] [G loss: 1.393506]\n",
            "8040 [D loss: 0.696341, acc.: 41.80%] [G loss: 1.394150]\n",
            "8060 [D loss: 0.697245, acc.: 48.44%] [G loss: 1.392660]\n",
            "8080 [D loss: 0.694503, acc.: 49.22%] [G loss: 1.399790]\n",
            "8100 [D loss: 0.699422, acc.: 40.23%] [G loss: 1.391258]\n",
            "8120 [D loss: 0.694864, acc.: 47.66%] [G loss: 1.396652]\n",
            "8140 [D loss: 0.694395, acc.: 45.70%] [G loss: 1.396786]\n",
            "8160 [D loss: 0.696896, acc.: 41.02%] [G loss: 1.390924]\n",
            "8180 [D loss: 0.697071, acc.: 45.31%] [G loss: 1.389972]\n",
            "8200 [D loss: 0.698414, acc.: 41.02%] [G loss: 1.395693]\n",
            "8220 [D loss: 0.696968, acc.: 42.97%] [G loss: 1.391768]\n",
            "8240 [D loss: 0.695341, acc.: 49.61%] [G loss: 1.391918]\n",
            "8260 [D loss: 0.696258, acc.: 42.97%] [G loss: 1.389055]\n",
            "8280 [D loss: 0.694510, acc.: 44.92%] [G loss: 1.391949]\n",
            "8300 [D loss: 0.693706, acc.: 51.17%] [G loss: 1.391831]\n",
            "8320 [D loss: 0.692619, acc.: 50.78%] [G loss: 1.394634]\n",
            "8340 [D loss: 0.692181, acc.: 45.31%] [G loss: 1.398439]\n",
            "8360 [D loss: 0.696840, acc.: 50.00%] [G loss: 1.386354]\n",
            "8380 [D loss: 0.696040, acc.: 45.70%] [G loss: 1.388847]\n",
            "8400 [D loss: 0.697098, acc.: 38.28%] [G loss: 1.385808]\n",
            "8420 [D loss: 0.696205, acc.: 48.44%] [G loss: 1.395315]\n",
            "8440 [D loss: 0.696577, acc.: 44.53%] [G loss: 1.395890]\n",
            "8460 [D loss: 0.698952, acc.: 40.23%] [G loss: 1.392299]\n",
            "8480 [D loss: 0.705718, acc.: 39.84%] [G loss: 1.425954]\n",
            "8500 [D loss: 0.692237, acc.: 47.66%] [G loss: 1.392538]\n",
            "8520 [D loss: 0.694271, acc.: 43.36%] [G loss: 1.400621]\n",
            "8540 [D loss: 0.697283, acc.: 43.36%] [G loss: 1.392912]\n",
            "8560 [D loss: 0.693932, acc.: 46.88%] [G loss: 1.389800]\n",
            "8580 [D loss: 0.698320, acc.: 41.02%] [G loss: 1.390708]\n",
            "8600 [D loss: 0.696863, acc.: 44.53%] [G loss: 1.393254]\n",
            "8620 [D loss: 0.693541, acc.: 48.83%] [G loss: 1.392359]\n",
            "8640 [D loss: 0.698448, acc.: 46.09%] [G loss: 1.390194]\n",
            "8660 [D loss: 0.698225, acc.: 43.36%] [G loss: 1.391702]\n",
            "8680 [D loss: 0.696173, acc.: 43.36%] [G loss: 1.393579]\n",
            "8700 [D loss: 0.698311, acc.: 39.84%] [G loss: 1.393775]\n",
            "8720 [D loss: 0.695746, acc.: 42.97%] [G loss: 1.389348]\n",
            "8740 [D loss: 0.697773, acc.: 45.31%] [G loss: 1.388839]\n",
            "8760 [D loss: 0.698051, acc.: 41.02%] [G loss: 1.387564]\n",
            "8780 [D loss: 0.695869, acc.: 41.80%] [G loss: 1.393906]\n",
            "8800 [D loss: 0.695220, acc.: 43.36%] [G loss: 1.384415]\n",
            "8820 [D loss: 0.694940, acc.: 43.36%] [G loss: 1.385797]\n",
            "8840 [D loss: 0.699507, acc.: 44.53%] [G loss: 1.392140]\n",
            "8860 [D loss: 0.698243, acc.: 41.02%] [G loss: 1.383003]\n",
            "8880 [D loss: 0.694200, acc.: 51.17%] [G loss: 1.389239]\n",
            "8900 [D loss: 0.700332, acc.: 39.45%] [G loss: 1.392415]\n",
            "8920 [D loss: 0.694468, acc.: 43.75%] [G loss: 1.398754]\n",
            "8940 [D loss: 0.693868, acc.: 46.09%] [G loss: 1.401220]\n",
            "8960 [D loss: 0.696201, acc.: 46.09%] [G loss: 1.390292]\n",
            "8980 [D loss: 0.700917, acc.: 37.11%] [G loss: 1.384135]\n",
            "9000 [D loss: 0.529419, acc.: 60.16%] [G loss: 3.451092]\n",
            "9020 [D loss: 0.714005, acc.: 34.77%] [G loss: 1.418368]\n",
            "9040 [D loss: 0.708870, acc.: 40.62%] [G loss: 1.395880]\n",
            "9060 [D loss: 0.705619, acc.: 41.80%] [G loss: 1.390461]\n",
            "9080 [D loss: 0.698528, acc.: 45.31%] [G loss: 1.404152]\n",
            "9100 [D loss: 0.699169, acc.: 39.06%] [G loss: 1.402751]\n",
            "9120 [D loss: 0.696621, acc.: 41.41%] [G loss: 1.406112]\n",
            "9140 [D loss: 0.698882, acc.: 44.92%] [G loss: 1.396840]\n",
            "9160 [D loss: 0.696500, acc.: 44.53%] [G loss: 1.399978]\n",
            "9180 [D loss: 0.694397, acc.: 50.39%] [G loss: 1.403586]\n",
            "9200 [D loss: 0.694435, acc.: 49.22%] [G loss: 1.399590]\n",
            "9220 [D loss: 0.696431, acc.: 43.36%] [G loss: 1.405650]\n",
            "9240 [D loss: 0.695898, acc.: 47.27%] [G loss: 1.392597]\n",
            "9260 [D loss: 0.698319, acc.: 42.58%] [G loss: 1.398521]\n",
            "9280 [D loss: 0.700153, acc.: 39.06%] [G loss: 1.388895]\n",
            "9300 [D loss: 0.693785, acc.: 47.27%] [G loss: 1.391394]\n",
            "9320 [D loss: 0.691352, acc.: 50.78%] [G loss: 1.394004]\n",
            "9340 [D loss: 0.694901, acc.: 43.36%] [G loss: 1.396414]\n",
            "9360 [D loss: 0.696790, acc.: 42.58%] [G loss: 1.395674]\n",
            "9380 [D loss: 0.692653, acc.: 48.83%] [G loss: 1.399105]\n",
            "9400 [D loss: 0.697056, acc.: 44.14%] [G loss: 1.392526]\n",
            "9420 [D loss: 0.699672, acc.: 42.97%] [G loss: 1.388630]\n",
            "9440 [D loss: 0.696520, acc.: 47.66%] [G loss: 1.388295]\n",
            "9460 [D loss: 0.692791, acc.: 49.22%] [G loss: 1.395600]\n",
            "9480 [D loss: 0.692895, acc.: 45.70%] [G loss: 1.389935]\n",
            "9500 [D loss: 0.697345, acc.: 44.92%] [G loss: 1.405686]\n",
            "9520 [D loss: 0.695038, acc.: 48.83%] [G loss: 1.398619]\n",
            "9540 [D loss: 0.693577, acc.: 49.22%] [G loss: 1.392874]\n",
            "9560 [D loss: 0.696086, acc.: 44.53%] [G loss: 1.395670]\n",
            "9580 [D loss: 0.694003, acc.: 47.66%] [G loss: 1.405685]\n",
            "9600 [D loss: 0.691519, acc.: 48.44%] [G loss: 1.388005]\n",
            "9620 [D loss: 0.701189, acc.: 41.41%] [G loss: 1.402854]\n",
            "9640 [D loss: 0.692627, acc.: 50.39%] [G loss: 1.407890]\n",
            "9660 [D loss: 0.699078, acc.: 45.31%] [G loss: 1.403761]\n",
            "9680 [D loss: 0.692119, acc.: 46.48%] [G loss: 1.422443]\n",
            "9700 [D loss: 0.694870, acc.: 47.27%] [G loss: 1.402087]\n",
            "9720 [D loss: 0.694653, acc.: 47.66%] [G loss: 1.401800]\n",
            "9740 [D loss: 0.691276, acc.: 53.52%] [G loss: 1.412094]\n",
            "9760 [D loss: 0.697540, acc.: 41.41%] [G loss: 1.401138]\n",
            "9780 [D loss: 0.697595, acc.: 46.09%] [G loss: 1.409874]\n",
            "9800 [D loss: 0.693005, acc.: 46.48%] [G loss: 1.400883]\n",
            "9820 [D loss: 0.692952, acc.: 54.30%] [G loss: 1.406455]\n",
            "9840 [D loss: 0.691773, acc.: 48.05%] [G loss: 1.401276]\n",
            "9860 [D loss: 0.691053, acc.: 51.17%] [G loss: 1.397547]\n",
            "9880 [D loss: 0.687381, acc.: 53.12%] [G loss: 1.407923]\n",
            "9900 [D loss: 0.692056, acc.: 47.27%] [G loss: 1.400826]\n",
            "9920 [D loss: 0.694512, acc.: 43.75%] [G loss: 1.410615]\n",
            "9940 [D loss: 0.693986, acc.: 45.70%] [G loss: 1.404719]\n",
            "9960 [D loss: 0.691524, acc.: 52.73%] [G loss: 1.402085]\n",
            "9980 [D loss: 0.695208, acc.: 48.05%] [G loss: 1.405380]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}