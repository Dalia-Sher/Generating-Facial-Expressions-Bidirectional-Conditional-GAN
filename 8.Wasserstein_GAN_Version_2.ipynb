{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "8_WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/tree/master/wgan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "14e1ae2d-3f7c-49ff-d6ae-21b6286a27fe"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c41ba41-86b1-47e8-a7cc-8a093bc07355"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLasjF73EZg1",
        "outputId": "7a2c85fa-0dce-4217-b1ce-b36ce69fd1ba"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "6906fe10-d4c4-4f91-da78-b4a8b0a53038"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoWGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBLIDqvQExmx"
      },
      "source": [
        "class BiCoWGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        self.clip_value = 0.01\n",
        "        optimizer = RMSprop(lr=0.00005)\n",
        "\n",
        "        # Build and compile the critic\n",
        "        self.critic = self.build_critic()\n",
        "        self.critic.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.critic.trainable = False\n",
        "\n",
        "        # The critic takes generated images as input and determines validity\n",
        "        valid = self.critic(img)\n",
        "\n",
        "        # The combined model  (stacked generator and critic)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 12 * 12, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 12x12 image\n",
        "        # n_nodes = 128 * 12 * 12\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((12, 12, 128)))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # generate\n",
        "        # model.add(Conv2D(self.channels, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print(\"generator\")\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_critic(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        print(\"critic\")\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # # Load the dataset\n",
        "        # (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # # Rescale -1 to 1\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake = np.ones((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "                \n",
        "                # Sample noise as generator input\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "                # Generate a batch of new images\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the critic\n",
        "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
        "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "                # Clip critic weights\n",
        "                for l in self.critic.layers:\n",
        "                    weights = l.get_weights()\n",
        "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                    l.set_weights(weights)\n",
        "\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/epoch_%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51thV6AAYkps",
        "outputId": "b1253b21-8a8c-4275-d62f-ca115d94d7ae"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicowgan = BiCoWGAN()\n",
        "    bicowgan.train(epochs=10000, batch_size=128, sample_interval=400)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "critic\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "generator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.999854] [G loss: 1.000287]\n",
            "20 [D loss: 0.999944] [G loss: 1.000092]\n",
            "40 [D loss: 0.999970] [G loss: 1.000063]\n",
            "60 [D loss: 0.999970] [G loss: 1.000068]\n",
            "80 [D loss: 0.999970] [G loss: 1.000064]\n",
            "100 [D loss: 0.999973] [G loss: 1.000064]\n",
            "120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "140 [D loss: 0.999969] [G loss: 1.000063]\n",
            "160 [D loss: 0.999970] [G loss: 1.000065]\n",
            "180 [D loss: 0.999969] [G loss: 1.000063]\n",
            "200 [D loss: 0.999971] [G loss: 1.000062]\n",
            "220 [D loss: 0.999970] [G loss: 1.000060]\n",
            "240 [D loss: 0.999969] [G loss: 1.000061]\n",
            "260 [D loss: 0.999971] [G loss: 1.000053]\n",
            "280 [D loss: 0.999973] [G loss: 1.000059]\n",
            "300 [D loss: 0.999977] [G loss: 1.000050]\n",
            "320 [D loss: 0.999979] [G loss: 1.000035]\n",
            "340 [D loss: 0.999989] [G loss: 1.000009]\n",
            "360 [D loss: 0.999989] [G loss: 1.000014]\n",
            "380 [D loss: 0.999993] [G loss: 1.000008]\n",
            "400 [D loss: 1.000024] [G loss: 0.999925]\n",
            "420 [D loss: 1.000012] [G loss: 0.999966]\n",
            "440 [D loss: 1.000013] [G loss: 0.999942]\n",
            "460 [D loss: 1.000001] [G loss: 0.999948]\n",
            "480 [D loss: 1.000003] [G loss: 0.999944]\n",
            "500 [D loss: 0.999993] [G loss: 0.999978]\n",
            "520 [D loss: 0.999973] [G loss: 1.000038]\n",
            "540 [D loss: 0.999972] [G loss: 1.000039]\n",
            "560 [D loss: 0.999974] [G loss: 1.000031]\n",
            "580 [D loss: 0.999974] [G loss: 1.000045]\n",
            "600 [D loss: 0.999965] [G loss: 1.000048]\n",
            "620 [D loss: 0.999968] [G loss: 1.000048]\n",
            "640 [D loss: 0.999970] [G loss: 1.000050]\n",
            "660 [D loss: 0.999967] [G loss: 1.000058]\n",
            "680 [D loss: 0.999971] [G loss: 1.000056]\n",
            "700 [D loss: 0.999968] [G loss: 1.000057]\n",
            "720 [D loss: 0.999969] [G loss: 1.000057]\n",
            "740 [D loss: 0.999970] [G loss: 1.000058]\n",
            "760 [D loss: 0.999965] [G loss: 1.000061]\n",
            "780 [D loss: 0.999971] [G loss: 1.000057]\n",
            "800 [D loss: 0.999970] [G loss: 1.000058]\n",
            "820 [D loss: 0.999968] [G loss: 1.000063]\n",
            "840 [D loss: 0.999968] [G loss: 1.000062]\n",
            "860 [D loss: 0.999971] [G loss: 1.000061]\n",
            "880 [D loss: 0.999969] [G loss: 1.000064]\n",
            "900 [D loss: 0.999970] [G loss: 1.000062]\n",
            "920 [D loss: 0.999968] [G loss: 1.000063]\n",
            "940 [D loss: 0.999970] [G loss: 1.000065]\n",
            "960 [D loss: 0.999972] [G loss: 1.000062]\n",
            "980 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1000 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1020 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1040 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1060 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1080 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1100 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1120 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1140 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1160 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1180 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1200 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1220 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1240 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1260 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1280 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1300 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1320 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1340 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1360 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1380 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1400 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1420 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1440 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1460 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1480 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1500 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1520 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1540 [D loss: 0.999968] [G loss: 1.000058]\n",
            "1560 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1600 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1620 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1640 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1660 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1680 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1700 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1720 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1740 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1760 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1780 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1800 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1820 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1840 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1860 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1880 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1900 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1920 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1940 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1960 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2000 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2020 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2040 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2060 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2080 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2100 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2120 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2140 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2160 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2180 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2200 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2220 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2240 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2260 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2280 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2300 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2320 [D loss: 0.999971] [G loss: 1.000058]\n",
            "2340 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2360 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2380 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2400 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2420 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2440 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2460 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2480 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2500 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2520 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2560 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2580 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2600 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2620 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2640 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2660 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2680 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2700 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2720 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2740 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2760 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2780 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2800 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2820 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2860 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2880 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2900 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2920 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2940 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2960 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2980 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3000 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3020 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3040 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3060 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3080 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3100 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3120 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3140 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3160 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3180 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3200 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3220 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3240 [D loss: 0.999969] [G loss: 1.000060]\n",
            "3260 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3280 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3300 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3320 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3340 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3360 [D loss: 0.999969] [G loss: 1.000061]\n",
            "3380 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3400 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3420 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3440 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3460 [D loss: 0.999970] [G loss: 1.000060]\n",
            "3480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3500 [D loss: 0.999970] [G loss: 1.000069]\n",
            "3520 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3540 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3560 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3580 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3600 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3620 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3640 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3660 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3680 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3700 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3720 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3740 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3760 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3780 [D loss: 0.999968] [G loss: 1.000064]\n",
            "3800 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3820 [D loss: 0.999972] [G loss: 1.000060]\n",
            "3840 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3860 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3880 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3900 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3920 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3940 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3960 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3980 [D loss: 0.999971] [G loss: 1.000062]\n",
            "4000 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4020 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4040 [D loss: 0.999973] [G loss: 1.000062]\n",
            "4060 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4080 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4100 [D loss: 0.999967] [G loss: 1.000067]\n",
            "4120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4140 [D loss: 0.999968] [G loss: 1.000065]\n",
            "4160 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4180 [D loss: 0.999970] [G loss: 1.000062]\n",
            "4200 [D loss: 0.999970] [G loss: 1.000063]\n",
            "4220 [D loss: 0.999969] [G loss: 1.000064]\n",
            "4240 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4260 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4280 [D loss: 0.999971] [G loss: 1.000068]\n",
            "4300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4320 [D loss: 0.999973] [G loss: 1.000063]\n",
            "4340 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4360 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4380 [D loss: 0.999971] [G loss: 1.000063]\n",
            "4400 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4420 [D loss: 0.999973] [G loss: 1.000065]\n",
            "4440 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4460 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4480 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4500 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4520 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4540 [D loss: 0.999971] [G loss: 1.000068]\n",
            "4560 [D loss: 0.999971] [G loss: 1.000062]\n",
            "4580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "4600 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4620 [D loss: 0.999972] [G loss: 1.000063]\n",
            "4640 [D loss: 0.999969] [G loss: 1.000063]\n",
            "4660 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4680 [D loss: 0.999972] [G loss: 1.000063]\n",
            "4700 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4720 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4740 [D loss: 0.999967] [G loss: 1.000061]\n",
            "4760 [D loss: 0.999971] [G loss: 1.000061]\n",
            "4780 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4800 [D loss: 0.999970] [G loss: 1.000062]\n",
            "4820 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4840 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4860 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4880 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4900 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4920 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4940 [D loss: 0.999968] [G loss: 1.000065]\n",
            "4960 [D loss: 0.999969] [G loss: 1.000062]\n",
            "4980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5000 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5020 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5040 [D loss: 0.999971] [G loss: 1.000068]\n",
            "5060 [D loss: 0.999969] [G loss: 1.000068]\n",
            "5080 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5100 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5120 [D loss: 0.999972] [G loss: 1.000065]\n",
            "5140 [D loss: 0.999971] [G loss: 1.000061]\n",
            "5160 [D loss: 0.999968] [G loss: 1.000066]\n",
            "5180 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5200 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5220 [D loss: 0.999970] [G loss: 1.000070]\n",
            "5240 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5260 [D loss: 0.999972] [G loss: 1.000061]\n",
            "5280 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5300 [D loss: 0.999968] [G loss: 1.000067]\n",
            "5320 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5340 [D loss: 0.999970] [G loss: 1.000063]\n",
            "5360 [D loss: 0.999970] [G loss: 1.000062]\n",
            "5380 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5400 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5420 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5440 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5460 [D loss: 0.999972] [G loss: 1.000063]\n",
            "5480 [D loss: 0.999972] [G loss: 1.000062]\n",
            "5500 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5520 [D loss: 0.999969] [G loss: 1.000066]\n",
            "5540 [D loss: 0.999971] [G loss: 1.000062]\n",
            "5560 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5580 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5600 [D loss: 0.999969] [G loss: 1.000067]\n",
            "5620 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5640 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5660 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5680 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5700 [D loss: 0.999973] [G loss: 1.000061]\n",
            "5720 [D loss: 0.999972] [G loss: 1.000066]\n",
            "5740 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5760 [D loss: 0.999969] [G loss: 1.000065]\n",
            "5780 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5800 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5820 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5840 [D loss: 0.999968] [G loss: 1.000066]\n",
            "5860 [D loss: 0.999969] [G loss: 1.000070]\n",
            "5880 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5900 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5920 [D loss: 0.999971] [G loss: 1.000061]\n",
            "5940 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5960 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6000 [D loss: 0.999971] [G loss: 1.000069]\n",
            "6020 [D loss: 0.999970] [G loss: 1.000063]\n",
            "6040 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6060 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6080 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6100 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6120 [D loss: 0.999969] [G loss: 1.000067]\n",
            "6140 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6160 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6180 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6200 [D loss: 0.999970] [G loss: 1.000068]\n",
            "6220 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6260 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6280 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6300 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6320 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6340 [D loss: 0.999970] [G loss: 1.000060]\n",
            "6360 [D loss: 0.999969] [G loss: 1.000068]\n",
            "6380 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6400 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6420 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6440 [D loss: 0.999967] [G loss: 1.000067]\n",
            "6460 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6480 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6500 [D loss: 0.999970] [G loss: 1.000063]\n",
            "6520 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6540 [D loss: 0.999973] [G loss: 1.000063]\n",
            "6560 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "6600 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6620 [D loss: 0.999972] [G loss: 1.000067]\n",
            "6640 [D loss: 0.999972] [G loss: 1.000068]\n",
            "6660 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6680 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6700 [D loss: 0.999971] [G loss: 1.000068]\n",
            "6720 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6740 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6760 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6780 [D loss: 0.999971] [G loss: 1.000062]\n",
            "6800 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6820 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6840 [D loss: 0.999970] [G loss: 1.000069]\n",
            "6860 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6880 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6900 [D loss: 0.999968] [G loss: 1.000066]\n",
            "6920 [D loss: 0.999971] [G loss: 1.000068]\n",
            "6940 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6960 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6980 [D loss: 0.999972] [G loss: 1.000066]\n",
            "7000 [D loss: 0.999969] [G loss: 1.000063]\n",
            "7020 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7060 [D loss: 0.999970] [G loss: 1.000065]\n",
            "7080 [D loss: 0.999967] [G loss: 1.000067]\n",
            "7100 [D loss: 0.999971] [G loss: 1.000064]\n",
            "7120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7140 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7160 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7180 [D loss: 0.999970] [G loss: 1.000069]\n",
            "7200 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7220 [D loss: 0.999971] [G loss: 1.000069]\n",
            "7240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7260 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7280 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7300 [D loss: 0.999971] [G loss: 1.000062]\n",
            "7320 [D loss: 0.999969] [G loss: 1.000068]\n",
            "7340 [D loss: 0.999971] [G loss: 1.000063]\n",
            "7360 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7380 [D loss: 0.999971] [G loss: 1.000068]\n",
            "7400 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7420 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7440 [D loss: 0.999972] [G loss: 1.000065]\n",
            "7460 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7480 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7500 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7520 [D loss: 0.999972] [G loss: 1.000063]\n",
            "7540 [D loss: 0.999970] [G loss: 1.000065]\n",
            "7560 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7600 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7620 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7640 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7660 [D loss: 0.999972] [G loss: 1.000066]\n",
            "7680 [D loss: 0.999967] [G loss: 1.000067]\n",
            "7700 [D loss: 0.999972] [G loss: 1.000065]\n",
            "7720 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7740 [D loss: 0.999971] [G loss: 1.000062]\n",
            "7760 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7780 [D loss: 0.999972] [G loss: 1.000062]\n",
            "7800 [D loss: 0.999969] [G loss: 1.000068]\n",
            "7820 [D loss: 0.999971] [G loss: 1.000068]\n",
            "7840 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7860 [D loss: 0.999972] [G loss: 1.000068]\n",
            "7880 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7900 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7920 [D loss: 0.999969] [G loss: 1.000067]\n",
            "7940 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7960 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7980 [D loss: 0.999970] [G loss: 1.000066]\n",
            "8000 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8020 [D loss: 0.999970] [G loss: 1.000068]\n",
            "8040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8060 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8080 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8100 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8120 [D loss: 0.999973] [G loss: 1.000065]\n",
            "8140 [D loss: 0.999972] [G loss: 1.000069]\n",
            "8160 [D loss: 0.999970] [G loss: 1.000063]\n",
            "8180 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8200 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8220 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8260 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8280 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8300 [D loss: 0.999973] [G loss: 1.000065]\n",
            "8320 [D loss: 0.999971] [G loss: 1.000062]\n",
            "8340 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8380 [D loss: 0.999971] [G loss: 1.000068]\n",
            "8400 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8420 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8440 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8460 [D loss: 0.999971] [G loss: 1.000067]\n",
            "8480 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8500 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8520 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8560 [D loss: 0.999970] [G loss: 1.000064]\n",
            "8580 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8600 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8620 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8640 [D loss: 0.999969] [G loss: 1.000063]\n",
            "8660 [D loss: 0.999970] [G loss: 1.000066]\n",
            "8680 [D loss: 0.999969] [G loss: 1.000067]\n",
            "8700 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8720 [D loss: 0.999970] [G loss: 1.000067]\n",
            "8740 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8760 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8780 [D loss: 0.999968] [G loss: 1.000068]\n",
            "8800 [D loss: 0.999969] [G loss: 1.000067]\n",
            "8820 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8840 [D loss: 0.999972] [G loss: 1.000066]\n",
            "8860 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8880 [D loss: 0.999969] [G loss: 1.000065]\n",
            "8900 [D loss: 0.999971] [G loss: 1.000068]\n",
            "8920 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8940 [D loss: 0.999972] [G loss: 1.000068]\n",
            "8960 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8980 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9000 [D loss: 0.999969] [G loss: 1.000067]\n",
            "9020 [D loss: 0.999967] [G loss: 1.000064]\n",
            "9040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9060 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9080 [D loss: 0.999970] [G loss: 1.000067]\n",
            "9100 [D loss: 0.999970] [G loss: 1.000064]\n",
            "9120 [D loss: 0.999972] [G loss: 1.000068]\n",
            "9140 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9160 [D loss: 0.999973] [G loss: 1.000065]\n",
            "9180 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9200 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9220 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9240 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9260 [D loss: 0.999972] [G loss: 1.000066]\n",
            "9280 [D loss: 0.999972] [G loss: 1.000061]\n",
            "9300 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9320 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9340 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9380 [D loss: 0.999970] [G loss: 1.000063]\n",
            "9400 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9420 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9440 [D loss: 0.999970] [G loss: 1.000068]\n",
            "9460 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9500 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9520 [D loss: 0.999972] [G loss: 1.000068]\n",
            "9540 [D loss: 0.999969] [G loss: 1.000061]\n",
            "9560 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9580 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9600 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9620 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9640 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9660 [D loss: 0.999969] [G loss: 1.000064]\n",
            "9680 [D loss: 0.999970] [G loss: 1.000067]\n",
            "9700 [D loss: 0.999971] [G loss: 1.000067]\n",
            "9720 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9740 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9760 [D loss: 0.999972] [G loss: 1.000066]\n",
            "9780 [D loss: 0.999969] [G loss: 1.000067]\n",
            "9800 [D loss: 0.999969] [G loss: 1.000062]\n",
            "9820 [D loss: 0.999967] [G loss: 1.000062]\n",
            "9840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9860 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9880 [D loss: 0.999970] [G loss: 1.000063]\n",
            "9900 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9920 [D loss: 0.999971] [G loss: 1.000063]\n",
            "9940 [D loss: 0.999971] [G loss: 1.000067]\n",
            "9960 [D loss: 0.999969] [G loss: 1.000067]\n",
            "9980 [D loss: 0.999971] [G loss: 1.000064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WZ_NTK2USqrB",
        "outputId": "d862b85c-701e-43ee-87e6-23243a463cc7"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicowgan = BiCoWGAN()\n",
        "    bicowgan.train(epochs=10000, batch_size=128, sample_interval=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "critic\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_19 (Conv2D)           (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_3 (ZeroPaddin (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_13 (Batc (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)   (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "generator\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_7 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.999858] [G loss: 1.000440]\n",
            "20 [D loss: 0.999944] [G loss: 1.000090]\n",
            "40 [D loss: 0.999979] [G loss: 1.000050]\n",
            "60 [D loss: 0.999966] [G loss: 1.000180]\n",
            "80 [D loss: 0.999974] [G loss: 1.000074]\n",
            "100 [D loss: 1.000328] [G loss: 0.999601]\n",
            "120 [D loss: 1.002187] [G loss: 0.997499]\n",
            "140 [D loss: 1.006244] [G loss: 0.994191]\n",
            "160 [D loss: 1.010634] [G loss: 0.989727]\n",
            "180 [D loss: 1.013361] [G loss: 0.984453]\n",
            "200 [D loss: 1.017708] [G loss: 0.978662]\n",
            "220 [D loss: 1.018418] [G loss: 0.972215]\n",
            "240 [D loss: 0.990527] [G loss: 1.023860]\n",
            "260 [D loss: 0.999532] [G loss: 1.010483]\n",
            "280 [D loss: 1.001830] [G loss: 1.009598]\n",
            "300 [D loss: 0.999504] [G loss: 1.034465]\n",
            "320 [D loss: 0.999568] [G loss: 1.025567]\n",
            "340 [D loss: 1.000222] [G loss: 1.021979]\n",
            "360 [D loss: 1.001012] [G loss: 1.018404]\n",
            "380 [D loss: 1.002424] [G loss: 1.012691]\n",
            "400 [D loss: 1.005221] [G loss: 0.997648]\n",
            "420 [D loss: 1.008362] [G loss: 0.978780]\n",
            "440 [D loss: 1.012945] [G loss: 0.960833]\n",
            "460 [D loss: 1.018356] [G loss: 0.947092]\n",
            "480 [D loss: 1.022666] [G loss: 0.937607]\n",
            "500 [D loss: 1.027511] [G loss: 0.931288]\n",
            "520 [D loss: 1.026533] [G loss: 0.923763]\n",
            "540 [D loss: 1.030584] [G loss: 0.917436]\n",
            "560 [D loss: 1.029449] [G loss: 0.911259]\n",
            "580 [D loss: 0.998359] [G loss: 1.005078]\n",
            "600 [D loss: 1.004420] [G loss: 1.055701]\n",
            "620 [D loss: 1.001824] [G loss: 1.054542]\n",
            "640 [D loss: 1.002569] [G loss: 1.039586]\n",
            "660 [D loss: 1.005188] [G loss: 1.012406]\n",
            "680 [D loss: 1.010383] [G loss: 0.976891]\n",
            "700 [D loss: 1.017513] [G loss: 0.941454]\n",
            "720 [D loss: 1.022624] [G loss: 0.923689]\n",
            "740 [D loss: 1.026433] [G loss: 0.914126]\n",
            "760 [D loss: 1.029318] [G loss: 0.908895]\n",
            "780 [D loss: 0.981805] [G loss: 1.020574]\n",
            "800 [D loss: 1.008015] [G loss: 1.060833]\n",
            "820 [D loss: 1.007790] [G loss: 1.040555]\n",
            "840 [D loss: 1.012971] [G loss: 0.974509]\n",
            "860 [D loss: 1.020653] [G loss: 0.929723]\n",
            "880 [D loss: 1.027275] [G loss: 0.904877]\n",
            "900 [D loss: 0.995026] [G loss: 1.001425]\n",
            "920 [D loss: 1.007300] [G loss: 1.059528]\n",
            "940 [D loss: 1.001621] [G loss: 1.048012]\n",
            "960 [D loss: 1.008290] [G loss: 0.990973]\n",
            "980 [D loss: 1.017014] [G loss: 0.935995]\n",
            "1000 [D loss: 1.005431] [G loss: 0.944400]\n",
            "1020 [D loss: 1.007742] [G loss: 1.049273]\n",
            "1040 [D loss: 1.000673] [G loss: 1.045504]\n",
            "1060 [D loss: 1.006585] [G loss: 0.980204]\n",
            "1080 [D loss: 0.999525] [G loss: 1.021877]\n",
            "1100 [D loss: 0.999094] [G loss: 1.002623]\n",
            "1120 [D loss: 0.999660] [G loss: 1.003705]\n",
            "1140 [D loss: 0.999747] [G loss: 1.002335]\n",
            "1160 [D loss: 0.999916] [G loss: 1.001094]\n",
            "1180 [D loss: 0.999859] [G loss: 1.001087]\n",
            "1200 [D loss: 1.000789] [G loss: 0.999225]\n",
            "1220 [D loss: 0.999979] [G loss: 1.002686]\n",
            "1240 [D loss: 0.999864] [G loss: 1.001205]\n",
            "1260 [D loss: 1.000273] [G loss: 1.004660]\n",
            "1280 [D loss: 1.002427] [G loss: 1.002764]\n",
            "1300 [D loss: 1.009174] [G loss: 0.992704]\n",
            "1320 [D loss: 1.014002] [G loss: 0.985762]\n",
            "1340 [D loss: 1.017505] [G loss: 0.980531]\n",
            "1360 [D loss: 1.024733] [G loss: 0.973083]\n",
            "1380 [D loss: 1.026935] [G loss: 0.963822]\n",
            "1400 [D loss: 1.032706] [G loss: 0.954702]\n",
            "1420 [D loss: 0.986147] [G loss: 1.026138]\n",
            "1440 [D loss: 0.999675] [G loss: 1.002669]\n",
            "1460 [D loss: 1.011943] [G loss: 1.002874]\n",
            "1480 [D loss: 0.998185] [G loss: 1.031703]\n",
            "1500 [D loss: 0.999012] [G loss: 1.021670]\n",
            "1520 [D loss: 0.999322] [G loss: 1.016047]\n",
            "1540 [D loss: 0.999674] [G loss: 1.012674]\n",
            "1560 [D loss: 0.999297] [G loss: 1.010582]\n",
            "1580 [D loss: 0.999718] [G loss: 1.007695]\n",
            "1600 [D loss: 0.999735] [G loss: 1.005596]\n",
            "1620 [D loss: 0.999976] [G loss: 1.003476]\n",
            "1640 [D loss: 1.000036] [G loss: 1.004002]\n",
            "1660 [D loss: 0.999802] [G loss: 1.002743]\n",
            "1680 [D loss: 0.999968] [G loss: 1.000925]\n",
            "1700 [D loss: 0.999958] [G loss: 1.000404]\n",
            "1720 [D loss: 0.999954] [G loss: 1.000206]\n",
            "1740 [D loss: 0.999967] [G loss: 1.000105]\n",
            "1760 [D loss: 0.999959] [G loss: 1.000118]\n",
            "1780 [D loss: 0.999955] [G loss: 1.000157]\n",
            "1800 [D loss: 0.999966] [G loss: 1.000080]\n",
            "1820 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1840 [D loss: 1.000104] [G loss: 0.999820]\n",
            "1860 [D loss: 1.000000] [G loss: 0.999977]\n",
            "1880 [D loss: 0.999978] [G loss: 1.000011]\n",
            "1900 [D loss: 0.999963] [G loss: 1.000072]\n",
            "1920 [D loss: 0.999987] [G loss: 1.000021]\n",
            "1940 [D loss: 1.000210] [G loss: 0.999351]\n",
            "1960 [D loss: 0.999271] [G loss: 1.001034]\n",
            "1980 [D loss: 0.998439] [G loss: 1.016279]\n",
            "2000 [D loss: 0.999719] [G loss: 1.002157]\n",
            "2020 [D loss: 1.003630] [G loss: 0.995125]\n",
            "2040 [D loss: 1.013565] [G loss: 0.995035]\n",
            "2060 [D loss: 1.105642] [G loss: 0.958735]\n",
            "2080 [D loss: 1.130018] [G loss: 0.924083]\n",
            "2100 [D loss: 1.138280] [G loss: 0.899254]\n",
            "2120 [D loss: 0.999729] [G loss: 1.162242]\n",
            "2140 [D loss: 0.986125] [G loss: 1.166147]\n",
            "2160 [D loss: 0.989285] [G loss: 1.118123]\n",
            "2180 [D loss: 0.994670] [G loss: 1.064705]\n",
            "2200 [D loss: 0.998007] [G loss: 1.030141]\n",
            "2220 [D loss: 1.000086] [G loss: 1.015246]\n",
            "2240 [D loss: 1.002807] [G loss: 1.015072]\n",
            "2260 [D loss: 1.004782] [G loss: 1.008821]\n",
            "2280 [D loss: 1.012536] [G loss: 0.996395]\n",
            "2300 [D loss: 1.017967] [G loss: 0.988304]\n",
            "2320 [D loss: 1.018677] [G loss: 0.979160]\n",
            "2340 [D loss: 0.999312] [G loss: 1.007142]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-039d33fbb19a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbicowgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiCoWGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbicowgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-83efd6c8bb35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;31m# Generate a batch of new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;31m# Train the critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func)\u001b[0m\n\u001b[1;32m   1835\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m     \"\"\"\n\u001b[0;32m-> 1837\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m   def interleave(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[1;32m   4283\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4284\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[0;32m-> 4285\u001b[0;31m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[1;32m   4286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4287\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3523\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3052\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3053\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3020\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    907\u001b[0m         args, arg_names, flat_shapes=arg_shapes)\n\u001b[1;32m    908\u001b[0m     func_kwargs = _get_defun_inputs_from_kwargs(\n\u001b[0;32m--> 909\u001b[0;31m         kwargs, flat_shapes=kwarg_shapes)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;31m# Convert all Tensors into TensorSpecs before saving the structured inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs_from_kwargs\u001b[0;34m(kwargs, flat_shapes)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1272\u001b[0m   return _get_defun_inputs(\n\u001b[0;32m-> 1273\u001b[0;31m       args, names, structure=kwargs, flat_shapes=flat_shapes)\n\u001b[0m\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_get_defun_inputs\u001b[0;34m(args, names, structure, flat_shapes)\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mfunction_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m   return nest.pack_sequence_as(structure, function_inputs,\n\u001b[0;32m-> 1262\u001b[0;31m                                expand_composites=True)\n\u001b[0m\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m   \"\"\"\n\u001b[0;32m--> 579\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    540\u001b[0m           \u001b[0;34m\"flat_sequence had %d elements.  Structure: %s, flat_sequence: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m           (len(flat_structure), len(flat_sequence), structure, flat_sequence))\n\u001b[0;32m--> 542\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msequence_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_sequence_like\u001b[0;34m(instance, args)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# ordered and plain dicts (e.g., flattening a dict but using a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;31m# corresponding `OrderedDict` to pack it back).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0minstance_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minstance_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j17vxFTEyv4",
        "outputId": "4eeeab62-f8d1-4762-8b03-536fd3701d1c"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicowgan = BiCoWGAN()\n",
        "    bicowgan.train(epochs=4000, batch_size=128, sample_interval=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "critic\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPaddin (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "generator\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.999851] [G loss: 1.000287]\n",
            "1 [D loss: 0.999867] [G loss: 1.000313]\n",
            "2 [D loss: 0.999875] [G loss: 1.000299]\n",
            "3 [D loss: 0.999873] [G loss: 1.000296]\n",
            "4 [D loss: 0.999875] [G loss: 1.000304]\n",
            "5 [D loss: 0.999875] [G loss: 1.000306]\n",
            "6 [D loss: 0.999875] [G loss: 1.000308]\n",
            "7 [D loss: 0.999874] [G loss: 1.000296]\n",
            "8 [D loss: 0.999874] [G loss: 1.000296]\n",
            "9 [D loss: 0.999876] [G loss: 1.000290]\n",
            "10 [D loss: 0.999876] [G loss: 1.000281]\n",
            "11 [D loss: 0.999880] [G loss: 1.000272]\n",
            "12 [D loss: 0.999883] [G loss: 1.000253]\n",
            "13 [D loss: 0.999888] [G loss: 1.000237]\n",
            "14 [D loss: 0.999894] [G loss: 1.000219]\n",
            "15 [D loss: 0.999897] [G loss: 1.000196]\n",
            "16 [D loss: 0.999904] [G loss: 1.000175]\n",
            "17 [D loss: 0.999915] [G loss: 1.000158]\n",
            "18 [D loss: 0.999920] [G loss: 1.000141]\n",
            "19 [D loss: 0.999927] [G loss: 1.000130]\n",
            "20 [D loss: 0.999932] [G loss: 1.000117]\n",
            "21 [D loss: 0.999938] [G loss: 1.000103]\n",
            "22 [D loss: 0.999943] [G loss: 1.000098]\n",
            "23 [D loss: 0.999949] [G loss: 1.000088]\n",
            "24 [D loss: 0.999953] [G loss: 1.000079]\n",
            "25 [D loss: 0.999957] [G loss: 1.000078]\n",
            "26 [D loss: 0.999958] [G loss: 1.000072]\n",
            "27 [D loss: 0.999965] [G loss: 1.000068]\n",
            "28 [D loss: 0.999965] [G loss: 1.000068]\n",
            "29 [D loss: 0.999966] [G loss: 1.000061]\n",
            "30 [D loss: 0.999967] [G loss: 1.000062]\n",
            "31 [D loss: 0.999968] [G loss: 1.000061]\n",
            "32 [D loss: 0.999970] [G loss: 1.000063]\n",
            "33 [D loss: 0.999967] [G loss: 1.000061]\n",
            "34 [D loss: 0.999969] [G loss: 1.000063]\n",
            "35 [D loss: 0.999968] [G loss: 1.000064]\n",
            "36 [D loss: 0.999971] [G loss: 1.000060]\n",
            "37 [D loss: 0.999971] [G loss: 1.000061]\n",
            "38 [D loss: 0.999972] [G loss: 1.000062]\n",
            "39 [D loss: 0.999971] [G loss: 1.000066]\n",
            "40 [D loss: 0.999973] [G loss: 1.000063]\n",
            "41 [D loss: 0.999969] [G loss: 1.000065]\n",
            "42 [D loss: 0.999971] [G loss: 1.000065]\n",
            "43 [D loss: 0.999969] [G loss: 1.000066]\n",
            "44 [D loss: 0.999970] [G loss: 1.000066]\n",
            "45 [D loss: 0.999970] [G loss: 1.000064]\n",
            "46 [D loss: 0.999972] [G loss: 1.000063]\n",
            "47 [D loss: 0.999971] [G loss: 1.000064]\n",
            "48 [D loss: 0.999970] [G loss: 1.000066]\n",
            "49 [D loss: 0.999969] [G loss: 1.000063]\n",
            "50 [D loss: 0.999969] [G loss: 1.000064]\n",
            "51 [D loss: 0.999971] [G loss: 1.000062]\n",
            "52 [D loss: 0.999968] [G loss: 1.000064]\n",
            "53 [D loss: 0.999969] [G loss: 1.000059]\n",
            "54 [D loss: 0.999970] [G loss: 1.000065]\n",
            "55 [D loss: 0.999971] [G loss: 1.000065]\n",
            "56 [D loss: 0.999968] [G loss: 1.000064]\n",
            "57 [D loss: 0.999970] [G loss: 1.000066]\n",
            "58 [D loss: 0.999970] [G loss: 1.000062]\n",
            "59 [D loss: 0.999970] [G loss: 1.000061]\n",
            "60 [D loss: 0.999970] [G loss: 1.000068]\n",
            "61 [D loss: 0.999971] [G loss: 1.000067]\n",
            "62 [D loss: 0.999970] [G loss: 1.000062]\n",
            "63 [D loss: 0.999971] [G loss: 1.000063]\n",
            "64 [D loss: 0.999969] [G loss: 1.000063]\n",
            "65 [D loss: 0.999970] [G loss: 1.000066]\n",
            "66 [D loss: 0.999970] [G loss: 1.000063]\n",
            "67 [D loss: 0.999969] [G loss: 1.000067]\n",
            "68 [D loss: 0.999967] [G loss: 1.000066]\n",
            "69 [D loss: 0.999969] [G loss: 1.000066]\n",
            "70 [D loss: 0.999969] [G loss: 1.000065]\n",
            "71 [D loss: 0.999971] [G loss: 1.000067]\n",
            "72 [D loss: 0.999969] [G loss: 1.000066]\n",
            "73 [D loss: 0.999971] [G loss: 1.000064]\n",
            "74 [D loss: 0.999970] [G loss: 1.000062]\n",
            "75 [D loss: 0.999971] [G loss: 1.000064]\n",
            "76 [D loss: 0.999970] [G loss: 1.000067]\n",
            "77 [D loss: 0.999969] [G loss: 1.000065]\n",
            "78 [D loss: 0.999970] [G loss: 1.000068]\n",
            "79 [D loss: 0.999970] [G loss: 1.000064]\n",
            "80 [D loss: 0.999970] [G loss: 1.000066]\n",
            "81 [D loss: 0.999970] [G loss: 1.000064]\n",
            "82 [D loss: 0.999966] [G loss: 1.000063]\n",
            "83 [D loss: 0.999969] [G loss: 1.000063]\n",
            "84 [D loss: 0.999969] [G loss: 1.000062]\n",
            "85 [D loss: 0.999968] [G loss: 1.000063]\n",
            "86 [D loss: 0.999967] [G loss: 1.000066]\n",
            "87 [D loss: 0.999969] [G loss: 1.000066]\n",
            "88 [D loss: 0.999970] [G loss: 1.000061]\n",
            "89 [D loss: 0.999970] [G loss: 1.000067]\n",
            "90 [D loss: 0.999971] [G loss: 1.000067]\n",
            "91 [D loss: 0.999970] [G loss: 1.000061]\n",
            "92 [D loss: 0.999969] [G loss: 1.000059]\n",
            "93 [D loss: 0.999971] [G loss: 1.000064]\n",
            "94 [D loss: 0.999968] [G loss: 1.000061]\n",
            "95 [D loss: 0.999969] [G loss: 1.000061]\n",
            "96 [D loss: 0.999971] [G loss: 1.000064]\n",
            "97 [D loss: 0.999970] [G loss: 1.000065]\n",
            "98 [D loss: 0.999970] [G loss: 1.000060]\n",
            "99 [D loss: 0.999969] [G loss: 1.000068]\n",
            "100 [D loss: 0.999969] [G loss: 1.000062]\n",
            "101 [D loss: 0.999969] [G loss: 1.000068]\n",
            "102 [D loss: 0.999972] [G loss: 1.000066]\n",
            "103 [D loss: 0.999969] [G loss: 1.000065]\n",
            "104 [D loss: 0.999969] [G loss: 1.000065]\n",
            "105 [D loss: 0.999968] [G loss: 1.000064]\n",
            "106 [D loss: 0.999969] [G loss: 1.000062]\n",
            "107 [D loss: 0.999969] [G loss: 1.000062]\n",
            "108 [D loss: 0.999969] [G loss: 1.000064]\n",
            "109 [D loss: 0.999969] [G loss: 1.000067]\n",
            "110 [D loss: 0.999971] [G loss: 1.000064]\n",
            "111 [D loss: 0.999970] [G loss: 1.000065]\n",
            "112 [D loss: 0.999969] [G loss: 1.000062]\n",
            "113 [D loss: 0.999969] [G loss: 1.000065]\n",
            "114 [D loss: 0.999971] [G loss: 1.000064]\n",
            "115 [D loss: 0.999970] [G loss: 1.000064]\n",
            "116 [D loss: 0.999970] [G loss: 1.000060]\n",
            "117 [D loss: 0.999967] [G loss: 1.000065]\n",
            "118 [D loss: 0.999969] [G loss: 1.000064]\n",
            "119 [D loss: 0.999969] [G loss: 1.000064]\n",
            "120 [D loss: 0.999969] [G loss: 1.000063]\n",
            "121 [D loss: 0.999970] [G loss: 1.000060]\n",
            "122 [D loss: 0.999971] [G loss: 1.000066]\n",
            "123 [D loss: 0.999970] [G loss: 1.000064]\n",
            "124 [D loss: 0.999968] [G loss: 1.000062]\n",
            "125 [D loss: 0.999968] [G loss: 1.000066]\n",
            "126 [D loss: 0.999971] [G loss: 1.000063]\n",
            "127 [D loss: 0.999972] [G loss: 1.000064]\n",
            "128 [D loss: 0.999969] [G loss: 1.000065]\n",
            "129 [D loss: 0.999969] [G loss: 1.000062]\n",
            "130 [D loss: 0.999969] [G loss: 1.000067]\n",
            "131 [D loss: 0.999970] [G loss: 1.000059]\n",
            "132 [D loss: 0.999970] [G loss: 1.000067]\n",
            "133 [D loss: 0.999970] [G loss: 1.000067]\n",
            "134 [D loss: 0.999971] [G loss: 1.000064]\n",
            "135 [D loss: 0.999968] [G loss: 1.000062]\n",
            "136 [D loss: 0.999971] [G loss: 1.000066]\n",
            "137 [D loss: 0.999970] [G loss: 1.000065]\n",
            "138 [D loss: 0.999969] [G loss: 1.000065]\n",
            "139 [D loss: 0.999969] [G loss: 1.000065]\n",
            "140 [D loss: 0.999969] [G loss: 1.000063]\n",
            "141 [D loss: 0.999972] [G loss: 1.000064]\n",
            "142 [D loss: 0.999969] [G loss: 1.000060]\n",
            "143 [D loss: 0.999971] [G loss: 1.000064]\n",
            "144 [D loss: 0.999969] [G loss: 1.000066]\n",
            "145 [D loss: 0.999971] [G loss: 1.000064]\n",
            "146 [D loss: 0.999968] [G loss: 1.000065]\n",
            "147 [D loss: 0.999972] [G loss: 1.000061]\n",
            "148 [D loss: 0.999969] [G loss: 1.000060]\n",
            "149 [D loss: 0.999969] [G loss: 1.000063]\n",
            "150 [D loss: 0.999969] [G loss: 1.000065]\n",
            "151 [D loss: 0.999969] [G loss: 1.000064]\n",
            "152 [D loss: 0.999970] [G loss: 1.000062]\n",
            "153 [D loss: 0.999970] [G loss: 1.000063]\n",
            "154 [D loss: 0.999974] [G loss: 1.000058]\n",
            "155 [D loss: 0.999970] [G loss: 1.000063]\n",
            "156 [D loss: 0.999972] [G loss: 1.000059]\n",
            "157 [D loss: 0.999971] [G loss: 1.000067]\n",
            "158 [D loss: 0.999972] [G loss: 1.000063]\n",
            "159 [D loss: 0.999972] [G loss: 1.000059]\n",
            "160 [D loss: 0.999969] [G loss: 1.000064]\n",
            "161 [D loss: 0.999969] [G loss: 1.000062]\n",
            "162 [D loss: 0.999969] [G loss: 1.000059]\n",
            "163 [D loss: 0.999970] [G loss: 1.000063]\n",
            "164 [D loss: 0.999970] [G loss: 1.000062]\n",
            "165 [D loss: 0.999969] [G loss: 1.000064]\n",
            "166 [D loss: 0.999969] [G loss: 1.000059]\n",
            "167 [D loss: 0.999967] [G loss: 1.000063]\n",
            "168 [D loss: 0.999970] [G loss: 1.000066]\n",
            "169 [D loss: 0.999972] [G loss: 1.000062]\n",
            "170 [D loss: 0.999971] [G loss: 1.000064]\n",
            "171 [D loss: 0.999972] [G loss: 1.000061]\n",
            "172 [D loss: 0.999971] [G loss: 1.000061]\n",
            "173 [D loss: 0.999970] [G loss: 1.000059]\n",
            "174 [D loss: 0.999969] [G loss: 1.000063]\n",
            "175 [D loss: 0.999972] [G loss: 1.000062]\n",
            "176 [D loss: 0.999971] [G loss: 1.000065]\n",
            "177 [D loss: 0.999972] [G loss: 1.000061]\n",
            "178 [D loss: 0.999968] [G loss: 1.000061]\n",
            "179 [D loss: 0.999968] [G loss: 1.000059]\n",
            "180 [D loss: 0.999971] [G loss: 1.000060]\n",
            "181 [D loss: 0.999969] [G loss: 1.000061]\n",
            "182 [D loss: 0.999968] [G loss: 1.000061]\n",
            "183 [D loss: 0.999971] [G loss: 1.000063]\n",
            "184 [D loss: 0.999969] [G loss: 1.000065]\n",
            "185 [D loss: 0.999969] [G loss: 1.000056]\n",
            "186 [D loss: 0.999971] [G loss: 1.000061]\n",
            "187 [D loss: 0.999971] [G loss: 1.000057]\n",
            "188 [D loss: 0.999971] [G loss: 1.000059]\n",
            "189 [D loss: 0.999972] [G loss: 1.000061]\n",
            "190 [D loss: 0.999969] [G loss: 1.000062]\n",
            "191 [D loss: 0.999969] [G loss: 1.000060]\n",
            "192 [D loss: 0.999971] [G loss: 1.000059]\n",
            "193 [D loss: 0.999972] [G loss: 1.000053]\n",
            "194 [D loss: 0.999973] [G loss: 1.000061]\n",
            "195 [D loss: 0.999969] [G loss: 1.000059]\n",
            "196 [D loss: 0.999970] [G loss: 1.000059]\n",
            "197 [D loss: 0.999974] [G loss: 1.000059]\n",
            "198 [D loss: 0.999970] [G loss: 1.000061]\n",
            "199 [D loss: 0.999970] [G loss: 1.000060]\n",
            "200 [D loss: 0.999969] [G loss: 1.000059]\n",
            "201 [D loss: 0.999972] [G loss: 1.000059]\n",
            "202 [D loss: 0.999971] [G loss: 1.000058]\n",
            "203 [D loss: 0.999970] [G loss: 1.000060]\n",
            "204 [D loss: 0.999971] [G loss: 1.000061]\n",
            "205 [D loss: 0.999972] [G loss: 1.000054]\n",
            "206 [D loss: 0.999973] [G loss: 1.000058]\n",
            "207 [D loss: 0.999969] [G loss: 1.000054]\n",
            "208 [D loss: 0.999971] [G loss: 1.000057]\n",
            "209 [D loss: 0.999971] [G loss: 1.000056]\n",
            "210 [D loss: 0.999974] [G loss: 1.000056]\n",
            "211 [D loss: 0.999973] [G loss: 1.000057]\n",
            "212 [D loss: 0.999973] [G loss: 1.000054]\n",
            "213 [D loss: 0.999972] [G loss: 1.000056]\n",
            "214 [D loss: 0.999972] [G loss: 1.000051]\n",
            "215 [D loss: 0.999972] [G loss: 1.000055]\n",
            "216 [D loss: 0.999972] [G loss: 1.000056]\n",
            "217 [D loss: 0.999971] [G loss: 1.000057]\n",
            "218 [D loss: 0.999973] [G loss: 1.000054]\n",
            "219 [D loss: 0.999974] [G loss: 1.000051]\n",
            "220 [D loss: 0.999975] [G loss: 1.000050]\n",
            "221 [D loss: 0.999972] [G loss: 1.000046]\n",
            "222 [D loss: 0.999972] [G loss: 1.000046]\n",
            "223 [D loss: 0.999972] [G loss: 1.000042]\n",
            "224 [D loss: 0.999975] [G loss: 1.000043]\n",
            "225 [D loss: 0.999975] [G loss: 1.000041]\n",
            "226 [D loss: 0.999975] [G loss: 1.000037]\n",
            "227 [D loss: 0.999975] [G loss: 1.000043]\n",
            "228 [D loss: 0.999977] [G loss: 1.000039]\n",
            "229 [D loss: 0.999979] [G loss: 1.000035]\n",
            "230 [D loss: 0.999979] [G loss: 1.000029]\n",
            "231 [D loss: 0.999975] [G loss: 1.000033]\n",
            "232 [D loss: 0.999977] [G loss: 1.000029]\n",
            "233 [D loss: 0.999980] [G loss: 1.000030]\n",
            "234 [D loss: 0.999981] [G loss: 1.000028]\n",
            "235 [D loss: 0.999982] [G loss: 1.000026]\n",
            "236 [D loss: 0.999980] [G loss: 1.000018]\n",
            "237 [D loss: 0.999983] [G loss: 1.000020]\n",
            "238 [D loss: 0.999985] [G loss: 1.000016]\n",
            "239 [D loss: 0.999986] [G loss: 1.000009]\n",
            "240 [D loss: 0.999985] [G loss: 1.000006]\n",
            "241 [D loss: 0.999988] [G loss: 1.000012]\n",
            "242 [D loss: 0.999986] [G loss: 1.000020]\n",
            "243 [D loss: 0.999984] [G loss: 1.000029]\n",
            "244 [D loss: 0.999985] [G loss: 1.000034]\n",
            "245 [D loss: 0.999984] [G loss: 1.000023]\n",
            "246 [D loss: 0.999982] [G loss: 1.000031]\n",
            "247 [D loss: 0.999978] [G loss: 1.000022]\n",
            "248 [D loss: 0.999979] [G loss: 1.000032]\n",
            "249 [D loss: 0.999985] [G loss: 1.000021]\n",
            "250 [D loss: 0.999986] [G loss: 1.000022]\n",
            "251 [D loss: 0.999991] [G loss: 1.000011]\n",
            "252 [D loss: 0.999997] [G loss: 1.000006]\n",
            "253 [D loss: 0.999999] [G loss: 1.000002]\n",
            "254 [D loss: 1.000001] [G loss: 0.999996]\n",
            "255 [D loss: 1.000003] [G loss: 1.000002]\n",
            "256 [D loss: 0.999999] [G loss: 1.000002]\n",
            "257 [D loss: 0.999998] [G loss: 0.999999]\n",
            "258 [D loss: 1.000009] [G loss: 0.999993]\n",
            "259 [D loss: 1.000019] [G loss: 0.999995]\n",
            "260 [D loss: 1.000022] [G loss: 0.999998]\n",
            "261 [D loss: 1.000011] [G loss: 1.000009]\n",
            "262 [D loss: 1.000002] [G loss: 1.000014]\n",
            "263 [D loss: 0.999991] [G loss: 1.000029]\n",
            "264 [D loss: 0.999991] [G loss: 1.000019]\n",
            "265 [D loss: 0.999989] [G loss: 1.000006]\n",
            "266 [D loss: 0.999994] [G loss: 1.000009]\n",
            "267 [D loss: 0.999994] [G loss: 1.000002]\n",
            "268 [D loss: 1.000002] [G loss: 0.999991]\n",
            "269 [D loss: 1.000009] [G loss: 0.999979]\n",
            "270 [D loss: 1.000003] [G loss: 0.999978]\n",
            "271 [D loss: 1.000014] [G loss: 0.999974]\n",
            "272 [D loss: 1.000013] [G loss: 0.999959]\n",
            "273 [D loss: 1.000025] [G loss: 0.999954]\n",
            "274 [D loss: 1.000034] [G loss: 0.999926]\n",
            "275 [D loss: 1.000036] [G loss: 0.999929]\n",
            "276 [D loss: 1.000035] [G loss: 0.999942]\n",
            "277 [D loss: 1.000036] [G loss: 0.999952]\n",
            "278 [D loss: 1.000037] [G loss: 0.999948]\n",
            "279 [D loss: 1.000037] [G loss: 0.999951]\n",
            "280 [D loss: 1.000031] [G loss: 0.999952]\n",
            "281 [D loss: 1.000027] [G loss: 0.999938]\n",
            "282 [D loss: 1.000038] [G loss: 0.999919]\n",
            "283 [D loss: 1.000031] [G loss: 0.999914]\n",
            "284 [D loss: 1.000027] [G loss: 0.999909]\n",
            "285 [D loss: 1.000031] [G loss: 0.999914]\n",
            "286 [D loss: 1.000029] [G loss: 0.999903]\n",
            "287 [D loss: 1.000032] [G loss: 0.999889]\n",
            "288 [D loss: 1.000037] [G loss: 0.999863]\n",
            "289 [D loss: 1.000029] [G loss: 0.999867]\n",
            "290 [D loss: 1.000020] [G loss: 0.999865]\n",
            "291 [D loss: 1.000013] [G loss: 0.999896]\n",
            "292 [D loss: 1.000025] [G loss: 0.999885]\n",
            "293 [D loss: 1.000031] [G loss: 0.999907]\n",
            "294 [D loss: 1.000063] [G loss: 0.999886]\n",
            "295 [D loss: 1.000096] [G loss: 0.999869]\n",
            "296 [D loss: 1.000119] [G loss: 0.999838]\n",
            "297 [D loss: 1.000136] [G loss: 0.999825]\n",
            "298 [D loss: 1.000148] [G loss: 0.999812]\n",
            "299 [D loss: 1.000153] [G loss: 0.999839]\n",
            "300 [D loss: 1.000136] [G loss: 0.999886]\n",
            "301 [D loss: 1.000138] [G loss: 0.999901]\n",
            "302 [D loss: 1.000091] [G loss: 0.999924]\n",
            "303 [D loss: 1.000058] [G loss: 0.999952]\n",
            "304 [D loss: 1.000034] [G loss: 0.999974]\n",
            "305 [D loss: 1.000007] [G loss: 0.999998]\n",
            "306 [D loss: 1.000000] [G loss: 0.999999]\n",
            "307 [D loss: 0.999995] [G loss: 0.999994]\n",
            "308 [D loss: 0.999999] [G loss: 0.999982]\n",
            "309 [D loss: 1.000005] [G loss: 0.999968]\n",
            "310 [D loss: 1.000005] [G loss: 0.999956]\n",
            "311 [D loss: 1.000014] [G loss: 0.999940]\n",
            "312 [D loss: 1.000033] [G loss: 0.999915]\n",
            "313 [D loss: 1.000045] [G loss: 0.999887]\n",
            "314 [D loss: 1.000057] [G loss: 0.999861]\n",
            "315 [D loss: 1.000057] [G loss: 0.999868]\n",
            "316 [D loss: 1.000068] [G loss: 0.999854]\n",
            "317 [D loss: 1.000085] [G loss: 0.999801]\n",
            "318 [D loss: 1.000077] [G loss: 0.999793]\n",
            "319 [D loss: 1.000096] [G loss: 0.999762]\n",
            "320 [D loss: 1.000095] [G loss: 0.999716]\n",
            "321 [D loss: 1.000131] [G loss: 0.999689]\n",
            "322 [D loss: 1.000120] [G loss: 0.999674]\n",
            "323 [D loss: 1.000108] [G loss: 0.999685]\n",
            "324 [D loss: 1.000120] [G loss: 0.999718]\n",
            "325 [D loss: 1.000117] [G loss: 0.999733]\n",
            "326 [D loss: 1.000060] [G loss: 0.999769]\n",
            "327 [D loss: 1.000032] [G loss: 0.999809]\n",
            "328 [D loss: 1.000029] [G loss: 0.999840]\n",
            "329 [D loss: 1.000053] [G loss: 0.999859]\n",
            "330 [D loss: 1.000074] [G loss: 0.999877]\n",
            "331 [D loss: 1.000095] [G loss: 0.999874]\n",
            "332 [D loss: 1.000111] [G loss: 0.999842]\n",
            "333 [D loss: 1.000114] [G loss: 0.999806]\n",
            "334 [D loss: 1.000108] [G loss: 0.999769]\n",
            "335 [D loss: 1.000109] [G loss: 0.999762]\n",
            "336 [D loss: 1.000114] [G loss: 0.999762]\n",
            "337 [D loss: 1.000108] [G loss: 0.999795]\n",
            "338 [D loss: 1.000104] [G loss: 0.999796]\n",
            "339 [D loss: 1.000106] [G loss: 0.999792]\n",
            "340 [D loss: 1.000092] [G loss: 0.999783]\n",
            "341 [D loss: 1.000112] [G loss: 0.999730]\n",
            "342 [D loss: 1.000128] [G loss: 0.999713]\n",
            "343 [D loss: 1.000146] [G loss: 0.999686]\n",
            "344 [D loss: 1.000115] [G loss: 0.999663]\n",
            "345 [D loss: 1.000141] [G loss: 0.999639]\n",
            "346 [D loss: 1.000117] [G loss: 0.999646]\n",
            "347 [D loss: 1.000101] [G loss: 0.999663]\n",
            "348 [D loss: 1.000124] [G loss: 0.999644]\n",
            "349 [D loss: 1.000140] [G loss: 0.999650]\n",
            "350 [D loss: 1.000156] [G loss: 0.999674]\n",
            "351 [D loss: 1.000156] [G loss: 0.999682]\n",
            "352 [D loss: 1.000115] [G loss: 0.999702]\n",
            "353 [D loss: 1.000070] [G loss: 0.999713]\n",
            "354 [D loss: 1.000112] [G loss: 0.999711]\n",
            "355 [D loss: 1.000146] [G loss: 0.999720]\n",
            "356 [D loss: 1.000142] [G loss: 0.999726]\n",
            "357 [D loss: 1.000151] [G loss: 0.999734]\n",
            "358 [D loss: 1.000151] [G loss: 0.999691]\n",
            "359 [D loss: 1.000167] [G loss: 0.999675]\n",
            "360 [D loss: 1.000179] [G loss: 0.999683]\n",
            "361 [D loss: 1.000182] [G loss: 0.999687]\n",
            "362 [D loss: 1.000178] [G loss: 0.999717]\n",
            "363 [D loss: 1.000180] [G loss: 0.999772]\n",
            "364 [D loss: 1.000173] [G loss: 0.999762]\n",
            "365 [D loss: 1.000162] [G loss: 0.999769]\n",
            "366 [D loss: 1.000148] [G loss: 0.999753]\n",
            "367 [D loss: 1.000124] [G loss: 0.999753]\n",
            "368 [D loss: 1.000153] [G loss: 0.999712]\n",
            "369 [D loss: 1.000132] [G loss: 0.999671]\n",
            "370 [D loss: 1.000131] [G loss: 0.999635]\n",
            "371 [D loss: 1.000148] [G loss: 0.999587]\n",
            "372 [D loss: 1.000155] [G loss: 0.999561]\n",
            "373 [D loss: 1.000172] [G loss: 0.999522]\n",
            "374 [D loss: 1.000147] [G loss: 0.999515]\n",
            "375 [D loss: 1.000155] [G loss: 0.999536]\n",
            "376 [D loss: 1.000160] [G loss: 0.999541]\n",
            "377 [D loss: 1.000118] [G loss: 0.999565]\n",
            "378 [D loss: 1.000118] [G loss: 0.999604]\n",
            "379 [D loss: 1.000120] [G loss: 0.999649]\n",
            "380 [D loss: 1.000144] [G loss: 0.999646]\n",
            "381 [D loss: 1.000152] [G loss: 0.999643]\n",
            "382 [D loss: 1.000131] [G loss: 0.999648]\n",
            "383 [D loss: 1.000140] [G loss: 0.999651]\n",
            "384 [D loss: 1.000130] [G loss: 0.999686]\n",
            "385 [D loss: 1.000150] [G loss: 0.999730]\n",
            "386 [D loss: 1.000103] [G loss: 0.999770]\n",
            "387 [D loss: 1.000058] [G loss: 0.999783]\n",
            "388 [D loss: 1.000081] [G loss: 0.999802]\n",
            "389 [D loss: 1.000086] [G loss: 0.999786]\n",
            "390 [D loss: 1.000071] [G loss: 0.999816]\n",
            "391 [D loss: 1.000088] [G loss: 0.999828]\n",
            "392 [D loss: 1.000107] [G loss: 0.999808]\n",
            "393 [D loss: 1.000089] [G loss: 0.999808]\n",
            "394 [D loss: 1.000104] [G loss: 0.999810]\n",
            "395 [D loss: 1.000105] [G loss: 0.999782]\n",
            "396 [D loss: 1.000098] [G loss: 0.999763]\n",
            "397 [D loss: 1.000107] [G loss: 0.999744]\n",
            "398 [D loss: 1.000122] [G loss: 0.999707]\n",
            "399 [D loss: 1.000124] [G loss: 0.999679]\n",
            "400 [D loss: 1.000113] [G loss: 0.999636]\n",
            "401 [D loss: 1.000140] [G loss: 0.999604]\n",
            "402 [D loss: 1.000130] [G loss: 0.999610]\n",
            "403 [D loss: 1.000140] [G loss: 0.999590]\n",
            "404 [D loss: 1.000151] [G loss: 0.999578]\n",
            "405 [D loss: 1.000118] [G loss: 0.999617]\n",
            "406 [D loss: 1.000126] [G loss: 0.999632]\n",
            "407 [D loss: 1.000101] [G loss: 0.999667]\n",
            "408 [D loss: 1.000102] [G loss: 0.999684]\n",
            "409 [D loss: 1.000083] [G loss: 0.999740]\n",
            "410 [D loss: 1.000096] [G loss: 0.999799]\n",
            "411 [D loss: 1.000086] [G loss: 0.999863]\n",
            "412 [D loss: 1.000066] [G loss: 0.999903]\n",
            "413 [D loss: 1.000061] [G loss: 0.999913]\n",
            "414 [D loss: 1.000055] [G loss: 0.999933]\n",
            "415 [D loss: 1.000063] [G loss: 0.999890]\n",
            "416 [D loss: 1.000038] [G loss: 0.999869]\n",
            "417 [D loss: 1.000043] [G loss: 0.999856]\n",
            "418 [D loss: 1.000042] [G loss: 0.999825]\n",
            "419 [D loss: 1.000045] [G loss: 0.999821]\n",
            "420 [D loss: 1.000063] [G loss: 0.999783]\n",
            "421 [D loss: 1.000040] [G loss: 0.999793]\n",
            "422 [D loss: 1.000057] [G loss: 0.999794]\n",
            "423 [D loss: 1.000018] [G loss: 0.999831]\n",
            "424 [D loss: 1.000036] [G loss: 0.999866]\n",
            "425 [D loss: 1.000054] [G loss: 0.999877]\n",
            "426 [D loss: 1.000050] [G loss: 0.999867]\n",
            "427 [D loss: 1.000067] [G loss: 0.999875]\n",
            "428 [D loss: 1.000031] [G loss: 0.999879]\n",
            "429 [D loss: 1.000053] [G loss: 0.999912]\n",
            "430 [D loss: 1.000051] [G loss: 0.999868]\n",
            "431 [D loss: 1.000041] [G loss: 0.999872]\n",
            "432 [D loss: 1.000028] [G loss: 0.999864]\n",
            "433 [D loss: 1.000043] [G loss: 0.999843]\n",
            "434 [D loss: 1.000049] [G loss: 0.999830]\n",
            "435 [D loss: 1.000038] [G loss: 0.999836]\n",
            "436 [D loss: 1.000064] [G loss: 0.999820]\n",
            "437 [D loss: 1.000026] [G loss: 0.999853]\n",
            "438 [D loss: 1.000056] [G loss: 0.999862]\n",
            "439 [D loss: 1.000031] [G loss: 0.999864]\n",
            "440 [D loss: 1.000038] [G loss: 0.999882]\n",
            "441 [D loss: 1.000041] [G loss: 0.999909]\n",
            "442 [D loss: 1.000037] [G loss: 0.999898]\n",
            "443 [D loss: 1.000031] [G loss: 0.999917]\n",
            "444 [D loss: 1.000020] [G loss: 0.999921]\n",
            "445 [D loss: 1.000015] [G loss: 0.999960]\n",
            "446 [D loss: 1.000004] [G loss: 0.999980]\n",
            "447 [D loss: 0.999999] [G loss: 0.999989]\n",
            "448 [D loss: 1.000006] [G loss: 0.999996]\n",
            "449 [D loss: 0.999995] [G loss: 0.999995]\n",
            "450 [D loss: 0.999993] [G loss: 0.999984]\n",
            "451 [D loss: 0.999989] [G loss: 1.000005]\n",
            "452 [D loss: 0.999983] [G loss: 1.000018]\n",
            "453 [D loss: 0.999988] [G loss: 0.999998]\n",
            "454 [D loss: 0.999987] [G loss: 0.999999]\n",
            "455 [D loss: 0.999976] [G loss: 1.000006]\n",
            "456 [D loss: 0.999972] [G loss: 1.000010]\n",
            "457 [D loss: 0.999985] [G loss: 0.999995]\n",
            "458 [D loss: 0.999979] [G loss: 1.000002]\n",
            "459 [D loss: 0.999984] [G loss: 1.000006]\n",
            "460 [D loss: 0.999977] [G loss: 1.000011]\n",
            "461 [D loss: 0.999981] [G loss: 0.999993]\n",
            "462 [D loss: 0.999979] [G loss: 1.000019]\n",
            "463 [D loss: 0.999985] [G loss: 1.000011]\n",
            "464 [D loss: 0.999985] [G loss: 0.999997]\n",
            "465 [D loss: 0.999988] [G loss: 0.999998]\n",
            "466 [D loss: 0.999982] [G loss: 1.000014]\n",
            "467 [D loss: 0.999986] [G loss: 0.999994]\n",
            "468 [D loss: 0.999987] [G loss: 1.000002]\n",
            "469 [D loss: 0.999977] [G loss: 1.000013]\n",
            "470 [D loss: 0.999986] [G loss: 1.000010]\n",
            "471 [D loss: 0.999976] [G loss: 1.000034]\n",
            "472 [D loss: 0.999980] [G loss: 1.000024]\n",
            "473 [D loss: 0.999979] [G loss: 1.000024]\n",
            "474 [D loss: 0.999982] [G loss: 1.000021]\n",
            "475 [D loss: 0.999970] [G loss: 1.000040]\n",
            "476 [D loss: 0.999970] [G loss: 1.000033]\n",
            "477 [D loss: 0.999970] [G loss: 1.000023]\n",
            "478 [D loss: 0.999975] [G loss: 1.000031]\n",
            "479 [D loss: 0.999973] [G loss: 1.000017]\n",
            "480 [D loss: 0.999973] [G loss: 1.000036]\n",
            "481 [D loss: 0.999976] [G loss: 1.000022]\n",
            "482 [D loss: 0.999976] [G loss: 1.000033]\n",
            "483 [D loss: 0.999975] [G loss: 1.000030]\n",
            "484 [D loss: 0.999975] [G loss: 1.000024]\n",
            "485 [D loss: 0.999979] [G loss: 0.999997]\n",
            "486 [D loss: 0.999973] [G loss: 1.000027]\n",
            "487 [D loss: 0.999984] [G loss: 1.000021]\n",
            "488 [D loss: 0.999972] [G loss: 1.000041]\n",
            "489 [D loss: 0.999977] [G loss: 1.000030]\n",
            "490 [D loss: 0.999974] [G loss: 1.000039]\n",
            "491 [D loss: 0.999971] [G loss: 1.000030]\n",
            "492 [D loss: 0.999970] [G loss: 1.000039]\n",
            "493 [D loss: 0.999976] [G loss: 1.000030]\n",
            "494 [D loss: 0.999971] [G loss: 1.000039]\n",
            "495 [D loss: 0.999970] [G loss: 1.000041]\n",
            "496 [D loss: 0.999974] [G loss: 1.000044]\n",
            "497 [D loss: 0.999969] [G loss: 1.000045]\n",
            "498 [D loss: 0.999974] [G loss: 1.000037]\n",
            "499 [D loss: 0.999973] [G loss: 1.000040]\n",
            "500 [D loss: 0.999968] [G loss: 1.000047]\n",
            "501 [D loss: 0.999970] [G loss: 1.000047]\n",
            "502 [D loss: 0.999968] [G loss: 1.000044]\n",
            "503 [D loss: 0.999972] [G loss: 1.000032]\n",
            "504 [D loss: 0.999967] [G loss: 1.000047]\n",
            "505 [D loss: 0.999966] [G loss: 1.000042]\n",
            "506 [D loss: 0.999972] [G loss: 1.000050]\n",
            "507 [D loss: 0.999969] [G loss: 1.000043]\n",
            "508 [D loss: 0.999970] [G loss: 1.000041]\n",
            "509 [D loss: 0.999972] [G loss: 1.000049]\n",
            "510 [D loss: 0.999974] [G loss: 1.000043]\n",
            "511 [D loss: 0.999964] [G loss: 1.000048]\n",
            "512 [D loss: 0.999971] [G loss: 1.000052]\n",
            "513 [D loss: 0.999970] [G loss: 1.000052]\n",
            "514 [D loss: 0.999971] [G loss: 1.000048]\n",
            "515 [D loss: 0.999972] [G loss: 1.000049]\n",
            "516 [D loss: 0.999965] [G loss: 1.000040]\n",
            "517 [D loss: 0.999970] [G loss: 1.000045]\n",
            "518 [D loss: 0.999974] [G loss: 1.000030]\n",
            "519 [D loss: 0.999972] [G loss: 1.000054]\n",
            "520 [D loss: 0.999971] [G loss: 1.000044]\n",
            "521 [D loss: 0.999969] [G loss: 1.000047]\n",
            "522 [D loss: 0.999968] [G loss: 1.000046]\n",
            "523 [D loss: 0.999973] [G loss: 1.000040]\n",
            "524 [D loss: 0.999966] [G loss: 1.000054]\n",
            "525 [D loss: 0.999968] [G loss: 1.000054]\n",
            "526 [D loss: 0.999969] [G loss: 1.000046]\n",
            "527 [D loss: 0.999971] [G loss: 1.000047]\n",
            "528 [D loss: 0.999968] [G loss: 1.000053]\n",
            "529 [D loss: 0.999968] [G loss: 1.000053]\n",
            "530 [D loss: 0.999971] [G loss: 1.000044]\n",
            "531 [D loss: 0.999972] [G loss: 1.000049]\n",
            "532 [D loss: 0.999971] [G loss: 1.000048]\n",
            "533 [D loss: 0.999972] [G loss: 1.000052]\n",
            "534 [D loss: 0.999972] [G loss: 1.000049]\n",
            "535 [D loss: 0.999963] [G loss: 1.000040]\n",
            "536 [D loss: 0.999970] [G loss: 1.000052]\n",
            "537 [D loss: 0.999970] [G loss: 1.000042]\n",
            "538 [D loss: 0.999976] [G loss: 1.000035]\n",
            "539 [D loss: 0.999973] [G loss: 1.000040]\n",
            "540 [D loss: 0.999969] [G loss: 1.000053]\n",
            "541 [D loss: 0.999975] [G loss: 1.000051]\n",
            "542 [D loss: 0.999970] [G loss: 1.000046]\n",
            "543 [D loss: 0.999972] [G loss: 1.000056]\n",
            "544 [D loss: 0.999967] [G loss: 1.000050]\n",
            "545 [D loss: 0.999969] [G loss: 1.000051]\n",
            "546 [D loss: 0.999972] [G loss: 1.000051]\n",
            "547 [D loss: 0.999968] [G loss: 1.000056]\n",
            "548 [D loss: 0.999971] [G loss: 1.000054]\n",
            "549 [D loss: 0.999970] [G loss: 1.000051]\n",
            "550 [D loss: 0.999968] [G loss: 1.000056]\n",
            "551 [D loss: 0.999971] [G loss: 1.000051]\n",
            "552 [D loss: 0.999973] [G loss: 1.000055]\n",
            "553 [D loss: 0.999969] [G loss: 1.000056]\n",
            "554 [D loss: 0.999968] [G loss: 1.000058]\n",
            "555 [D loss: 0.999968] [G loss: 1.000056]\n",
            "556 [D loss: 0.999969] [G loss: 1.000053]\n",
            "557 [D loss: 0.999970] [G loss: 1.000051]\n",
            "558 [D loss: 0.999971] [G loss: 1.000052]\n",
            "559 [D loss: 0.999970] [G loss: 1.000051]\n",
            "560 [D loss: 0.999973] [G loss: 1.000047]\n",
            "561 [D loss: 0.999969] [G loss: 1.000050]\n",
            "562 [D loss: 0.999969] [G loss: 1.000048]\n",
            "563 [D loss: 0.999970] [G loss: 1.000050]\n",
            "564 [D loss: 0.999971] [G loss: 1.000054]\n",
            "565 [D loss: 0.999967] [G loss: 1.000053]\n",
            "566 [D loss: 0.999969] [G loss: 1.000056]\n",
            "567 [D loss: 0.999970] [G loss: 1.000050]\n",
            "568 [D loss: 0.999973] [G loss: 1.000054]\n",
            "569 [D loss: 0.999967] [G loss: 1.000053]\n",
            "570 [D loss: 0.999973] [G loss: 1.000057]\n",
            "571 [D loss: 0.999973] [G loss: 1.000052]\n",
            "572 [D loss: 0.999970] [G loss: 1.000056]\n",
            "573 [D loss: 0.999968] [G loss: 1.000052]\n",
            "574 [D loss: 0.999970] [G loss: 1.000056]\n",
            "575 [D loss: 0.999972] [G loss: 1.000056]\n",
            "576 [D loss: 0.999969] [G loss: 1.000056]\n",
            "577 [D loss: 0.999972] [G loss: 1.000053]\n",
            "578 [D loss: 0.999969] [G loss: 1.000057]\n",
            "579 [D loss: 0.999968] [G loss: 1.000056]\n",
            "580 [D loss: 0.999968] [G loss: 1.000055]\n",
            "581 [D loss: 0.999968] [G loss: 1.000061]\n",
            "582 [D loss: 0.999966] [G loss: 1.000054]\n",
            "583 [D loss: 0.999971] [G loss: 1.000058]\n",
            "584 [D loss: 0.999970] [G loss: 1.000052]\n",
            "585 [D loss: 0.999971] [G loss: 1.000057]\n",
            "586 [D loss: 0.999967] [G loss: 1.000056]\n",
            "587 [D loss: 0.999972] [G loss: 1.000053]\n",
            "588 [D loss: 0.999974] [G loss: 1.000052]\n",
            "589 [D loss: 0.999970] [G loss: 1.000053]\n",
            "590 [D loss: 0.999968] [G loss: 1.000056]\n",
            "591 [D loss: 0.999972] [G loss: 1.000059]\n",
            "592 [D loss: 0.999972] [G loss: 1.000053]\n",
            "593 [D loss: 0.999970] [G loss: 1.000055]\n",
            "594 [D loss: 0.999967] [G loss: 1.000056]\n",
            "595 [D loss: 0.999970] [G loss: 1.000059]\n",
            "596 [D loss: 0.999971] [G loss: 1.000053]\n",
            "597 [D loss: 0.999972] [G loss: 1.000056]\n",
            "598 [D loss: 0.999969] [G loss: 1.000055]\n",
            "599 [D loss: 0.999970] [G loss: 1.000058]\n",
            "600 [D loss: 0.999967] [G loss: 1.000058]\n",
            "601 [D loss: 0.999970] [G loss: 1.000057]\n",
            "602 [D loss: 0.999971] [G loss: 1.000055]\n",
            "603 [D loss: 0.999971] [G loss: 1.000052]\n",
            "604 [D loss: 0.999971] [G loss: 1.000057]\n",
            "605 [D loss: 0.999969] [G loss: 1.000057]\n",
            "606 [D loss: 0.999971] [G loss: 1.000053]\n",
            "607 [D loss: 0.999968] [G loss: 1.000061]\n",
            "608 [D loss: 0.999971] [G loss: 1.000052]\n",
            "609 [D loss: 0.999971] [G loss: 1.000059]\n",
            "610 [D loss: 0.999971] [G loss: 1.000048]\n",
            "611 [D loss: 0.999972] [G loss: 1.000053]\n",
            "612 [D loss: 0.999974] [G loss: 1.000049]\n",
            "613 [D loss: 0.999973] [G loss: 1.000057]\n",
            "614 [D loss: 0.999970] [G loss: 1.000053]\n",
            "615 [D loss: 0.999970] [G loss: 1.000052]\n",
            "616 [D loss: 0.999970] [G loss: 1.000053]\n",
            "617 [D loss: 0.999972] [G loss: 1.000054]\n",
            "618 [D loss: 0.999974] [G loss: 1.000055]\n",
            "619 [D loss: 0.999974] [G loss: 1.000058]\n",
            "620 [D loss: 0.999967] [G loss: 1.000057]\n",
            "621 [D loss: 0.999970] [G loss: 1.000048]\n",
            "622 [D loss: 0.999974] [G loss: 1.000047]\n",
            "623 [D loss: 0.999974] [G loss: 1.000051]\n",
            "624 [D loss: 0.999972] [G loss: 1.000056]\n",
            "625 [D loss: 0.999969] [G loss: 1.000062]\n",
            "626 [D loss: 0.999971] [G loss: 1.000058]\n",
            "627 [D loss: 0.999969] [G loss: 1.000054]\n",
            "628 [D loss: 0.999970] [G loss: 1.000051]\n",
            "629 [D loss: 0.999969] [G loss: 1.000047]\n",
            "630 [D loss: 0.999972] [G loss: 1.000054]\n",
            "631 [D loss: 0.999967] [G loss: 1.000054]\n",
            "632 [D loss: 0.999969] [G loss: 1.000056]\n",
            "633 [D loss: 0.999970] [G loss: 1.000051]\n",
            "634 [D loss: 0.999971] [G loss: 1.000055]\n",
            "635 [D loss: 0.999970] [G loss: 1.000057]\n",
            "636 [D loss: 0.999968] [G loss: 1.000053]\n",
            "637 [D loss: 0.999967] [G loss: 1.000051]\n",
            "638 [D loss: 0.999968] [G loss: 1.000059]\n",
            "639 [D loss: 0.999971] [G loss: 1.000056]\n",
            "640 [D loss: 0.999973] [G loss: 1.000055]\n",
            "641 [D loss: 0.999972] [G loss: 1.000051]\n",
            "642 [D loss: 0.999976] [G loss: 1.000052]\n",
            "643 [D loss: 0.999971] [G loss: 1.000053]\n",
            "644 [D loss: 0.999973] [G loss: 1.000057]\n",
            "645 [D loss: 0.999971] [G loss: 1.000057]\n",
            "646 [D loss: 0.999968] [G loss: 1.000050]\n",
            "647 [D loss: 0.999973] [G loss: 1.000051]\n",
            "648 [D loss: 0.999971] [G loss: 1.000053]\n",
            "649 [D loss: 0.999970] [G loss: 1.000053]\n",
            "650 [D loss: 0.999970] [G loss: 1.000053]\n",
            "651 [D loss: 0.999973] [G loss: 1.000056]\n",
            "652 [D loss: 0.999969] [G loss: 1.000058]\n",
            "653 [D loss: 0.999970] [G loss: 1.000060]\n",
            "654 [D loss: 0.999968] [G loss: 1.000057]\n",
            "655 [D loss: 0.999973] [G loss: 1.000055]\n",
            "656 [D loss: 0.999970] [G loss: 1.000058]\n",
            "657 [D loss: 0.999974] [G loss: 1.000054]\n",
            "658 [D loss: 0.999971] [G loss: 1.000058]\n",
            "659 [D loss: 0.999971] [G loss: 1.000052]\n",
            "660 [D loss: 0.999970] [G loss: 1.000059]\n",
            "661 [D loss: 0.999971] [G loss: 1.000054]\n",
            "662 [D loss: 0.999970] [G loss: 1.000047]\n",
            "663 [D loss: 0.999974] [G loss: 1.000047]\n",
            "664 [D loss: 0.999970] [G loss: 1.000057]\n",
            "665 [D loss: 0.999969] [G loss: 1.000058]\n",
            "666 [D loss: 0.999970] [G loss: 1.000053]\n",
            "667 [D loss: 0.999971] [G loss: 1.000054]\n",
            "668 [D loss: 0.999974] [G loss: 1.000049]\n",
            "669 [D loss: 0.999972] [G loss: 1.000051]\n",
            "670 [D loss: 0.999973] [G loss: 1.000053]\n",
            "671 [D loss: 0.999972] [G loss: 1.000059]\n",
            "672 [D loss: 0.999970] [G loss: 1.000054]\n",
            "673 [D loss: 0.999973] [G loss: 1.000048]\n",
            "674 [D loss: 0.999975] [G loss: 1.000045]\n",
            "675 [D loss: 0.999969] [G loss: 1.000056]\n",
            "676 [D loss: 0.999969] [G loss: 1.000060]\n",
            "677 [D loss: 0.999969] [G loss: 1.000062]\n",
            "678 [D loss: 0.999970] [G loss: 1.000060]\n",
            "679 [D loss: 0.999965] [G loss: 1.000054]\n",
            "680 [D loss: 0.999972] [G loss: 1.000050]\n",
            "681 [D loss: 0.999974] [G loss: 1.000039]\n",
            "682 [D loss: 0.999974] [G loss: 1.000048]\n",
            "683 [D loss: 0.999970] [G loss: 1.000058]\n",
            "684 [D loss: 0.999969] [G loss: 1.000052]\n",
            "685 [D loss: 0.999971] [G loss: 1.000056]\n",
            "686 [D loss: 0.999976] [G loss: 1.000045]\n",
            "687 [D loss: 0.999969] [G loss: 1.000056]\n",
            "688 [D loss: 0.999968] [G loss: 1.000054]\n",
            "689 [D loss: 0.999967] [G loss: 1.000061]\n",
            "690 [D loss: 0.999969] [G loss: 1.000054]\n",
            "691 [D loss: 0.999973] [G loss: 1.000058]\n",
            "692 [D loss: 0.999970] [G loss: 1.000060]\n",
            "693 [D loss: 0.999970] [G loss: 1.000061]\n",
            "694 [D loss: 0.999974] [G loss: 1.000054]\n",
            "695 [D loss: 0.999972] [G loss: 1.000053]\n",
            "696 [D loss: 0.999972] [G loss: 1.000058]\n",
            "697 [D loss: 0.999970] [G loss: 1.000057]\n",
            "698 [D loss: 0.999965] [G loss: 1.000059]\n",
            "699 [D loss: 0.999970] [G loss: 1.000058]\n",
            "700 [D loss: 0.999972] [G loss: 1.000057]\n",
            "701 [D loss: 0.999970] [G loss: 1.000057]\n",
            "702 [D loss: 0.999970] [G loss: 1.000058]\n",
            "703 [D loss: 0.999971] [G loss: 1.000052]\n",
            "704 [D loss: 0.999976] [G loss: 1.000045]\n",
            "705 [D loss: 0.999973] [G loss: 1.000056]\n",
            "706 [D loss: 0.999968] [G loss: 1.000058]\n",
            "707 [D loss: 0.999968] [G loss: 1.000063]\n",
            "708 [D loss: 0.999970] [G loss: 1.000057]\n",
            "709 [D loss: 0.999972] [G loss: 1.000050]\n",
            "710 [D loss: 0.999973] [G loss: 1.000057]\n",
            "711 [D loss: 0.999971] [G loss: 1.000053]\n",
            "712 [D loss: 0.999970] [G loss: 1.000050]\n",
            "713 [D loss: 0.999973] [G loss: 1.000051]\n",
            "714 [D loss: 0.999971] [G loss: 1.000055]\n",
            "715 [D loss: 0.999968] [G loss: 1.000055]\n",
            "716 [D loss: 0.999974] [G loss: 1.000051]\n",
            "717 [D loss: 0.999969] [G loss: 1.000060]\n",
            "718 [D loss: 0.999974] [G loss: 1.000056]\n",
            "719 [D loss: 0.999971] [G loss: 1.000058]\n",
            "720 [D loss: 0.999972] [G loss: 1.000057]\n",
            "721 [D loss: 0.999974] [G loss: 1.000054]\n",
            "722 [D loss: 0.999974] [G loss: 1.000053]\n",
            "723 [D loss: 0.999973] [G loss: 1.000048]\n",
            "724 [D loss: 0.999971] [G loss: 1.000055]\n",
            "725 [D loss: 0.999972] [G loss: 1.000054]\n",
            "726 [D loss: 0.999975] [G loss: 1.000051]\n",
            "727 [D loss: 0.999970] [G loss: 1.000058]\n",
            "728 [D loss: 0.999974] [G loss: 1.000053]\n",
            "729 [D loss: 0.999971] [G loss: 1.000047]\n",
            "730 [D loss: 0.999976] [G loss: 1.000050]\n",
            "731 [D loss: 0.999971] [G loss: 1.000059]\n",
            "732 [D loss: 0.999969] [G loss: 1.000061]\n",
            "733 [D loss: 0.999971] [G loss: 1.000054]\n",
            "734 [D loss: 0.999972] [G loss: 1.000061]\n",
            "735 [D loss: 0.999970] [G loss: 1.000058]\n",
            "736 [D loss: 0.999974] [G loss: 1.000049]\n",
            "737 [D loss: 0.999969] [G loss: 1.000051]\n",
            "738 [D loss: 0.999973] [G loss: 1.000061]\n",
            "739 [D loss: 0.999974] [G loss: 1.000057]\n",
            "740 [D loss: 0.999973] [G loss: 1.000054]\n",
            "741 [D loss: 0.999970] [G loss: 1.000059]\n",
            "742 [D loss: 0.999973] [G loss: 1.000047]\n",
            "743 [D loss: 0.999972] [G loss: 1.000059]\n",
            "744 [D loss: 0.999971] [G loss: 1.000055]\n",
            "745 [D loss: 0.999971] [G loss: 1.000054]\n",
            "746 [D loss: 0.999972] [G loss: 1.000054]\n",
            "747 [D loss: 0.999972] [G loss: 1.000050]\n",
            "748 [D loss: 0.999974] [G loss: 1.000058]\n",
            "749 [D loss: 0.999973] [G loss: 1.000057]\n",
            "750 [D loss: 0.999972] [G loss: 1.000060]\n",
            "751 [D loss: 0.999969] [G loss: 1.000052]\n",
            "752 [D loss: 0.999973] [G loss: 1.000051]\n",
            "753 [D loss: 0.999968] [G loss: 1.000056]\n",
            "754 [D loss: 0.999968] [G loss: 1.000060]\n",
            "755 [D loss: 0.999974] [G loss: 1.000050]\n",
            "756 [D loss: 0.999970] [G loss: 1.000059]\n",
            "757 [D loss: 0.999972] [G loss: 1.000054]\n",
            "758 [D loss: 0.999972] [G loss: 1.000053]\n",
            "759 [D loss: 0.999972] [G loss: 1.000056]\n",
            "760 [D loss: 0.999970] [G loss: 1.000064]\n",
            "761 [D loss: 0.999971] [G loss: 1.000061]\n",
            "762 [D loss: 0.999972] [G loss: 1.000058]\n",
            "763 [D loss: 0.999968] [G loss: 1.000058]\n",
            "764 [D loss: 0.999970] [G loss: 1.000054]\n",
            "765 [D loss: 0.999971] [G loss: 1.000056]\n",
            "766 [D loss: 0.999969] [G loss: 1.000063]\n",
            "767 [D loss: 0.999972] [G loss: 1.000058]\n",
            "768 [D loss: 0.999971] [G loss: 1.000058]\n",
            "769 [D loss: 0.999973] [G loss: 1.000057]\n",
            "770 [D loss: 0.999968] [G loss: 1.000054]\n",
            "771 [D loss: 0.999974] [G loss: 1.000052]\n",
            "772 [D loss: 0.999971] [G loss: 1.000053]\n",
            "773 [D loss: 0.999969] [G loss: 1.000060]\n",
            "774 [D loss: 0.999969] [G loss: 1.000062]\n",
            "775 [D loss: 0.999966] [G loss: 1.000060]\n",
            "776 [D loss: 0.999972] [G loss: 1.000051]\n",
            "777 [D loss: 0.999972] [G loss: 1.000045]\n",
            "778 [D loss: 0.999974] [G loss: 1.000048]\n",
            "779 [D loss: 0.999969] [G loss: 1.000057]\n",
            "780 [D loss: 0.999968] [G loss: 1.000066]\n",
            "781 [D loss: 0.999970] [G loss: 1.000062]\n",
            "782 [D loss: 0.999967] [G loss: 1.000057]\n",
            "783 [D loss: 0.999970] [G loss: 1.000060]\n",
            "784 [D loss: 0.999971] [G loss: 1.000061]\n",
            "785 [D loss: 0.999968] [G loss: 1.000060]\n",
            "786 [D loss: 0.999971] [G loss: 1.000064]\n",
            "787 [D loss: 0.999967] [G loss: 1.000059]\n",
            "788 [D loss: 0.999973] [G loss: 1.000057]\n",
            "789 [D loss: 0.999974] [G loss: 1.000047]\n",
            "790 [D loss: 0.999970] [G loss: 1.000050]\n",
            "791 [D loss: 0.999972] [G loss: 1.000058]\n",
            "792 [D loss: 0.999968] [G loss: 1.000061]\n",
            "793 [D loss: 0.999968] [G loss: 1.000061]\n",
            "794 [D loss: 0.999971] [G loss: 1.000059]\n",
            "795 [D loss: 0.999969] [G loss: 1.000061]\n",
            "796 [D loss: 0.999968] [G loss: 1.000056]\n",
            "797 [D loss: 0.999970] [G loss: 1.000058]\n",
            "798 [D loss: 0.999969] [G loss: 1.000064]\n",
            "799 [D loss: 0.999970] [G loss: 1.000064]\n",
            "800 [D loss: 0.999969] [G loss: 1.000061]\n",
            "801 [D loss: 0.999970] [G loss: 1.000056]\n",
            "802 [D loss: 0.999972] [G loss: 1.000051]\n",
            "803 [D loss: 0.999971] [G loss: 1.000059]\n",
            "804 [D loss: 0.999967] [G loss: 1.000065]\n",
            "805 [D loss: 0.999969] [G loss: 1.000062]\n",
            "806 [D loss: 0.999971] [G loss: 1.000058]\n",
            "807 [D loss: 0.999972] [G loss: 1.000055]\n",
            "808 [D loss: 0.999969] [G loss: 1.000057]\n",
            "809 [D loss: 0.999967] [G loss: 1.000061]\n",
            "810 [D loss: 0.999971] [G loss: 1.000060]\n",
            "811 [D loss: 0.999974] [G loss: 1.000058]\n",
            "812 [D loss: 0.999968] [G loss: 1.000062]\n",
            "813 [D loss: 0.999971] [G loss: 1.000063]\n",
            "814 [D loss: 0.999969] [G loss: 1.000065]\n",
            "815 [D loss: 0.999970] [G loss: 1.000061]\n",
            "816 [D loss: 0.999969] [G loss: 1.000057]\n",
            "817 [D loss: 0.999971] [G loss: 1.000051]\n",
            "818 [D loss: 0.999975] [G loss: 1.000041]\n",
            "819 [D loss: 0.999969] [G loss: 1.000067]\n",
            "820 [D loss: 0.999972] [G loss: 1.000057]\n",
            "821 [D loss: 0.999968] [G loss: 1.000061]\n",
            "822 [D loss: 0.999967] [G loss: 1.000060]\n",
            "823 [D loss: 0.999969] [G loss: 1.000064]\n",
            "824 [D loss: 0.999968] [G loss: 1.000061]\n",
            "825 [D loss: 0.999970] [G loss: 1.000069]\n",
            "826 [D loss: 0.999971] [G loss: 1.000065]\n",
            "827 [D loss: 0.999967] [G loss: 1.000066]\n",
            "828 [D loss: 0.999969] [G loss: 1.000062]\n",
            "829 [D loss: 0.999969] [G loss: 1.000060]\n",
            "830 [D loss: 0.999970] [G loss: 1.000061]\n",
            "831 [D loss: 0.999972] [G loss: 1.000064]\n",
            "832 [D loss: 0.999970] [G loss: 1.000062]\n",
            "833 [D loss: 0.999971] [G loss: 1.000060]\n",
            "834 [D loss: 0.999971] [G loss: 1.000063]\n",
            "835 [D loss: 0.999970] [G loss: 1.000064]\n",
            "836 [D loss: 0.999970] [G loss: 1.000066]\n",
            "837 [D loss: 0.999968] [G loss: 1.000059]\n",
            "838 [D loss: 0.999970] [G loss: 1.000062]\n",
            "839 [D loss: 0.999971] [G loss: 1.000061]\n",
            "840 [D loss: 0.999968] [G loss: 1.000055]\n",
            "841 [D loss: 0.999969] [G loss: 1.000062]\n",
            "842 [D loss: 0.999971] [G loss: 1.000062]\n",
            "843 [D loss: 0.999971] [G loss: 1.000062]\n",
            "844 [D loss: 0.999970] [G loss: 1.000064]\n",
            "845 [D loss: 0.999969] [G loss: 1.000062]\n",
            "846 [D loss: 0.999972] [G loss: 1.000063]\n",
            "847 [D loss: 0.999972] [G loss: 1.000063]\n",
            "848 [D loss: 0.999973] [G loss: 1.000062]\n",
            "849 [D loss: 0.999972] [G loss: 1.000055]\n",
            "850 [D loss: 0.999968] [G loss: 1.000059]\n",
            "851 [D loss: 0.999969] [G loss: 1.000066]\n",
            "852 [D loss: 0.999971] [G loss: 1.000061]\n",
            "853 [D loss: 0.999970] [G loss: 1.000064]\n",
            "854 [D loss: 0.999970] [G loss: 1.000064]\n",
            "855 [D loss: 0.999968] [G loss: 1.000063]\n",
            "856 [D loss: 0.999971] [G loss: 1.000060]\n",
            "857 [D loss: 0.999970] [G loss: 1.000062]\n",
            "858 [D loss: 0.999972] [G loss: 1.000063]\n",
            "859 [D loss: 0.999970] [G loss: 1.000060]\n",
            "860 [D loss: 0.999971] [G loss: 1.000055]\n",
            "861 [D loss: 0.999974] [G loss: 1.000050]\n",
            "862 [D loss: 0.999970] [G loss: 1.000061]\n",
            "863 [D loss: 0.999968] [G loss: 1.000063]\n",
            "864 [D loss: 0.999966] [G loss: 1.000063]\n",
            "865 [D loss: 0.999970] [G loss: 1.000061]\n",
            "866 [D loss: 0.999968] [G loss: 1.000064]\n",
            "867 [D loss: 0.999971] [G loss: 1.000064]\n",
            "868 [D loss: 0.999971] [G loss: 1.000065]\n",
            "869 [D loss: 0.999968] [G loss: 1.000064]\n",
            "870 [D loss: 0.999968] [G loss: 1.000060]\n",
            "871 [D loss: 0.999969] [G loss: 1.000060]\n",
            "872 [D loss: 0.999972] [G loss: 1.000063]\n",
            "873 [D loss: 0.999970] [G loss: 1.000066]\n",
            "874 [D loss: 0.999971] [G loss: 1.000064]\n",
            "875 [D loss: 0.999969] [G loss: 1.000060]\n",
            "876 [D loss: 0.999969] [G loss: 1.000064]\n",
            "877 [D loss: 0.999968] [G loss: 1.000062]\n",
            "878 [D loss: 0.999969] [G loss: 1.000061]\n",
            "879 [D loss: 0.999970] [G loss: 1.000063]\n",
            "880 [D loss: 0.999971] [G loss: 1.000059]\n",
            "881 [D loss: 0.999969] [G loss: 1.000064]\n",
            "882 [D loss: 0.999968] [G loss: 1.000065]\n",
            "883 [D loss: 0.999971] [G loss: 1.000060]\n",
            "884 [D loss: 0.999966] [G loss: 1.000054]\n",
            "885 [D loss: 0.999970] [G loss: 1.000060]\n",
            "886 [D loss: 0.999970] [G loss: 1.000063]\n",
            "887 [D loss: 0.999973] [G loss: 1.000062]\n",
            "888 [D loss: 0.999971] [G loss: 1.000064]\n",
            "889 [D loss: 0.999969] [G loss: 1.000057]\n",
            "890 [D loss: 0.999967] [G loss: 1.000065]\n",
            "891 [D loss: 0.999969] [G loss: 1.000061]\n",
            "892 [D loss: 0.999969] [G loss: 1.000062]\n",
            "893 [D loss: 0.999972] [G loss: 1.000060]\n",
            "894 [D loss: 0.999966] [G loss: 1.000064]\n",
            "895 [D loss: 0.999967] [G loss: 1.000057]\n",
            "896 [D loss: 0.999969] [G loss: 1.000064]\n",
            "897 [D loss: 0.999970] [G loss: 1.000061]\n",
            "898 [D loss: 0.999967] [G loss: 1.000065]\n",
            "899 [D loss: 0.999972] [G loss: 1.000058]\n",
            "900 [D loss: 0.999970] [G loss: 1.000063]\n",
            "901 [D loss: 0.999970] [G loss: 1.000063]\n",
            "902 [D loss: 0.999970] [G loss: 1.000062]\n",
            "903 [D loss: 0.999971] [G loss: 1.000061]\n",
            "904 [D loss: 0.999971] [G loss: 1.000060]\n",
            "905 [D loss: 0.999971] [G loss: 1.000063]\n",
            "906 [D loss: 0.999969] [G loss: 1.000059]\n",
            "907 [D loss: 0.999972] [G loss: 1.000056]\n",
            "908 [D loss: 0.999967] [G loss: 1.000063]\n",
            "909 [D loss: 0.999971] [G loss: 1.000061]\n",
            "910 [D loss: 0.999971] [G loss: 1.000061]\n",
            "911 [D loss: 0.999972] [G loss: 1.000060]\n",
            "912 [D loss: 0.999969] [G loss: 1.000060]\n",
            "913 [D loss: 0.999970] [G loss: 1.000064]\n",
            "914 [D loss: 0.999970] [G loss: 1.000064]\n",
            "915 [D loss: 0.999968] [G loss: 1.000064]\n",
            "916 [D loss: 0.999969] [G loss: 1.000062]\n",
            "917 [D loss: 0.999971] [G loss: 1.000063]\n",
            "918 [D loss: 0.999969] [G loss: 1.000064]\n",
            "919 [D loss: 0.999969] [G loss: 1.000062]\n",
            "920 [D loss: 0.999970] [G loss: 1.000059]\n",
            "921 [D loss: 0.999968] [G loss: 1.000062]\n",
            "922 [D loss: 0.999969] [G loss: 1.000056]\n",
            "923 [D loss: 0.999973] [G loss: 1.000060]\n",
            "924 [D loss: 0.999971] [G loss: 1.000060]\n",
            "925 [D loss: 0.999972] [G loss: 1.000059]\n",
            "926 [D loss: 0.999970] [G loss: 1.000061]\n",
            "927 [D loss: 0.999970] [G loss: 1.000056]\n",
            "928 [D loss: 0.999973] [G loss: 1.000064]\n",
            "929 [D loss: 0.999969] [G loss: 1.000062]\n",
            "930 [D loss: 0.999969] [G loss: 1.000064]\n",
            "931 [D loss: 0.999970] [G loss: 1.000066]\n",
            "932 [D loss: 0.999970] [G loss: 1.000060]\n",
            "933 [D loss: 0.999969] [G loss: 1.000057]\n",
            "934 [D loss: 0.999967] [G loss: 1.000063]\n",
            "935 [D loss: 0.999972] [G loss: 1.000062]\n",
            "936 [D loss: 0.999970] [G loss: 1.000064]\n",
            "937 [D loss: 0.999970] [G loss: 1.000060]\n",
            "938 [D loss: 0.999971] [G loss: 1.000062]\n",
            "939 [D loss: 0.999972] [G loss: 1.000060]\n",
            "940 [D loss: 0.999970] [G loss: 1.000061]\n",
            "941 [D loss: 0.999969] [G loss: 1.000060]\n",
            "942 [D loss: 0.999971] [G loss: 1.000059]\n",
            "943 [D loss: 0.999968] [G loss: 1.000061]\n",
            "944 [D loss: 0.999970] [G loss: 1.000062]\n",
            "945 [D loss: 0.999968] [G loss: 1.000065]\n",
            "946 [D loss: 0.999968] [G loss: 1.000061]\n",
            "947 [D loss: 0.999972] [G loss: 1.000063]\n",
            "948 [D loss: 0.999969] [G loss: 1.000064]\n",
            "949 [D loss: 0.999969] [G loss: 1.000064]\n",
            "950 [D loss: 0.999970] [G loss: 1.000059]\n",
            "951 [D loss: 0.999970] [G loss: 1.000063]\n",
            "952 [D loss: 0.999971] [G loss: 1.000067]\n",
            "953 [D loss: 0.999970] [G loss: 1.000063]\n",
            "954 [D loss: 0.999971] [G loss: 1.000059]\n",
            "955 [D loss: 0.999969] [G loss: 1.000058]\n",
            "956 [D loss: 0.999970] [G loss: 1.000066]\n",
            "957 [D loss: 0.999970] [G loss: 1.000064]\n",
            "958 [D loss: 0.999968] [G loss: 1.000059]\n",
            "959 [D loss: 0.999972] [G loss: 1.000060]\n",
            "960 [D loss: 0.999971] [G loss: 1.000063]\n",
            "961 [D loss: 0.999969] [G loss: 1.000062]\n",
            "962 [D loss: 0.999967] [G loss: 1.000061]\n",
            "963 [D loss: 0.999971] [G loss: 1.000063]\n",
            "964 [D loss: 0.999972] [G loss: 1.000060]\n",
            "965 [D loss: 0.999968] [G loss: 1.000062]\n",
            "966 [D loss: 0.999970] [G loss: 1.000059]\n",
            "967 [D loss: 0.999969] [G loss: 1.000065]\n",
            "968 [D loss: 0.999969] [G loss: 1.000061]\n",
            "969 [D loss: 0.999972] [G loss: 1.000064]\n",
            "970 [D loss: 0.999969] [G loss: 1.000066]\n",
            "971 [D loss: 0.999968] [G loss: 1.000065]\n",
            "972 [D loss: 0.999971] [G loss: 1.000061]\n",
            "973 [D loss: 0.999966] [G loss: 1.000063]\n",
            "974 [D loss: 0.999969] [G loss: 1.000058]\n",
            "975 [D loss: 0.999969] [G loss: 1.000063]\n",
            "976 [D loss: 0.999970] [G loss: 1.000060]\n",
            "977 [D loss: 0.999970] [G loss: 1.000064]\n",
            "978 [D loss: 0.999967] [G loss: 1.000064]\n",
            "979 [D loss: 0.999970] [G loss: 1.000066]\n",
            "980 [D loss: 0.999969] [G loss: 1.000064]\n",
            "981 [D loss: 0.999970] [G loss: 1.000066]\n",
            "982 [D loss: 0.999968] [G loss: 1.000067]\n",
            "983 [D loss: 0.999969] [G loss: 1.000061]\n",
            "984 [D loss: 0.999968] [G loss: 1.000063]\n",
            "985 [D loss: 0.999970] [G loss: 1.000061]\n",
            "986 [D loss: 0.999972] [G loss: 1.000063]\n",
            "987 [D loss: 0.999971] [G loss: 1.000061]\n",
            "988 [D loss: 0.999970] [G loss: 1.000061]\n",
            "989 [D loss: 0.999970] [G loss: 1.000065]\n",
            "990 [D loss: 0.999969] [G loss: 1.000059]\n",
            "991 [D loss: 0.999968] [G loss: 1.000064]\n",
            "992 [D loss: 0.999970] [G loss: 1.000064]\n",
            "993 [D loss: 0.999968] [G loss: 1.000059]\n",
            "994 [D loss: 0.999964] [G loss: 1.000061]\n",
            "995 [D loss: 0.999968] [G loss: 1.000063]\n",
            "996 [D loss: 0.999969] [G loss: 1.000065]\n",
            "997 [D loss: 0.999967] [G loss: 1.000066]\n",
            "998 [D loss: 0.999970] [G loss: 1.000065]\n",
            "999 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1000 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1001 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1002 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1003 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1004 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1005 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1006 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1007 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1008 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1009 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1010 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1011 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1012 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1013 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1014 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1015 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1016 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1017 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1018 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1019 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1020 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1021 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1022 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1023 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1024 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1025 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1026 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1027 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1028 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1029 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1030 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1031 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1032 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1033 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1034 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1035 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1036 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1037 [D loss: 0.999966] [G loss: 1.000062]\n",
            "1038 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1039 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1040 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1041 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1042 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1043 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1044 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1045 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1046 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1047 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1048 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1049 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1050 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1051 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1052 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1053 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1054 [D loss: 0.999972] [G loss: 1.000054]\n",
            "1055 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1056 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1057 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1058 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1059 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1060 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1061 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1062 [D loss: 0.999965] [G loss: 1.000062]\n",
            "1063 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1064 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1065 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1066 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1067 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1068 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1069 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1070 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1071 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1072 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1073 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1074 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1075 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1076 [D loss: 0.999973] [G loss: 1.000056]\n",
            "1077 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1078 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1079 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1080 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1081 [D loss: 0.999971] [G loss: 1.000055]\n",
            "1082 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1083 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1084 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1085 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1086 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1087 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1088 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1089 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1090 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1091 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1092 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1093 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1094 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1095 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1096 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1097 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1098 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1099 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1100 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1101 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1102 [D loss: 0.999967] [G loss: 1.000061]\n",
            "1103 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1104 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1105 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1106 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1107 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1108 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1109 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1110 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1111 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1112 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1113 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1114 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1115 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1116 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1117 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1118 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1119 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1120 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1121 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1122 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1123 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1124 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1125 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1126 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1127 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1128 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1129 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1130 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1131 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1132 [D loss: 0.999969] [G loss: 1.000056]\n",
            "1133 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1134 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1135 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1136 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1137 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1138 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1139 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1140 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1141 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1142 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1143 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1144 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1145 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1146 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1147 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1148 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1149 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1150 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1151 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1152 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1153 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1154 [D loss: 0.999972] [G loss: 1.000053]\n",
            "1155 [D loss: 0.999972] [G loss: 1.000053]\n",
            "1156 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1157 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1158 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1159 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1160 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1161 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1162 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1163 [D loss: 0.999971] [G loss: 1.000056]\n",
            "1164 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1165 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1166 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1167 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1168 [D loss: 0.999965] [G loss: 1.000061]\n",
            "1169 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1170 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1171 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1172 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1173 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1174 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1175 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1176 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1177 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1178 [D loss: 0.999971] [G loss: 1.000054]\n",
            "1179 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1180 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1181 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1182 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1183 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1184 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1185 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1186 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1187 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1188 [D loss: 0.999971] [G loss: 1.000068]\n",
            "1189 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1190 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1191 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1192 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1193 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1194 [D loss: 0.999969] [G loss: 1.000055]\n",
            "1195 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1196 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1197 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1198 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1199 [D loss: 0.999971] [G loss: 1.000056]\n",
            "1200 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1201 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1202 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1203 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1204 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1205 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1206 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1207 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1208 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1209 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1210 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1211 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1212 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1213 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1214 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1215 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1216 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1217 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1218 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1219 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1220 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1221 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1222 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1223 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1224 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1225 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1226 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1227 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1228 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1229 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1230 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1231 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1232 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1233 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1234 [D loss: 0.999969] [G loss: 1.000058]\n",
            "1235 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1236 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1237 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1238 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1239 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1240 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1241 [D loss: 0.999973] [G loss: 1.000068]\n",
            "1242 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1243 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1244 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1245 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1246 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1247 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1248 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1249 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1250 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1251 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1252 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1253 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1254 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1255 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1256 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1257 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1258 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1259 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1260 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1261 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1262 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1263 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1264 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1265 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1266 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1267 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1268 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1269 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1270 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1271 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1272 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1273 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1274 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1275 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1276 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1277 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1278 [D loss: 0.999973] [G loss: 1.000059]\n",
            "1279 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1280 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1281 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1282 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1283 [D loss: 0.999967] [G loss: 1.000060]\n",
            "1284 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1285 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1286 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1287 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1288 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1289 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1290 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1291 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1292 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1293 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1294 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1295 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1296 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1297 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1298 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1299 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1300 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1301 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1302 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1303 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1304 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1305 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1306 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1307 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1308 [D loss: 0.999974] [G loss: 1.000064]\n",
            "1309 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1310 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1311 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1312 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1313 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1314 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1315 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1316 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1317 [D loss: 0.999968] [G loss: 1.000056]\n",
            "1318 [D loss: 0.999973] [G loss: 1.000055]\n",
            "1319 [D loss: 0.999971] [G loss: 1.000059]\n",
            "1320 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1321 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1322 [D loss: 0.999966] [G loss: 1.000063]\n",
            "1323 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1324 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1325 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1326 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1327 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1328 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1329 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1330 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1331 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1332 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1333 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1334 [D loss: 0.999969] [G loss: 1.000058]\n",
            "1335 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1336 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1337 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1338 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1339 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1340 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1341 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1342 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1343 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1344 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1345 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1346 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1347 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1348 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1349 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1350 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1351 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1352 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1353 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1354 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1355 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1356 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1357 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1358 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1359 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1360 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1361 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1362 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1363 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1364 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1365 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1366 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1367 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1368 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1369 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1370 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1371 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1372 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1373 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1374 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1375 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1376 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1377 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1378 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1379 [D loss: 0.999969] [G loss: 1.000058]\n",
            "1380 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1381 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1382 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1383 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1384 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1385 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1386 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1387 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1388 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1389 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1390 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1391 [D loss: 0.999967] [G loss: 1.000061]\n",
            "1392 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1393 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1394 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1395 [D loss: 0.999970] [G loss: 1.000069]\n",
            "1396 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1397 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1398 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1399 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1400 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1401 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1402 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1403 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1404 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1405 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1406 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1407 [D loss: 0.999971] [G loss: 1.000053]\n",
            "1408 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1409 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1410 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1411 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1412 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1413 [D loss: 0.999971] [G loss: 1.000056]\n",
            "1414 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1415 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1416 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1417 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1418 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1419 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1420 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1421 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1422 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1423 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1424 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1425 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1426 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1427 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1428 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1429 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1430 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1431 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1432 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1433 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1434 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1435 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1436 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1437 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1438 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1439 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1440 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1441 [D loss: 0.999973] [G loss: 1.000066]\n",
            "1442 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1443 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1444 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1445 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1446 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1447 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1448 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1449 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1450 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1451 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1452 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1453 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1454 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1455 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1456 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1457 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1458 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1459 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1460 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1461 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1462 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1463 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1464 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1465 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1466 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1467 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1468 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1469 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1470 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1471 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1472 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1473 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1474 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1475 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1476 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1477 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1478 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1479 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1480 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1481 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1482 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1483 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1484 [D loss: 0.999972] [G loss: 1.000057]\n",
            "1485 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1486 [D loss: 0.999968] [G loss: 1.000058]\n",
            "1487 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1488 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1489 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1490 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1491 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1492 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1493 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1494 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1495 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1496 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1497 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1498 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1499 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1500 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1501 [D loss: 0.999969] [G loss: 1.000058]\n",
            "1502 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1503 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1504 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1505 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1506 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1507 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1508 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1509 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1510 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1511 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1512 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1513 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1514 [D loss: 0.999969] [G loss: 1.000068]\n",
            "1515 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1516 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1517 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1518 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1519 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1520 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1521 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1522 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1523 [D loss: 0.999968] [G loss: 1.000059]\n",
            "1524 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1525 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1526 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1527 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1528 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1529 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1530 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1531 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1532 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1533 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1534 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1535 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1536 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1537 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1538 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1539 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1540 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1541 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1542 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1543 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1544 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1545 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1546 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1547 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1548 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1549 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1550 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1551 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1552 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1553 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1554 [D loss: 0.999968] [G loss: 1.000056]\n",
            "1555 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1556 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1557 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1558 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1559 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1560 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1561 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1562 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1563 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1564 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1565 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1566 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1567 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1568 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1569 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1570 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1571 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1572 [D loss: 0.999966] [G loss: 1.000066]\n",
            "1573 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1574 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1575 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1576 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1577 [D loss: 0.999972] [G loss: 1.000054]\n",
            "1578 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1579 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1581 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1582 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1583 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1584 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1585 [D loss: 0.999967] [G loss: 1.000063]\n",
            "1586 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1587 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1588 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1589 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1590 [D loss: 0.999968] [G loss: 1.000057]\n",
            "1591 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1592 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1593 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1594 [D loss: 0.999969] [G loss: 1.000069]\n",
            "1595 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1596 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1597 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1598 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1599 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1600 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1601 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1602 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1603 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1604 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1605 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1606 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1607 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1608 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1609 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1610 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1611 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1612 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1613 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1614 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1615 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1616 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1617 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1618 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1619 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1620 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1621 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1622 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1623 [D loss: 0.999966] [G loss: 1.000062]\n",
            "1624 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1625 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1626 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1627 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1628 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1629 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1630 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1631 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1632 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1633 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1634 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1635 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1636 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1637 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1638 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1639 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1640 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1641 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1642 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1643 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1644 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1645 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1646 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1647 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1648 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1649 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1650 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1651 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1652 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1653 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1654 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1655 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1656 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1657 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1658 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1659 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1660 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1661 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1662 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1663 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1664 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1665 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1666 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1667 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1668 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1669 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1670 [D loss: 0.999966] [G loss: 1.000062]\n",
            "1671 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1672 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1673 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1674 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1675 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1676 [D loss: 0.999968] [G loss: 1.000067]\n",
            "1677 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1678 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1679 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1680 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1681 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1682 [D loss: 0.999971] [G loss: 1.000055]\n",
            "1683 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1684 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1685 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1686 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1687 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1688 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1689 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1690 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1691 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1692 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1693 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1694 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1695 [D loss: 0.999965] [G loss: 1.000064]\n",
            "1696 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1697 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1698 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1699 [D loss: 0.999973] [G loss: 1.000056]\n",
            "1700 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1701 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1702 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1703 [D loss: 0.999971] [G loss: 1.000058]\n",
            "1704 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1705 [D loss: 0.999971] [G loss: 1.000057]\n",
            "1706 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1707 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1708 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1709 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1710 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1711 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1712 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1713 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1714 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1715 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1716 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1717 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1718 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1719 [D loss: 0.999973] [G loss: 1.000059]\n",
            "1720 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1721 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1722 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1723 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1724 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1725 [D loss: 0.999973] [G loss: 1.000066]\n",
            "1726 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1727 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1728 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1729 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1730 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1731 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1732 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1733 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1734 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1735 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1736 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1737 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1738 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1739 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1740 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1741 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1742 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1743 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1744 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1745 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1746 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1747 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1748 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1749 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1750 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1751 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1752 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1753 [D loss: 0.999972] [G loss: 1.000067]\n",
            "1754 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1755 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1756 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1757 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1758 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1759 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1760 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1761 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1762 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1763 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1764 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1765 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1766 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1767 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1768 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1769 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1770 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1771 [D loss: 0.999969] [G loss: 1.000060]\n",
            "1772 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1773 [D loss: 0.999965] [G loss: 1.000062]\n",
            "1774 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1775 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1776 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1777 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1778 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1779 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1780 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1781 [D loss: 0.999965] [G loss: 1.000060]\n",
            "1782 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1783 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1784 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1785 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1786 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1787 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1788 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1789 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1790 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1791 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1792 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1793 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1794 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1795 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1796 [D loss: 0.999971] [G loss: 1.000056]\n",
            "1797 [D loss: 0.999973] [G loss: 1.000064]\n",
            "1798 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1799 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1800 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1801 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1802 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1803 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1804 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1805 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1806 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1807 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1808 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1809 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1810 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1811 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1812 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1813 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1814 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1815 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1816 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1817 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1818 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1819 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1820 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1821 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1822 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1823 [D loss: 0.999972] [G loss: 1.000061]\n",
            "1824 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1825 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1826 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1827 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1828 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1829 [D loss: 0.999968] [G loss: 1.000069]\n",
            "1830 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1831 [D loss: 0.999972] [G loss: 1.000059]\n",
            "1832 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1833 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1834 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1835 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1836 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1837 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1838 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1839 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1840 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1841 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1842 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1843 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1844 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1845 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1846 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1847 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1848 [D loss: 0.999966] [G loss: 1.000064]\n",
            "1849 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1850 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1851 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1852 [D loss: 0.999972] [G loss: 1.000056]\n",
            "1853 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1854 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1855 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1856 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1857 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1858 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1859 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1860 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1861 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1862 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1863 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1864 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1865 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1866 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1867 [D loss: 0.999973] [G loss: 1.000061]\n",
            "1868 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1869 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1870 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1871 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1872 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1873 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1874 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1875 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1876 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1877 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1878 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1879 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1880 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1881 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1882 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1883 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1884 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1885 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1886 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1887 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1888 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1889 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1890 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1891 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1892 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1893 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1894 [D loss: 0.999967] [G loss: 1.000062]\n",
            "1895 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1896 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1897 [D loss: 0.999971] [G loss: 1.000069]\n",
            "1898 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1899 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1900 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1901 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1902 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1903 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1904 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1905 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1906 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1907 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1908 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1909 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1910 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1911 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1912 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1913 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1914 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1915 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1916 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1917 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1918 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1919 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1920 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1921 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1922 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1923 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1924 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1925 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1926 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1927 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1928 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1929 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1930 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1931 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1932 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1933 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1934 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1935 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1936 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1937 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1938 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1939 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1940 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1941 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1942 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1943 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1944 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1945 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1946 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1947 [D loss: 0.999971] [G loss: 1.000068]\n",
            "1948 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1949 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1950 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1951 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1952 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1953 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1954 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1955 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1956 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1957 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1958 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1959 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1960 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1961 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1962 [D loss: 0.999970] [G loss: 1.000068]\n",
            "1963 [D loss: 0.999973] [G loss: 1.000063]\n",
            "1964 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1965 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1966 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1967 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1968 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1969 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1970 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1971 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1972 [D loss: 0.999967] [G loss: 1.000067]\n",
            "1973 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1974 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1975 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1976 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1977 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1978 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1979 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1980 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1981 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1982 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1983 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1984 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1985 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1986 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1987 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1988 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1989 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1990 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1991 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1992 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1993 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1994 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1995 [D loss: 0.999971] [G loss: 1.000066]\n",
            "1996 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1997 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1998 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1999 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2000 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2001 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2002 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2003 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2004 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2005 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2006 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2007 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2008 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2009 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2010 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2011 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2012 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2013 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2014 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2015 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2016 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2017 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2018 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2019 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2020 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2021 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2022 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2023 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2024 [D loss: 0.999970] [G loss: 1.000069]\n",
            "2025 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2026 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2027 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2028 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2029 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2030 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2031 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2032 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2033 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2034 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2035 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2036 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2037 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2038 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2039 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2040 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2041 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2042 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2043 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2044 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2045 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2046 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2047 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2048 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2049 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2050 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2051 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2052 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2053 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2054 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2055 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2056 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2057 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2058 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2059 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2060 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2061 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2062 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2063 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2064 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2065 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2066 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2067 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2068 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2069 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2070 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2071 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2072 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2073 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2074 [D loss: 0.999967] [G loss: 1.000063]\n",
            "2075 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2076 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2077 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2078 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2079 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2080 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2081 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2082 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2083 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2084 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2085 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2086 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2087 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2088 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2089 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2090 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2091 [D loss: 0.999974] [G loss: 1.000063]\n",
            "2092 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2093 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2094 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2095 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2096 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2097 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2098 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2099 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2100 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2101 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2102 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2103 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2104 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2105 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2106 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2107 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2108 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2109 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2110 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2111 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2112 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2113 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2114 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2115 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2116 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2117 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2118 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2119 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2120 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2121 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2122 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2123 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2124 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2125 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2126 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2127 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2128 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2129 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2130 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2131 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2132 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2133 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2134 [D loss: 0.999969] [G loss: 1.000061]\n",
            "2135 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2136 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2137 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2138 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2139 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2140 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2141 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2142 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2143 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2144 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2145 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2146 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2147 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2148 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2149 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2150 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2151 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2152 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2153 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2154 [D loss: 0.999966] [G loss: 1.000065]\n",
            "2155 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2156 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2157 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2158 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2159 [D loss: 0.999968] [G loss: 1.000067]\n",
            "2160 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2161 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2162 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2163 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2164 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2165 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2166 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2167 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2168 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2169 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2170 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2171 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2172 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2173 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2174 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2175 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2176 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2177 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2178 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2179 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2180 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2181 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2182 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2183 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2184 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2185 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2186 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2187 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2188 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2189 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2190 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2191 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2192 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2193 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2194 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2195 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2196 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2197 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2198 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2199 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2200 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2201 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2202 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2203 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2204 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2205 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2206 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2207 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2208 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2209 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2210 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2211 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2212 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2213 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2214 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2215 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2216 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2217 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2218 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2219 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2220 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2221 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2222 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2223 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2224 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2225 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2226 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2227 [D loss: 0.999966] [G loss: 1.000064]\n",
            "2228 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2229 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2230 [D loss: 0.999968] [G loss: 1.000067]\n",
            "2231 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2232 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2233 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2234 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2235 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2236 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2237 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2238 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2239 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2240 [D loss: 0.999973] [G loss: 1.000068]\n",
            "2241 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2242 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2243 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2244 [D loss: 0.999969] [G loss: 1.000070]\n",
            "2245 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2246 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2247 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2248 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2249 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2250 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2251 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2252 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2253 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2254 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2255 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2256 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2257 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2258 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2259 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2260 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2261 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2262 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2263 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2264 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2265 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2266 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2267 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2268 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2269 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2270 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2271 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2272 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2273 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2274 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2275 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2276 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2277 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2278 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2279 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2280 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2281 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2282 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2283 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2284 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2285 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2286 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2287 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2288 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2289 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2290 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2291 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2292 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2293 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2294 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2295 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2296 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2297 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2298 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2299 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2300 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2301 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2302 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2303 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2304 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2305 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2306 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2307 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2308 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2309 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2310 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2311 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2312 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2313 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2314 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2315 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2316 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2317 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2318 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2319 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2320 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2321 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2322 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2323 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2324 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2325 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2326 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2327 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2328 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2329 [D loss: 0.999971] [G loss: 1.000069]\n",
            "2330 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2331 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2332 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2333 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2334 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2335 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2336 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2337 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2338 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2339 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2340 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2341 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2342 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2343 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2344 [D loss: 0.999966] [G loss: 1.000066]\n",
            "2345 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2346 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2347 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2348 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2349 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2350 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2351 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2352 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2353 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2354 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2355 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2356 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2357 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2358 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2359 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2360 [D loss: 0.999967] [G loss: 1.000064]\n",
            "2361 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2362 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2363 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2364 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2365 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2366 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2367 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2368 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2369 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2370 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2371 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2372 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2373 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2374 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2375 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2376 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2377 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2378 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2379 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2380 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2381 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2382 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2383 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2384 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2385 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2386 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2387 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2388 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2389 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2390 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2391 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2392 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2393 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2394 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2395 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2396 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2397 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2398 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2399 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2400 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2401 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2402 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2403 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2404 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2405 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2406 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2407 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2408 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2409 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2410 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2411 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2412 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2413 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2414 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2415 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2416 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2417 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2418 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2419 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2420 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2421 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2422 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2423 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2424 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2425 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2426 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2427 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2428 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2429 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2430 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2431 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2432 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2433 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2434 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2435 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2436 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2437 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2438 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2439 [D loss: 0.999967] [G loss: 1.000065]\n",
            "2440 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2441 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2442 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2443 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2444 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2445 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2446 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2447 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2448 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2449 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2450 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2451 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2452 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2453 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2454 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2455 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2456 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2457 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2458 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2459 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2460 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2461 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2462 [D loss: 0.999969] [G loss: 1.000069]\n",
            "2463 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2464 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2465 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2466 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2467 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2468 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2469 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2470 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2471 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2472 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2473 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2474 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2475 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2476 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2477 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2478 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2479 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2480 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2481 [D loss: 0.999973] [G loss: 1.000067]\n",
            "2482 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2483 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2484 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2485 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2486 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2487 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2488 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2489 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2490 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2491 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2492 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2493 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2494 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2495 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2496 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2497 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2498 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2499 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2500 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2501 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2502 [D loss: 0.999968] [G loss: 1.000067]\n",
            "2503 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2504 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2505 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2506 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2507 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2508 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2509 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2510 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2511 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2512 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2513 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2514 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2515 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2516 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2517 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2518 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2519 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2520 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2521 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2522 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2523 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2524 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2525 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2526 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2527 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2528 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2529 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2530 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2531 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2532 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2533 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2534 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2535 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2536 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2537 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2538 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2539 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2541 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2542 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2543 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2544 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2545 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2546 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2547 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2548 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2549 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2550 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2551 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2552 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2553 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2554 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2555 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2556 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2557 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2558 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2559 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2560 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2561 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2562 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2563 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2564 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2565 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2566 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2567 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2568 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2569 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2570 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2571 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2572 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2573 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2574 [D loss: 0.999969] [G loss: 1.000069]\n",
            "2575 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2576 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2577 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2578 [D loss: 0.999967] [G loss: 1.000065]\n",
            "2579 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2580 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2581 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2582 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2583 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2584 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2585 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2586 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2587 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2588 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2589 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2590 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2591 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2592 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2593 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2594 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2595 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2596 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2597 [D loss: 0.999970] [G loss: 1.000061]\n",
            "2598 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2599 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2600 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2601 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2602 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2603 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2604 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2605 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2606 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2607 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2608 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2609 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2610 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2611 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2612 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2613 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2614 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2615 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2616 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2617 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2618 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2619 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2620 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2621 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2622 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2623 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2624 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2625 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2626 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2627 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2628 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2629 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2630 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2631 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2632 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2633 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2634 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2635 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2636 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2637 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2638 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2639 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2640 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2641 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2642 [D loss: 0.999967] [G loss: 1.000066]\n",
            "2643 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2644 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2645 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2646 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2647 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2648 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2649 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2650 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2651 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2652 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2653 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2654 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2655 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2656 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2657 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2658 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2659 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2660 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2661 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2662 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2663 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2664 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2665 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2666 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2667 [D loss: 0.999966] [G loss: 1.000066]\n",
            "2668 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2669 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2670 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2671 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2672 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2673 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2674 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2675 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2676 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2677 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2678 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2679 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2680 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2681 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2682 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2683 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2684 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2685 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2686 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2687 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2688 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2689 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2690 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2691 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2692 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2693 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2694 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2695 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2696 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2697 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2698 [D loss: 0.999968] [G loss: 1.000067]\n",
            "2699 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2700 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2701 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2702 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2703 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2704 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2705 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2706 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2707 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2708 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2709 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2710 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2711 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2712 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2713 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2714 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2715 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2716 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2717 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2718 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2719 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2720 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2721 [D loss: 0.999969] [G loss: 1.000061]\n",
            "2722 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2723 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2724 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2725 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2726 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2727 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2728 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2729 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2730 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2731 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2732 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2733 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2734 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2735 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2736 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2737 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2738 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2739 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2740 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2741 [D loss: 0.999969] [G loss: 1.000061]\n",
            "2742 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2743 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2744 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2745 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2746 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2747 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2748 [D loss: 0.999971] [G loss: 1.000069]\n",
            "2749 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2750 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2751 [D loss: 0.999967] [G loss: 1.000063]\n",
            "2752 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2753 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2754 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2755 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2756 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2757 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2758 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2759 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2760 [D loss: 0.999968] [G loss: 1.000060]\n",
            "2761 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2762 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2763 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2764 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2765 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2766 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2767 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2768 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2769 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2770 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2771 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2772 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2773 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2774 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2775 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2776 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2777 [D loss: 0.999973] [G loss: 1.000067]\n",
            "2778 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2779 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2780 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2781 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2782 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2783 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2784 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2785 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2786 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2787 [D loss: 0.999973] [G loss: 1.000062]\n",
            "2788 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2789 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2790 [D loss: 0.999971] [G loss: 1.000069]\n",
            "2791 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2792 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2793 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2794 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2795 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2796 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2797 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2798 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2799 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2800 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2801 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2802 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2803 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2804 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2805 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2806 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2807 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2808 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2809 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2810 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2811 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2812 [D loss: 0.999967] [G loss: 1.000066]\n",
            "2813 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2814 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2815 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2816 [D loss: 0.999967] [G loss: 1.000066]\n",
            "2817 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2818 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2819 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2820 [D loss: 0.999973] [G loss: 1.000061]\n",
            "2821 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2822 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2823 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2824 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2825 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2826 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2827 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2828 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2829 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2830 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2831 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2832 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2833 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2834 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2835 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2836 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2837 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2838 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2839 [D loss: 0.999968] [G loss: 1.000062]\n",
            "2840 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2841 [D loss: 0.999972] [G loss: 1.000060]\n",
            "2842 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2843 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2844 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2845 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2846 [D loss: 0.999973] [G loss: 1.000064]\n",
            "2847 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2848 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2849 [D loss: 0.999967] [G loss: 1.000066]\n",
            "2850 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2851 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2852 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2853 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2854 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2855 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2856 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2857 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2858 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2859 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2860 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2861 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2862 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2863 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2864 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2865 [D loss: 0.999973] [G loss: 1.000067]\n",
            "2866 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2867 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2868 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2869 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2870 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2871 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2872 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2873 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2874 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2875 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2876 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2877 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2878 [D loss: 0.999973] [G loss: 1.000069]\n",
            "2879 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2880 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2881 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2882 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2883 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2884 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2885 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2886 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2887 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2888 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2889 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2890 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2891 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2892 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2893 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2894 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2895 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2896 [D loss: 0.999967] [G loss: 1.000064]\n",
            "2897 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2898 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2899 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2900 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2901 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2902 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2903 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2904 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2905 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2906 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2907 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2908 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2909 [D loss: 0.999970] [G loss: 1.000061]\n",
            "2910 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2911 [D loss: 0.999967] [G loss: 1.000062]\n",
            "2912 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2913 [D loss: 0.999969] [G loss: 1.000063]\n",
            "2914 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2915 [D loss: 0.999971] [G loss: 1.000060]\n",
            "2916 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2917 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2918 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2919 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2920 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2921 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2922 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2923 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2924 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2925 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2926 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2927 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2928 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2929 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2930 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2931 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2932 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2933 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2934 [D loss: 0.999968] [G loss: 1.000065]\n",
            "2935 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2936 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2937 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2938 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2939 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2940 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2941 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2942 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2943 [D loss: 0.999971] [G loss: 1.000059]\n",
            "2944 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2945 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2946 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2947 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2948 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2949 [D loss: 0.999966] [G loss: 1.000066]\n",
            "2950 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2951 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2952 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2953 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2954 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2955 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2956 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2957 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2958 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2959 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2960 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2961 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2962 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2963 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2964 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2965 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2966 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2967 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2968 [D loss: 0.999968] [G loss: 1.000067]\n",
            "2969 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2970 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2971 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2972 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2973 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2974 [D loss: 0.999969] [G loss: 1.000069]\n",
            "2975 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2976 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2977 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2978 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2979 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2980 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2981 [D loss: 0.999970] [G loss: 1.000061]\n",
            "2982 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2983 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2984 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2985 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2986 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2987 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2988 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2989 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2990 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2991 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2992 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2993 [D loss: 0.999972] [G loss: 1.000068]\n",
            "2994 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2995 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2996 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2997 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2998 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2999 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3000 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3001 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3002 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3003 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3004 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3005 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3006 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3007 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3008 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3009 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3010 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3011 [D loss: 0.999972] [G loss: 1.000068]\n",
            "3012 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3013 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3014 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3015 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3016 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3017 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3018 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3019 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3020 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3021 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3022 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3023 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3024 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3025 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3026 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3027 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3028 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3029 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3030 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3031 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3032 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3033 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3034 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3035 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3036 [D loss: 0.999970] [G loss: 1.000069]\n",
            "3037 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3038 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3039 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3040 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3041 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3042 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3043 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3044 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3045 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3046 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3047 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3048 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3049 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3050 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3051 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3052 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3053 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3054 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3055 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3056 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3057 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3058 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3059 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3060 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3061 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3062 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3063 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3064 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3065 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3066 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3067 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3068 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3069 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3070 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3071 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3072 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3073 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3074 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3075 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3076 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3077 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3078 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3079 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3080 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3081 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3082 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3083 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3084 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3085 [D loss: 0.999968] [G loss: 1.000068]\n",
            "3086 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3087 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3088 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3089 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3090 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3091 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3092 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3093 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3094 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3095 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3096 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3097 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3098 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3099 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3100 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3101 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3102 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3103 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3104 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3105 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3106 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3107 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3108 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3109 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3110 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3111 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3112 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3113 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3114 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3115 [D loss: 0.999973] [G loss: 1.000063]\n",
            "3116 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3117 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3118 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3119 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3120 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3121 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3122 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3123 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3124 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3125 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3126 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3127 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3128 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3129 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3130 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3131 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3132 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3133 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3134 [D loss: 0.999973] [G loss: 1.000063]\n",
            "3135 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3136 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3137 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3138 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3139 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3140 [D loss: 0.999972] [G loss: 1.000068]\n",
            "3141 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3142 [D loss: 0.999967] [G loss: 1.000067]\n",
            "3143 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3144 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3145 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3146 [D loss: 0.999973] [G loss: 1.000065]\n",
            "3147 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3148 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3149 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3150 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3151 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3152 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3153 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3154 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3155 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3156 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3157 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3158 [D loss: 0.999972] [G loss: 1.000068]\n",
            "3159 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3160 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3161 [D loss: 0.999968] [G loss: 1.000069]\n",
            "3162 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3163 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3164 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3165 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3166 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3167 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3168 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3169 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3170 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3171 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3172 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3173 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3174 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3175 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3176 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3177 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3178 [D loss: 0.999966] [G loss: 1.000066]\n",
            "3179 [D loss: 0.999967] [G loss: 1.000064]\n",
            "3180 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3181 [D loss: 0.999973] [G loss: 1.000064]\n",
            "3182 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3183 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3184 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3185 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3186 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3187 [D loss: 0.999972] [G loss: 1.000068]\n",
            "3188 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3189 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3190 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3191 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3192 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3193 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3194 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3195 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3196 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3197 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3198 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3199 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3200 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3201 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3202 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3203 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3204 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3205 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3206 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3207 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3208 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3209 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3210 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3211 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3212 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3213 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3214 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3215 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3216 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3217 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3218 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3219 [D loss: 0.999968] [G loss: 1.000068]\n",
            "3220 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3221 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3222 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3223 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3224 [D loss: 0.999967] [G loss: 1.000066]\n",
            "3225 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3226 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3227 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3228 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3229 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3230 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3231 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3232 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3233 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3234 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3235 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3236 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3237 [D loss: 0.999973] [G loss: 1.000063]\n",
            "3238 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3239 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3240 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3241 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3242 [D loss: 0.999973] [G loss: 1.000064]\n",
            "3243 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3244 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3245 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3246 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3247 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3248 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3249 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3250 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3251 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3252 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3253 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3254 [D loss: 0.999970] [G loss: 1.000060]\n",
            "3255 [D loss: 0.999971] [G loss: 1.000069]\n",
            "3256 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3257 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3258 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3259 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3260 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3261 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3262 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3263 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3264 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3265 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3266 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3267 [D loss: 0.999972] [G loss: 1.000069]\n",
            "3268 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3269 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3270 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3271 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3272 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3273 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3274 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3275 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3276 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3277 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3278 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3279 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3280 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3281 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3282 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3283 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3284 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3285 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3286 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3287 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3288 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3289 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3290 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3291 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3292 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3293 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3294 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3295 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3296 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3297 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3298 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3299 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3300 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3301 [D loss: 0.999973] [G loss: 1.000063]\n",
            "3302 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3303 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3304 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3305 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3306 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3307 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3308 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3309 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3310 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3311 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3312 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3313 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3314 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3315 [D loss: 0.999971] [G loss: 1.000069]\n",
            "3316 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3317 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3318 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3319 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3320 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3321 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3322 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3323 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3324 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3325 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3326 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3327 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3328 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3329 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3330 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3331 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3332 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3333 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3334 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3335 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3336 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3337 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3338 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3339 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3340 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3341 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3342 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3343 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3344 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3345 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3346 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3347 [D loss: 0.999973] [G loss: 1.000065]\n",
            "3348 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3349 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3350 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3351 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3352 [D loss: 0.999972] [G loss: 1.000059]\n",
            "3353 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3354 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3355 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3356 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3357 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3358 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3359 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3360 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3361 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3362 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3363 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3364 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3365 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3366 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3367 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3368 [D loss: 0.999967] [G loss: 1.000066]\n",
            "3369 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3370 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3371 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3372 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3373 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3374 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3375 [D loss: 0.999972] [G loss: 1.000068]\n",
            "3376 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3377 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3378 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3379 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3380 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3381 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3382 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3383 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3384 [D loss: 0.999971] [G loss: 1.000069]\n",
            "3385 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3386 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3387 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3388 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3389 [D loss: 0.999974] [G loss: 1.000062]\n",
            "3390 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3391 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3392 [D loss: 0.999971] [G loss: 1.000059]\n",
            "3393 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3394 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3395 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3396 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3397 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3398 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3399 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3400 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3401 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3402 [D loss: 0.999973] [G loss: 1.000061]\n",
            "3403 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3404 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3405 [D loss: 0.999966] [G loss: 1.000066]\n",
            "3406 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3407 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3408 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3409 [D loss: 0.999973] [G loss: 1.000064]\n",
            "3410 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3411 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3412 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3413 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3414 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3415 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3416 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3417 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3418 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3419 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3420 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3421 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3422 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3423 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3424 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3425 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3426 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3427 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3428 [D loss: 0.999972] [G loss: 1.000068]\n",
            "3429 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3430 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3431 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3432 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3433 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3434 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3435 [D loss: 0.999970] [G loss: 1.000060]\n",
            "3436 [D loss: 0.999973] [G loss: 1.000067]\n",
            "3437 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3438 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3439 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3440 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3441 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3442 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3443 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3444 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3445 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3446 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3447 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3448 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3449 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3450 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3451 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3452 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3453 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3454 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3455 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3456 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3457 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3458 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3459 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3460 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3461 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3462 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3463 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3464 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3465 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3466 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3467 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3468 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3469 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3470 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3471 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3472 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3473 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3474 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3475 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3476 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3477 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3478 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3479 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3480 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3481 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3482 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3483 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3484 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3485 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3486 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3487 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3488 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3489 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3490 [D loss: 0.999968] [G loss: 1.000063]\n",
            "3491 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3492 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3493 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3494 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3495 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3496 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3497 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3498 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3499 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3500 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3501 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3502 [D loss: 0.999973] [G loss: 1.000065]\n",
            "3503 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3504 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3505 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3506 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3507 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3508 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3509 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3510 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3511 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3512 [D loss: 0.999972] [G loss: 1.000069]\n",
            "3513 [D loss: 0.999972] [G loss: 1.000059]\n",
            "3514 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3515 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3516 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3517 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3518 [D loss: 0.999968] [G loss: 1.000064]\n",
            "3519 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3520 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3521 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3522 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3523 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3524 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3525 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3526 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3527 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3528 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3529 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3530 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3531 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3532 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3533 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3534 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3535 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3536 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3537 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3538 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3539 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3541 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3542 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3543 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3544 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3545 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3546 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3547 [D loss: 0.999973] [G loss: 1.000061]\n",
            "3548 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3549 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3550 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3551 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3552 [D loss: 0.999970] [G loss: 1.000069]\n",
            "3553 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3554 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3555 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3556 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3557 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3558 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3559 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3560 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3561 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3562 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3563 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3564 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3565 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3566 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3567 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3568 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3569 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3570 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3571 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3572 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3573 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3574 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3575 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3576 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3577 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3578 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3579 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3580 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3581 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3582 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3583 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3584 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3585 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3586 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3587 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3588 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3589 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3590 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3591 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3592 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3593 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3594 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3595 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3596 [D loss: 0.999973] [G loss: 1.000063]\n",
            "3597 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3598 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3599 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3600 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3601 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3602 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3603 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3604 [D loss: 0.999968] [G loss: 1.000064]\n",
            "3605 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3606 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3607 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3608 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3609 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3610 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3611 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3612 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3613 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3614 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3615 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3616 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3617 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3618 [D loss: 0.999971] [G loss: 1.000069]\n",
            "3619 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3620 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3621 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3622 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3623 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3624 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3625 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3626 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3627 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3628 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3629 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3630 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3631 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3632 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3633 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3634 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3635 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3636 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3637 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3638 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3639 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3640 [D loss: 0.999971] [G loss: 1.000069]\n",
            "3641 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3642 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3643 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3644 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3645 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3646 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3647 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3648 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3649 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3650 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3651 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3652 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3653 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3654 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3655 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3656 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3657 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3658 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3659 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3660 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3661 [D loss: 0.999973] [G loss: 1.000063]\n",
            "3662 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3663 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3664 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3665 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3666 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3667 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3668 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3669 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3670 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3671 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3672 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3673 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3674 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3675 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3676 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3677 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3678 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3679 [D loss: 0.999968] [G loss: 1.000068]\n",
            "3680 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3681 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3682 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3683 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3684 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3685 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3686 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3687 [D loss: 0.999968] [G loss: 1.000064]\n",
            "3688 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3689 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3690 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3691 [D loss: 0.999969] [G loss: 1.000060]\n",
            "3692 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3693 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3694 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3695 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3696 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3697 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3698 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3699 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3700 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3701 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3702 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3703 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3704 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3705 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3706 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3707 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3708 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3709 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3710 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3711 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3712 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3713 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3714 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3715 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3716 [D loss: 0.999973] [G loss: 1.000064]\n",
            "3717 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3718 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3719 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3720 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3721 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3722 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3723 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3724 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3725 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3726 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3727 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3728 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3729 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3730 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3731 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3732 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3733 [D loss: 0.999972] [G loss: 1.000060]\n",
            "3734 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3735 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3736 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3737 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3738 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3739 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3740 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3741 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3742 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3743 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3744 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3745 [D loss: 0.999973] [G loss: 1.000065]\n",
            "3746 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3747 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3748 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3749 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3750 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3751 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3752 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3753 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3754 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3755 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3756 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3757 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3758 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3759 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3760 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3761 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3762 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3763 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3764 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3765 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3766 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3767 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3768 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3769 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3770 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3771 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3772 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3773 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3774 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3775 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3776 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3777 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3778 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3779 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3780 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3781 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3782 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3783 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3784 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3785 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3786 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3787 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3788 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3789 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3790 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3791 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3792 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3793 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3794 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3795 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3796 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3797 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3798 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3799 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3800 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3801 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3802 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3803 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3804 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3805 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3806 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3807 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3808 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3809 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3810 [D loss: 0.999973] [G loss: 1.000065]\n",
            "3811 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3812 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3813 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3814 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3815 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3816 [D loss: 0.999973] [G loss: 1.000067]\n",
            "3817 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3818 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3819 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3820 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3821 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3822 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3823 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3824 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3825 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3826 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3827 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3828 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3829 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3830 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3831 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3832 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3833 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3834 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3835 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3836 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3837 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3838 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3839 [D loss: 0.999971] [G loss: 1.000070]\n",
            "3840 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3841 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3842 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3843 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3844 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3845 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3846 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3847 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3848 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3849 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3850 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3851 [D loss: 0.999973] [G loss: 1.000064]\n",
            "3852 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3853 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3854 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3855 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3856 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3857 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3858 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3859 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3860 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3861 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3862 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3863 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3864 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3865 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3866 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3867 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3868 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3869 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3870 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3871 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3872 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3873 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3874 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3875 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3876 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3877 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3878 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3879 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3880 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3881 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3882 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3883 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3884 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3885 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3886 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3887 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3888 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3889 [D loss: 0.999969] [G loss: 1.000060]\n",
            "3890 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3891 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3892 [D loss: 0.999972] [G loss: 1.000061]\n",
            "3893 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3894 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3895 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3896 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3897 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3898 [D loss: 0.999968] [G loss: 1.000063]\n",
            "3899 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3900 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3901 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3902 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3903 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3904 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3905 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3906 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3907 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3908 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3909 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3910 [D loss: 0.999972] [G loss: 1.000061]\n",
            "3911 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3912 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3913 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3914 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3915 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3916 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3917 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3918 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3919 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3920 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3921 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3922 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3923 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3924 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3925 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3926 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3927 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3928 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3929 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3930 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3931 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3932 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3933 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3934 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3935 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3936 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3937 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3938 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3939 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3940 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3941 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3942 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3943 [D loss: 0.999973] [G loss: 1.000067]\n",
            "3944 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3945 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3946 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3947 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3948 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3949 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3950 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3951 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3952 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3953 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3954 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3955 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3956 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3957 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3958 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3959 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3960 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3961 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3962 [D loss: 0.999973] [G loss: 1.000069]\n",
            "3963 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3964 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3965 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3966 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3967 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3968 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3969 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3970 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3971 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3972 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3973 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3974 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3975 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3976 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3977 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3978 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3979 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3980 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3981 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3982 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3983 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3984 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3985 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3986 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3987 [D loss: 0.999970] [G loss: 1.000068]\n",
            "3988 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3989 [D loss: 0.999973] [G loss: 1.000066]\n",
            "3990 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3991 [D loss: 0.999972] [G loss: 1.000067]\n",
            "3992 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3993 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3994 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3995 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3996 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3997 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3998 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3999 [D loss: 0.999971] [G loss: 1.000066]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnh1CQpGKk-t"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 7\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        # noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,))\n",
        "        # img = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        zn = Dense(512, activation='relu')(zn)\n",
        "        zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        # (_, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Configure input\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\n",
        "        y_train = y.to_numpy().reshape(-1, 1)\n",
        "        # y_train = y.reshape(-1, 1)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 7, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    # def sample_images(self, epoch):\n",
        "    #     r, c = 1, 7\n",
        "    #     noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    #     sampled_labels = np.arange(0, 7).reshape(-1, 1)\n",
        "\n",
        "    #     gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "    #     print(gen_imgs.shape)\n",
        "\n",
        "    #     # Rescale images 0 - 1\n",
        "    #     gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    #     fig, axs = plt.subplots(r, c)\n",
        "    #     cnt = 0\n",
        "    #     for i in range(r):\n",
        "    #         print(i)\n",
        "    #         for j in range(c):\n",
        "    #             print(j)\n",
        "    #             axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "    #             axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
        "    #             axs[i,j].axis('off')\n",
        "    #             cnt += 1\n",
        "    #     fig.savefig(\"images/%d.png\" % epoch)\n",
        "    #     plt.close()\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 7\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 7).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"label: %d\" % sampled_labels[cnt])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images_3/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKyOoPom5Zz",
        "outputId": "c8d3af2f-3729-4904-f9a0-9cb8c6d9a8f8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 48, 48, 128)  3328        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 100)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          51712       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 2304)      16128       input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1152)         0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2304)         0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_15[0][0]                 \n",
            "                                                                 flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            3969        concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.775621, acc.: 20.31%] [G loss: 1.527493]\n",
            "20 [D loss: 1.136945, acc.: 31.64%] [G loss: 1.106470]\n",
            "40 [D loss: 0.436015, acc.: 49.61%] [G loss: 17.784233]\n",
            "60 [D loss: 0.537491, acc.: 73.83%] [G loss: 4.473651]\n",
            "80 [D loss: 0.834621, acc.: 22.66%] [G loss: 1.869293]\n",
            "100 [D loss: 0.741704, acc.: 39.45%] [G loss: 1.638891]\n",
            "120 [D loss: 0.575083, acc.: 82.81%] [G loss: 2.042169]\n",
            "140 [D loss: 0.455398, acc.: 78.12%] [G loss: 3.064530]\n",
            "160 [D loss: 1.021226, acc.: 2.73%] [G loss: 1.166114]\n",
            "180 [D loss: 0.558926, acc.: 76.17%] [G loss: 2.472870]\n",
            "200 [D loss: 0.770760, acc.: 41.80%] [G loss: 1.451331]\n",
            "220 [D loss: 0.876846, acc.: 49.22%] [G loss: 3.595408]\n",
            "240 [D loss: 0.186598, acc.: 100.00%] [G loss: 7.332498]\n",
            "260 [D loss: 0.682056, acc.: 68.75%] [G loss: 1.890666]\n",
            "280 [D loss: 0.209264, acc.: 95.31%] [G loss: 10.532845]\n",
            "300 [D loss: 0.691736, acc.: 60.55%] [G loss: 1.672787]\n",
            "320 [D loss: 0.672510, acc.: 71.48%] [G loss: 1.888943]\n",
            "340 [D loss: 0.385103, acc.: 89.06%] [G loss: 4.332753]\n",
            "360 [D loss: 0.380074, acc.: 84.77%] [G loss: 4.375402]\n",
            "380 [D loss: 0.762896, acc.: 57.81%] [G loss: 1.722982]\n",
            "400 [D loss: 0.730825, acc.: 46.09%] [G loss: 2.234101]\n",
            "420 [D loss: 0.644904, acc.: 57.81%] [G loss: 2.380552]\n",
            "440 [D loss: 0.686306, acc.: 59.77%] [G loss: 2.222342]\n",
            "460 [D loss: 0.746045, acc.: 54.69%] [G loss: 1.957719]\n",
            "480 [D loss: 0.601959, acc.: 67.58%] [G loss: 2.125723]\n",
            "500 [D loss: 0.650282, acc.: 62.11%] [G loss: 2.268615]\n",
            "520 [D loss: 0.609233, acc.: 67.58%] [G loss: 2.602067]\n",
            "540 [D loss: 0.572285, acc.: 69.53%] [G loss: 2.928072]\n",
            "560 [D loss: 0.618016, acc.: 66.41%] [G loss: 2.138952]\n",
            "580 [D loss: 0.671801, acc.: 58.20%] [G loss: 1.966664]\n",
            "600 [D loss: 0.712011, acc.: 48.83%] [G loss: 1.836228]\n",
            "620 [D loss: 0.744234, acc.: 47.27%] [G loss: 1.852621]\n",
            "640 [D loss: 0.625981, acc.: 65.23%] [G loss: 2.130571]\n",
            "660 [D loss: 0.605573, acc.: 68.75%] [G loss: 2.238631]\n",
            "680 [D loss: 0.745590, acc.: 51.17%] [G loss: 1.863794]\n",
            "700 [D loss: 0.541753, acc.: 73.05%] [G loss: 2.539672]\n",
            "720 [D loss: 0.546152, acc.: 75.78%] [G loss: 2.414199]\n",
            "740 [D loss: 0.617598, acc.: 65.62%] [G loss: 2.347521]\n",
            "760 [D loss: 0.445802, acc.: 83.98%] [G loss: 3.243727]\n",
            "780 [D loss: 0.495237, acc.: 78.12%] [G loss: 2.732445]\n",
            "800 [D loss: 0.560501, acc.: 75.39%] [G loss: 2.522155]\n",
            "820 [D loss: 0.555277, acc.: 67.97%] [G loss: 2.861588]\n",
            "840 [D loss: 0.536461, acc.: 76.95%] [G loss: 2.788723]\n",
            "860 [D loss: 0.578135, acc.: 70.70%] [G loss: 3.652322]\n",
            "880 [D loss: 0.623166, acc.: 65.23%] [G loss: 2.995444]\n",
            "900 [D loss: 0.660175, acc.: 62.89%] [G loss: 2.713311]\n",
            "920 [D loss: 0.448838, acc.: 81.25%] [G loss: 3.552639]\n",
            "940 [D loss: 0.507613, acc.: 76.56%] [G loss: 3.297152]\n",
            "960 [D loss: 0.538144, acc.: 73.05%] [G loss: 3.129484]\n",
            "980 [D loss: 0.657594, acc.: 61.72%] [G loss: 3.054131]\n",
            "1000 [D loss: 0.435557, acc.: 84.77%] [G loss: 3.602971]\n",
            "1020 [D loss: 0.389269, acc.: 85.16%] [G loss: 4.215716]\n",
            "1040 [D loss: 0.441645, acc.: 80.47%] [G loss: 3.759854]\n",
            "1060 [D loss: 0.417450, acc.: 81.64%] [G loss: 3.896385]\n",
            "1080 [D loss: 0.648562, acc.: 64.84%] [G loss: 3.626264]\n",
            "1100 [D loss: 0.402189, acc.: 83.98%] [G loss: 3.678422]\n",
            "1120 [D loss: 0.418486, acc.: 82.81%] [G loss: 3.632628]\n",
            "1140 [D loss: 0.467013, acc.: 76.95%] [G loss: 3.824034]\n",
            "1160 [D loss: 0.570863, acc.: 70.31%] [G loss: 3.044311]\n",
            "1180 [D loss: 0.511243, acc.: 76.95%] [G loss: 3.353204]\n",
            "1200 [D loss: 0.478253, acc.: 78.12%] [G loss: 3.580424]\n",
            "1220 [D loss: 0.470860, acc.: 77.73%] [G loss: 3.401241]\n",
            "1240 [D loss: 0.509738, acc.: 75.00%] [G loss: 3.294276]\n",
            "1260 [D loss: 0.510494, acc.: 78.91%] [G loss: 3.284930]\n",
            "1280 [D loss: 0.589470, acc.: 67.19%] [G loss: 3.138620]\n",
            "1300 [D loss: 0.555125, acc.: 73.05%] [G loss: 2.883266]\n",
            "1320 [D loss: 0.479775, acc.: 79.30%] [G loss: 3.127534]\n",
            "1340 [D loss: 0.498140, acc.: 75.78%] [G loss: 3.167541]\n",
            "1360 [D loss: 0.530504, acc.: 76.56%] [G loss: 2.911742]\n",
            "1380 [D loss: 0.482003, acc.: 77.34%] [G loss: 3.266805]\n",
            "1400 [D loss: 0.470014, acc.: 80.47%] [G loss: 3.325715]\n",
            "1420 [D loss: 0.546466, acc.: 73.44%] [G loss: 3.093244]\n",
            "1440 [D loss: 0.548517, acc.: 73.83%] [G loss: 3.152225]\n",
            "1460 [D loss: 0.533497, acc.: 71.88%] [G loss: 3.297659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1-QlEvelsO",
        "outputId": "d5e6db9b-1b83-4118-fa9f-1173864d140b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      16128       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_73 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_33 (Flatten)            (None, 2304)         0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_32 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4708)         0           input_73[0][0]                   \n",
            "                                                                 flatten_33[0][0]                 \n",
            "                                                                 flatten_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 1024)         4822016     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, 1024)         0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 1024)         0           leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1024)         1049600     dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1024)         0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 1024)         0           leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 1024)         1049600     dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1024)         0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 1024)         0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 1)            1025        dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.849750, acc.: 30.47%] [G loss: 5.367685]\n",
            "20 [D loss: 0.341891, acc.: 91.41%] [G loss: 8.554846]\n",
            "40 [D loss: 0.710111, acc.: 44.14%] [G loss: 2.083147]\n",
            "60 [D loss: 0.715512, acc.: 46.48%] [G loss: 1.738583]\n",
            "80 [D loss: 0.648917, acc.: 56.25%] [G loss: 1.863081]\n",
            "100 [D loss: 0.701115, acc.: 44.92%] [G loss: 1.767049]\n",
            "120 [D loss: 0.732758, acc.: 40.23%] [G loss: 1.580268]\n",
            "140 [D loss: 0.706723, acc.: 48.05%] [G loss: 1.537512]\n",
            "160 [D loss: 0.717456, acc.: 40.23%] [G loss: 1.437199]\n",
            "180 [D loss: 0.710523, acc.: 42.97%] [G loss: 1.439812]\n",
            "200 [D loss: 0.711228, acc.: 45.70%] [G loss: 1.451416]\n",
            "220 [D loss: 0.721137, acc.: 35.55%] [G loss: 1.414149]\n",
            "240 [D loss: 0.714687, acc.: 42.58%] [G loss: 1.432792]\n",
            "260 [D loss: 0.703062, acc.: 46.09%] [G loss: 1.423483]\n",
            "280 [D loss: 0.724828, acc.: 36.72%] [G loss: 1.404998]\n",
            "300 [D loss: 0.716795, acc.: 42.58%] [G loss: 1.396966]\n",
            "320 [D loss: 0.720376, acc.: 42.97%] [G loss: 1.444355]\n",
            "340 [D loss: 0.706069, acc.: 46.88%] [G loss: 1.423882]\n",
            "360 [D loss: 0.709865, acc.: 39.06%] [G loss: 1.404603]\n",
            "380 [D loss: 0.714929, acc.: 43.75%] [G loss: 1.404289]\n",
            "400 [D loss: 0.711042, acc.: 42.97%] [G loss: 1.414505]\n",
            "420 [D loss: 0.717568, acc.: 40.23%] [G loss: 1.403865]\n",
            "440 [D loss: 0.711510, acc.: 40.23%] [G loss: 1.410104]\n",
            "460 [D loss: 0.714272, acc.: 41.41%] [G loss: 1.419285]\n",
            "480 [D loss: 0.708251, acc.: 41.02%] [G loss: 1.413074]\n",
            "500 [D loss: 0.706564, acc.: 40.62%] [G loss: 1.414759]\n",
            "520 [D loss: 0.711570, acc.: 39.84%] [G loss: 1.416901]\n",
            "540 [D loss: 0.707966, acc.: 44.14%] [G loss: 1.396562]\n",
            "560 [D loss: 0.714514, acc.: 40.62%] [G loss: 1.400588]\n",
            "580 [D loss: 0.708786, acc.: 39.06%] [G loss: 1.401240]\n",
            "600 [D loss: 0.715925, acc.: 37.50%] [G loss: 1.401176]\n",
            "620 [D loss: 0.700743, acc.: 42.19%] [G loss: 1.417309]\n",
            "640 [D loss: 0.706829, acc.: 40.23%] [G loss: 1.403668]\n",
            "660 [D loss: 0.707380, acc.: 39.84%] [G loss: 1.389831]\n",
            "680 [D loss: 0.707225, acc.: 39.84%] [G loss: 1.398783]\n",
            "700 [D loss: 0.703080, acc.: 40.62%] [G loss: 1.396986]\n",
            "720 [D loss: 0.718166, acc.: 34.38%] [G loss: 1.389264]\n",
            "740 [D loss: 0.713579, acc.: 40.23%] [G loss: 1.403226]\n",
            "760 [D loss: 0.705897, acc.: 45.31%] [G loss: 1.437489]\n",
            "780 [D loss: 0.707634, acc.: 40.62%] [G loss: 1.401700]\n",
            "800 [D loss: 0.701959, acc.: 39.06%] [G loss: 1.398977]\n",
            "820 [D loss: 0.706529, acc.: 42.19%] [G loss: 1.400484]\n",
            "840 [D loss: 0.700337, acc.: 41.02%] [G loss: 1.400969]\n",
            "860 [D loss: 0.706927, acc.: 38.67%] [G loss: 1.405506]\n",
            "880 [D loss: 0.708806, acc.: 38.28%] [G loss: 1.387069]\n",
            "900 [D loss: 0.706434, acc.: 41.41%] [G loss: 1.398114]\n",
            "920 [D loss: 0.704907, acc.: 35.94%] [G loss: 1.406425]\n",
            "940 [D loss: 0.696471, acc.: 47.66%] [G loss: 1.392628]\n",
            "960 [D loss: 0.705585, acc.: 41.02%] [G loss: 1.396953]\n",
            "980 [D loss: 0.705313, acc.: 39.84%] [G loss: 1.393413]\n",
            "1000 [D loss: 0.693701, acc.: 48.44%] [G loss: 1.410988]\n",
            "1020 [D loss: 0.707825, acc.: 38.28%] [G loss: 1.393575]\n",
            "1040 [D loss: 0.700687, acc.: 44.53%] [G loss: 1.400779]\n",
            "1060 [D loss: 0.703935, acc.: 39.84%] [G loss: 1.387593]\n",
            "1080 [D loss: 0.706629, acc.: 39.84%] [G loss: 1.389481]\n",
            "1100 [D loss: 0.707315, acc.: 37.89%] [G loss: 1.378371]\n",
            "1120 [D loss: 0.707380, acc.: 42.97%] [G loss: 1.383276]\n",
            "1140 [D loss: 0.701792, acc.: 45.31%] [G loss: 1.392983]\n",
            "1160 [D loss: 0.698018, acc.: 41.41%] [G loss: 1.396971]\n",
            "1180 [D loss: 0.710089, acc.: 35.94%] [G loss: 1.389826]\n",
            "1200 [D loss: 0.704007, acc.: 40.23%] [G loss: 1.392146]\n",
            "1220 [D loss: 0.704774, acc.: 42.97%] [G loss: 1.406959]\n",
            "1240 [D loss: 0.699959, acc.: 41.41%] [G loss: 1.412235]\n",
            "1260 [D loss: 0.702670, acc.: 44.53%] [G loss: 1.408520]\n",
            "1280 [D loss: 0.704078, acc.: 41.41%] [G loss: 1.384988]\n",
            "1300 [D loss: 0.707281, acc.: 40.62%] [G loss: 1.399330]\n",
            "1320 [D loss: 0.709640, acc.: 39.06%] [G loss: 1.400736]\n",
            "1340 [D loss: 0.697176, acc.: 46.88%] [G loss: 1.408559]\n",
            "1360 [D loss: 0.690890, acc.: 46.09%] [G loss: 1.407980]\n",
            "1380 [D loss: 0.695711, acc.: 48.44%] [G loss: 1.384939]\n",
            "1400 [D loss: 0.696284, acc.: 41.41%] [G loss: 1.391898]\n",
            "1420 [D loss: 0.702413, acc.: 41.02%] [G loss: 1.391516]\n",
            "1440 [D loss: 0.702305, acc.: 43.75%] [G loss: 1.404577]\n",
            "1460 [D loss: 0.702467, acc.: 40.62%] [G loss: 1.401769]\n",
            "1480 [D loss: 0.700135, acc.: 39.84%] [G loss: 1.391395]\n",
            "1500 [D loss: 0.701792, acc.: 41.80%] [G loss: 1.390833]\n",
            "1520 [D loss: 0.700585, acc.: 42.19%] [G loss: 1.394488]\n",
            "1540 [D loss: 0.704487, acc.: 41.41%] [G loss: 1.397046]\n",
            "1560 [D loss: 0.695279, acc.: 46.48%] [G loss: 1.408600]\n",
            "1580 [D loss: 0.695438, acc.: 48.83%] [G loss: 1.392757]\n",
            "1600 [D loss: 0.704662, acc.: 38.67%] [G loss: 1.390574]\n",
            "1620 [D loss: 0.699533, acc.: 46.09%] [G loss: 1.401004]\n",
            "1640 [D loss: 0.702735, acc.: 43.75%] [G loss: 1.398700]\n",
            "1660 [D loss: 0.697723, acc.: 43.75%] [G loss: 1.401226]\n",
            "1680 [D loss: 0.697773, acc.: 41.80%] [G loss: 1.410201]\n",
            "1700 [D loss: 0.696773, acc.: 43.36%] [G loss: 1.407255]\n",
            "1720 [D loss: 0.701552, acc.: 39.45%] [G loss: 1.395131]\n",
            "1740 [D loss: 0.696195, acc.: 48.44%] [G loss: 1.396674]\n",
            "1760 [D loss: 0.702511, acc.: 43.36%] [G loss: 1.383245]\n",
            "1780 [D loss: 0.704975, acc.: 40.62%] [G loss: 1.399527]\n",
            "1800 [D loss: 0.698895, acc.: 41.41%] [G loss: 1.391627]\n",
            "1820 [D loss: 0.702525, acc.: 37.50%] [G loss: 1.391825]\n",
            "1840 [D loss: 0.698295, acc.: 46.48%] [G loss: 1.401908]\n",
            "1860 [D loss: 0.699764, acc.: 42.97%] [G loss: 1.399934]\n",
            "1880 [D loss: 0.702305, acc.: 41.80%] [G loss: 1.396282]\n",
            "1900 [D loss: 0.705898, acc.: 42.19%] [G loss: 1.384895]\n",
            "1920 [D loss: 0.700531, acc.: 44.92%] [G loss: 1.391573]\n",
            "1940 [D loss: 0.707957, acc.: 41.02%] [G loss: 1.386891]\n",
            "1960 [D loss: 0.700286, acc.: 41.41%] [G loss: 1.396966]\n",
            "1980 [D loss: 0.697184, acc.: 42.19%] [G loss: 1.398432]\n",
            "2000 [D loss: 0.701151, acc.: 46.09%] [G loss: 1.408123]\n",
            "2020 [D loss: 0.707549, acc.: 41.80%] [G loss: 1.400710]\n",
            "2040 [D loss: 0.706370, acc.: 41.80%] [G loss: 1.382454]\n",
            "2060 [D loss: 0.696820, acc.: 44.14%] [G loss: 1.416820]\n",
            "2080 [D loss: 0.704532, acc.: 39.84%] [G loss: 1.388172]\n",
            "2100 [D loss: 0.698797, acc.: 41.80%] [G loss: 1.401481]\n",
            "2120 [D loss: 0.701321, acc.: 42.58%] [G loss: 1.405037]\n",
            "2140 [D loss: 0.700277, acc.: 42.58%] [G loss: 1.388465]\n",
            "2160 [D loss: 0.706183, acc.: 34.77%] [G loss: 1.395349]\n",
            "2180 [D loss: 0.700379, acc.: 44.53%] [G loss: 1.399946]\n",
            "2200 [D loss: 0.706776, acc.: 35.55%] [G loss: 1.397158]\n",
            "2220 [D loss: 0.690010, acc.: 49.61%] [G loss: 1.415059]\n",
            "2240 [D loss: 0.700388, acc.: 45.31%] [G loss: 1.396531]\n",
            "2260 [D loss: 0.698710, acc.: 41.41%] [G loss: 1.387498]\n",
            "2280 [D loss: 0.699097, acc.: 44.53%] [G loss: 1.400677]\n",
            "2300 [D loss: 0.698183, acc.: 41.41%] [G loss: 1.393741]\n",
            "2320 [D loss: 0.700072, acc.: 44.92%] [G loss: 1.387109]\n",
            "2340 [D loss: 0.702045, acc.: 43.75%] [G loss: 1.396456]\n",
            "2360 [D loss: 0.698545, acc.: 42.97%] [G loss: 1.386749]\n",
            "2380 [D loss: 0.701191, acc.: 44.53%] [G loss: 1.390651]\n",
            "2400 [D loss: 0.702897, acc.: 41.02%] [G loss: 1.385156]\n",
            "2420 [D loss: 0.705316, acc.: 41.02%] [G loss: 1.401502]\n",
            "2440 [D loss: 0.706111, acc.: 37.11%] [G loss: 1.388617]\n",
            "2460 [D loss: 0.702543, acc.: 41.02%] [G loss: 1.407373]\n",
            "2480 [D loss: 0.705135, acc.: 44.92%] [G loss: 1.386979]\n",
            "2500 [D loss: 0.699529, acc.: 43.75%] [G loss: 1.433604]\n",
            "2520 [D loss: 0.701719, acc.: 43.36%] [G loss: 1.395393]\n",
            "2540 [D loss: 0.699508, acc.: 42.58%] [G loss: 1.396002]\n",
            "2560 [D loss: 0.703406, acc.: 38.67%] [G loss: 1.390184]\n",
            "2580 [D loss: 0.702974, acc.: 38.67%] [G loss: 1.384311]\n",
            "2600 [D loss: 0.697291, acc.: 44.92%] [G loss: 1.391137]\n",
            "2620 [D loss: 0.698857, acc.: 46.48%] [G loss: 1.397241]\n",
            "2640 [D loss: 0.702192, acc.: 41.02%] [G loss: 1.391943]\n",
            "2660 [D loss: 0.703083, acc.: 35.94%] [G loss: 1.392532]\n",
            "2680 [D loss: 0.704199, acc.: 41.80%] [G loss: 1.383410]\n",
            "2700 [D loss: 0.705399, acc.: 40.62%] [G loss: 1.386579]\n",
            "2720 [D loss: 0.702500, acc.: 42.19%] [G loss: 1.398633]\n",
            "2740 [D loss: 0.701238, acc.: 41.41%] [G loss: 1.387710]\n",
            "2760 [D loss: 0.695434, acc.: 47.66%] [G loss: 1.398785]\n",
            "2780 [D loss: 0.698381, acc.: 46.48%] [G loss: 1.409770]\n",
            "2800 [D loss: 0.703224, acc.: 41.02%] [G loss: 1.398382]\n",
            "2820 [D loss: 0.704637, acc.: 42.97%] [G loss: 1.391324]\n",
            "2840 [D loss: 0.695273, acc.: 46.48%] [G loss: 1.403218]\n",
            "2860 [D loss: 0.703234, acc.: 45.31%] [G loss: 1.395308]\n",
            "2880 [D loss: 0.699889, acc.: 41.80%] [G loss: 1.389093]\n",
            "2900 [D loss: 0.704404, acc.: 42.19%] [G loss: 1.391987]\n",
            "2920 [D loss: 0.700413, acc.: 43.36%] [G loss: 1.394257]\n",
            "2940 [D loss: 0.701555, acc.: 45.31%] [G loss: 1.386916]\n",
            "2960 [D loss: 0.704574, acc.: 41.02%] [G loss: 1.397935]\n",
            "2980 [D loss: 0.701216, acc.: 38.28%] [G loss: 1.397118]\n",
            "3000 [D loss: 0.703859, acc.: 43.75%] [G loss: 1.386564]\n",
            "3020 [D loss: 0.697224, acc.: 47.27%] [G loss: 1.404107]\n",
            "3040 [D loss: 0.700284, acc.: 44.14%] [G loss: 1.408972]\n",
            "3060 [D loss: 0.710935, acc.: 39.45%] [G loss: 1.398036]\n",
            "3080 [D loss: 0.701705, acc.: 43.75%] [G loss: 1.385907]\n",
            "3100 [D loss: 0.703351, acc.: 42.19%] [G loss: 1.400969]\n",
            "3120 [D loss: 0.705543, acc.: 35.55%] [G loss: 1.395355]\n",
            "3140 [D loss: 0.705566, acc.: 41.02%] [G loss: 1.389833]\n",
            "3160 [D loss: 0.694014, acc.: 50.78%] [G loss: 1.399073]\n",
            "3180 [D loss: 0.696958, acc.: 47.27%] [G loss: 1.398360]\n",
            "3200 [D loss: 0.697163, acc.: 45.31%] [G loss: 1.396299]\n",
            "3220 [D loss: 0.704172, acc.: 39.45%] [G loss: 1.401600]\n",
            "3240 [D loss: 0.706146, acc.: 35.55%] [G loss: 1.386641]\n",
            "3260 [D loss: 0.693955, acc.: 46.88%] [G loss: 1.392742]\n",
            "3280 [D loss: 0.694101, acc.: 48.83%] [G loss: 1.400743]\n",
            "3300 [D loss: 0.699886, acc.: 42.58%] [G loss: 1.380824]\n",
            "3320 [D loss: 0.709387, acc.: 47.27%] [G loss: 1.398697]\n",
            "3340 [D loss: 0.699752, acc.: 48.44%] [G loss: 1.394596]\n",
            "3360 [D loss: 0.706852, acc.: 40.62%] [G loss: 1.405188]\n",
            "3380 [D loss: 0.699275, acc.: 44.53%] [G loss: 1.398795]\n",
            "3400 [D loss: 0.696006, acc.: 44.92%] [G loss: 1.421854]\n",
            "3420 [D loss: 0.696535, acc.: 48.05%] [G loss: 1.411503]\n",
            "3440 [D loss: 0.702331, acc.: 43.36%] [G loss: 1.399069]\n",
            "3460 [D loss: 0.705177, acc.: 42.19%] [G loss: 1.388886]\n",
            "3480 [D loss: 0.703387, acc.: 36.33%] [G loss: 1.393602]\n",
            "3500 [D loss: 0.701317, acc.: 38.67%] [G loss: 1.405229]\n",
            "3520 [D loss: 0.700779, acc.: 36.33%] [G loss: 1.394291]\n",
            "3540 [D loss: 0.704368, acc.: 43.75%] [G loss: 1.388223]\n",
            "3560 [D loss: 0.698320, acc.: 44.92%] [G loss: 1.408260]\n",
            "3580 [D loss: 0.699920, acc.: 41.80%] [G loss: 1.396066]\n",
            "3600 [D loss: 0.703215, acc.: 42.97%] [G loss: 1.393963]\n",
            "3620 [D loss: 0.704095, acc.: 42.58%] [G loss: 1.389001]\n",
            "3640 [D loss: 0.696503, acc.: 46.48%] [G loss: 1.391987]\n",
            "3660 [D loss: 0.706459, acc.: 39.45%] [G loss: 1.403823]\n",
            "3680 [D loss: 0.701097, acc.: 45.70%] [G loss: 1.391359]\n",
            "3700 [D loss: 0.700797, acc.: 40.62%] [G loss: 1.401428]\n",
            "3720 [D loss: 0.706842, acc.: 39.45%] [G loss: 1.401338]\n",
            "3740 [D loss: 0.695790, acc.: 48.44%] [G loss: 1.410640]\n",
            "3760 [D loss: 0.700871, acc.: 41.02%] [G loss: 1.389013]\n",
            "3780 [D loss: 0.698214, acc.: 45.31%] [G loss: 1.391579]\n",
            "3800 [D loss: 0.702306, acc.: 41.02%] [G loss: 1.390742]\n",
            "3820 [D loss: 0.699549, acc.: 44.92%] [G loss: 1.388456]\n",
            "3840 [D loss: 0.726881, acc.: 41.02%] [G loss: 1.427503]\n",
            "3860 [D loss: 0.706836, acc.: 40.23%] [G loss: 1.408166]\n",
            "3880 [D loss: 0.708650, acc.: 37.89%] [G loss: 1.388743]\n",
            "3900 [D loss: 0.703004, acc.: 42.19%] [G loss: 1.413317]\n",
            "3920 [D loss: 0.701894, acc.: 43.75%] [G loss: 1.391886]\n",
            "3940 [D loss: 0.695446, acc.: 49.22%] [G loss: 1.405801]\n",
            "3960 [D loss: 0.704857, acc.: 41.02%] [G loss: 1.401155]\n",
            "3980 [D loss: 0.699231, acc.: 42.58%] [G loss: 1.404975]\n",
            "4000 [D loss: 0.699956, acc.: 39.84%] [G loss: 1.397395]\n",
            "4020 [D loss: 0.693868, acc.: 48.83%] [G loss: 1.414370]\n",
            "4040 [D loss: 0.699592, acc.: 46.48%] [G loss: 1.402936]\n",
            "4060 [D loss: 0.697715, acc.: 45.31%] [G loss: 1.424391]\n",
            "4080 [D loss: 0.708574, acc.: 40.23%] [G loss: 1.381411]\n",
            "4100 [D loss: 1.072704, acc.: 84.38%] [G loss: 26.749468]\n",
            "4120 [D loss: 0.717382, acc.: 67.97%] [G loss: 6.265586]\n",
            "4140 [D loss: 1.001233, acc.: 50.00%] [G loss: 3.090046]\n",
            "4160 [D loss: 0.768635, acc.: 51.56%] [G loss: 2.125261]\n",
            "4180 [D loss: 0.740759, acc.: 46.48%] [G loss: 1.783273]\n",
            "4200 [D loss: 0.722959, acc.: 47.66%] [G loss: 1.520224]\n",
            "4220 [D loss: 0.734547, acc.: 45.31%] [G loss: 1.466105]\n",
            "4240 [D loss: 0.688544, acc.: 57.03%] [G loss: 1.615793]\n",
            "4260 [D loss: 0.738717, acc.: 43.75%] [G loss: 1.405132]\n",
            "4280 [D loss: 0.722575, acc.: 42.19%] [G loss: 1.447261]\n",
            "4300 [D loss: 0.690146, acc.: 49.61%] [G loss: 1.476141]\n",
            "4320 [D loss: 0.719582, acc.: 43.75%] [G loss: 1.429009]\n",
            "4340 [D loss: 0.720578, acc.: 45.70%] [G loss: 1.434604]\n",
            "4360 [D loss: 0.706739, acc.: 45.70%] [G loss: 1.414366]\n",
            "4380 [D loss: 0.702524, acc.: 48.44%] [G loss: 1.432420]\n",
            "4400 [D loss: 0.710964, acc.: 48.83%] [G loss: 1.441298]\n",
            "4420 [D loss: 0.711855, acc.: 46.48%] [G loss: 1.427701]\n",
            "4440 [D loss: 0.710840, acc.: 47.66%] [G loss: 1.458087]\n",
            "4460 [D loss: 0.705373, acc.: 44.53%] [G loss: 1.425750]\n",
            "4480 [D loss: 0.704472, acc.: 48.83%] [G loss: 1.433243]\n",
            "4500 [D loss: 0.698071, acc.: 51.17%] [G loss: 1.431295]\n",
            "4520 [D loss: 0.716092, acc.: 41.80%] [G loss: 1.414632]\n",
            "4540 [D loss: 0.709937, acc.: 44.53%] [G loss: 1.402517]\n",
            "4560 [D loss: 0.706339, acc.: 43.75%] [G loss: 1.417696]\n",
            "4580 [D loss: 0.701099, acc.: 49.61%] [G loss: 1.398619]\n",
            "4600 [D loss: 0.705118, acc.: 43.75%] [G loss: 1.420572]\n",
            "4620 [D loss: 0.706447, acc.: 43.75%] [G loss: 1.411010]\n",
            "4640 [D loss: 0.701780, acc.: 48.44%] [G loss: 1.398860]\n",
            "4660 [D loss: 0.705210, acc.: 48.05%] [G loss: 1.383090]\n",
            "4680 [D loss: 0.708879, acc.: 46.09%] [G loss: 1.397547]\n",
            "4700 [D loss: 0.710725, acc.: 42.97%] [G loss: 1.417038]\n",
            "4720 [D loss: 0.704634, acc.: 45.70%] [G loss: 1.393745]\n",
            "4740 [D loss: 0.696728, acc.: 51.56%] [G loss: 1.395010]\n",
            "4760 [D loss: 0.702551, acc.: 48.83%] [G loss: 1.404335]\n",
            "4780 [D loss: 0.699379, acc.: 46.48%] [G loss: 1.415344]\n",
            "4800 [D loss: 0.700585, acc.: 49.22%] [G loss: 1.406814]\n",
            "4820 [D loss: 0.698388, acc.: 46.88%] [G loss: 1.390556]\n",
            "4840 [D loss: 0.694551, acc.: 51.56%] [G loss: 1.385368]\n",
            "4860 [D loss: 0.694786, acc.: 53.12%] [G loss: 1.396678]\n",
            "4880 [D loss: 0.703839, acc.: 44.53%] [G loss: 1.405728]\n",
            "4900 [D loss: 0.705474, acc.: 44.53%] [G loss: 1.391244]\n",
            "4920 [D loss: 0.698867, acc.: 46.09%] [G loss: 1.391430]\n",
            "4940 [D loss: 0.705874, acc.: 39.45%] [G loss: 1.395215]\n",
            "4960 [D loss: 0.697893, acc.: 49.61%] [G loss: 1.396267]\n",
            "4980 [D loss: 0.709537, acc.: 39.84%] [G loss: 1.394031]\n",
            "5000 [D loss: 0.704992, acc.: 42.19%] [G loss: 1.393940]\n",
            "5020 [D loss: 0.702913, acc.: 48.05%] [G loss: 1.390844]\n",
            "5040 [D loss: 0.697565, acc.: 48.83%] [G loss: 1.388696]\n",
            "5060 [D loss: 0.700234, acc.: 42.58%] [G loss: 1.404058]\n",
            "5080 [D loss: 0.703479, acc.: 44.92%] [G loss: 1.406952]\n",
            "5100 [D loss: 0.693664, acc.: 49.22%] [G loss: 1.407932]\n",
            "5120 [D loss: 0.706131, acc.: 42.97%] [G loss: 1.404334]\n",
            "5140 [D loss: 0.700269, acc.: 48.44%] [G loss: 1.391555]\n",
            "5160 [D loss: 0.696768, acc.: 45.31%] [G loss: 1.396595]\n",
            "5180 [D loss: 0.699592, acc.: 46.09%] [G loss: 1.397417]\n",
            "5200 [D loss: 0.695383, acc.: 47.27%] [G loss: 1.387535]\n",
            "5220 [D loss: 0.693621, acc.: 47.27%] [G loss: 1.408859]\n",
            "5240 [D loss: 0.695037, acc.: 47.66%] [G loss: 1.403044]\n",
            "5260 [D loss: 0.700985, acc.: 41.41%] [G loss: 1.395171]\n",
            "5280 [D loss: 0.695701, acc.: 47.66%] [G loss: 1.403005]\n",
            "5300 [D loss: 0.700270, acc.: 42.19%] [G loss: 1.394384]\n",
            "5320 [D loss: 0.698988, acc.: 44.92%] [G loss: 1.401334]\n",
            "5340 [D loss: 0.694417, acc.: 53.12%] [G loss: 1.388224]\n",
            "5360 [D loss: 0.696847, acc.: 50.78%] [G loss: 1.402042]\n",
            "5380 [D loss: 0.699440, acc.: 46.48%] [G loss: 1.400354]\n",
            "5400 [D loss: 0.695333, acc.: 46.88%] [G loss: 1.389589]\n",
            "5420 [D loss: 0.693278, acc.: 52.34%] [G loss: 1.395435]\n",
            "5440 [D loss: 0.697515, acc.: 52.73%] [G loss: 1.403365]\n",
            "5460 [D loss: 0.700039, acc.: 44.92%] [G loss: 1.385313]\n",
            "5480 [D loss: 0.701342, acc.: 41.80%] [G loss: 1.400771]\n",
            "5500 [D loss: 0.691950, acc.: 52.73%] [G loss: 1.405167]\n",
            "5520 [D loss: 0.697928, acc.: 47.66%] [G loss: 1.407117]\n",
            "5540 [D loss: 0.694162, acc.: 50.39%] [G loss: 1.412841]\n",
            "5560 [D loss: 0.696665, acc.: 49.22%] [G loss: 1.398718]\n",
            "5580 [D loss: 0.700371, acc.: 45.31%] [G loss: 1.393484]\n",
            "5600 [D loss: 0.702304, acc.: 42.19%] [G loss: 1.393507]\n",
            "5620 [D loss: 0.694285, acc.: 50.39%] [G loss: 1.394578]\n",
            "5640 [D loss: 0.697163, acc.: 46.88%] [G loss: 1.395253]\n",
            "5660 [D loss: 0.693129, acc.: 50.78%] [G loss: 1.392592]\n",
            "5680 [D loss: 0.696335, acc.: 45.31%] [G loss: 1.386523]\n",
            "5700 [D loss: 0.697125, acc.: 50.00%] [G loss: 1.412799]\n",
            "5720 [D loss: 0.697893, acc.: 47.27%] [G loss: 1.399107]\n",
            "5740 [D loss: 0.695130, acc.: 49.22%] [G loss: 1.400753]\n",
            "5760 [D loss: 0.698657, acc.: 44.14%] [G loss: 1.395851]\n",
            "5780 [D loss: 0.697089, acc.: 47.66%] [G loss: 1.392132]\n",
            "5800 [D loss: 0.700125, acc.: 46.09%] [G loss: 1.401990]\n",
            "5820 [D loss: 0.697709, acc.: 47.27%] [G loss: 1.394505]\n",
            "5840 [D loss: 0.693451, acc.: 50.39%] [G loss: 1.400309]\n",
            "5860 [D loss: 0.697274, acc.: 45.31%] [G loss: 1.386672]\n",
            "5880 [D loss: 0.700695, acc.: 44.53%] [G loss: 1.398656]\n",
            "5900 [D loss: 0.697150, acc.: 47.27%] [G loss: 1.390164]\n",
            "5920 [D loss: 0.696779, acc.: 44.14%] [G loss: 1.396117]\n",
            "5940 [D loss: 0.693782, acc.: 50.00%] [G loss: 1.390036]\n",
            "5960 [D loss: 0.700256, acc.: 51.17%] [G loss: 1.398869]\n",
            "5980 [D loss: 0.696114, acc.: 49.22%] [G loss: 1.395630]\n",
            "6000 [D loss: 0.697736, acc.: 45.70%] [G loss: 1.393245]\n",
            "6020 [D loss: 0.699556, acc.: 43.75%] [G loss: 1.396108]\n",
            "6040 [D loss: 0.693481, acc.: 50.39%] [G loss: 1.397763]\n",
            "6060 [D loss: 0.703812, acc.: 44.14%] [G loss: 1.388477]\n",
            "6080 [D loss: 0.697451, acc.: 43.36%] [G loss: 1.389992]\n",
            "6100 [D loss: 0.692298, acc.: 46.88%] [G loss: 1.387400]\n",
            "6120 [D loss: 0.699595, acc.: 45.70%] [G loss: 1.405494]\n",
            "6140 [D loss: 0.697563, acc.: 45.70%] [G loss: 1.396657]\n",
            "6160 [D loss: 0.695493, acc.: 48.83%] [G loss: 1.383792]\n",
            "6180 [D loss: 0.697858, acc.: 43.75%] [G loss: 1.381215]\n",
            "6200 [D loss: 0.695829, acc.: 48.83%] [G loss: 1.407797]\n",
            "6220 [D loss: 0.717301, acc.: 37.11%] [G loss: 1.387314]\n",
            "6240 [D loss: 0.697440, acc.: 46.88%] [G loss: 1.404925]\n",
            "6260 [D loss: 0.693353, acc.: 48.05%] [G loss: 1.386393]\n",
            "6280 [D loss: 0.691351, acc.: 53.52%] [G loss: 1.411113]\n",
            "6300 [D loss: 0.697193, acc.: 48.44%] [G loss: 1.393546]\n",
            "6320 [D loss: 0.693712, acc.: 48.05%] [G loss: 1.386686]\n",
            "6340 [D loss: 0.701024, acc.: 43.75%] [G loss: 1.396646]\n",
            "6360 [D loss: 0.701343, acc.: 46.09%] [G loss: 1.392262]\n",
            "6380 [D loss: 0.696558, acc.: 48.05%] [G loss: 1.396049]\n",
            "6400 [D loss: 0.699096, acc.: 45.31%] [G loss: 1.396192]\n",
            "6420 [D loss: 0.695062, acc.: 48.44%] [G loss: 1.393358]\n",
            "6440 [D loss: 0.699274, acc.: 41.80%] [G loss: 1.398998]\n",
            "6460 [D loss: 0.698745, acc.: 42.97%] [G loss: 1.413985]\n",
            "6480 [D loss: 0.696595, acc.: 49.22%] [G loss: 1.407619]\n",
            "6500 [D loss: 0.699104, acc.: 44.53%] [G loss: 1.392327]\n",
            "6520 [D loss: 0.696238, acc.: 47.66%] [G loss: 1.400739]\n",
            "6540 [D loss: 0.694688, acc.: 49.61%] [G loss: 1.409534]\n",
            "6560 [D loss: 0.700760, acc.: 41.80%] [G loss: 1.394047]\n",
            "6580 [D loss: 0.690933, acc.: 51.56%] [G loss: 1.394747]\n",
            "6600 [D loss: 0.698814, acc.: 42.58%] [G loss: 1.389279]\n",
            "6620 [D loss: 0.700194, acc.: 44.53%] [G loss: 1.398422]\n",
            "6640 [D loss: 0.698942, acc.: 47.27%] [G loss: 1.386726]\n",
            "6660 [D loss: 0.700201, acc.: 48.05%] [G loss: 1.389932]\n",
            "6680 [D loss: 0.697296, acc.: 46.88%] [G loss: 1.393817]\n",
            "6700 [D loss: 0.700214, acc.: 42.97%] [G loss: 1.383901]\n",
            "6720 [D loss: 0.697019, acc.: 46.88%] [G loss: 1.392673]\n",
            "6740 [D loss: 0.701964, acc.: 48.05%] [G loss: 1.392654]\n",
            "6760 [D loss: 0.696323, acc.: 52.34%] [G loss: 1.397271]\n",
            "6780 [D loss: 0.699229, acc.: 43.75%] [G loss: 1.387532]\n",
            "6800 [D loss: 0.693560, acc.: 50.78%] [G loss: 1.384406]\n",
            "6820 [D loss: 0.699900, acc.: 46.48%] [G loss: 1.396095]\n",
            "6840 [D loss: 0.700351, acc.: 46.09%] [G loss: 1.400817]\n",
            "6860 [D loss: 0.696565, acc.: 50.39%] [G loss: 1.393807]\n",
            "6880 [D loss: 0.694871, acc.: 49.22%] [G loss: 1.389832]\n",
            "6900 [D loss: 0.700529, acc.: 46.88%] [G loss: 1.386945]\n",
            "6920 [D loss: 0.695679, acc.: 47.27%] [G loss: 1.394230]\n",
            "6940 [D loss: 0.697947, acc.: 43.75%] [G loss: 1.383168]\n",
            "6960 [D loss: 0.697765, acc.: 47.27%] [G loss: 1.401471]\n",
            "6980 [D loss: 0.699596, acc.: 42.97%] [G loss: 1.397855]\n",
            "7000 [D loss: 0.701152, acc.: 42.19%] [G loss: 1.395025]\n",
            "7020 [D loss: 0.697597, acc.: 44.53%] [G loss: 1.392945]\n",
            "7040 [D loss: 0.696670, acc.: 44.14%] [G loss: 1.395268]\n",
            "7060 [D loss: 0.701981, acc.: 41.80%] [G loss: 1.394562]\n",
            "7080 [D loss: 0.698433, acc.: 48.83%] [G loss: 1.398969]\n",
            "7100 [D loss: 0.699508, acc.: 44.92%] [G loss: 1.388082]\n",
            "7120 [D loss: 0.698022, acc.: 46.88%] [G loss: 1.393650]\n",
            "7140 [D loss: 0.694677, acc.: 44.53%] [G loss: 1.384647]\n",
            "7160 [D loss: 0.698689, acc.: 47.27%] [G loss: 1.392577]\n",
            "7180 [D loss: 0.698242, acc.: 42.58%] [G loss: 1.391645]\n",
            "7200 [D loss: 0.700687, acc.: 43.75%] [G loss: 1.396249]\n",
            "7220 [D loss: 0.696790, acc.: 44.14%] [G loss: 1.388211]\n",
            "7240 [D loss: 0.692794, acc.: 46.48%] [G loss: 1.397785]\n",
            "7260 [D loss: 0.698412, acc.: 43.36%] [G loss: 1.383541]\n",
            "7280 [D loss: 0.696848, acc.: 45.70%] [G loss: 1.394438]\n",
            "7300 [D loss: 0.694678, acc.: 45.70%] [G loss: 1.395237]\n",
            "7320 [D loss: 0.693550, acc.: 47.66%] [G loss: 1.395023]\n",
            "7340 [D loss: 0.697784, acc.: 46.09%] [G loss: 1.393231]\n",
            "7360 [D loss: 0.699125, acc.: 43.36%] [G loss: 1.386775]\n",
            "7380 [D loss: 0.698780, acc.: 42.97%] [G loss: 1.399514]\n",
            "7400 [D loss: 0.693878, acc.: 49.61%] [G loss: 1.394288]\n",
            "7420 [D loss: 0.694396, acc.: 50.39%] [G loss: 1.387681]\n",
            "7440 [D loss: 0.699487, acc.: 42.58%] [G loss: 1.391767]\n",
            "7460 [D loss: 0.695520, acc.: 47.27%] [G loss: 1.400790]\n",
            "7480 [D loss: 0.696784, acc.: 43.75%] [G loss: 1.389204]\n",
            "7500 [D loss: 0.705090, acc.: 40.23%] [G loss: 1.392731]\n",
            "7520 [D loss: 0.698871, acc.: 41.02%] [G loss: 1.388768]\n",
            "7540 [D loss: 0.697078, acc.: 46.09%] [G loss: 1.396813]\n",
            "7560 [D loss: 0.696970, acc.: 43.36%] [G loss: 1.395193]\n",
            "7580 [D loss: 0.696497, acc.: 46.09%] [G loss: 1.390957]\n",
            "7600 [D loss: 0.702041, acc.: 41.02%] [G loss: 1.395289]\n",
            "7620 [D loss: 0.697056, acc.: 43.75%] [G loss: 1.393222]\n",
            "7640 [D loss: 0.696136, acc.: 47.27%] [G loss: 1.394576]\n",
            "7660 [D loss: 0.694996, acc.: 47.27%] [G loss: 1.393033]\n",
            "7680 [D loss: 0.695782, acc.: 46.48%] [G loss: 1.385630]\n",
            "7700 [D loss: 0.693979, acc.: 50.39%] [G loss: 1.398277]\n",
            "7720 [D loss: 0.702364, acc.: 45.70%] [G loss: 1.391881]\n",
            "7740 [D loss: 0.699747, acc.: 38.67%] [G loss: 1.395062]\n",
            "7760 [D loss: 0.697299, acc.: 44.53%] [G loss: 1.393300]\n",
            "7780 [D loss: 0.697575, acc.: 44.14%] [G loss: 1.388515]\n",
            "7800 [D loss: 0.696327, acc.: 47.27%] [G loss: 1.399309]\n",
            "7820 [D loss: 0.699796, acc.: 39.45%] [G loss: 1.394260]\n",
            "7840 [D loss: 0.697553, acc.: 44.92%] [G loss: 1.389583]\n",
            "7860 [D loss: 0.698274, acc.: 40.62%] [G loss: 1.385870]\n",
            "7880 [D loss: 0.695591, acc.: 50.39%] [G loss: 1.394403]\n",
            "7900 [D loss: 0.697667, acc.: 42.97%] [G loss: 1.399807]\n",
            "7920 [D loss: 0.699197, acc.: 43.36%] [G loss: 1.389332]\n",
            "7940 [D loss: 0.695461, acc.: 43.75%] [G loss: 1.395153]\n",
            "7960 [D loss: 0.693985, acc.: 50.78%] [G loss: 1.396758]\n",
            "7980 [D loss: 0.694616, acc.: 49.61%] [G loss: 1.394727]\n",
            "8000 [D loss: 0.698273, acc.: 47.66%] [G loss: 1.394009]\n",
            "8020 [D loss: 0.697546, acc.: 46.09%] [G loss: 1.393506]\n",
            "8040 [D loss: 0.696341, acc.: 41.80%] [G loss: 1.394150]\n",
            "8060 [D loss: 0.697245, acc.: 48.44%] [G loss: 1.392660]\n",
            "8080 [D loss: 0.694503, acc.: 49.22%] [G loss: 1.399790]\n",
            "8100 [D loss: 0.699422, acc.: 40.23%] [G loss: 1.391258]\n",
            "8120 [D loss: 0.694864, acc.: 47.66%] [G loss: 1.396652]\n",
            "8140 [D loss: 0.694395, acc.: 45.70%] [G loss: 1.396786]\n",
            "8160 [D loss: 0.696896, acc.: 41.02%] [G loss: 1.390924]\n",
            "8180 [D loss: 0.697071, acc.: 45.31%] [G loss: 1.389972]\n",
            "8200 [D loss: 0.698414, acc.: 41.02%] [G loss: 1.395693]\n",
            "8220 [D loss: 0.696968, acc.: 42.97%] [G loss: 1.391768]\n",
            "8240 [D loss: 0.695341, acc.: 49.61%] [G loss: 1.391918]\n",
            "8260 [D loss: 0.696258, acc.: 42.97%] [G loss: 1.389055]\n",
            "8280 [D loss: 0.694510, acc.: 44.92%] [G loss: 1.391949]\n",
            "8300 [D loss: 0.693706, acc.: 51.17%] [G loss: 1.391831]\n",
            "8320 [D loss: 0.692619, acc.: 50.78%] [G loss: 1.394634]\n",
            "8340 [D loss: 0.692181, acc.: 45.31%] [G loss: 1.398439]\n",
            "8360 [D loss: 0.696840, acc.: 50.00%] [G loss: 1.386354]\n",
            "8380 [D loss: 0.696040, acc.: 45.70%] [G loss: 1.388847]\n",
            "8400 [D loss: 0.697098, acc.: 38.28%] [G loss: 1.385808]\n",
            "8420 [D loss: 0.696205, acc.: 48.44%] [G loss: 1.395315]\n",
            "8440 [D loss: 0.696577, acc.: 44.53%] [G loss: 1.395890]\n",
            "8460 [D loss: 0.698952, acc.: 40.23%] [G loss: 1.392299]\n",
            "8480 [D loss: 0.705718, acc.: 39.84%] [G loss: 1.425954]\n",
            "8500 [D loss: 0.692237, acc.: 47.66%] [G loss: 1.392538]\n",
            "8520 [D loss: 0.694271, acc.: 43.36%] [G loss: 1.400621]\n",
            "8540 [D loss: 0.697283, acc.: 43.36%] [G loss: 1.392912]\n",
            "8560 [D loss: 0.693932, acc.: 46.88%] [G loss: 1.389800]\n",
            "8580 [D loss: 0.698320, acc.: 41.02%] [G loss: 1.390708]\n",
            "8600 [D loss: 0.696863, acc.: 44.53%] [G loss: 1.393254]\n",
            "8620 [D loss: 0.693541, acc.: 48.83%] [G loss: 1.392359]\n",
            "8640 [D loss: 0.698448, acc.: 46.09%] [G loss: 1.390194]\n",
            "8660 [D loss: 0.698225, acc.: 43.36%] [G loss: 1.391702]\n",
            "8680 [D loss: 0.696173, acc.: 43.36%] [G loss: 1.393579]\n",
            "8700 [D loss: 0.698311, acc.: 39.84%] [G loss: 1.393775]\n",
            "8720 [D loss: 0.695746, acc.: 42.97%] [G loss: 1.389348]\n",
            "8740 [D loss: 0.697773, acc.: 45.31%] [G loss: 1.388839]\n",
            "8760 [D loss: 0.698051, acc.: 41.02%] [G loss: 1.387564]\n",
            "8780 [D loss: 0.695869, acc.: 41.80%] [G loss: 1.393906]\n",
            "8800 [D loss: 0.695220, acc.: 43.36%] [G loss: 1.384415]\n",
            "8820 [D loss: 0.694940, acc.: 43.36%] [G loss: 1.385797]\n",
            "8840 [D loss: 0.699507, acc.: 44.53%] [G loss: 1.392140]\n",
            "8860 [D loss: 0.698243, acc.: 41.02%] [G loss: 1.383003]\n",
            "8880 [D loss: 0.694200, acc.: 51.17%] [G loss: 1.389239]\n",
            "8900 [D loss: 0.700332, acc.: 39.45%] [G loss: 1.392415]\n",
            "8920 [D loss: 0.694468, acc.: 43.75%] [G loss: 1.398754]\n",
            "8940 [D loss: 0.693868, acc.: 46.09%] [G loss: 1.401220]\n",
            "8960 [D loss: 0.696201, acc.: 46.09%] [G loss: 1.390292]\n",
            "8980 [D loss: 0.700917, acc.: 37.11%] [G loss: 1.384135]\n",
            "9000 [D loss: 0.529419, acc.: 60.16%] [G loss: 3.451092]\n",
            "9020 [D loss: 0.714005, acc.: 34.77%] [G loss: 1.418368]\n",
            "9040 [D loss: 0.708870, acc.: 40.62%] [G loss: 1.395880]\n",
            "9060 [D loss: 0.705619, acc.: 41.80%] [G loss: 1.390461]\n",
            "9080 [D loss: 0.698528, acc.: 45.31%] [G loss: 1.404152]\n",
            "9100 [D loss: 0.699169, acc.: 39.06%] [G loss: 1.402751]\n",
            "9120 [D loss: 0.696621, acc.: 41.41%] [G loss: 1.406112]\n",
            "9140 [D loss: 0.698882, acc.: 44.92%] [G loss: 1.396840]\n",
            "9160 [D loss: 0.696500, acc.: 44.53%] [G loss: 1.399978]\n",
            "9180 [D loss: 0.694397, acc.: 50.39%] [G loss: 1.403586]\n",
            "9200 [D loss: 0.694435, acc.: 49.22%] [G loss: 1.399590]\n",
            "9220 [D loss: 0.696431, acc.: 43.36%] [G loss: 1.405650]\n",
            "9240 [D loss: 0.695898, acc.: 47.27%] [G loss: 1.392597]\n",
            "9260 [D loss: 0.698319, acc.: 42.58%] [G loss: 1.398521]\n",
            "9280 [D loss: 0.700153, acc.: 39.06%] [G loss: 1.388895]\n",
            "9300 [D loss: 0.693785, acc.: 47.27%] [G loss: 1.391394]\n",
            "9320 [D loss: 0.691352, acc.: 50.78%] [G loss: 1.394004]\n",
            "9340 [D loss: 0.694901, acc.: 43.36%] [G loss: 1.396414]\n",
            "9360 [D loss: 0.696790, acc.: 42.58%] [G loss: 1.395674]\n",
            "9380 [D loss: 0.692653, acc.: 48.83%] [G loss: 1.399105]\n",
            "9400 [D loss: 0.697056, acc.: 44.14%] [G loss: 1.392526]\n",
            "9420 [D loss: 0.699672, acc.: 42.97%] [G loss: 1.388630]\n",
            "9440 [D loss: 0.696520, acc.: 47.66%] [G loss: 1.388295]\n",
            "9460 [D loss: 0.692791, acc.: 49.22%] [G loss: 1.395600]\n",
            "9480 [D loss: 0.692895, acc.: 45.70%] [G loss: 1.389935]\n",
            "9500 [D loss: 0.697345, acc.: 44.92%] [G loss: 1.405686]\n",
            "9520 [D loss: 0.695038, acc.: 48.83%] [G loss: 1.398619]\n",
            "9540 [D loss: 0.693577, acc.: 49.22%] [G loss: 1.392874]\n",
            "9560 [D loss: 0.696086, acc.: 44.53%] [G loss: 1.395670]\n",
            "9580 [D loss: 0.694003, acc.: 47.66%] [G loss: 1.405685]\n",
            "9600 [D loss: 0.691519, acc.: 48.44%] [G loss: 1.388005]\n",
            "9620 [D loss: 0.701189, acc.: 41.41%] [G loss: 1.402854]\n",
            "9640 [D loss: 0.692627, acc.: 50.39%] [G loss: 1.407890]\n",
            "9660 [D loss: 0.699078, acc.: 45.31%] [G loss: 1.403761]\n",
            "9680 [D loss: 0.692119, acc.: 46.48%] [G loss: 1.422443]\n",
            "9700 [D loss: 0.694870, acc.: 47.27%] [G loss: 1.402087]\n",
            "9720 [D loss: 0.694653, acc.: 47.66%] [G loss: 1.401800]\n",
            "9740 [D loss: 0.691276, acc.: 53.52%] [G loss: 1.412094]\n",
            "9760 [D loss: 0.697540, acc.: 41.41%] [G loss: 1.401138]\n",
            "9780 [D loss: 0.697595, acc.: 46.09%] [G loss: 1.409874]\n",
            "9800 [D loss: 0.693005, acc.: 46.48%] [G loss: 1.400883]\n",
            "9820 [D loss: 0.692952, acc.: 54.30%] [G loss: 1.406455]\n",
            "9840 [D loss: 0.691773, acc.: 48.05%] [G loss: 1.401276]\n",
            "9860 [D loss: 0.691053, acc.: 51.17%] [G loss: 1.397547]\n",
            "9880 [D loss: 0.687381, acc.: 53.12%] [G loss: 1.407923]\n",
            "9900 [D loss: 0.692056, acc.: 47.27%] [G loss: 1.400826]\n",
            "9920 [D loss: 0.694512, acc.: 43.75%] [G loss: 1.410615]\n",
            "9940 [D loss: 0.693986, acc.: 45.70%] [G loss: 1.404719]\n",
            "9960 [D loss: 0.691524, acc.: 52.73%] [G loss: 1.402085]\n",
            "9980 [D loss: 0.695208, acc.: 48.05%] [G loss: 1.405380]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}