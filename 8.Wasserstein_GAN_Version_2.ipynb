{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "8.Wasserstein_GAN_Version_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code based on https://github.com/eriklindernoren/Keras-GAN/tree/master/wgan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "b2e3d3f1-0db4-459f-bbd2-8e8cdd69d2a5"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98d2636-33bc-4c72-f653-aa4f5acc5abc"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLasjF73EZg1",
        "outputId": "d56df05d-4b0f-4f0b-a7e6-3742114942ad"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "70efe708-20eb-4291-bf9c-d068d70359d4"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## WGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBLIDqvQExmx"
      },
      "source": [
        "class WGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        self.clip_value = 0.01\n",
        "        optimizer = RMSprop(lr=0.00005)\n",
        "\n",
        "        # Build and compile the critic\n",
        "        self.critic = self.build_critic()\n",
        "        self.critic.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.critic.trainable = False\n",
        "\n",
        "        # The critic takes generated images as input and determines validity\n",
        "        valid = self.critic(img)\n",
        "\n",
        "        # The combined model  (stacked generator and critic)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 12 * 12, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 12x12 image\n",
        "        # n_nodes = 128 * 12 * 12\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((12, 12, 128)))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # generate\n",
        "        # model.add(Conv2D(self.channels, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print(\"generator\")\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_critic(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        print(\"critic\")\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake = np.ones((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------\n",
        "                #  Train Critic\n",
        "                # ---------------\n",
        "\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "                \n",
        "                # Sample noise as generator input\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "                # Generate a batch of new images\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the critic\n",
        "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
        "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "                # Clip critic weights\n",
        "                for l in self.critic.layers:\n",
        "                    weights = l.get_weights()\n",
        "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                    l.set_weights(weights)\n",
        "\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/epoch_%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDxAH9OmyLYC",
        "outputId": "1fe5d7a9-017d-4403-d07d-37babe88be1b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    wgan = WGAN()\n",
        "    wgan.train(epochs=20000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "critic\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "generator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.999856] [G loss: 1.000101]\n",
            "20 [D loss: 0.999935] [G loss: 1.000102]\n",
            "40 [D loss: 0.999970] [G loss: 1.000061]\n",
            "60 [D loss: 0.999969] [G loss: 1.000064]\n",
            "80 [D loss: 0.999969] [G loss: 1.000060]\n",
            "100 [D loss: 0.999967] [G loss: 1.000066]\n",
            "120 [D loss: 0.999970] [G loss: 1.000061]\n",
            "140 [D loss: 0.999971] [G loss: 1.000065]\n",
            "160 [D loss: 0.999971] [G loss: 1.000062]\n",
            "180 [D loss: 0.999974] [G loss: 1.000060]\n",
            "200 [D loss: 0.999973] [G loss: 1.000054]\n",
            "220 [D loss: 0.999975] [G loss: 1.000051]\n",
            "240 [D loss: 0.999981] [G loss: 1.000033]\n",
            "260 [D loss: 0.999981] [G loss: 1.000031]\n",
            "280 [D loss: 0.999998] [G loss: 0.999994]\n",
            "300 [D loss: 0.999992] [G loss: 0.999996]\n",
            "320 [D loss: 1.000012] [G loss: 0.999942]\n",
            "340 [D loss: 1.000011] [G loss: 0.999953]\n",
            "360 [D loss: 0.999999] [G loss: 0.999972]\n",
            "380 [D loss: 0.999994] [G loss: 0.999959]\n",
            "400 [D loss: 0.999993] [G loss: 0.999984]\n",
            "420 [D loss: 0.999981] [G loss: 1.000000]\n",
            "440 [D loss: 0.999971] [G loss: 1.000040]\n",
            "460 [D loss: 0.999966] [G loss: 1.000051]\n",
            "480 [D loss: 0.999965] [G loss: 1.000055]\n",
            "500 [D loss: 0.999966] [G loss: 1.000050]\n",
            "520 [D loss: 0.999968] [G loss: 1.000053]\n",
            "540 [D loss: 0.999969] [G loss: 1.000053]\n",
            "560 [D loss: 0.999969] [G loss: 1.000062]\n",
            "580 [D loss: 0.999970] [G loss: 1.000058]\n",
            "600 [D loss: 0.999970] [G loss: 1.000058]\n",
            "620 [D loss: 0.999970] [G loss: 1.000056]\n",
            "640 [D loss: 0.999969] [G loss: 1.000062]\n",
            "660 [D loss: 0.999967] [G loss: 1.000062]\n",
            "680 [D loss: 0.999970] [G loss: 1.000060]\n",
            "700 [D loss: 0.999971] [G loss: 1.000058]\n",
            "720 [D loss: 0.999971] [G loss: 1.000064]\n",
            "740 [D loss: 0.999968] [G loss: 1.000059]\n",
            "760 [D loss: 0.999971] [G loss: 1.000056]\n",
            "780 [D loss: 0.999970] [G loss: 1.000061]\n",
            "800 [D loss: 0.999971] [G loss: 1.000057]\n",
            "820 [D loss: 0.999970] [G loss: 1.000060]\n",
            "840 [D loss: 0.999968] [G loss: 1.000061]\n",
            "860 [D loss: 0.999970] [G loss: 1.000060]\n",
            "880 [D loss: 0.999970] [G loss: 1.000062]\n",
            "900 [D loss: 0.999970] [G loss: 1.000066]\n",
            "920 [D loss: 0.999970] [G loss: 1.000064]\n",
            "940 [D loss: 0.999969] [G loss: 1.000065]\n",
            "960 [D loss: 0.999971] [G loss: 1.000063]\n",
            "980 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1000 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1020 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1040 [D loss: 0.999972] [G loss: 1.000058]\n",
            "1060 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1080 [D loss: 0.999974] [G loss: 1.000062]\n",
            "1100 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1120 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1140 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1160 [D loss: 0.999970] [G loss: 1.000058]\n",
            "1180 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1200 [D loss: 0.999973] [G loss: 1.000058]\n",
            "1220 [D loss: 0.999974] [G loss: 1.000054]\n",
            "1240 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1260 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1280 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1300 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1320 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1340 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1360 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1380 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1400 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1420 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1440 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1460 [D loss: 0.999965] [G loss: 1.000066]\n",
            "1480 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1500 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1520 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1540 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1560 [D loss: 0.999970] [G loss: 1.000067]\n",
            "1580 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1600 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1620 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1640 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1660 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1680 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1700 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1720 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1740 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1760 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1780 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1800 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1820 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1840 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1860 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1880 [D loss: 0.999968] [G loss: 1.000065]\n",
            "1900 [D loss: 0.999973] [G loss: 1.000062]\n",
            "1920 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1940 [D loss: 0.999972] [G loss: 1.000065]\n",
            "1960 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1980 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2000 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2020 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2040 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2060 [D loss: 0.999972] [G loss: 1.000065]\n",
            "2080 [D loss: 0.999971] [G loss: 1.000067]\n",
            "2100 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2120 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2140 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2160 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2180 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2200 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2220 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2240 [D loss: 0.999969] [G loss: 1.000062]\n",
            "2260 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2280 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2300 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2320 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2340 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2360 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2380 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2400 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2420 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2440 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2460 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2480 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2500 [D loss: 0.999973] [G loss: 1.000063]\n",
            "2520 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2540 [D loss: 0.999969] [G loss: 1.000064]\n",
            "2560 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2580 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2600 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2620 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2640 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2660 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2680 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2700 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2720 [D loss: 0.999973] [G loss: 1.000065]\n",
            "2740 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2760 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2780 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2800 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2820 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2840 [D loss: 0.999972] [G loss: 1.000066]\n",
            "2860 [D loss: 0.999972] [G loss: 1.000061]\n",
            "2880 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2900 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2920 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2940 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2960 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2980 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3000 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3020 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3040 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3060 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3080 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3100 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3120 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3140 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3160 [D loss: 0.999973] [G loss: 1.000064]\n",
            "3180 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3200 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3220 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3240 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3260 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3280 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3300 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3320 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3340 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3360 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3380 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3400 [D loss: 0.999973] [G loss: 1.000065]\n",
            "3420 [D loss: 0.999973] [G loss: 1.000062]\n",
            "3440 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3460 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3500 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3520 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3560 [D loss: 0.999967] [G loss: 1.000061]\n",
            "3580 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3600 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3620 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3640 [D loss: 0.999968] [G loss: 1.000070]\n",
            "3660 [D loss: 0.999972] [G loss: 1.000064]\n",
            "3680 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3700 [D loss: 0.999970] [G loss: 1.000069]\n",
            "3720 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3740 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3760 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3780 [D loss: 0.999968] [G loss: 1.000063]\n",
            "3800 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3820 [D loss: 0.999969] [G loss: 1.000063]\n",
            "3840 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3860 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3880 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3900 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3920 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3940 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3960 [D loss: 0.999972] [G loss: 1.000062]\n",
            "3980 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4000 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4020 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4040 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4060 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4080 [D loss: 0.999972] [G loss: 1.000062]\n",
            "4100 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4120 [D loss: 0.999972] [G loss: 1.000067]\n",
            "4140 [D loss: 0.999973] [G loss: 1.000066]\n",
            "4160 [D loss: 0.999969] [G loss: 1.000067]\n",
            "4180 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4200 [D loss: 0.999970] [G loss: 1.000068]\n",
            "4220 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4240 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4260 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4280 [D loss: 0.999970] [G loss: 1.000063]\n",
            "4300 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4320 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4340 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4360 [D loss: 0.999970] [G loss: 1.000063]\n",
            "4380 [D loss: 0.999970] [G loss: 1.000062]\n",
            "4400 [D loss: 0.999968] [G loss: 1.000068]\n",
            "4420 [D loss: 0.999971] [G loss: 1.000063]\n",
            "4440 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4460 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4480 [D loss: 0.999969] [G loss: 1.000067]\n",
            "4500 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4520 [D loss: 0.999969] [G loss: 1.000064]\n",
            "4540 [D loss: 0.999972] [G loss: 1.000066]\n",
            "4560 [D loss: 0.999969] [G loss: 1.000063]\n",
            "4580 [D loss: 0.999972] [G loss: 1.000066]\n",
            "4600 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4620 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4640 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4660 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4680 [D loss: 0.999972] [G loss: 1.000063]\n",
            "4700 [D loss: 0.999966] [G loss: 1.000066]\n",
            "4720 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4740 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4760 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4780 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4800 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4820 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4860 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4880 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4900 [D loss: 0.999971] [G loss: 1.000062]\n",
            "4920 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4940 [D loss: 0.999970] [G loss: 1.000068]\n",
            "4960 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4980 [D loss: 0.999969] [G loss: 1.000064]\n",
            "5000 [D loss: 0.999969] [G loss: 1.000063]\n",
            "5020 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5040 [D loss: 0.999970] [G loss: 1.000062]\n",
            "5060 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5080 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5100 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5120 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5140 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5160 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5180 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5200 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5220 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5260 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5280 [D loss: 0.999970] [G loss: 1.000062]\n",
            "5300 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5320 [D loss: 0.999969] [G loss: 1.000066]\n",
            "5340 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5360 [D loss: 0.999970] [G loss: 1.000060]\n",
            "5380 [D loss: 0.999970] [G loss: 1.000068]\n",
            "5400 [D loss: 0.999969] [G loss: 1.000065]\n",
            "5420 [D loss: 0.999971] [G loss: 1.000062]\n",
            "5440 [D loss: 0.999968] [G loss: 1.000065]\n",
            "5460 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5480 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5500 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5520 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5540 [D loss: 0.999969] [G loss: 1.000065]\n",
            "5560 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5580 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5600 [D loss: 0.999971] [G loss: 1.000061]\n",
            "5620 [D loss: 0.999969] [G loss: 1.000064]\n",
            "5640 [D loss: 0.999971] [G loss: 1.000068]\n",
            "5660 [D loss: 0.999972] [G loss: 1.000063]\n",
            "5680 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5700 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5720 [D loss: 0.999969] [G loss: 1.000065]\n",
            "5740 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5760 [D loss: 0.999971] [G loss: 1.000067]\n",
            "5780 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5800 [D loss: 0.999971] [G loss: 1.000067]\n",
            "5820 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5840 [D loss: 0.999971] [G loss: 1.000068]\n",
            "5860 [D loss: 0.999968] [G loss: 1.000065]\n",
            "5880 [D loss: 0.999971] [G loss: 1.000067]\n",
            "5900 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5920 [D loss: 0.999969] [G loss: 1.000066]\n",
            "5940 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5960 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5980 [D loss: 0.999970] [G loss: 1.000068]\n",
            "6000 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6020 [D loss: 0.999972] [G loss: 1.000064]\n",
            "6040 [D loss: 0.999969] [G loss: 1.000062]\n",
            "6060 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6080 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6100 [D loss: 0.999972] [G loss: 1.000068]\n",
            "6120 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6140 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6160 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6180 [D loss: 0.999970] [G loss: 1.000063]\n",
            "6200 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6220 [D loss: 0.999972] [G loss: 1.000066]\n",
            "6240 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6260 [D loss: 0.999972] [G loss: 1.000063]\n",
            "6280 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6300 [D loss: 0.999973] [G loss: 1.000066]\n",
            "6320 [D loss: 0.999968] [G loss: 1.000068]\n",
            "6340 [D loss: 0.999972] [G loss: 1.000068]\n",
            "6360 [D loss: 0.999972] [G loss: 1.000066]\n",
            "6380 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6400 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6420 [D loss: 0.999969] [G loss: 1.000067]\n",
            "6440 [D loss: 0.999973] [G loss: 1.000064]\n",
            "6460 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6480 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6500 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6520 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6540 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6560 [D loss: 0.999972] [G loss: 1.000066]\n",
            "6580 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6600 [D loss: 0.999969] [G loss: 1.000067]\n",
            "6620 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6640 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6660 [D loss: 0.999968] [G loss: 1.000065]\n",
            "6680 [D loss: 0.999967] [G loss: 1.000067]\n",
            "6700 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6720 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6740 [D loss: 0.999968] [G loss: 1.000066]\n",
            "6760 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6780 [D loss: 0.999969] [G loss: 1.000065]\n",
            "6800 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6820 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6840 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6860 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6880 [D loss: 0.999968] [G loss: 1.000064]\n",
            "6900 [D loss: 0.999969] [G loss: 1.000064]\n",
            "6920 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6940 [D loss: 0.999969] [G loss: 1.000067]\n",
            "6960 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6980 [D loss: 0.999971] [G loss: 1.000064]\n",
            "7000 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7020 [D loss: 0.999968] [G loss: 1.000068]\n",
            "7040 [D loss: 0.999969] [G loss: 1.000066]\n",
            "7060 [D loss: 0.999969] [G loss: 1.000068]\n",
            "7080 [D loss: 0.999973] [G loss: 1.000063]\n",
            "7100 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7120 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7140 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7160 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7180 [D loss: 0.999971] [G loss: 1.000061]\n",
            "7200 [D loss: 0.999969] [G loss: 1.000066]\n",
            "7220 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7240 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7260 [D loss: 0.999969] [G loss: 1.000067]\n",
            "7280 [D loss: 0.999971] [G loss: 1.000069]\n",
            "7300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "7320 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7340 [D loss: 0.999971] [G loss: 1.000063]\n",
            "7360 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7380 [D loss: 0.999970] [G loss: 1.000065]\n",
            "7400 [D loss: 0.999972] [G loss: 1.000068]\n",
            "7420 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7440 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7460 [D loss: 0.999972] [G loss: 1.000065]\n",
            "7480 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7500 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7520 [D loss: 0.999971] [G loss: 1.000068]\n",
            "7540 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7560 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7580 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7600 [D loss: 0.999970] [G loss: 1.000067]\n",
            "7620 [D loss: 0.999970] [G loss: 1.000068]\n",
            "7640 [D loss: 0.999969] [G loss: 1.000062]\n",
            "7660 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7680 [D loss: 0.999972] [G loss: 1.000066]\n",
            "7700 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7720 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7740 [D loss: 0.999970] [G loss: 1.000058]\n",
            "7760 [D loss: 0.999971] [G loss: 1.000061]\n",
            "7780 [D loss: 0.999972] [G loss: 1.000066]\n",
            "7800 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7820 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7840 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7860 [D loss: 0.999971] [G loss: 1.000063]\n",
            "7880 [D loss: 0.999971] [G loss: 1.000061]\n",
            "7900 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7920 [D loss: 0.999967] [G loss: 1.000066]\n",
            "7940 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7960 [D loss: 0.999970] [G loss: 1.000059]\n",
            "7980 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8000 [D loss: 0.999973] [G loss: 1.000064]\n",
            "8020 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8040 [D loss: 0.999970] [G loss: 1.000063]\n",
            "8060 [D loss: 0.999968] [G loss: 1.000063]\n",
            "8080 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8100 [D loss: 0.999969] [G loss: 1.000063]\n",
            "8120 [D loss: 0.999969] [G loss: 1.000066]\n",
            "8140 [D loss: 0.999969] [G loss: 1.000065]\n",
            "8160 [D loss: 0.999973] [G loss: 1.000066]\n",
            "8180 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8200 [D loss: 0.999969] [G loss: 1.000065]\n",
            "8220 [D loss: 0.999972] [G loss: 1.000064]\n",
            "8240 [D loss: 0.999973] [G loss: 1.000065]\n",
            "8260 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8280 [D loss: 0.999972] [G loss: 1.000061]\n",
            "8300 [D loss: 0.999972] [G loss: 1.000062]\n",
            "8320 [D loss: 0.999969] [G loss: 1.000066]\n",
            "8340 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8380 [D loss: 0.999969] [G loss: 1.000066]\n",
            "8400 [D loss: 0.999972] [G loss: 1.000065]\n",
            "8420 [D loss: 0.999972] [G loss: 1.000066]\n",
            "8440 [D loss: 0.999969] [G loss: 1.000065]\n",
            "8460 [D loss: 0.999971] [G loss: 1.000060]\n",
            "8480 [D loss: 0.999970] [G loss: 1.000064]\n",
            "8500 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8520 [D loss: 0.999969] [G loss: 1.000061]\n",
            "8540 [D loss: 0.999968] [G loss: 1.000066]\n",
            "8560 [D loss: 0.999971] [G loss: 1.000060]\n",
            "8580 [D loss: 0.999971] [G loss: 1.000062]\n",
            "8600 [D loss: 0.999971] [G loss: 1.000068]\n",
            "8620 [D loss: 0.999971] [G loss: 1.000067]\n",
            "8640 [D loss: 0.999973] [G loss: 1.000064]\n",
            "8660 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8680 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8700 [D loss: 0.999970] [G loss: 1.000066]\n",
            "8720 [D loss: 0.999970] [G loss: 1.000067]\n",
            "8740 [D loss: 0.999969] [G loss: 1.000065]\n",
            "8760 [D loss: 0.999970] [G loss: 1.000062]\n",
            "8780 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8800 [D loss: 0.999971] [G loss: 1.000067]\n",
            "8820 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8840 [D loss: 0.999972] [G loss: 1.000065]\n",
            "8860 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8880 [D loss: 0.999972] [G loss: 1.000059]\n",
            "8900 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8920 [D loss: 0.999970] [G loss: 1.000066]\n",
            "8940 [D loss: 0.999972] [G loss: 1.000061]\n",
            "8960 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8980 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9000 [D loss: 0.999969] [G loss: 1.000061]\n",
            "9020 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9040 [D loss: 0.999969] [G loss: 1.000063]\n",
            "9060 [D loss: 0.999971] [G loss: 1.000063]\n",
            "9080 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9100 [D loss: 0.999970] [G loss: 1.000066]\n",
            "9120 [D loss: 0.999972] [G loss: 1.000066]\n",
            "9140 [D loss: 0.999969] [G loss: 1.000063]\n",
            "9160 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9180 [D loss: 0.999970] [G loss: 1.000067]\n",
            "9200 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9220 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9240 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9260 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9280 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9300 [D loss: 0.999970] [G loss: 1.000066]\n",
            "9320 [D loss: 0.999968] [G loss: 1.000066]\n",
            "9340 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9360 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9380 [D loss: 0.999971] [G loss: 1.000067]\n",
            "9400 [D loss: 0.999970] [G loss: 1.000064]\n",
            "9420 [D loss: 0.999968] [G loss: 1.000064]\n",
            "9440 [D loss: 0.999969] [G loss: 1.000064]\n",
            "9460 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9480 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9500 [D loss: 0.999970] [G loss: 1.000066]\n",
            "9520 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9540 [D loss: 0.999970] [G loss: 1.000066]\n",
            "9560 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9580 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9600 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9620 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9640 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9660 [D loss: 0.999972] [G loss: 1.000066]\n",
            "9680 [D loss: 0.999970] [G loss: 1.000066]\n",
            "9700 [D loss: 0.999970] [G loss: 1.000064]\n",
            "9720 [D loss: 0.999968] [G loss: 1.000063]\n",
            "9740 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9760 [D loss: 0.999970] [G loss: 1.000064]\n",
            "9780 [D loss: 0.999973] [G loss: 1.000067]\n",
            "9800 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9820 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9840 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9860 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9880 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9900 [D loss: 0.999970] [G loss: 1.000064]\n",
            "9920 [D loss: 0.999971] [G loss: 1.000067]\n",
            "9940 [D loss: 0.999968] [G loss: 1.000062]\n",
            "9960 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9980 [D loss: 0.999970] [G loss: 1.000064]\n",
            "10000 [D loss: 0.999972] [G loss: 1.000067]\n",
            "10020 [D loss: 0.999970] [G loss: 1.000065]\n",
            "10040 [D loss: 0.999970] [G loss: 1.000064]\n",
            "10060 [D loss: 0.999970] [G loss: 1.000065]\n",
            "10080 [D loss: 0.999971] [G loss: 1.000063]\n",
            "10100 [D loss: 0.999969] [G loss: 1.000065]\n",
            "10120 [D loss: 0.999972] [G loss: 1.000063]\n",
            "10140 [D loss: 0.999972] [G loss: 1.000068]\n",
            "10160 [D loss: 0.999970] [G loss: 1.000066]\n",
            "10180 [D loss: 0.999970] [G loss: 1.000067]\n",
            "10200 [D loss: 0.999971] [G loss: 1.000066]\n",
            "10220 [D loss: 0.999971] [G loss: 1.000062]\n",
            "10240 [D loss: 0.999970] [G loss: 1.000064]\n",
            "10260 [D loss: 0.999972] [G loss: 1.000064]\n",
            "10280 [D loss: 0.999972] [G loss: 1.000065]\n",
            "10300 [D loss: 0.999969] [G loss: 1.000068]\n",
            "10320 [D loss: 0.999972] [G loss: 1.000067]\n",
            "10340 [D loss: 0.999971] [G loss: 1.000066]\n",
            "10360 [D loss: 0.999971] [G loss: 1.000068]\n",
            "10380 [D loss: 0.999969] [G loss: 1.000065]\n",
            "10400 [D loss: 0.999969] [G loss: 1.000066]\n",
            "10420 [D loss: 0.999970] [G loss: 1.000067]\n",
            "10440 [D loss: 0.999971] [G loss: 1.000065]\n",
            "10460 [D loss: 0.999970] [G loss: 1.000064]\n",
            "10480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "10500 [D loss: 0.999968] [G loss: 1.000064]\n",
            "10520 [D loss: 0.999971] [G loss: 1.000069]\n",
            "10540 [D loss: 0.999971] [G loss: 1.000065]\n",
            "10560 [D loss: 0.999973] [G loss: 1.000063]\n",
            "10580 [D loss: 0.999970] [G loss: 1.000065]\n",
            "10600 [D loss: 0.999970] [G loss: 1.000067]\n",
            "10620 [D loss: 0.999970] [G loss: 1.000064]\n",
            "10640 [D loss: 0.999970] [G loss: 1.000066]\n",
            "10660 [D loss: 0.999969] [G loss: 1.000065]\n",
            "10680 [D loss: 0.999970] [G loss: 1.000065]\n",
            "10700 [D loss: 0.999970] [G loss: 1.000064]\n",
            "10720 [D loss: 0.999971] [G loss: 1.000060]\n",
            "10740 [D loss: 0.999969] [G loss: 1.000067]\n",
            "10760 [D loss: 0.999971] [G loss: 1.000066]\n",
            "10780 [D loss: 0.999971] [G loss: 1.000067]\n",
            "10800 [D loss: 0.999970] [G loss: 1.000067]\n",
            "10820 [D loss: 0.999971] [G loss: 1.000064]\n",
            "10840 [D loss: 0.999972] [G loss: 1.000067]\n",
            "10860 [D loss: 0.999969] [G loss: 1.000067]\n",
            "10880 [D loss: 0.999971] [G loss: 1.000066]\n",
            "10900 [D loss: 0.999969] [G loss: 1.000065]\n",
            "10920 [D loss: 0.999971] [G loss: 1.000067]\n",
            "10940 [D loss: 0.999970] [G loss: 1.000068]\n",
            "10960 [D loss: 0.999972] [G loss: 1.000061]\n",
            "10980 [D loss: 0.999970] [G loss: 1.000067]\n",
            "11000 [D loss: 0.999970] [G loss: 1.000067]\n",
            "11020 [D loss: 0.999972] [G loss: 1.000065]\n",
            "11040 [D loss: 0.999969] [G loss: 1.000066]\n",
            "11060 [D loss: 0.999967] [G loss: 1.000066]\n",
            "11080 [D loss: 0.999970] [G loss: 1.000065]\n",
            "11100 [D loss: 0.999970] [G loss: 1.000061]\n",
            "11120 [D loss: 0.999971] [G loss: 1.000066]\n",
            "11140 [D loss: 0.999973] [G loss: 1.000063]\n",
            "11160 [D loss: 0.999970] [G loss: 1.000066]\n",
            "11180 [D loss: 0.999970] [G loss: 1.000063]\n",
            "11200 [D loss: 0.999969] [G loss: 1.000061]\n",
            "11220 [D loss: 0.999969] [G loss: 1.000065]\n",
            "11240 [D loss: 0.999971] [G loss: 1.000067]\n",
            "11260 [D loss: 0.999971] [G loss: 1.000067]\n",
            "11280 [D loss: 0.999972] [G loss: 1.000063]\n",
            "11300 [D loss: 0.999970] [G loss: 1.000065]\n",
            "11320 [D loss: 0.999968] [G loss: 1.000062]\n",
            "11340 [D loss: 0.999972] [G loss: 1.000063]\n",
            "11360 [D loss: 0.999970] [G loss: 1.000064]\n",
            "11380 [D loss: 0.999971] [G loss: 1.000061]\n",
            "11400 [D loss: 0.999970] [G loss: 1.000063]\n",
            "11420 [D loss: 0.999972] [G loss: 1.000065]\n",
            "11440 [D loss: 0.999969] [G loss: 1.000068]\n",
            "11460 [D loss: 0.999972] [G loss: 1.000065]\n",
            "11480 [D loss: 0.999970] [G loss: 1.000066]\n",
            "11500 [D loss: 0.999971] [G loss: 1.000065]\n",
            "11520 [D loss: 0.999972] [G loss: 1.000062]\n",
            "11540 [D loss: 0.999969] [G loss: 1.000065]\n",
            "11560 [D loss: 0.999972] [G loss: 1.000063]\n",
            "11580 [D loss: 0.999969] [G loss: 1.000063]\n",
            "11600 [D loss: 0.999970] [G loss: 1.000065]\n",
            "11620 [D loss: 0.999971] [G loss: 1.000062]\n",
            "11640 [D loss: 0.999971] [G loss: 1.000066]\n",
            "11660 [D loss: 0.999971] [G loss: 1.000066]\n",
            "11680 [D loss: 0.999968] [G loss: 1.000064]\n",
            "11700 [D loss: 0.999970] [G loss: 1.000060]\n",
            "11720 [D loss: 0.999969] [G loss: 1.000065]\n",
            "11740 [D loss: 0.999970] [G loss: 1.000066]\n",
            "11760 [D loss: 0.999971] [G loss: 1.000063]\n",
            "11780 [D loss: 0.999970] [G loss: 1.000067]\n",
            "11800 [D loss: 0.999971] [G loss: 1.000065]\n",
            "11820 [D loss: 0.999968] [G loss: 1.000065]\n",
            "11840 [D loss: 0.999970] [G loss: 1.000066]\n",
            "11860 [D loss: 0.999972] [G loss: 1.000066]\n",
            "11880 [D loss: 0.999971] [G loss: 1.000068]\n",
            "11900 [D loss: 0.999972] [G loss: 1.000065]\n",
            "11920 [D loss: 0.999970] [G loss: 1.000063]\n",
            "11940 [D loss: 0.999972] [G loss: 1.000066]\n",
            "11960 [D loss: 0.999971] [G loss: 1.000063]\n",
            "11980 [D loss: 0.999971] [G loss: 1.000061]\n",
            "12000 [D loss: 0.999971] [G loss: 1.000066]\n",
            "12020 [D loss: 0.999972] [G loss: 1.000065]\n",
            "12040 [D loss: 0.999970] [G loss: 1.000067]\n",
            "12060 [D loss: 0.999968] [G loss: 1.000065]\n",
            "12080 [D loss: 0.999970] [G loss: 1.000067]\n",
            "12100 [D loss: 0.999971] [G loss: 1.000069]\n",
            "12120 [D loss: 0.999971] [G loss: 1.000065]\n",
            "12140 [D loss: 0.999970] [G loss: 1.000067]\n",
            "12160 [D loss: 0.999970] [G loss: 1.000064]\n",
            "12180 [D loss: 0.999971] [G loss: 1.000068]\n",
            "12200 [D loss: 0.999970] [G loss: 1.000062]\n",
            "12220 [D loss: 0.999968] [G loss: 1.000064]\n",
            "12240 [D loss: 0.999968] [G loss: 1.000065]\n",
            "12260 [D loss: 0.999971] [G loss: 1.000063]\n",
            "12280 [D loss: 0.999973] [G loss: 1.000063]\n",
            "12300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "12320 [D loss: 0.999971] [G loss: 1.000066]\n",
            "12340 [D loss: 0.999969] [G loss: 1.000066]\n",
            "12360 [D loss: 0.999969] [G loss: 1.000067]\n",
            "12380 [D loss: 0.999971] [G loss: 1.000061]\n",
            "12400 [D loss: 0.999972] [G loss: 1.000063]\n",
            "12420 [D loss: 0.999971] [G loss: 1.000067]\n",
            "12440 [D loss: 0.999970] [G loss: 1.000064]\n",
            "12460 [D loss: 0.999970] [G loss: 1.000067]\n",
            "12480 [D loss: 0.999972] [G loss: 1.000064]\n",
            "12500 [D loss: 0.999972] [G loss: 1.000067]\n",
            "12520 [D loss: 0.999969] [G loss: 1.000068]\n",
            "12540 [D loss: 0.999970] [G loss: 1.000065]\n",
            "12560 [D loss: 0.999973] [G loss: 1.000065]\n",
            "12580 [D loss: 0.999971] [G loss: 1.000064]\n",
            "12600 [D loss: 0.999969] [G loss: 1.000068]\n",
            "12620 [D loss: 0.999971] [G loss: 1.000064]\n",
            "12640 [D loss: 0.999971] [G loss: 1.000062]\n",
            "12660 [D loss: 0.999973] [G loss: 1.000064]\n",
            "12680 [D loss: 0.999972] [G loss: 1.000065]\n",
            "12700 [D loss: 0.999970] [G loss: 1.000063]\n",
            "12720 [D loss: 0.999971] [G loss: 1.000066]\n",
            "12740 [D loss: 0.999972] [G loss: 1.000067]\n",
            "12760 [D loss: 0.999971] [G loss: 1.000062]\n",
            "12780 [D loss: 0.999970] [G loss: 1.000065]\n",
            "12800 [D loss: 0.999969] [G loss: 1.000068]\n",
            "12820 [D loss: 0.999972] [G loss: 1.000065]\n",
            "12840 [D loss: 0.999970] [G loss: 1.000062]\n",
            "12860 [D loss: 0.999967] [G loss: 1.000067]\n",
            "12880 [D loss: 0.999971] [G loss: 1.000063]\n",
            "12900 [D loss: 0.999970] [G loss: 1.000065]\n",
            "12920 [D loss: 0.999971] [G loss: 1.000064]\n",
            "12940 [D loss: 0.999971] [G loss: 1.000067]\n",
            "12960 [D loss: 0.999972] [G loss: 1.000064]\n",
            "12980 [D loss: 0.999970] [G loss: 1.000065]\n",
            "13000 [D loss: 0.999972] [G loss: 1.000067]\n",
            "13020 [D loss: 0.999972] [G loss: 1.000067]\n",
            "13040 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13060 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13080 [D loss: 0.999972] [G loss: 1.000063]\n",
            "13100 [D loss: 0.999971] [G loss: 1.000067]\n",
            "13120 [D loss: 0.999972] [G loss: 1.000065]\n",
            "13140 [D loss: 0.999972] [G loss: 1.000064]\n",
            "13160 [D loss: 0.999971] [G loss: 1.000064]\n",
            "13180 [D loss: 0.999970] [G loss: 1.000068]\n",
            "13200 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13220 [D loss: 0.999970] [G loss: 1.000063]\n",
            "13240 [D loss: 0.999972] [G loss: 1.000064]\n",
            "13260 [D loss: 0.999971] [G loss: 1.000067]\n",
            "13280 [D loss: 0.999969] [G loss: 1.000066]\n",
            "13300 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13320 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13340 [D loss: 0.999972] [G loss: 1.000061]\n",
            "13360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "13380 [D loss: 0.999969] [G loss: 1.000065]\n",
            "13400 [D loss: 0.999970] [G loss: 1.000065]\n",
            "13420 [D loss: 0.999970] [G loss: 1.000067]\n",
            "13440 [D loss: 0.999970] [G loss: 1.000067]\n",
            "13460 [D loss: 0.999969] [G loss: 1.000066]\n",
            "13480 [D loss: 0.999969] [G loss: 1.000065]\n",
            "13500 [D loss: 0.999972] [G loss: 1.000067]\n",
            "13520 [D loss: 0.999969] [G loss: 1.000065]\n",
            "13540 [D loss: 0.999970] [G loss: 1.000069]\n",
            "13560 [D loss: 0.999971] [G loss: 1.000066]\n",
            "13580 [D loss: 0.999971] [G loss: 1.000064]\n",
            "13600 [D loss: 0.999969] [G loss: 1.000064]\n",
            "13620 [D loss: 0.999972] [G loss: 1.000066]\n",
            "13640 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13660 [D loss: 0.999973] [G loss: 1.000067]\n",
            "13680 [D loss: 0.999968] [G loss: 1.000065]\n",
            "13700 [D loss: 0.999970] [G loss: 1.000065]\n",
            "13720 [D loss: 0.999973] [G loss: 1.000067]\n",
            "13740 [D loss: 0.999972] [G loss: 1.000067]\n",
            "13760 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13780 [D loss: 0.999970] [G loss: 1.000066]\n",
            "13800 [D loss: 0.999970] [G loss: 1.000068]\n",
            "13820 [D loss: 0.999970] [G loss: 1.000068]\n",
            "13840 [D loss: 0.999970] [G loss: 1.000064]\n",
            "13860 [D loss: 0.999968] [G loss: 1.000063]\n",
            "13880 [D loss: 0.999970] [G loss: 1.000068]\n",
            "13900 [D loss: 0.999970] [G loss: 1.000065]\n",
            "13920 [D loss: 0.999968] [G loss: 1.000066]\n",
            "13940 [D loss: 0.999970] [G loss: 1.000068]\n",
            "13960 [D loss: 0.999970] [G loss: 1.000065]\n",
            "13980 [D loss: 0.999970] [G loss: 1.000067]\n",
            "14000 [D loss: 0.999971] [G loss: 1.000066]\n",
            "14020 [D loss: 0.999970] [G loss: 1.000066]\n",
            "14040 [D loss: 0.999970] [G loss: 1.000063]\n",
            "14060 [D loss: 0.999971] [G loss: 1.000067]\n",
            "14080 [D loss: 0.999969] [G loss: 1.000066]\n",
            "14100 [D loss: 0.999972] [G loss: 1.000063]\n",
            "14120 [D loss: 0.999970] [G loss: 1.000066]\n",
            "14140 [D loss: 0.999972] [G loss: 1.000068]\n",
            "14160 [D loss: 0.999971] [G loss: 1.000065]\n",
            "14180 [D loss: 0.999971] [G loss: 1.000062]\n",
            "14200 [D loss: 0.999971] [G loss: 1.000064]\n",
            "14220 [D loss: 0.999969] [G loss: 1.000067]\n",
            "14240 [D loss: 0.999972] [G loss: 1.000068]\n",
            "14260 [D loss: 0.999971] [G loss: 1.000065]\n",
            "14280 [D loss: 0.999968] [G loss: 1.000066]\n",
            "14300 [D loss: 0.999969] [G loss: 1.000065]\n",
            "14320 [D loss: 0.999972] [G loss: 1.000065]\n",
            "14340 [D loss: 0.999970] [G loss: 1.000067]\n",
            "14360 [D loss: 0.999971] [G loss: 1.000063]\n",
            "14380 [D loss: 0.999970] [G loss: 1.000066]\n",
            "14400 [D loss: 0.999969] [G loss: 1.000064]\n",
            "14420 [D loss: 0.999971] [G loss: 1.000068]\n",
            "14440 [D loss: 0.999970] [G loss: 1.000064]\n",
            "14460 [D loss: 0.999970] [G loss: 1.000065]\n",
            "14480 [D loss: 0.999972] [G loss: 1.000065]\n",
            "14500 [D loss: 0.999971] [G loss: 1.000064]\n",
            "14520 [D loss: 0.999972] [G loss: 1.000066]\n",
            "14540 [D loss: 0.999971] [G loss: 1.000065]\n",
            "14560 [D loss: 0.999969] [G loss: 1.000064]\n",
            "14580 [D loss: 0.999971] [G loss: 1.000066]\n",
            "14600 [D loss: 0.999971] [G loss: 1.000064]\n",
            "14620 [D loss: 0.999970] [G loss: 1.000062]\n",
            "14640 [D loss: 0.999969] [G loss: 1.000065]\n",
            "14660 [D loss: 0.999969] [G loss: 1.000064]\n",
            "14680 [D loss: 0.999969] [G loss: 1.000064]\n",
            "14700 [D loss: 0.999970] [G loss: 1.000063]\n",
            "14720 [D loss: 0.999972] [G loss: 1.000066]\n",
            "14740 [D loss: 0.999971] [G loss: 1.000066]\n",
            "14760 [D loss: 0.999970] [G loss: 1.000067]\n",
            "14780 [D loss: 0.999972] [G loss: 1.000067]\n",
            "14800 [D loss: 0.999969] [G loss: 1.000066]\n",
            "14820 [D loss: 0.999973] [G loss: 1.000065]\n",
            "14840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "14860 [D loss: 0.999971] [G loss: 1.000067]\n",
            "14880 [D loss: 0.999970] [G loss: 1.000062]\n",
            "14900 [D loss: 0.999971] [G loss: 1.000062]\n",
            "14920 [D loss: 0.999969] [G loss: 1.000063]\n",
            "14940 [D loss: 0.999971] [G loss: 1.000068]\n",
            "14960 [D loss: 0.999973] [G loss: 1.000065]\n",
            "14980 [D loss: 0.999972] [G loss: 1.000065]\n",
            "15000 [D loss: 0.999970] [G loss: 1.000064]\n",
            "15020 [D loss: 0.999971] [G loss: 1.000067]\n",
            "15040 [D loss: 0.999971] [G loss: 1.000064]\n",
            "15060 [D loss: 0.999971] [G loss: 1.000063]\n",
            "15080 [D loss: 0.999967] [G loss: 1.000069]\n",
            "15100 [D loss: 0.999971] [G loss: 1.000066]\n",
            "15120 [D loss: 0.999972] [G loss: 1.000066]\n",
            "15140 [D loss: 0.999971] [G loss: 1.000065]\n",
            "15160 [D loss: 0.999969] [G loss: 1.000063]\n",
            "15180 [D loss: 0.999971] [G loss: 1.000065]\n",
            "15200 [D loss: 0.999971] [G loss: 1.000066]\n",
            "15220 [D loss: 0.999969] [G loss: 1.000065]\n",
            "15240 [D loss: 0.999970] [G loss: 1.000066]\n",
            "15260 [D loss: 0.999972] [G loss: 1.000066]\n",
            "15280 [D loss: 0.999971] [G loss: 1.000067]\n",
            "15300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "15320 [D loss: 0.999972] [G loss: 1.000064]\n",
            "15340 [D loss: 0.999970] [G loss: 1.000067]\n",
            "15360 [D loss: 0.999972] [G loss: 1.000064]\n",
            "15380 [D loss: 0.999970] [G loss: 1.000067]\n",
            "15400 [D loss: 0.999971] [G loss: 1.000064]\n",
            "15420 [D loss: 0.999971] [G loss: 1.000067]\n",
            "15440 [D loss: 0.999973] [G loss: 1.000066]\n",
            "15460 [D loss: 0.999973] [G loss: 1.000065]\n",
            "15480 [D loss: 0.999969] [G loss: 1.000065]\n",
            "15500 [D loss: 0.999968] [G loss: 1.000066]\n",
            "15520 [D loss: 0.999969] [G loss: 1.000065]\n",
            "15540 [D loss: 0.999970] [G loss: 1.000066]\n",
            "15560 [D loss: 0.999971] [G loss: 1.000065]\n",
            "15580 [D loss: 0.999969] [G loss: 1.000065]\n",
            "15600 [D loss: 0.999971] [G loss: 1.000067]\n",
            "15620 [D loss: 0.999970] [G loss: 1.000062]\n",
            "15640 [D loss: 0.999972] [G loss: 1.000059]\n",
            "15660 [D loss: 0.999971] [G loss: 1.000064]\n",
            "15680 [D loss: 0.999972] [G loss: 1.000062]\n",
            "15700 [D loss: 0.999969] [G loss: 1.000067]\n",
            "15720 [D loss: 0.999970] [G loss: 1.000060]\n",
            "15740 [D loss: 0.999970] [G loss: 1.000065]\n",
            "15760 [D loss: 0.999971] [G loss: 1.000065]\n",
            "15780 [D loss: 0.999971] [G loss: 1.000064]\n",
            "15800 [D loss: 0.999971] [G loss: 1.000066]\n",
            "15820 [D loss: 0.999972] [G loss: 1.000067]\n",
            "15840 [D loss: 0.999969] [G loss: 1.000064]\n",
            "15860 [D loss: 0.999970] [G loss: 1.000066]\n",
            "15880 [D loss: 0.999970] [G loss: 1.000064]\n",
            "15900 [D loss: 0.999972] [G loss: 1.000064]\n",
            "15920 [D loss: 0.999972] [G loss: 1.000065]\n",
            "15940 [D loss: 0.999971] [G loss: 1.000068]\n",
            "15960 [D loss: 0.999970] [G loss: 1.000065]\n",
            "15980 [D loss: 0.999970] [G loss: 1.000067]\n",
            "16000 [D loss: 0.999971] [G loss: 1.000066]\n",
            "16020 [D loss: 0.999970] [G loss: 1.000065]\n",
            "16040 [D loss: 0.999969] [G loss: 1.000068]\n",
            "16060 [D loss: 0.999968] [G loss: 1.000067]\n",
            "16080 [D loss: 0.999968] [G loss: 1.000064]\n",
            "16100 [D loss: 0.999973] [G loss: 1.000066]\n",
            "16120 [D loss: 0.999972] [G loss: 1.000061]\n",
            "16140 [D loss: 0.999973] [G loss: 1.000067]\n",
            "16160 [D loss: 0.999973] [G loss: 1.000066]\n",
            "16180 [D loss: 0.999969] [G loss: 1.000065]\n",
            "16200 [D loss: 0.999971] [G loss: 1.000064]\n",
            "16220 [D loss: 0.999971] [G loss: 1.000065]\n",
            "16240 [D loss: 0.999969] [G loss: 1.000064]\n",
            "16260 [D loss: 0.999971] [G loss: 1.000063]\n",
            "16280 [D loss: 0.999971] [G loss: 1.000065]\n",
            "16300 [D loss: 0.999969] [G loss: 1.000066]\n",
            "16320 [D loss: 0.999970] [G loss: 1.000067]\n",
            "16340 [D loss: 0.999970] [G loss: 1.000066]\n",
            "16360 [D loss: 0.999972] [G loss: 1.000060]\n",
            "16380 [D loss: 0.999971] [G loss: 1.000062]\n",
            "16400 [D loss: 0.999972] [G loss: 1.000069]\n",
            "16420 [D loss: 0.999970] [G loss: 1.000064]\n",
            "16440 [D loss: 0.999970] [G loss: 1.000068]\n",
            "16460 [D loss: 0.999972] [G loss: 1.000064]\n",
            "16480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "16500 [D loss: 0.999972] [G loss: 1.000063]\n",
            "16520 [D loss: 0.999972] [G loss: 1.000066]\n",
            "16540 [D loss: 0.999967] [G loss: 1.000066]\n",
            "16560 [D loss: 0.999972] [G loss: 1.000063]\n",
            "16580 [D loss: 0.999969] [G loss: 1.000065]\n",
            "16600 [D loss: 0.999971] [G loss: 1.000064]\n",
            "16620 [D loss: 0.999969] [G loss: 1.000066]\n",
            "16640 [D loss: 0.999970] [G loss: 1.000061]\n",
            "16660 [D loss: 0.999972] [G loss: 1.000068]\n",
            "16680 [D loss: 0.999969] [G loss: 1.000066]\n",
            "16700 [D loss: 0.999969] [G loss: 1.000067]\n",
            "16720 [D loss: 0.999973] [G loss: 1.000065]\n",
            "16740 [D loss: 0.999970] [G loss: 1.000065]\n",
            "16760 [D loss: 0.999969] [G loss: 1.000066]\n",
            "16780 [D loss: 0.999970] [G loss: 1.000066]\n",
            "16800 [D loss: 0.999972] [G loss: 1.000063]\n",
            "16820 [D loss: 0.999971] [G loss: 1.000067]\n",
            "16840 [D loss: 0.999971] [G loss: 1.000067]\n",
            "16860 [D loss: 0.999971] [G loss: 1.000063]\n",
            "16880 [D loss: 0.999972] [G loss: 1.000065]\n",
            "16900 [D loss: 0.999971] [G loss: 1.000065]\n",
            "16920 [D loss: 0.999969] [G loss: 1.000067]\n",
            "16940 [D loss: 0.999970] [G loss: 1.000068]\n",
            "16960 [D loss: 0.999970] [G loss: 1.000059]\n",
            "16980 [D loss: 0.999969] [G loss: 1.000065]\n",
            "17000 [D loss: 0.999971] [G loss: 1.000069]\n",
            "17020 [D loss: 0.999969] [G loss: 1.000068]\n",
            "17040 [D loss: 0.999969] [G loss: 1.000065]\n",
            "17060 [D loss: 0.999969] [G loss: 1.000065]\n",
            "17080 [D loss: 0.999970] [G loss: 1.000067]\n",
            "17100 [D loss: 0.999972] [G loss: 1.000066]\n",
            "17120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "17140 [D loss: 0.999971] [G loss: 1.000064]\n",
            "17160 [D loss: 0.999971] [G loss: 1.000066]\n",
            "17180 [D loss: 0.999971] [G loss: 1.000066]\n",
            "17200 [D loss: 0.999970] [G loss: 1.000066]\n",
            "17220 [D loss: 0.999970] [G loss: 1.000065]\n",
            "17240 [D loss: 0.999972] [G loss: 1.000067]\n",
            "17260 [D loss: 0.999970] [G loss: 1.000067]\n",
            "17280 [D loss: 0.999971] [G loss: 1.000063]\n",
            "17300 [D loss: 0.999971] [G loss: 1.000064]\n",
            "17320 [D loss: 0.999971] [G loss: 1.000061]\n",
            "17340 [D loss: 0.999971] [G loss: 1.000065]\n",
            "17360 [D loss: 0.999971] [G loss: 1.000059]\n",
            "17380 [D loss: 0.999970] [G loss: 1.000067]\n",
            "17400 [D loss: 0.999972] [G loss: 1.000068]\n",
            "17420 [D loss: 0.999971] [G loss: 1.000068]\n",
            "17440 [D loss: 0.999971] [G loss: 1.000068]\n",
            "17460 [D loss: 0.999971] [G loss: 1.000062]\n",
            "17480 [D loss: 0.999969] [G loss: 1.000065]\n",
            "17500 [D loss: 0.999971] [G loss: 1.000065]\n",
            "17520 [D loss: 0.999971] [G loss: 1.000068]\n",
            "17540 [D loss: 0.999973] [G loss: 1.000063]\n",
            "17560 [D loss: 0.999972] [G loss: 1.000067]\n",
            "17580 [D loss: 0.999970] [G loss: 1.000066]\n",
            "17600 [D loss: 0.999971] [G loss: 1.000063]\n",
            "17620 [D loss: 0.999973] [G loss: 1.000064]\n",
            "17640 [D loss: 0.999972] [G loss: 1.000062]\n",
            "17660 [D loss: 0.999970] [G loss: 1.000066]\n",
            "17680 [D loss: 0.999972] [G loss: 1.000064]\n",
            "17700 [D loss: 0.999968] [G loss: 1.000067]\n",
            "17720 [D loss: 0.999971] [G loss: 1.000061]\n",
            "17740 [D loss: 0.999971] [G loss: 1.000065]\n",
            "17760 [D loss: 0.999971] [G loss: 1.000062]\n",
            "17780 [D loss: 0.999970] [G loss: 1.000066]\n",
            "17800 [D loss: 0.999972] [G loss: 1.000065]\n",
            "17820 [D loss: 0.999970] [G loss: 1.000062]\n",
            "17840 [D loss: 0.999971] [G loss: 1.000065]\n",
            "17860 [D loss: 0.999972] [G loss: 1.000064]\n",
            "17880 [D loss: 0.999971] [G loss: 1.000065]\n",
            "17900 [D loss: 0.999972] [G loss: 1.000067]\n",
            "17920 [D loss: 0.999969] [G loss: 1.000065]\n",
            "17940 [D loss: 0.999971] [G loss: 1.000066]\n",
            "17960 [D loss: 0.999970] [G loss: 1.000066]\n",
            "17980 [D loss: 0.999972] [G loss: 1.000067]\n",
            "18000 [D loss: 0.999971] [G loss: 1.000066]\n",
            "18020 [D loss: 0.999970] [G loss: 1.000061]\n",
            "18040 [D loss: 0.999973] [G loss: 1.000060]\n",
            "18060 [D loss: 0.999972] [G loss: 1.000068]\n",
            "18080 [D loss: 0.999969] [G loss: 1.000064]\n",
            "18100 [D loss: 0.999970] [G loss: 1.000065]\n",
            "18120 [D loss: 0.999971] [G loss: 1.000068]\n",
            "18140 [D loss: 0.999971] [G loss: 1.000064]\n",
            "18160 [D loss: 0.999970] [G loss: 1.000066]\n",
            "18180 [D loss: 0.999970] [G loss: 1.000063]\n",
            "18200 [D loss: 0.999972] [G loss: 1.000064]\n",
            "18220 [D loss: 0.999972] [G loss: 1.000066]\n",
            "18240 [D loss: 0.999968] [G loss: 1.000064]\n",
            "18260 [D loss: 0.999971] [G loss: 1.000068]\n",
            "18280 [D loss: 0.999971] [G loss: 1.000064]\n",
            "18300 [D loss: 0.999971] [G loss: 1.000063]\n",
            "18320 [D loss: 0.999971] [G loss: 1.000064]\n",
            "18340 [D loss: 0.999969] [G loss: 1.000063]\n",
            "18360 [D loss: 0.999970] [G loss: 1.000067]\n",
            "18380 [D loss: 0.999971] [G loss: 1.000064]\n",
            "18400 [D loss: 0.999970] [G loss: 1.000066]\n",
            "18420 [D loss: 0.999970] [G loss: 1.000066]\n",
            "18440 [D loss: 0.999969] [G loss: 1.000065]\n",
            "18460 [D loss: 0.999971] [G loss: 1.000063]\n",
            "18480 [D loss: 0.999970] [G loss: 1.000061]\n",
            "18500 [D loss: 0.999972] [G loss: 1.000066]\n",
            "18520 [D loss: 0.999970] [G loss: 1.000067]\n",
            "18540 [D loss: 0.999970] [G loss: 1.000067]\n",
            "18560 [D loss: 0.999970] [G loss: 1.000065]\n",
            "18580 [D loss: 0.999973] [G loss: 1.000064]\n",
            "18600 [D loss: 0.999970] [G loss: 1.000067]\n",
            "18620 [D loss: 0.999971] [G loss: 1.000065]\n",
            "18640 [D loss: 0.999970] [G loss: 1.000064]\n",
            "18660 [D loss: 0.999970] [G loss: 1.000063]\n",
            "18680 [D loss: 0.999972] [G loss: 1.000068]\n",
            "18700 [D loss: 0.999970] [G loss: 1.000064]\n",
            "18720 [D loss: 0.999971] [G loss: 1.000067]\n",
            "18740 [D loss: 0.999969] [G loss: 1.000067]\n",
            "18760 [D loss: 0.999971] [G loss: 1.000063]\n",
            "18780 [D loss: 0.999971] [G loss: 1.000068]\n",
            "18800 [D loss: 0.999970] [G loss: 1.000065]\n",
            "18820 [D loss: 0.999971] [G loss: 1.000063]\n",
            "18840 [D loss: 0.999971] [G loss: 1.000062]\n",
            "18860 [D loss: 0.999971] [G loss: 1.000064]\n",
            "18880 [D loss: 0.999971] [G loss: 1.000065]\n",
            "18900 [D loss: 0.999972] [G loss: 1.000066]\n",
            "18920 [D loss: 0.999969] [G loss: 1.000066]\n",
            "18940 [D loss: 0.999969] [G loss: 1.000064]\n",
            "18960 [D loss: 0.999970] [G loss: 1.000066]\n",
            "18980 [D loss: 0.999969] [G loss: 1.000066]\n",
            "19000 [D loss: 0.999969] [G loss: 1.000067]\n",
            "19020 [D loss: 0.999969] [G loss: 1.000067]\n",
            "19040 [D loss: 0.999971] [G loss: 1.000066]\n",
            "19060 [D loss: 0.999972] [G loss: 1.000066]\n",
            "19080 [D loss: 0.999972] [G loss: 1.000062]\n",
            "19100 [D loss: 0.999970] [G loss: 1.000067]\n",
            "19120 [D loss: 0.999971] [G loss: 1.000066]\n",
            "19140 [D loss: 0.999970] [G loss: 1.000064]\n",
            "19160 [D loss: 0.999971] [G loss: 1.000065]\n",
            "19180 [D loss: 0.999972] [G loss: 1.000064]\n",
            "19200 [D loss: 0.999972] [G loss: 1.000066]\n",
            "19220 [D loss: 0.999972] [G loss: 1.000067]\n",
            "19240 [D loss: 0.999972] [G loss: 1.000065]\n",
            "19260 [D loss: 0.999969] [G loss: 1.000063]\n",
            "19280 [D loss: 0.999970] [G loss: 1.000064]\n",
            "19300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "19320 [D loss: 0.999971] [G loss: 1.000065]\n",
            "19340 [D loss: 0.999973] [G loss: 1.000063]\n",
            "19360 [D loss: 0.999971] [G loss: 1.000064]\n",
            "19380 [D loss: 0.999971] [G loss: 1.000065]\n",
            "19400 [D loss: 0.999971] [G loss: 1.000065]\n",
            "19420 [D loss: 0.999972] [G loss: 1.000066]\n",
            "19440 [D loss: 0.999971] [G loss: 1.000066]\n",
            "19460 [D loss: 0.999969] [G loss: 1.000064]\n",
            "19480 [D loss: 0.999971] [G loss: 1.000066]\n",
            "19500 [D loss: 0.999969] [G loss: 1.000065]\n",
            "19520 [D loss: 0.999969] [G loss: 1.000063]\n",
            "19540 [D loss: 0.999972] [G loss: 1.000064]\n",
            "19560 [D loss: 0.999971] [G loss: 1.000065]\n",
            "19580 [D loss: 0.999968] [G loss: 1.000067]\n",
            "19600 [D loss: 0.999970] [G loss: 1.000068]\n",
            "19620 [D loss: 0.999973] [G loss: 1.000063]\n",
            "19640 [D loss: 0.999971] [G loss: 1.000064]\n",
            "19660 [D loss: 0.999968] [G loss: 1.000065]\n",
            "19680 [D loss: 0.999972] [G loss: 1.000061]\n",
            "19700 [D loss: 0.999972] [G loss: 1.000064]\n",
            "19720 [D loss: 0.999972] [G loss: 1.000065]\n",
            "19740 [D loss: 0.999970] [G loss: 1.000067]\n",
            "19760 [D loss: 0.999972] [G loss: 1.000065]\n",
            "19780 [D loss: 0.999972] [G loss: 1.000065]\n",
            "19800 [D loss: 0.999973] [G loss: 1.000067]\n",
            "19820 [D loss: 0.999967] [G loss: 1.000066]\n",
            "19840 [D loss: 0.999974] [G loss: 1.000065]\n",
            "19860 [D loss: 0.999971] [G loss: 1.000064]\n",
            "19880 [D loss: 0.999971] [G loss: 1.000063]\n",
            "19900 [D loss: 0.999972] [G loss: 1.000067]\n",
            "19920 [D loss: 0.999968] [G loss: 1.000064]\n",
            "19940 [D loss: 0.999971] [G loss: 1.000066]\n",
            "19960 [D loss: 0.999970] [G loss: 1.000067]\n",
            "19980 [D loss: 0.999972] [G loss: 1.000066]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}