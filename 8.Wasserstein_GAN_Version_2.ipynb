{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "8_WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code based on https://github.com/eriklindernoren/Keras-GAN/tree/master/wgan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "6ac3806f-3234-4823-b95a-9f191dfb7e8a"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c555a0-0c1d-4a40-d5a7-84c120c381da"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLasjF73EZg1",
        "outputId": "114edaab-d5f9-452e-c12f-6ab60e111851"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "e47efccc-0170-4b6c-8fc1-1786494f81ef"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## WGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBLIDqvQExmx"
      },
      "source": [
        "class WGAN():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        self.clip_value = 0.01\n",
        "        optimizer = RMSprop(lr=0.00005)\n",
        "\n",
        "        # Build and compile the critic\n",
        "        self.critic = self.build_critic()\n",
        "        self.critic.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generated imgs\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.critic.trainable = False\n",
        "\n",
        "        # The critic takes generated images as input and determines validity\n",
        "        valid = self.critic(img)\n",
        "\n",
        "        # The combined model  (stacked generator and critic)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 12 * 12, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 12x12 image\n",
        "        # n_nodes = 128 * 12 * 12\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((12, 12, 128)))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # generate\n",
        "        # model.add(Conv2D(self.channels, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print(\"generator\")\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_critic(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        print(\"critic\")\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake = np.ones((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------\n",
        "                #  Train Critic\n",
        "                # ---------------\n",
        "\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "                \n",
        "                # Sample noise as generator input\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "                # Generate a batch of new images\n",
        "                gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "                # Train the critic\n",
        "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
        "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
        "\n",
        "                # Clip critic weights\n",
        "                for l in self.critic.layers:\n",
        "                    weights = l.get_weights()\n",
        "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                    l.set_weights(weights)\n",
        "\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/epoch_%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TDVWeIcCXig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae213987-a382-415f-bc9b-d7740c033d44"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    wgan = WGAN()\n",
        "    wgan.train(epochs=20000, batch_size=128, sample_interval=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "critic\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "generator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.999843] [G loss: 0.999877]\n",
            "20 [D loss: 0.999940] [G loss: 1.000096]\n",
            "40 [D loss: 0.999971] [G loss: 1.000065]\n",
            "60 [D loss: 0.999970] [G loss: 1.000068]\n",
            "80 [D loss: 0.999969] [G loss: 1.000066]\n",
            "100 [D loss: 0.999967] [G loss: 1.000067]\n",
            "120 [D loss: 0.999970] [G loss: 1.000065]\n",
            "140 [D loss: 0.999968] [G loss: 1.000064]\n",
            "160 [D loss: 0.999968] [G loss: 1.000061]\n",
            "180 [D loss: 0.999970] [G loss: 1.000061]\n",
            "200 [D loss: 1.000011] [G loss: 1.000011]\n",
            "220 [D loss: 1.000037] [G loss: 0.999993]\n",
            "240 [D loss: 0.999969] [G loss: 1.000064]\n",
            "260 [D loss: 0.999973] [G loss: 1.000074]\n",
            "280 [D loss: 0.999971] [G loss: 1.000062]\n",
            "300 [D loss: 0.999969] [G loss: 1.000057]\n",
            "320 [D loss: 0.999969] [G loss: 1.000055]\n",
            "340 [D loss: 0.999968] [G loss: 1.000059]\n",
            "360 [D loss: 0.999974] [G loss: 1.000056]\n",
            "380 [D loss: 0.999972] [G loss: 1.000057]\n",
            "400 [D loss: 0.999976] [G loss: 1.000049]\n",
            "420 [D loss: 0.999970] [G loss: 1.000053]\n",
            "440 [D loss: 0.999974] [G loss: 1.000044]\n",
            "460 [D loss: 0.999975] [G loss: 1.000043]\n",
            "480 [D loss: 0.999976] [G loss: 1.000038]\n",
            "500 [D loss: 0.999971] [G loss: 1.000045]\n",
            "520 [D loss: 0.999972] [G loss: 1.000042]\n",
            "540 [D loss: 0.999973] [G loss: 1.000042]\n",
            "560 [D loss: 0.999972] [G loss: 1.000041]\n",
            "580 [D loss: 0.999969] [G loss: 1.000046]\n",
            "600 [D loss: 0.999968] [G loss: 1.000043]\n",
            "620 [D loss: 0.999974] [G loss: 1.000035]\n",
            "640 [D loss: 0.999970] [G loss: 1.000043]\n",
            "660 [D loss: 0.999969] [G loss: 1.000043]\n",
            "680 [D loss: 0.999971] [G loss: 1.000043]\n",
            "700 [D loss: 0.999970] [G loss: 1.000046]\n",
            "720 [D loss: 0.999968] [G loss: 1.000053]\n",
            "740 [D loss: 0.999967] [G loss: 1.000056]\n",
            "760 [D loss: 0.999968] [G loss: 1.000061]\n",
            "780 [D loss: 0.999970] [G loss: 1.000054]\n",
            "800 [D loss: 0.999970] [G loss: 1.000058]\n",
            "820 [D loss: 0.999966] [G loss: 1.000059]\n",
            "840 [D loss: 0.999970] [G loss: 1.000064]\n",
            "860 [D loss: 0.999970] [G loss: 1.000065]\n",
            "880 [D loss: 0.999971] [G loss: 1.000059]\n",
            "900 [D loss: 0.999970] [G loss: 1.000061]\n",
            "920 [D loss: 0.999967] [G loss: 1.000063]\n",
            "940 [D loss: 0.999968] [G loss: 1.000064]\n",
            "960 [D loss: 0.999969] [G loss: 1.000064]\n",
            "980 [D loss: 0.999970] [G loss: 1.000060]\n",
            "1000 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1020 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1040 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1060 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1080 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1100 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1120 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1140 [D loss: 0.999971] [G loss: 1.000056]\n",
            "1160 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1180 [D loss: 0.999966] [G loss: 1.000065]\n",
            "1200 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1220 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1240 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1260 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1280 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1300 [D loss: 0.999969] [G loss: 1.000065]\n",
            "1320 [D loss: 0.999970] [G loss: 1.000064]\n",
            "1340 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1360 [D loss: 0.999967] [G loss: 1.000066]\n",
            "1380 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1400 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1420 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1440 [D loss: 0.999968] [G loss: 1.000066]\n",
            "1460 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1500 [D loss: 0.999973] [G loss: 1.000065]\n",
            "1520 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1540 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1560 [D loss: 0.999972] [G loss: 1.000064]\n",
            "1580 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1600 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1620 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1640 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1660 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1680 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1700 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1720 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1740 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1760 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1780 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1800 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1820 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1840 [D loss: 0.999969] [G loss: 1.000061]\n",
            "1860 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1880 [D loss: 0.999968] [G loss: 1.000062]\n",
            "1900 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1920 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1940 [D loss: 0.999971] [G loss: 1.000063]\n",
            "1960 [D loss: 0.999973] [G loss: 1.000067]\n",
            "1980 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2000 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2020 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2060 [D loss: 0.999973] [G loss: 1.000059]\n",
            "2080 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2100 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2120 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2140 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2160 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2180 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2200 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2220 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2240 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2260 [D loss: 0.999970] [G loss: 1.000058]\n",
            "2280 [D loss: 0.999972] [G loss: 1.000062]\n",
            "2300 [D loss: 0.999972] [G loss: 1.000067]\n",
            "2320 [D loss: 0.999972] [G loss: 1.000059]\n",
            "2340 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2380 [D loss: 0.999971] [G loss: 1.000059]\n",
            "2400 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2420 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2440 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2460 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2500 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2520 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2540 [D loss: 0.999970] [G loss: 1.000066]\n",
            "2560 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2580 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2600 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2620 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2640 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2660 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2680 [D loss: 0.999972] [G loss: 1.000059]\n",
            "2700 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2720 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2740 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2760 [D loss: 0.999970] [G loss: 1.000062]\n",
            "2780 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2800 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2820 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2840 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2860 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2880 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2900 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2920 [D loss: 0.999973] [G loss: 1.000070]\n",
            "2940 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2960 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2980 [D loss: 0.999971] [G loss: 1.000067]\n",
            "3000 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3020 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3040 [D loss: 0.999972] [G loss: 1.000069]\n",
            "3060 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3080 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3100 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3120 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3140 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3160 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3180 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3200 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3220 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3240 [D loss: 0.999971] [G loss: 1.000068]\n",
            "3260 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3280 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3300 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3320 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3340 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3360 [D loss: 0.999968] [G loss: 1.000064]\n",
            "3380 [D loss: 0.999972] [G loss: 1.000061]\n",
            "3400 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3420 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3440 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3460 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3480 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3500 [D loss: 0.999968] [G loss: 1.000067]\n",
            "3520 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3560 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3580 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3600 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3620 [D loss: 0.999969] [G loss: 1.000062]\n",
            "3640 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3660 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3680 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3700 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3720 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3740 [D loss: 0.999970] [G loss: 1.000062]\n",
            "3760 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3780 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3800 [D loss: 0.999973] [G loss: 1.000068]\n",
            "3820 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3840 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3860 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3880 [D loss: 0.999971] [G loss: 1.000061]\n",
            "3900 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3920 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3940 [D loss: 0.999970] [G loss: 1.000067]\n",
            "3960 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3980 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4000 [D loss: 0.999968] [G loss: 1.000067]\n",
            "4020 [D loss: 0.999971] [G loss: 1.000062]\n",
            "4040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4060 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4080 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4100 [D loss: 0.999973] [G loss: 1.000065]\n",
            "4120 [D loss: 0.999968] [G loss: 1.000067]\n",
            "4140 [D loss: 0.999971] [G loss: 1.000063]\n",
            "4160 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4180 [D loss: 0.999972] [G loss: 1.000066]\n",
            "4200 [D loss: 0.999968] [G loss: 1.000061]\n",
            "4220 [D loss: 0.999972] [G loss: 1.000066]\n",
            "4240 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4260 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4280 [D loss: 0.999970] [G loss: 1.000068]\n",
            "4300 [D loss: 0.999969] [G loss: 1.000065]\n",
            "4320 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4340 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4360 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4380 [D loss: 0.999969] [G loss: 1.000065]\n",
            "4400 [D loss: 0.999972] [G loss: 1.000060]\n",
            "4420 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4440 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4460 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4480 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4500 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4520 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4540 [D loss: 0.999969] [G loss: 1.000069]\n",
            "4560 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4580 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4600 [D loss: 0.999968] [G loss: 1.000065]\n",
            "4620 [D loss: 0.999969] [G loss: 1.000064]\n",
            "4640 [D loss: 0.999970] [G loss: 1.000062]\n",
            "4660 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4680 [D loss: 0.999973] [G loss: 1.000064]\n",
            "4700 [D loss: 0.999973] [G loss: 1.000063]\n",
            "4720 [D loss: 0.999968] [G loss: 1.000067]\n",
            "4740 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4760 [D loss: 0.999972] [G loss: 1.000067]\n",
            "4780 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4800 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4820 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4840 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4860 [D loss: 0.999966] [G loss: 1.000066]\n",
            "4880 [D loss: 0.999971] [G loss: 1.000068]\n",
            "4900 [D loss: 0.999970] [G loss: 1.000068]\n",
            "4920 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4940 [D loss: 0.999970] [G loss: 1.000068]\n",
            "4960 [D loss: 0.999973] [G loss: 1.000063]\n",
            "4980 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5000 [D loss: 0.999968] [G loss: 1.000063]\n",
            "5020 [D loss: 0.999969] [G loss: 1.000064]\n",
            "5040 [D loss: 0.999969] [G loss: 1.000067]\n",
            "5060 [D loss: 0.999969] [G loss: 1.000061]\n",
            "5080 [D loss: 0.999967] [G loss: 1.000064]\n",
            "5100 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5120 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5140 [D loss: 0.999972] [G loss: 1.000060]\n",
            "5160 [D loss: 0.999970] [G loss: 1.000068]\n",
            "5180 [D loss: 0.999969] [G loss: 1.000067]\n",
            "5200 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5220 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5240 [D loss: 0.999972] [G loss: 1.000063]\n",
            "5260 [D loss: 0.999973] [G loss: 1.000063]\n",
            "5280 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5320 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5340 [D loss: 0.999972] [G loss: 1.000061]\n",
            "5360 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5380 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5400 [D loss: 0.999969] [G loss: 1.000065]\n",
            "5420 [D loss: 0.999971] [G loss: 1.000068]\n",
            "5440 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5460 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5480 [D loss: 0.999971] [G loss: 1.000061]\n",
            "5500 [D loss: 0.999969] [G loss: 1.000063]\n",
            "5520 [D loss: 0.999971] [G loss: 1.000067]\n",
            "5540 [D loss: 0.999968] [G loss: 1.000066]\n",
            "5560 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5580 [D loss: 0.999968] [G loss: 1.000066]\n",
            "5600 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5620 [D loss: 0.999973] [G loss: 1.000064]\n",
            "5640 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5660 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5680 [D loss: 0.999970] [G loss: 1.000063]\n",
            "5700 [D loss: 0.999969] [G loss: 1.000067]\n",
            "5720 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5740 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5760 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5780 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5800 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5820 [D loss: 0.999968] [G loss: 1.000065]\n",
            "5840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "5860 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5880 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5900 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5920 [D loss: 0.999971] [G loss: 1.000057]\n",
            "5940 [D loss: 0.999972] [G loss: 1.000063]\n",
            "5960 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5980 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6000 [D loss: 0.999970] [G loss: 1.000068]\n",
            "6020 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6040 [D loss: 0.999969] [G loss: 1.000067]\n",
            "6060 [D loss: 0.999971] [G loss: 1.000061]\n",
            "6080 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6100 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6120 [D loss: 0.999969] [G loss: 1.000063]\n",
            "6140 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6160 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6180 [D loss: 0.999970] [G loss: 1.000068]\n",
            "6200 [D loss: 0.999968] [G loss: 1.000063]\n",
            "6220 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6240 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6260 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6280 [D loss: 0.999969] [G loss: 1.000065]\n",
            "6300 [D loss: 0.999969] [G loss: 1.000064]\n",
            "6320 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6340 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6360 [D loss: 0.999971] [G loss: 1.000059]\n",
            "6380 [D loss: 0.999969] [G loss: 1.000064]\n",
            "6400 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6420 [D loss: 0.999972] [G loss: 1.000067]\n",
            "6440 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6460 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6480 [D loss: 0.999969] [G loss: 1.000063]\n",
            "6500 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6520 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6540 [D loss: 0.999968] [G loss: 1.000066]\n",
            "6560 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6580 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6600 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6620 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6640 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6660 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6680 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6700 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6720 [D loss: 0.999970] [G loss: 1.000062]\n",
            "6740 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6760 [D loss: 0.999972] [G loss: 1.000063]\n",
            "6780 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6800 [D loss: 0.999971] [G loss: 1.000068]\n",
            "6820 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6840 [D loss: 0.999969] [G loss: 1.000064]\n",
            "6860 [D loss: 0.999970] [G loss: 1.000063]\n",
            "6880 [D loss: 0.999971] [G loss: 1.000062]\n",
            "6900 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6920 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6940 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6960 [D loss: 0.999967] [G loss: 1.000066]\n",
            "6980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7000 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7020 [D loss: 0.999970] [G loss: 1.000063]\n",
            "7040 [D loss: 0.999971] [G loss: 1.000064]\n",
            "7060 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7080 [D loss: 0.999970] [G loss: 1.000065]\n",
            "7100 [D loss: 0.999968] [G loss: 1.000064]\n",
            "7120 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7140 [D loss: 0.999973] [G loss: 1.000066]\n",
            "7160 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7180 [D loss: 0.999969] [G loss: 1.000063]\n",
            "7200 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7220 [D loss: 0.999971] [G loss: 1.000063]\n",
            "7240 [D loss: 0.999971] [G loss: 1.000064]\n",
            "7260 [D loss: 0.999970] [G loss: 1.000061]\n",
            "7280 [D loss: 0.999972] [G loss: 1.000063]\n",
            "7300 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7320 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7340 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7360 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7380 [D loss: 0.999970] [G loss: 1.000067]\n",
            "7400 [D loss: 0.999969] [G loss: 1.000063]\n",
            "7420 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7440 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7460 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7480 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7500 [D loss: 0.999969] [G loss: 1.000062]\n",
            "7520 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7540 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7560 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7580 [D loss: 0.999971] [G loss: 1.000065]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51thV6AAYkps",
        "outputId": "b1253b21-8a8c-4275-d62f-ca115d94d7ae"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    wgan = WGAN()\n",
        "    wgan.train(epochs=10000, batch_size=128, sample_interval=400)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "critic\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "generator\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.999854] [G loss: 1.000287]\n",
            "20 [D loss: 0.999944] [G loss: 1.000092]\n",
            "40 [D loss: 0.999970] [G loss: 1.000063]\n",
            "60 [D loss: 0.999970] [G loss: 1.000068]\n",
            "80 [D loss: 0.999970] [G loss: 1.000064]\n",
            "100 [D loss: 0.999973] [G loss: 1.000064]\n",
            "120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "140 [D loss: 0.999969] [G loss: 1.000063]\n",
            "160 [D loss: 0.999970] [G loss: 1.000065]\n",
            "180 [D loss: 0.999969] [G loss: 1.000063]\n",
            "200 [D loss: 0.999971] [G loss: 1.000062]\n",
            "220 [D loss: 0.999970] [G loss: 1.000060]\n",
            "240 [D loss: 0.999969] [G loss: 1.000061]\n",
            "260 [D loss: 0.999971] [G loss: 1.000053]\n",
            "280 [D loss: 0.999973] [G loss: 1.000059]\n",
            "300 [D loss: 0.999977] [G loss: 1.000050]\n",
            "320 [D loss: 0.999979] [G loss: 1.000035]\n",
            "340 [D loss: 0.999989] [G loss: 1.000009]\n",
            "360 [D loss: 0.999989] [G loss: 1.000014]\n",
            "380 [D loss: 0.999993] [G loss: 1.000008]\n",
            "400 [D loss: 1.000024] [G loss: 0.999925]\n",
            "420 [D loss: 1.000012] [G loss: 0.999966]\n",
            "440 [D loss: 1.000013] [G loss: 0.999942]\n",
            "460 [D loss: 1.000001] [G loss: 0.999948]\n",
            "480 [D loss: 1.000003] [G loss: 0.999944]\n",
            "500 [D loss: 0.999993] [G loss: 0.999978]\n",
            "520 [D loss: 0.999973] [G loss: 1.000038]\n",
            "540 [D loss: 0.999972] [G loss: 1.000039]\n",
            "560 [D loss: 0.999974] [G loss: 1.000031]\n",
            "580 [D loss: 0.999974] [G loss: 1.000045]\n",
            "600 [D loss: 0.999965] [G loss: 1.000048]\n",
            "620 [D loss: 0.999968] [G loss: 1.000048]\n",
            "640 [D loss: 0.999970] [G loss: 1.000050]\n",
            "660 [D loss: 0.999967] [G loss: 1.000058]\n",
            "680 [D loss: 0.999971] [G loss: 1.000056]\n",
            "700 [D loss: 0.999968] [G loss: 1.000057]\n",
            "720 [D loss: 0.999969] [G loss: 1.000057]\n",
            "740 [D loss: 0.999970] [G loss: 1.000058]\n",
            "760 [D loss: 0.999965] [G loss: 1.000061]\n",
            "780 [D loss: 0.999971] [G loss: 1.000057]\n",
            "800 [D loss: 0.999970] [G loss: 1.000058]\n",
            "820 [D loss: 0.999968] [G loss: 1.000063]\n",
            "840 [D loss: 0.999968] [G loss: 1.000062]\n",
            "860 [D loss: 0.999971] [G loss: 1.000061]\n",
            "880 [D loss: 0.999969] [G loss: 1.000064]\n",
            "900 [D loss: 0.999970] [G loss: 1.000062]\n",
            "920 [D loss: 0.999968] [G loss: 1.000063]\n",
            "940 [D loss: 0.999970] [G loss: 1.000065]\n",
            "960 [D loss: 0.999972] [G loss: 1.000062]\n",
            "980 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1000 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1020 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1040 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1060 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1080 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1100 [D loss: 0.999972] [G loss: 1.000060]\n",
            "1120 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1140 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1160 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1180 [D loss: 0.999970] [G loss: 1.000057]\n",
            "1200 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1220 [D loss: 0.999971] [G loss: 1.000067]\n",
            "1240 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1260 [D loss: 0.999971] [G loss: 1.000061]\n",
            "1280 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1300 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1320 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1340 [D loss: 0.999969] [G loss: 1.000067]\n",
            "1360 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1380 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1400 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1420 [D loss: 0.999969] [G loss: 1.000062]\n",
            "1440 [D loss: 0.999971] [G loss: 1.000065]\n",
            "1460 [D loss: 0.999972] [G loss: 1.000062]\n",
            "1480 [D loss: 0.999971] [G loss: 1.000064]\n",
            "1500 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1520 [D loss: 0.999970] [G loss: 1.000059]\n",
            "1540 [D loss: 0.999968] [G loss: 1.000058]\n",
            "1560 [D loss: 0.999970] [G loss: 1.000066]\n",
            "1580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "1600 [D loss: 0.999968] [G loss: 1.000060]\n",
            "1620 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1640 [D loss: 0.999969] [G loss: 1.000066]\n",
            "1660 [D loss: 0.999970] [G loss: 1.000065]\n",
            "1680 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1700 [D loss: 0.999969] [G loss: 1.000059]\n",
            "1720 [D loss: 0.999969] [G loss: 1.000063]\n",
            "1740 [D loss: 0.999967] [G loss: 1.000064]\n",
            "1760 [D loss: 0.999971] [G loss: 1.000060]\n",
            "1780 [D loss: 0.999968] [G loss: 1.000064]\n",
            "1800 [D loss: 0.999972] [G loss: 1.000063]\n",
            "1820 [D loss: 0.999970] [G loss: 1.000061]\n",
            "1840 [D loss: 0.999970] [G loss: 1.000062]\n",
            "1860 [D loss: 0.999967] [G loss: 1.000065]\n",
            "1880 [D loss: 0.999970] [G loss: 1.000063]\n",
            "1900 [D loss: 0.999968] [G loss: 1.000061]\n",
            "1920 [D loss: 0.999968] [G loss: 1.000063]\n",
            "1940 [D loss: 0.999971] [G loss: 1.000062]\n",
            "1960 [D loss: 0.999972] [G loss: 1.000066]\n",
            "1980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2000 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2020 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2040 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2060 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2080 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2100 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2120 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2140 [D loss: 0.999970] [G loss: 1.000068]\n",
            "2160 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2180 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2200 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2220 [D loss: 0.999971] [G loss: 1.000065]\n",
            "2240 [D loss: 0.999970] [G loss: 1.000064]\n",
            "2260 [D loss: 0.999971] [G loss: 1.000063]\n",
            "2280 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2300 [D loss: 0.999969] [G loss: 1.000065]\n",
            "2320 [D loss: 0.999971] [G loss: 1.000058]\n",
            "2340 [D loss: 0.999971] [G loss: 1.000068]\n",
            "2360 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2380 [D loss: 0.999971] [G loss: 1.000062]\n",
            "2400 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2420 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2440 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2460 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2480 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2500 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2520 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "2560 [D loss: 0.999968] [G loss: 1.000066]\n",
            "2580 [D loss: 0.999968] [G loss: 1.000064]\n",
            "2600 [D loss: 0.999969] [G loss: 1.000067]\n",
            "2620 [D loss: 0.999970] [G loss: 1.000063]\n",
            "2640 [D loss: 0.999968] [G loss: 1.000068]\n",
            "2660 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2680 [D loss: 0.999972] [G loss: 1.000063]\n",
            "2700 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2720 [D loss: 0.999970] [G loss: 1.000065]\n",
            "2740 [D loss: 0.999968] [G loss: 1.000063]\n",
            "2760 [D loss: 0.999974] [G loss: 1.000065]\n",
            "2780 [D loss: 0.999971] [G loss: 1.000061]\n",
            "2800 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2820 [D loss: 0.999973] [G loss: 1.000066]\n",
            "2840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "2860 [D loss: 0.999972] [G loss: 1.000064]\n",
            "2880 [D loss: 0.999970] [G loss: 1.000067]\n",
            "2900 [D loss: 0.999969] [G loss: 1.000068]\n",
            "2920 [D loss: 0.999970] [G loss: 1.000060]\n",
            "2940 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2960 [D loss: 0.999969] [G loss: 1.000066]\n",
            "2980 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3000 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3020 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3040 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3060 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3080 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3100 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3120 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3140 [D loss: 0.999971] [G loss: 1.000062]\n",
            "3160 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3180 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3200 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3220 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3240 [D loss: 0.999969] [G loss: 1.000060]\n",
            "3260 [D loss: 0.999969] [G loss: 1.000066]\n",
            "3280 [D loss: 0.999971] [G loss: 1.000064]\n",
            "3300 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3320 [D loss: 0.999969] [G loss: 1.000065]\n",
            "3340 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3360 [D loss: 0.999969] [G loss: 1.000061]\n",
            "3380 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3400 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3420 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3440 [D loss: 0.999972] [G loss: 1.000065]\n",
            "3460 [D loss: 0.999970] [G loss: 1.000060]\n",
            "3480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "3500 [D loss: 0.999970] [G loss: 1.000069]\n",
            "3520 [D loss: 0.999968] [G loss: 1.000066]\n",
            "3540 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3560 [D loss: 0.999969] [G loss: 1.000068]\n",
            "3580 [D loss: 0.999969] [G loss: 1.000067]\n",
            "3600 [D loss: 0.999970] [G loss: 1.000061]\n",
            "3620 [D loss: 0.999968] [G loss: 1.000065]\n",
            "3640 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3660 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3680 [D loss: 0.999971] [G loss: 1.000060]\n",
            "3700 [D loss: 0.999971] [G loss: 1.000065]\n",
            "3720 [D loss: 0.999970] [G loss: 1.000066]\n",
            "3740 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3760 [D loss: 0.999969] [G loss: 1.000064]\n",
            "3780 [D loss: 0.999968] [G loss: 1.000064]\n",
            "3800 [D loss: 0.999972] [G loss: 1.000063]\n",
            "3820 [D loss: 0.999972] [G loss: 1.000060]\n",
            "3840 [D loss: 0.999972] [G loss: 1.000066]\n",
            "3860 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3880 [D loss: 0.999970] [G loss: 1.000063]\n",
            "3900 [D loss: 0.999971] [G loss: 1.000066]\n",
            "3920 [D loss: 0.999971] [G loss: 1.000063]\n",
            "3940 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3960 [D loss: 0.999970] [G loss: 1.000064]\n",
            "3980 [D loss: 0.999971] [G loss: 1.000062]\n",
            "4000 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4020 [D loss: 0.999971] [G loss: 1.000067]\n",
            "4040 [D loss: 0.999973] [G loss: 1.000062]\n",
            "4060 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4080 [D loss: 0.999970] [G loss: 1.000066]\n",
            "4100 [D loss: 0.999967] [G loss: 1.000067]\n",
            "4120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4140 [D loss: 0.999968] [G loss: 1.000065]\n",
            "4160 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4180 [D loss: 0.999970] [G loss: 1.000062]\n",
            "4200 [D loss: 0.999970] [G loss: 1.000063]\n",
            "4220 [D loss: 0.999969] [G loss: 1.000064]\n",
            "4240 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4260 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4280 [D loss: 0.999971] [G loss: 1.000068]\n",
            "4300 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4320 [D loss: 0.999973] [G loss: 1.000063]\n",
            "4340 [D loss: 0.999970] [G loss: 1.000067]\n",
            "4360 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4380 [D loss: 0.999971] [G loss: 1.000063]\n",
            "4400 [D loss: 0.999971] [G loss: 1.000064]\n",
            "4420 [D loss: 0.999973] [G loss: 1.000065]\n",
            "4440 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4460 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4480 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4500 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4520 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4540 [D loss: 0.999971] [G loss: 1.000068]\n",
            "4560 [D loss: 0.999971] [G loss: 1.000062]\n",
            "4580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "4600 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4620 [D loss: 0.999972] [G loss: 1.000063]\n",
            "4640 [D loss: 0.999969] [G loss: 1.000063]\n",
            "4660 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4680 [D loss: 0.999972] [G loss: 1.000063]\n",
            "4700 [D loss: 0.999969] [G loss: 1.000066]\n",
            "4720 [D loss: 0.999972] [G loss: 1.000065]\n",
            "4740 [D loss: 0.999967] [G loss: 1.000061]\n",
            "4760 [D loss: 0.999971] [G loss: 1.000061]\n",
            "4780 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4800 [D loss: 0.999970] [G loss: 1.000062]\n",
            "4820 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4840 [D loss: 0.999972] [G loss: 1.000064]\n",
            "4860 [D loss: 0.999971] [G loss: 1.000065]\n",
            "4880 [D loss: 0.999970] [G loss: 1.000065]\n",
            "4900 [D loss: 0.999970] [G loss: 1.000064]\n",
            "4920 [D loss: 0.999971] [G loss: 1.000066]\n",
            "4940 [D loss: 0.999968] [G loss: 1.000065]\n",
            "4960 [D loss: 0.999969] [G loss: 1.000062]\n",
            "4980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5000 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5020 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5040 [D loss: 0.999971] [G loss: 1.000068]\n",
            "5060 [D loss: 0.999969] [G loss: 1.000068]\n",
            "5080 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5100 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5120 [D loss: 0.999972] [G loss: 1.000065]\n",
            "5140 [D loss: 0.999971] [G loss: 1.000061]\n",
            "5160 [D loss: 0.999968] [G loss: 1.000066]\n",
            "5180 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5200 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5220 [D loss: 0.999970] [G loss: 1.000070]\n",
            "5240 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5260 [D loss: 0.999972] [G loss: 1.000061]\n",
            "5280 [D loss: 0.999970] [G loss: 1.000067]\n",
            "5300 [D loss: 0.999968] [G loss: 1.000067]\n",
            "5320 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5340 [D loss: 0.999970] [G loss: 1.000063]\n",
            "5360 [D loss: 0.999970] [G loss: 1.000062]\n",
            "5380 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5400 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5420 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5440 [D loss: 0.999970] [G loss: 1.000065]\n",
            "5460 [D loss: 0.999972] [G loss: 1.000063]\n",
            "5480 [D loss: 0.999972] [G loss: 1.000062]\n",
            "5500 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5520 [D loss: 0.999969] [G loss: 1.000066]\n",
            "5540 [D loss: 0.999971] [G loss: 1.000062]\n",
            "5560 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5580 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5600 [D loss: 0.999969] [G loss: 1.000067]\n",
            "5620 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5640 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5660 [D loss: 0.999971] [G loss: 1.000063]\n",
            "5680 [D loss: 0.999972] [G loss: 1.000067]\n",
            "5700 [D loss: 0.999973] [G loss: 1.000061]\n",
            "5720 [D loss: 0.999972] [G loss: 1.000066]\n",
            "5740 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5760 [D loss: 0.999969] [G loss: 1.000065]\n",
            "5780 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5800 [D loss: 0.999972] [G loss: 1.000064]\n",
            "5820 [D loss: 0.999968] [G loss: 1.000064]\n",
            "5840 [D loss: 0.999968] [G loss: 1.000066]\n",
            "5860 [D loss: 0.999969] [G loss: 1.000070]\n",
            "5880 [D loss: 0.999971] [G loss: 1.000065]\n",
            "5900 [D loss: 0.999971] [G loss: 1.000064]\n",
            "5920 [D loss: 0.999971] [G loss: 1.000061]\n",
            "5940 [D loss: 0.999970] [G loss: 1.000066]\n",
            "5960 [D loss: 0.999970] [G loss: 1.000064]\n",
            "5980 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6000 [D loss: 0.999971] [G loss: 1.000069]\n",
            "6020 [D loss: 0.999970] [G loss: 1.000063]\n",
            "6040 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6060 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6080 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6100 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6120 [D loss: 0.999969] [G loss: 1.000067]\n",
            "6140 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6160 [D loss: 0.999970] [G loss: 1.000064]\n",
            "6180 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6200 [D loss: 0.999970] [G loss: 1.000068]\n",
            "6220 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6260 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6280 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6300 [D loss: 0.999970] [G loss: 1.000066]\n",
            "6320 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6340 [D loss: 0.999970] [G loss: 1.000060]\n",
            "6360 [D loss: 0.999969] [G loss: 1.000068]\n",
            "6380 [D loss: 0.999969] [G loss: 1.000066]\n",
            "6400 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6420 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6440 [D loss: 0.999967] [G loss: 1.000067]\n",
            "6460 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6480 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6500 [D loss: 0.999970] [G loss: 1.000063]\n",
            "6520 [D loss: 0.999971] [G loss: 1.000063]\n",
            "6540 [D loss: 0.999973] [G loss: 1.000063]\n",
            "6560 [D loss: 0.999970] [G loss: 1.000067]\n",
            "6580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "6600 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6620 [D loss: 0.999972] [G loss: 1.000067]\n",
            "6640 [D loss: 0.999972] [G loss: 1.000068]\n",
            "6660 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6680 [D loss: 0.999971] [G loss: 1.000065]\n",
            "6700 [D loss: 0.999971] [G loss: 1.000068]\n",
            "6720 [D loss: 0.999972] [G loss: 1.000065]\n",
            "6740 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6760 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6780 [D loss: 0.999971] [G loss: 1.000062]\n",
            "6800 [D loss: 0.999970] [G loss: 1.000065]\n",
            "6820 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6840 [D loss: 0.999970] [G loss: 1.000069]\n",
            "6860 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6880 [D loss: 0.999971] [G loss: 1.000067]\n",
            "6900 [D loss: 0.999968] [G loss: 1.000066]\n",
            "6920 [D loss: 0.999971] [G loss: 1.000068]\n",
            "6940 [D loss: 0.999971] [G loss: 1.000066]\n",
            "6960 [D loss: 0.999971] [G loss: 1.000064]\n",
            "6980 [D loss: 0.999972] [G loss: 1.000066]\n",
            "7000 [D loss: 0.999969] [G loss: 1.000063]\n",
            "7020 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7060 [D loss: 0.999970] [G loss: 1.000065]\n",
            "7080 [D loss: 0.999967] [G loss: 1.000067]\n",
            "7100 [D loss: 0.999971] [G loss: 1.000064]\n",
            "7120 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7140 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7160 [D loss: 0.999971] [G loss: 1.000067]\n",
            "7180 [D loss: 0.999970] [G loss: 1.000069]\n",
            "7200 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7220 [D loss: 0.999971] [G loss: 1.000069]\n",
            "7240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7260 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7280 [D loss: 0.999969] [G loss: 1.000065]\n",
            "7300 [D loss: 0.999971] [G loss: 1.000062]\n",
            "7320 [D loss: 0.999969] [G loss: 1.000068]\n",
            "7340 [D loss: 0.999971] [G loss: 1.000063]\n",
            "7360 [D loss: 0.999970] [G loss: 1.000064]\n",
            "7380 [D loss: 0.999971] [G loss: 1.000068]\n",
            "7400 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7420 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7440 [D loss: 0.999972] [G loss: 1.000065]\n",
            "7460 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7480 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7500 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7520 [D loss: 0.999972] [G loss: 1.000063]\n",
            "7540 [D loss: 0.999970] [G loss: 1.000065]\n",
            "7560 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7580 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7600 [D loss: 0.999970] [G loss: 1.000066]\n",
            "7620 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7640 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7660 [D loss: 0.999972] [G loss: 1.000066]\n",
            "7680 [D loss: 0.999967] [G loss: 1.000067]\n",
            "7700 [D loss: 0.999972] [G loss: 1.000065]\n",
            "7720 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7740 [D loss: 0.999971] [G loss: 1.000062]\n",
            "7760 [D loss: 0.999971] [G loss: 1.000065]\n",
            "7780 [D loss: 0.999972] [G loss: 1.000062]\n",
            "7800 [D loss: 0.999969] [G loss: 1.000068]\n",
            "7820 [D loss: 0.999971] [G loss: 1.000068]\n",
            "7840 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7860 [D loss: 0.999972] [G loss: 1.000068]\n",
            "7880 [D loss: 0.999971] [G loss: 1.000066]\n",
            "7900 [D loss: 0.999972] [G loss: 1.000064]\n",
            "7920 [D loss: 0.999969] [G loss: 1.000067]\n",
            "7940 [D loss: 0.999969] [G loss: 1.000064]\n",
            "7960 [D loss: 0.999972] [G loss: 1.000067]\n",
            "7980 [D loss: 0.999970] [G loss: 1.000066]\n",
            "8000 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8020 [D loss: 0.999970] [G loss: 1.000068]\n",
            "8040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8060 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8080 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8100 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8120 [D loss: 0.999973] [G loss: 1.000065]\n",
            "8140 [D loss: 0.999972] [G loss: 1.000069]\n",
            "8160 [D loss: 0.999970] [G loss: 1.000063]\n",
            "8180 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8200 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8220 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8240 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8260 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8280 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8300 [D loss: 0.999973] [G loss: 1.000065]\n",
            "8320 [D loss: 0.999971] [G loss: 1.000062]\n",
            "8340 [D loss: 0.999972] [G loss: 1.000063]\n",
            "8360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8380 [D loss: 0.999971] [G loss: 1.000068]\n",
            "8400 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8420 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8440 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8460 [D loss: 0.999971] [G loss: 1.000067]\n",
            "8480 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8500 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8520 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8540 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8560 [D loss: 0.999970] [G loss: 1.000064]\n",
            "8580 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8600 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8620 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8640 [D loss: 0.999969] [G loss: 1.000063]\n",
            "8660 [D loss: 0.999970] [G loss: 1.000066]\n",
            "8680 [D loss: 0.999969] [G loss: 1.000067]\n",
            "8700 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8720 [D loss: 0.999970] [G loss: 1.000067]\n",
            "8740 [D loss: 0.999971] [G loss: 1.000065]\n",
            "8760 [D loss: 0.999971] [G loss: 1.000064]\n",
            "8780 [D loss: 0.999968] [G loss: 1.000068]\n",
            "8800 [D loss: 0.999969] [G loss: 1.000067]\n",
            "8820 [D loss: 0.999971] [G loss: 1.000063]\n",
            "8840 [D loss: 0.999972] [G loss: 1.000066]\n",
            "8860 [D loss: 0.999971] [G loss: 1.000066]\n",
            "8880 [D loss: 0.999969] [G loss: 1.000065]\n",
            "8900 [D loss: 0.999971] [G loss: 1.000068]\n",
            "8920 [D loss: 0.999970] [G loss: 1.000065]\n",
            "8940 [D loss: 0.999972] [G loss: 1.000068]\n",
            "8960 [D loss: 0.999972] [G loss: 1.000067]\n",
            "8980 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9000 [D loss: 0.999969] [G loss: 1.000067]\n",
            "9020 [D loss: 0.999967] [G loss: 1.000064]\n",
            "9040 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9060 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9080 [D loss: 0.999970] [G loss: 1.000067]\n",
            "9100 [D loss: 0.999970] [G loss: 1.000064]\n",
            "9120 [D loss: 0.999972] [G loss: 1.000068]\n",
            "9140 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9160 [D loss: 0.999973] [G loss: 1.000065]\n",
            "9180 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9200 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9220 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9240 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9260 [D loss: 0.999972] [G loss: 1.000066]\n",
            "9280 [D loss: 0.999972] [G loss: 1.000061]\n",
            "9300 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9320 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9340 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9360 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9380 [D loss: 0.999970] [G loss: 1.000063]\n",
            "9400 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9420 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9440 [D loss: 0.999970] [G loss: 1.000068]\n",
            "9460 [D loss: 0.999972] [G loss: 1.000064]\n",
            "9480 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9500 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9520 [D loss: 0.999972] [G loss: 1.000068]\n",
            "9540 [D loss: 0.999969] [G loss: 1.000061]\n",
            "9560 [D loss: 0.999972] [G loss: 1.000067]\n",
            "9580 [D loss: 0.999971] [G loss: 1.000065]\n",
            "9600 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9620 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9640 [D loss: 0.999969] [G loss: 1.000066]\n",
            "9660 [D loss: 0.999969] [G loss: 1.000064]\n",
            "9680 [D loss: 0.999970] [G loss: 1.000067]\n",
            "9700 [D loss: 0.999971] [G loss: 1.000067]\n",
            "9720 [D loss: 0.999969] [G loss: 1.000065]\n",
            "9740 [D loss: 0.999972] [G loss: 1.000065]\n",
            "9760 [D loss: 0.999972] [G loss: 1.000066]\n",
            "9780 [D loss: 0.999969] [G loss: 1.000067]\n",
            "9800 [D loss: 0.999969] [G loss: 1.000062]\n",
            "9820 [D loss: 0.999967] [G loss: 1.000062]\n",
            "9840 [D loss: 0.999971] [G loss: 1.000066]\n",
            "9860 [D loss: 0.999970] [G loss: 1.000065]\n",
            "9880 [D loss: 0.999970] [G loss: 1.000063]\n",
            "9900 [D loss: 0.999971] [G loss: 1.000064]\n",
            "9920 [D loss: 0.999971] [G loss: 1.000063]\n",
            "9940 [D loss: 0.999971] [G loss: 1.000067]\n",
            "9960 [D loss: 0.999969] [G loss: 1.000067]\n",
            "9980 [D loss: 0.999971] [G loss: 1.000064]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}