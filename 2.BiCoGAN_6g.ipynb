{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "6_Bidirectional_Conditional_GAN_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "200fb3b7-28f8-4151-8b13-3f13ac8a8c8e"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0b53d8-f087-4777-8d18-51a3b89f3d08"
      },
      "source": [
        "data.Usage.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training       28709\n",
              "PrivateTest     3589\n",
              "PublicTest      3589\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 7\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "262fb1b9-e365-4e68-e297-9b8605e6d1d5"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnh1CQpGKk-t"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 7\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        # noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,))\n",
        "        # img = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 12x12 image\n",
        "        # n_nodes = 128 * 12 * 12\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((12, 12, 128)))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # generate\n",
        "        # model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 6x6 feature maps\n",
        "        n_nodes = 128 * 6 * 6\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((6, 6, 128)))\n",
        "        # upsample to 12x12\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # output layer 48x48x1\n",
        "        model.add(Conv2D(1, (6,6), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        zn = Dense(512, activation='relu')(zn)\n",
        "        zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        # (_, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Configure input\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\n",
        "        y_train = y.to_numpy().reshape(-1, 1)\n",
        "        # y_train = y.reshape(-1, 1)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 7, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    # def sample_images(self, epoch):\n",
        "    #     r, c = 1, 7\n",
        "    #     noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    #     sampled_labels = np.arange(0, 7).reshape(-1, 1)\n",
        "\n",
        "    #     gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "    #     print(gen_imgs.shape)\n",
        "\n",
        "    #     # Rescale images 0 - 1\n",
        "    #     gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    #     fig, axs = plt.subplots(r, c)\n",
        "    #     cnt = 0\n",
        "    #     for i in range(r):\n",
        "    #         print(i)\n",
        "    #         for j in range(c):\n",
        "    #             print(j)\n",
        "    #             axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "    #             axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
        "    #             axs[i,j].axis('off')\n",
        "    #             cnt += 1\n",
        "    #     fig.savefig(\"images/%d.png\" % epoch)\n",
        "    #     plt.close()\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 7\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 7).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"label: %d\" % sampled_labels[cnt])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7OvsALJTfRi",
        "outputId": "12a355d3-9d4b-49ca-ad2b-aba5aa6f7fad"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 48, 48, 128)  3328        input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_11 (Flatten)            (None, 100)          0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 512)          51712       flatten_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, 1, 2304)      16128       input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 512)          0           dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_10 (Flatten)            (None, 1152)         0           leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_12 (Flatten)            (None, 2304)         0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3968)         0           dropout_2[0][0]                  \n",
            "                                                                 flatten_10[0][0]                 \n",
            "                                                                 flatten_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 1)            3969        concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 4608)              465408    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_24 (LeakyReLU)   (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "reshape_2 (Reshape)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 12, 12, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_25 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_8 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_26 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_9 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_27 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 48, 48, 1)         4609      \n",
            "=================================================================\n",
            "Total params: 1,256,833\n",
            "Trainable params: 1,256,833\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.692794, acc.: 41.80%] [G loss: 1.363244]\n",
            "20 [D loss: 0.997228, acc.: 49.22%] [G loss: 1.079654]\n",
            "40 [D loss: 10.192451, acc.: 38.28%] [G loss: 16.920563]\n",
            "60 [D loss: 4.623682, acc.: 29.30%] [G loss: 7.144755]\n",
            "80 [D loss: 0.145405, acc.: 96.88%] [G loss: 3.568455]\n",
            "100 [D loss: 0.171819, acc.: 96.09%] [G loss: 4.876417]\n",
            "120 [D loss: 0.059815, acc.: 100.00%] [G loss: 8.702084]\n",
            "140 [D loss: 0.277375, acc.: 89.45%] [G loss: 4.059882]\n",
            "160 [D loss: 0.139765, acc.: 94.53%] [G loss: 6.267387]\n",
            "180 [D loss: 0.098693, acc.: 96.48%] [G loss: 7.450905]\n",
            "200 [D loss: 0.075423, acc.: 96.88%] [G loss: 9.491319]\n",
            "220 [D loss: 0.860310, acc.: 76.17%] [G loss: 7.120339]\n",
            "240 [D loss: 0.021870, acc.: 100.00%] [G loss: 10.763273]\n",
            "260 [D loss: 0.283057, acc.: 98.83%] [G loss: 4.523032]\n",
            "280 [D loss: 0.064968, acc.: 99.22%] [G loss: 8.937262]\n",
            "300 [D loss: 0.257237, acc.: 98.05%] [G loss: 5.262499]\n",
            "320 [D loss: 0.638951, acc.: 46.88%] [G loss: 5.962174]\n",
            "340 [D loss: 0.419628, acc.: 89.84%] [G loss: 4.417611]\n",
            "360 [D loss: 0.347575, acc.: 93.36%] [G loss: 3.552187]\n",
            "380 [D loss: 0.969137, acc.: 64.84%] [G loss: 2.598330]\n",
            "400 [D loss: 0.632267, acc.: 78.52%] [G loss: 3.372662]\n",
            "420 [D loss: 0.525285, acc.: 83.98%] [G loss: 2.757557]\n",
            "440 [D loss: 0.400338, acc.: 88.28%] [G loss: 5.184896]\n",
            "460 [D loss: 0.484532, acc.: 89.84%] [G loss: 5.033338]\n",
            "480 [D loss: 0.526991, acc.: 77.34%] [G loss: 3.353721]\n",
            "500 [D loss: 0.484876, acc.: 87.11%] [G loss: 3.852768]\n",
            "520 [D loss: 0.238678, acc.: 94.14%] [G loss: 6.053322]\n",
            "540 [D loss: 0.310701, acc.: 91.41%] [G loss: 4.480524]\n",
            "560 [D loss: 0.386147, acc.: 92.19%] [G loss: 5.068025]\n",
            "580 [D loss: 0.441447, acc.: 82.42%] [G loss: 8.714972]\n",
            "600 [D loss: 2.068088, acc.: 35.16%] [G loss: 5.693320]\n",
            "620 [D loss: 0.076214, acc.: 96.88%] [G loss: 15.504116]\n",
            "640 [D loss: 0.402508, acc.: 89.84%] [G loss: 5.015415]\n",
            "660 [D loss: 0.291974, acc.: 86.33%] [G loss: 6.800432]\n",
            "680 [D loss: 0.365668, acc.: 85.94%] [G loss: 4.809776]\n",
            "700 [D loss: 0.375825, acc.: 87.89%] [G loss: 7.323451]\n",
            "720 [D loss: 0.584250, acc.: 77.34%] [G loss: 3.898723]\n",
            "740 [D loss: 0.480237, acc.: 77.34%] [G loss: 3.939720]\n",
            "760 [D loss: 0.615871, acc.: 68.75%] [G loss: 3.039476]\n",
            "780 [D loss: 0.518439, acc.: 75.39%] [G loss: 3.012846]\n",
            "800 [D loss: 0.591483, acc.: 67.58%] [G loss: 2.936316]\n",
            "820 [D loss: 0.580957, acc.: 68.75%] [G loss: 3.117126]\n",
            "840 [D loss: 0.453159, acc.: 84.77%] [G loss: 3.259461]\n",
            "860 [D loss: 0.419408, acc.: 83.20%] [G loss: 4.634182]\n",
            "880 [D loss: 0.313598, acc.: 90.62%] [G loss: 4.173212]\n",
            "900 [D loss: 0.333613, acc.: 86.33%] [G loss: 5.602916]\n",
            "920 [D loss: 0.195415, acc.: 96.48%] [G loss: 6.447292]\n",
            "940 [D loss: 0.307183, acc.: 89.45%] [G loss: 5.055152]\n",
            "960 [D loss: 0.452245, acc.: 81.64%] [G loss: 4.614908]\n",
            "980 [D loss: 0.375906, acc.: 84.38%] [G loss: 4.065977]\n",
            "1000 [D loss: 0.322876, acc.: 87.50%] [G loss: 4.418130]\n",
            "1020 [D loss: 0.586964, acc.: 73.05%] [G loss: 3.883324]\n",
            "1040 [D loss: 0.422864, acc.: 80.08%] [G loss: 3.679060]\n",
            "1060 [D loss: 0.452724, acc.: 79.30%] [G loss: 4.292919]\n",
            "1080 [D loss: 0.270108, acc.: 92.19%] [G loss: 5.464712]\n",
            "1100 [D loss: 0.379701, acc.: 83.98%] [G loss: 5.048302]\n",
            "1120 [D loss: 0.425299, acc.: 85.55%] [G loss: 4.107570]\n",
            "1140 [D loss: 0.346484, acc.: 86.33%] [G loss: 4.969882]\n",
            "1160 [D loss: 0.475160, acc.: 74.61%] [G loss: 4.161832]\n",
            "1180 [D loss: 0.556998, acc.: 73.83%] [G loss: 3.082120]\n",
            "1200 [D loss: 0.484912, acc.: 76.56%] [G loss: 3.433202]\n",
            "1220 [D loss: 0.389804, acc.: 84.77%] [G loss: 3.793457]\n",
            "1240 [D loss: 0.383044, acc.: 83.20%] [G loss: 4.044407]\n",
            "1260 [D loss: 0.557780, acc.: 74.61%] [G loss: 3.227710]\n",
            "1280 [D loss: 0.454065, acc.: 78.52%] [G loss: 3.505677]\n",
            "1300 [D loss: 0.447838, acc.: 81.25%] [G loss: 3.583960]\n",
            "1320 [D loss: 0.614067, acc.: 71.88%] [G loss: 3.790469]\n",
            "1340 [D loss: 0.397807, acc.: 83.59%] [G loss: 3.764839]\n",
            "1360 [D loss: 0.807834, acc.: 53.91%] [G loss: 3.551185]\n",
            "1380 [D loss: 0.439475, acc.: 81.64%] [G loss: 3.677790]\n",
            "1400 [D loss: 0.483154, acc.: 77.34%] [G loss: 3.529718]\n",
            "1420 [D loss: 0.475700, acc.: 76.56%] [G loss: 3.799642]\n",
            "1440 [D loss: 0.545951, acc.: 74.61%] [G loss: 3.470526]\n",
            "1460 [D loss: 0.441273, acc.: 81.64%] [G loss: 3.969632]\n",
            "1480 [D loss: 0.387720, acc.: 82.81%] [G loss: 4.333671]\n",
            "1500 [D loss: 0.557913, acc.: 71.88%] [G loss: 3.855313]\n",
            "1520 [D loss: 0.368076, acc.: 82.81%] [G loss: 4.123768]\n",
            "1540 [D loss: 0.410299, acc.: 82.42%] [G loss: 4.277707]\n",
            "1560 [D loss: 0.400476, acc.: 82.81%] [G loss: 3.751390]\n",
            "1580 [D loss: 0.449101, acc.: 79.30%] [G loss: 3.734212]\n",
            "1600 [D loss: 0.415674, acc.: 81.25%] [G loss: 3.873059]\n",
            "1620 [D loss: 0.265750, acc.: 92.19%] [G loss: 2.964638]\n",
            "1640 [D loss: 0.489255, acc.: 76.17%] [G loss: 3.519682]\n",
            "1660 [D loss: 0.477191, acc.: 78.91%] [G loss: 4.055447]\n",
            "1680 [D loss: 0.386211, acc.: 82.03%] [G loss: 4.089319]\n",
            "1700 [D loss: 0.438694, acc.: 80.86%] [G loss: 3.858937]\n",
            "1720 [D loss: 0.324894, acc.: 87.11%] [G loss: 4.758214]\n",
            "1740 [D loss: 0.508147, acc.: 73.83%] [G loss: 3.894225]\n",
            "1760 [D loss: 0.466030, acc.: 81.25%] [G loss: 4.541659]\n",
            "1780 [D loss: 0.499243, acc.: 77.34%] [G loss: 3.977340]\n",
            "1800 [D loss: 0.441533, acc.: 81.64%] [G loss: 3.896728]\n",
            "1820 [D loss: 0.476543, acc.: 77.34%] [G loss: 4.024343]\n",
            "1840 [D loss: 0.380871, acc.: 82.81%] [G loss: 4.468583]\n",
            "1860 [D loss: 0.521969, acc.: 74.22%] [G loss: 4.363077]\n",
            "1880 [D loss: 0.398990, acc.: 85.16%] [G loss: 4.545360]\n",
            "1900 [D loss: 0.352253, acc.: 84.77%] [G loss: 4.854161]\n",
            "1920 [D loss: 0.411604, acc.: 80.86%] [G loss: 4.394051]\n",
            "1940 [D loss: 0.348949, acc.: 86.72%] [G loss: 4.684855]\n",
            "1960 [D loss: 0.408454, acc.: 83.59%] [G loss: 4.370540]\n",
            "1980 [D loss: 0.445645, acc.: 78.12%] [G loss: 4.173965]\n",
            "2000 [D loss: 0.382557, acc.: 83.59%] [G loss: 4.587084]\n",
            "2020 [D loss: 0.528059, acc.: 77.73%] [G loss: 4.250171]\n",
            "2040 [D loss: 0.434418, acc.: 81.64%] [G loss: 3.984667]\n",
            "2060 [D loss: 0.396934, acc.: 83.59%] [G loss: 4.381786]\n",
            "2080 [D loss: 0.406715, acc.: 81.64%] [G loss: 4.359241]\n",
            "2100 [D loss: 0.498469, acc.: 76.17%] [G loss: 4.309250]\n",
            "2120 [D loss: 0.392552, acc.: 81.64%] [G loss: 4.542624]\n",
            "2140 [D loss: 0.497858, acc.: 75.39%] [G loss: 4.256995]\n",
            "2160 [D loss: 0.431669, acc.: 78.52%] [G loss: 4.374342]\n",
            "2180 [D loss: 0.386353, acc.: 82.03%] [G loss: 4.030497]\n",
            "2200 [D loss: 0.393525, acc.: 84.38%] [G loss: 4.327544]\n",
            "2220 [D loss: 0.471047, acc.: 76.17%] [G loss: 4.148203]\n",
            "2240 [D loss: 0.431076, acc.: 80.86%] [G loss: 4.362601]\n",
            "2260 [D loss: 0.376126, acc.: 82.81%] [G loss: 4.311936]\n",
            "2280 [D loss: 0.435135, acc.: 82.03%] [G loss: 4.216527]\n",
            "2300 [D loss: 0.412959, acc.: 80.47%] [G loss: 4.111159]\n",
            "2320 [D loss: 0.550280, acc.: 71.88%] [G loss: 3.931458]\n",
            "2340 [D loss: 0.335698, acc.: 87.50%] [G loss: 4.733627]\n",
            "2360 [D loss: 0.425331, acc.: 80.86%] [G loss: 4.688098]\n",
            "2380 [D loss: 0.424558, acc.: 82.81%] [G loss: 4.332030]\n",
            "2400 [D loss: 0.427221, acc.: 78.91%] [G loss: 4.370893]\n",
            "2420 [D loss: 0.318654, acc.: 88.28%] [G loss: 4.803578]\n",
            "2440 [D loss: 0.413908, acc.: 83.59%] [G loss: 4.504460]\n",
            "2460 [D loss: 0.421730, acc.: 79.69%] [G loss: 4.413567]\n",
            "2480 [D loss: 0.419884, acc.: 81.64%] [G loss: 4.405695]\n",
            "2500 [D loss: 0.424117, acc.: 82.42%] [G loss: 4.691920]\n",
            "2520 [D loss: 0.453394, acc.: 80.47%] [G loss: 4.406740]\n",
            "2540 [D loss: 0.374935, acc.: 84.77%] [G loss: 4.736284]\n",
            "2560 [D loss: 0.321794, acc.: 87.50%] [G loss: 5.267558]\n",
            "2580 [D loss: 0.518656, acc.: 72.27%] [G loss: 4.482098]\n",
            "2600 [D loss: 0.388319, acc.: 83.98%] [G loss: 4.813517]\n",
            "2620 [D loss: 0.328663, acc.: 86.33%] [G loss: 4.836767]\n",
            "2640 [D loss: 0.318099, acc.: 85.55%] [G loss: 5.641196]\n",
            "2660 [D loss: 0.326650, acc.: 87.50%] [G loss: 5.069518]\n",
            "2680 [D loss: 0.447819, acc.: 77.34%] [G loss: 5.226392]\n",
            "2700 [D loss: 0.347934, acc.: 87.50%] [G loss: 4.671406]\n",
            "2720 [D loss: 0.453368, acc.: 76.56%] [G loss: 4.273150]\n",
            "2740 [D loss: 0.432023, acc.: 78.12%] [G loss: 4.159589]\n",
            "2760 [D loss: 0.420880, acc.: 80.47%] [G loss: 4.599861]\n",
            "2780 [D loss: 0.566641, acc.: 69.92%] [G loss: 4.225712]\n",
            "2800 [D loss: 0.463328, acc.: 76.95%] [G loss: 4.600976]\n",
            "2820 [D loss: 0.350038, acc.: 83.98%] [G loss: 4.546372]\n",
            "2840 [D loss: 0.403359, acc.: 81.25%] [G loss: 5.237429]\n",
            "2860 [D loss: 0.345739, acc.: 85.94%] [G loss: 4.741522]\n",
            "2880 [D loss: 0.354460, acc.: 85.16%] [G loss: 4.596492]\n",
            "2900 [D loss: 0.375729, acc.: 80.86%] [G loss: 4.914245]\n",
            "2920 [D loss: 0.322705, acc.: 86.33%] [G loss: 5.045888]\n",
            "2940 [D loss: 0.247954, acc.: 91.41%] [G loss: 5.091659]\n",
            "2960 [D loss: 0.557924, acc.: 72.27%] [G loss: 4.150468]\n",
            "2980 [D loss: 0.327608, acc.: 89.06%] [G loss: 4.811913]\n",
            "3000 [D loss: 0.412486, acc.: 79.30%] [G loss: 4.731018]\n",
            "3020 [D loss: 0.344638, acc.: 87.11%] [G loss: 5.435119]\n",
            "3040 [D loss: 0.331938, acc.: 83.59%] [G loss: 4.884459]\n",
            "3060 [D loss: 0.328882, acc.: 83.98%] [G loss: 5.597978]\n",
            "3080 [D loss: 0.380475, acc.: 81.25%] [G loss: 4.764737]\n",
            "3100 [D loss: 0.393132, acc.: 81.64%] [G loss: 4.988447]\n",
            "3120 [D loss: 0.417215, acc.: 81.64%] [G loss: 4.992435]\n",
            "3140 [D loss: 0.358083, acc.: 83.59%] [G loss: 5.373530]\n",
            "3160 [D loss: 0.343837, acc.: 85.16%] [G loss: 5.502664]\n",
            "3180 [D loss: 0.242313, acc.: 91.41%] [G loss: 5.834956]\n",
            "3200 [D loss: 0.299378, acc.: 90.23%] [G loss: 5.601436]\n",
            "3220 [D loss: 0.500149, acc.: 76.17%] [G loss: 5.214738]\n",
            "3240 [D loss: 0.355832, acc.: 85.16%] [G loss: 5.202427]\n",
            "3260 [D loss: 0.290968, acc.: 88.28%] [G loss: 6.063551]\n",
            "3280 [D loss: 0.327741, acc.: 87.89%] [G loss: 5.059925]\n",
            "3300 [D loss: 0.451675, acc.: 79.30%] [G loss: 3.925858]\n",
            "3320 [D loss: 0.313318, acc.: 86.33%] [G loss: 5.141754]\n",
            "3340 [D loss: 0.371400, acc.: 84.77%] [G loss: 5.368063]\n",
            "3360 [D loss: 0.335157, acc.: 87.89%] [G loss: 5.477335]\n",
            "3380 [D loss: 0.408369, acc.: 82.81%] [G loss: 4.978508]\n",
            "3400 [D loss: 0.377747, acc.: 82.03%] [G loss: 5.000229]\n",
            "3420 [D loss: 0.411301, acc.: 80.08%] [G loss: 4.702883]\n",
            "3440 [D loss: 0.389751, acc.: 81.25%] [G loss: 5.163847]\n",
            "3460 [D loss: 0.364691, acc.: 84.38%] [G loss: 4.934480]\n",
            "3480 [D loss: 0.313698, acc.: 87.89%] [G loss: 5.449514]\n",
            "3500 [D loss: 0.315051, acc.: 88.67%] [G loss: 5.151597]\n",
            "3520 [D loss: 0.294548, acc.: 88.28%] [G loss: 5.393147]\n",
            "3540 [D loss: 0.360445, acc.: 83.20%] [G loss: 5.641194]\n",
            "3560 [D loss: 0.415940, acc.: 80.47%] [G loss: 5.054404]\n",
            "3580 [D loss: 0.353312, acc.: 84.38%] [G loss: 5.222958]\n",
            "3600 [D loss: 0.288212, acc.: 88.28%] [G loss: 5.577487]\n",
            "3620 [D loss: 0.343285, acc.: 85.55%] [G loss: 5.528058]\n",
            "3640 [D loss: 0.379959, acc.: 82.42%] [G loss: 4.946277]\n",
            "3660 [D loss: 0.299427, acc.: 85.94%] [G loss: 5.427139]\n",
            "3680 [D loss: 0.308425, acc.: 86.72%] [G loss: 5.204938]\n",
            "3700 [D loss: 0.379670, acc.: 83.98%] [G loss: 4.750484]\n",
            "3720 [D loss: 0.323518, acc.: 89.06%] [G loss: 6.012178]\n",
            "3740 [D loss: 0.311610, acc.: 88.67%] [G loss: 5.523414]\n",
            "3760 [D loss: 0.424161, acc.: 83.98%] [G loss: 5.111194]\n",
            "3780 [D loss: 0.404188, acc.: 80.47%] [G loss: 4.824166]\n",
            "3800 [D loss: 0.348751, acc.: 83.98%] [G loss: 5.565083]\n",
            "3820 [D loss: 0.362564, acc.: 82.03%] [G loss: 5.672406]\n",
            "3840 [D loss: 0.274154, acc.: 90.23%] [G loss: 5.396473]\n",
            "3860 [D loss: 0.397751, acc.: 82.42%] [G loss: 5.311002]\n",
            "3880 [D loss: 0.281521, acc.: 89.84%] [G loss: 5.293602]\n",
            "3900 [D loss: 0.239948, acc.: 93.36%] [G loss: 4.203655]\n",
            "3920 [D loss: 0.388629, acc.: 83.20%] [G loss: 5.298659]\n",
            "3940 [D loss: 0.416746, acc.: 82.42%] [G loss: 5.064167]\n",
            "3960 [D loss: 0.326796, acc.: 89.06%] [G loss: 5.618279]\n",
            "3980 [D loss: 0.406631, acc.: 81.64%] [G loss: 5.007459]\n",
            "4000 [D loss: 0.359046, acc.: 83.98%] [G loss: 5.233293]\n",
            "4020 [D loss: 0.309346, acc.: 87.11%] [G loss: 5.397418]\n",
            "4040 [D loss: 0.397237, acc.: 83.59%] [G loss: 5.136081]\n",
            "4060 [D loss: 0.263437, acc.: 89.45%] [G loss: 5.792728]\n",
            "4080 [D loss: 0.375356, acc.: 83.59%] [G loss: 5.597792]\n",
            "4100 [D loss: 0.425198, acc.: 77.73%] [G loss: 5.603860]\n",
            "4120 [D loss: 0.355144, acc.: 85.16%] [G loss: 4.942945]\n",
            "4140 [D loss: 0.310467, acc.: 87.50%] [G loss: 5.567299]\n",
            "4160 [D loss: 0.415152, acc.: 81.64%] [G loss: 5.416082]\n",
            "4180 [D loss: 0.411281, acc.: 82.42%] [G loss: 4.888762]\n",
            "4200 [D loss: 0.355689, acc.: 84.77%] [G loss: 5.024950]\n",
            "4220 [D loss: 0.364526, acc.: 83.59%] [G loss: 5.475099]\n",
            "4240 [D loss: 0.337517, acc.: 87.89%] [G loss: 5.470329]\n",
            "4260 [D loss: 0.354351, acc.: 84.77%] [G loss: 5.294339]\n",
            "4280 [D loss: 0.390846, acc.: 82.42%] [G loss: 5.764386]\n",
            "4300 [D loss: 0.399027, acc.: 82.81%] [G loss: 4.830018]\n",
            "4320 [D loss: 0.411984, acc.: 79.69%] [G loss: 5.266260]\n",
            "4340 [D loss: 0.371327, acc.: 83.98%] [G loss: 4.919018]\n",
            "4360 [D loss: 0.377299, acc.: 86.72%] [G loss: 5.610697]\n",
            "4380 [D loss: 0.404603, acc.: 83.20%] [G loss: 4.994227]\n",
            "4400 [D loss: 0.330091, acc.: 87.89%] [G loss: 5.244478]\n",
            "4420 [D loss: 0.374008, acc.: 82.81%] [G loss: 5.226240]\n",
            "4440 [D loss: 0.343358, acc.: 86.33%] [G loss: 5.445497]\n",
            "4460 [D loss: 0.418696, acc.: 81.64%] [G loss: 4.754268]\n",
            "4480 [D loss: 0.319797, acc.: 86.72%] [G loss: 5.319033]\n",
            "4500 [D loss: 0.287284, acc.: 88.67%] [G loss: 5.353791]\n",
            "4520 [D loss: 0.341780, acc.: 87.11%] [G loss: 4.987555]\n",
            "4540 [D loss: 0.390417, acc.: 85.16%] [G loss: 5.194599]\n",
            "4560 [D loss: 0.325254, acc.: 89.06%] [G loss: 5.506325]\n",
            "4580 [D loss: 0.397132, acc.: 81.64%] [G loss: 5.209028]\n",
            "4600 [D loss: 0.335165, acc.: 86.72%] [G loss: 5.436874]\n",
            "4620 [D loss: 0.194968, acc.: 94.53%] [G loss: 3.942992]\n",
            "4640 [D loss: 0.283390, acc.: 89.06%] [G loss: 5.332844]\n",
            "4660 [D loss: 0.310230, acc.: 87.89%] [G loss: 6.026187]\n",
            "4680 [D loss: 0.429914, acc.: 79.30%] [G loss: 5.205091]\n",
            "4700 [D loss: 0.346587, acc.: 86.33%] [G loss: 5.363153]\n",
            "4720 [D loss: 0.315344, acc.: 87.11%] [G loss: 5.414298]\n",
            "4740 [D loss: 0.379832, acc.: 81.64%] [G loss: 4.749491]\n",
            "4760 [D loss: 0.429490, acc.: 81.25%] [G loss: 5.200512]\n",
            "4780 [D loss: 0.391730, acc.: 85.16%] [G loss: 5.007379]\n",
            "4800 [D loss: 0.334838, acc.: 85.16%] [G loss: 5.388161]\n",
            "4820 [D loss: 0.325584, acc.: 88.67%] [G loss: 5.573364]\n",
            "4840 [D loss: 0.339214, acc.: 83.20%] [G loss: 5.809021]\n",
            "4860 [D loss: 0.338332, acc.: 84.77%] [G loss: 5.561607]\n",
            "4880 [D loss: 0.374262, acc.: 85.16%] [G loss: 5.383989]\n",
            "4900 [D loss: 0.379931, acc.: 86.33%] [G loss: 5.251416]\n",
            "4920 [D loss: 0.331883, acc.: 87.11%] [G loss: 5.405425]\n",
            "4940 [D loss: 0.334563, acc.: 84.38%] [G loss: 5.640156]\n",
            "4960 [D loss: 0.439441, acc.: 81.25%] [G loss: 5.522693]\n",
            "4980 [D loss: 0.289872, acc.: 89.84%] [G loss: 5.592683]\n",
            "5000 [D loss: 0.394560, acc.: 84.38%] [G loss: 5.213978]\n",
            "5020 [D loss: 0.306190, acc.: 87.11%] [G loss: 5.533834]\n",
            "5040 [D loss: 0.349813, acc.: 86.72%] [G loss: 5.110530]\n",
            "5060 [D loss: 0.300289, acc.: 91.02%] [G loss: 5.372245]\n",
            "5080 [D loss: 0.360505, acc.: 83.20%] [G loss: 5.911191]\n",
            "5100 [D loss: 0.319047, acc.: 86.72%] [G loss: 5.747505]\n",
            "5120 [D loss: 0.363069, acc.: 84.77%] [G loss: 5.584892]\n",
            "5140 [D loss: 0.385717, acc.: 83.59%] [G loss: 5.609719]\n",
            "5160 [D loss: 0.362564, acc.: 83.98%] [G loss: 5.778864]\n",
            "5180 [D loss: 0.317650, acc.: 85.55%] [G loss: 6.104589]\n",
            "5200 [D loss: 0.251684, acc.: 91.02%] [G loss: 5.762933]\n",
            "5220 [D loss: 0.301229, acc.: 87.89%] [G loss: 5.769100]\n",
            "5240 [D loss: 0.306466, acc.: 88.28%] [G loss: 5.430788]\n",
            "5260 [D loss: 0.298505, acc.: 87.50%] [G loss: 6.293819]\n",
            "5280 [D loss: 0.282071, acc.: 86.72%] [G loss: 5.871499]\n",
            "5300 [D loss: 0.314756, acc.: 87.11%] [G loss: 6.444508]\n",
            "5320 [D loss: 0.384698, acc.: 82.42%] [G loss: 5.484086]\n",
            "5340 [D loss: 0.300356, acc.: 88.28%] [G loss: 5.790524]\n",
            "5360 [D loss: 0.294371, acc.: 85.94%] [G loss: 5.791577]\n",
            "5380 [D loss: 0.342749, acc.: 85.16%] [G loss: 4.439356]\n",
            "5400 [D loss: 0.235910, acc.: 91.80%] [G loss: 5.700068]\n",
            "5420 [D loss: 0.365386, acc.: 83.98%] [G loss: 5.132436]\n",
            "5440 [D loss: 0.331750, acc.: 87.11%] [G loss: 5.891356]\n",
            "5460 [D loss: 0.262088, acc.: 89.06%] [G loss: 5.571550]\n",
            "5480 [D loss: 0.314808, acc.: 87.11%] [G loss: 5.646025]\n",
            "5500 [D loss: 0.233844, acc.: 92.19%] [G loss: 6.248126]\n",
            "5520 [D loss: 0.347749, acc.: 85.55%] [G loss: 5.817762]\n",
            "5540 [D loss: 0.362213, acc.: 84.77%] [G loss: 5.883071]\n",
            "5560 [D loss: 0.292542, acc.: 88.67%] [G loss: 6.394390]\n",
            "5580 [D loss: 0.268725, acc.: 89.45%] [G loss: 6.440895]\n",
            "5600 [D loss: 0.316412, acc.: 85.55%] [G loss: 6.040981]\n",
            "5620 [D loss: 0.222469, acc.: 92.19%] [G loss: 6.744613]\n",
            "5640 [D loss: 0.263532, acc.: 87.89%] [G loss: 6.116303]\n",
            "5660 [D loss: 0.286192, acc.: 89.45%] [G loss: 6.010793]\n",
            "5680 [D loss: 0.296819, acc.: 88.28%] [G loss: 5.903666]\n",
            "5700 [D loss: 0.282366, acc.: 87.89%] [G loss: 5.365356]\n",
            "5720 [D loss: 0.395449, acc.: 80.08%] [G loss: 5.242566]\n",
            "5740 [D loss: 0.326615, acc.: 85.55%] [G loss: 5.571678]\n",
            "5760 [D loss: 0.332601, acc.: 84.38%] [G loss: 5.675895]\n",
            "5780 [D loss: 0.385176, acc.: 81.25%] [G loss: 5.950945]\n",
            "5800 [D loss: 0.382811, acc.: 83.20%] [G loss: 5.254795]\n",
            "5820 [D loss: 0.333592, acc.: 87.50%] [G loss: 5.504701]\n",
            "5840 [D loss: 0.419590, acc.: 82.42%] [G loss: 5.453451]\n",
            "5860 [D loss: 0.273522, acc.: 91.80%] [G loss: 5.945286]\n",
            "5880 [D loss: 0.345261, acc.: 86.33%] [G loss: 5.456802]\n",
            "5900 [D loss: 0.327052, acc.: 83.59%] [G loss: 5.362304]\n",
            "5920 [D loss: 0.342705, acc.: 83.59%] [G loss: 5.935745]\n",
            "5940 [D loss: 0.264386, acc.: 89.06%] [G loss: 3.793453]\n",
            "5960 [D loss: 0.301584, acc.: 87.89%] [G loss: 5.095552]\n",
            "5980 [D loss: 0.363940, acc.: 84.77%] [G loss: 5.450327]\n",
            "6000 [D loss: 0.402290, acc.: 80.47%] [G loss: 4.837795]\n",
            "6020 [D loss: 0.348144, acc.: 83.98%] [G loss: 6.261810]\n",
            "6040 [D loss: 0.287552, acc.: 88.67%] [G loss: 5.299012]\n",
            "6060 [D loss: 0.445677, acc.: 77.34%] [G loss: 5.946408]\n",
            "6080 [D loss: 0.227243, acc.: 90.62%] [G loss: 6.162244]\n",
            "6100 [D loss: 0.353216, acc.: 83.20%] [G loss: 6.010668]\n",
            "6120 [D loss: 0.375577, acc.: 82.81%] [G loss: 5.656889]\n",
            "6140 [D loss: 0.345884, acc.: 86.33%] [G loss: 5.737312]\n",
            "6160 [D loss: 0.334394, acc.: 86.72%] [G loss: 5.291653]\n",
            "6180 [D loss: 0.300782, acc.: 85.94%] [G loss: 5.146754]\n",
            "6200 [D loss: 0.281913, acc.: 90.62%] [G loss: 5.748967]\n",
            "6220 [D loss: 0.334877, acc.: 85.94%] [G loss: 5.493914]\n",
            "6240 [D loss: 0.337611, acc.: 85.55%] [G loss: 5.100847]\n",
            "6260 [D loss: 0.324709, acc.: 84.77%] [G loss: 5.309240]\n",
            "6280 [D loss: 0.383528, acc.: 83.20%] [G loss: 5.204239]\n",
            "6300 [D loss: 0.456260, acc.: 78.12%] [G loss: 4.921412]\n",
            "6320 [D loss: 0.319257, acc.: 88.28%] [G loss: 4.922379]\n",
            "6340 [D loss: 0.347562, acc.: 85.16%] [G loss: 5.244738]\n",
            "6360 [D loss: 0.268125, acc.: 89.84%] [G loss: 5.325434]\n",
            "6380 [D loss: 0.284629, acc.: 87.50%] [G loss: 5.339823]\n",
            "6400 [D loss: 0.453858, acc.: 76.56%] [G loss: 5.390445]\n",
            "6420 [D loss: 0.307804, acc.: 86.72%] [G loss: 5.089771]\n",
            "6440 [D loss: 0.370477, acc.: 85.55%] [G loss: 5.368149]\n",
            "6460 [D loss: 0.339559, acc.: 85.16%] [G loss: 5.092026]\n",
            "6480 [D loss: 0.411319, acc.: 81.25%] [G loss: 4.833236]\n",
            "6500 [D loss: 0.332855, acc.: 87.50%] [G loss: 5.338369]\n",
            "6520 [D loss: 0.360680, acc.: 84.77%] [G loss: 5.107018]\n",
            "6540 [D loss: 0.360588, acc.: 82.81%] [G loss: 5.363896]\n",
            "6560 [D loss: 0.259306, acc.: 88.67%] [G loss: 5.369337]\n",
            "6580 [D loss: 0.339890, acc.: 83.20%] [G loss: 4.808709]\n",
            "6600 [D loss: 0.140767, acc.: 94.53%] [G loss: 5.386733]\n",
            "6620 [D loss: 0.347659, acc.: 87.50%] [G loss: 5.324479]\n",
            "6640 [D loss: 0.380079, acc.: 83.98%] [G loss: 4.730723]\n",
            "6660 [D loss: 0.366442, acc.: 83.59%] [G loss: 6.020202]\n",
            "6680 [D loss: 0.331211, acc.: 86.72%] [G loss: 5.635776]\n",
            "6700 [D loss: 0.316687, acc.: 87.11%] [G loss: 5.261915]\n",
            "6720 [D loss: 0.263666, acc.: 87.50%] [G loss: 5.789947]\n",
            "6740 [D loss: 0.450168, acc.: 77.34%] [G loss: 5.162899]\n",
            "6760 [D loss: 0.289071, acc.: 90.62%] [G loss: 5.552494]\n",
            "6780 [D loss: 0.316516, acc.: 86.33%] [G loss: 5.380622]\n",
            "6800 [D loss: 0.350055, acc.: 86.72%] [G loss: 5.656824]\n",
            "6820 [D loss: 0.322616, acc.: 87.50%] [G loss: 5.498057]\n",
            "6840 [D loss: 0.322218, acc.: 86.33%] [G loss: 5.597958]\n",
            "6860 [D loss: 0.340773, acc.: 84.77%] [G loss: 5.283210]\n",
            "6880 [D loss: 0.350483, acc.: 87.89%] [G loss: 5.216579]\n",
            "6900 [D loss: 0.244983, acc.: 89.84%] [G loss: 5.237999]\n",
            "6920 [D loss: 0.318166, acc.: 87.11%] [G loss: 5.810130]\n",
            "6940 [D loss: 0.416369, acc.: 79.69%] [G loss: 5.419020]\n",
            "6960 [D loss: 0.433403, acc.: 82.81%] [G loss: 6.122477]\n",
            "6980 [D loss: 0.337892, acc.: 86.33%] [G loss: 5.497224]\n",
            "7000 [D loss: 0.373318, acc.: 83.20%] [G loss: 5.608348]\n",
            "7020 [D loss: 0.269202, acc.: 88.67%] [G loss: 5.577579]\n",
            "7040 [D loss: 0.088703, acc.: 98.44%] [G loss: 3.912928]\n",
            "7060 [D loss: 0.328243, acc.: 85.94%] [G loss: 4.819490]\n",
            "7080 [D loss: 0.420681, acc.: 78.12%] [G loss: 5.446097]\n",
            "7100 [D loss: 0.448072, acc.: 79.69%] [G loss: 4.985517]\n",
            "7120 [D loss: 0.429468, acc.: 79.30%] [G loss: 5.316717]\n",
            "7140 [D loss: 0.312819, acc.: 87.89%] [G loss: 5.414249]\n",
            "7160 [D loss: 0.332111, acc.: 86.72%] [G loss: 5.827503]\n",
            "7180 [D loss: 0.238801, acc.: 89.45%] [G loss: 5.157704]\n",
            "7200 [D loss: 0.307728, acc.: 85.55%] [G loss: 5.356025]\n",
            "7220 [D loss: 0.321008, acc.: 83.98%] [G loss: 5.497461]\n",
            "7240 [D loss: 0.357018, acc.: 84.77%] [G loss: 5.168530]\n",
            "7260 [D loss: 0.404600, acc.: 82.03%] [G loss: 5.734508]\n",
            "7280 [D loss: 0.351773, acc.: 81.64%] [G loss: 5.576539]\n",
            "7300 [D loss: 0.271966, acc.: 88.67%] [G loss: 5.642989]\n",
            "7320 [D loss: 0.271960, acc.: 87.50%] [G loss: 5.433709]\n",
            "7340 [D loss: 0.310674, acc.: 85.55%] [G loss: 5.472804]\n",
            "7360 [D loss: 0.297896, acc.: 87.89%] [G loss: 5.940438]\n",
            "7380 [D loss: 0.315322, acc.: 85.55%] [G loss: 5.497225]\n",
            "7400 [D loss: 0.260705, acc.: 90.62%] [G loss: 5.451283]\n",
            "7420 [D loss: 0.347332, acc.: 83.20%] [G loss: 5.647743]\n",
            "7440 [D loss: 0.437393, acc.: 80.86%] [G loss: 5.248051]\n",
            "7460 [D loss: 0.309130, acc.: 87.89%] [G loss: 5.564451]\n",
            "7480 [D loss: 0.258613, acc.: 90.23%] [G loss: 5.398925]\n",
            "7500 [D loss: 0.373239, acc.: 85.55%] [G loss: 5.228782]\n",
            "7520 [D loss: 0.362169, acc.: 83.59%] [G loss: 5.316082]\n",
            "7540 [D loss: 0.237251, acc.: 89.45%] [G loss: 5.718632]\n",
            "7560 [D loss: 0.389103, acc.: 81.64%] [G loss: 5.932398]\n",
            "7580 [D loss: 0.286184, acc.: 89.06%] [G loss: 5.214523]\n",
            "7600 [D loss: 0.367061, acc.: 85.55%] [G loss: 5.895893]\n",
            "7620 [D loss: 0.338533, acc.: 85.94%] [G loss: 5.206831]\n",
            "7640 [D loss: 0.390912, acc.: 82.42%] [G loss: 5.079202]\n",
            "7660 [D loss: 0.307353, acc.: 89.45%] [G loss: 4.937565]\n",
            "7680 [D loss: 0.323087, acc.: 88.28%] [G loss: 5.009768]\n",
            "7700 [D loss: 0.308532, acc.: 89.06%] [G loss: 5.077505]\n",
            "7720 [D loss: 0.298557, acc.: 89.06%] [G loss: 5.603384]\n",
            "7740 [D loss: 0.280219, acc.: 88.28%] [G loss: 5.810865]\n",
            "7760 [D loss: 0.355162, acc.: 83.98%] [G loss: 5.248317]\n",
            "7780 [D loss: 0.324720, acc.: 87.50%] [G loss: 5.954989]\n",
            "7800 [D loss: 0.315485, acc.: 87.50%] [G loss: 5.386941]\n",
            "7820 [D loss: 0.510927, acc.: 75.00%] [G loss: 5.247611]\n",
            "7840 [D loss: 0.834379, acc.: 64.45%] [G loss: 4.532631]\n",
            "7860 [D loss: 0.543879, acc.: 72.27%] [G loss: 3.511292]\n",
            "7880 [D loss: 0.712902, acc.: 58.20%] [G loss: 2.758089]\n",
            "7900 [D loss: 0.697044, acc.: 58.98%] [G loss: 2.727075]\n",
            "7920 [D loss: 0.569789, acc.: 72.66%] [G loss: 3.217299]\n",
            "7940 [D loss: 0.593531, acc.: 68.36%] [G loss: 3.060927]\n",
            "7960 [D loss: 0.450351, acc.: 81.64%] [G loss: 3.634347]\n",
            "7980 [D loss: 0.427071, acc.: 76.95%] [G loss: 4.088515]\n",
            "8000 [D loss: 0.403474, acc.: 81.64%] [G loss: 4.158008]\n",
            "8020 [D loss: 0.264738, acc.: 89.06%] [G loss: 4.936428]\n",
            "8040 [D loss: 0.573446, acc.: 75.39%] [G loss: 3.190813]\n",
            "8060 [D loss: 0.286453, acc.: 86.33%] [G loss: 5.337687]\n",
            "8080 [D loss: 0.353029, acc.: 86.33%] [G loss: 5.581841]\n",
            "8100 [D loss: 0.340519, acc.: 84.38%] [G loss: 6.170039]\n",
            "8120 [D loss: 0.395155, acc.: 82.03%] [G loss: 5.883152]\n",
            "8140 [D loss: 0.319650, acc.: 83.98%] [G loss: 5.560670]\n",
            "8160 [D loss: 0.368742, acc.: 85.16%] [G loss: 5.223540]\n",
            "8180 [D loss: 0.317306, acc.: 86.72%] [G loss: 6.455637]\n",
            "8200 [D loss: 0.360140, acc.: 87.11%] [G loss: 5.370908]\n",
            "8220 [D loss: 0.297327, acc.: 88.67%] [G loss: 6.195693]\n",
            "8240 [D loss: 0.468766, acc.: 77.73%] [G loss: 4.452073]\n",
            "8260 [D loss: 0.342802, acc.: 83.98%] [G loss: 5.458196]\n",
            "8280 [D loss: 0.330584, acc.: 85.55%] [G loss: 4.843324]\n",
            "8300 [D loss: 0.348265, acc.: 86.33%] [G loss: 4.559250]\n",
            "8320 [D loss: 0.306799, acc.: 86.72%] [G loss: 5.830043]\n",
            "8340 [D loss: 0.432454, acc.: 82.03%] [G loss: 4.482040]\n",
            "8360 [D loss: 0.376301, acc.: 83.98%] [G loss: 5.358859]\n",
            "8380 [D loss: 0.282868, acc.: 88.67%] [G loss: 5.618600]\n",
            "8400 [D loss: 0.336804, acc.: 82.81%] [G loss: 5.274965]\n",
            "8420 [D loss: 0.363905, acc.: 83.20%] [G loss: 5.295593]\n",
            "8440 [D loss: 0.443801, acc.: 77.34%] [G loss: 4.648054]\n",
            "8460 [D loss: 0.347363, acc.: 86.72%] [G loss: 4.994941]\n",
            "8480 [D loss: 0.335500, acc.: 85.94%] [G loss: 4.914373]\n",
            "8500 [D loss: 0.316940, acc.: 87.89%] [G loss: 4.927286]\n",
            "8520 [D loss: 0.316858, acc.: 85.55%] [G loss: 4.926901]\n",
            "8540 [D loss: 0.358397, acc.: 84.38%] [G loss: 4.686515]\n",
            "8560 [D loss: 0.340775, acc.: 86.33%] [G loss: 4.704080]\n",
            "8580 [D loss: 0.328553, acc.: 84.77%] [G loss: 4.910675]\n",
            "8600 [D loss: 0.311557, acc.: 89.84%] [G loss: 4.430941]\n",
            "8620 [D loss: 0.300677, acc.: 88.28%] [G loss: 5.097423]\n",
            "8640 [D loss: 0.379789, acc.: 85.16%] [G loss: 4.683819]\n",
            "8660 [D loss: 0.383746, acc.: 85.55%] [G loss: 4.533529]\n",
            "8680 [D loss: 0.366701, acc.: 83.20%] [G loss: 4.618659]\n",
            "8700 [D loss: 0.438025, acc.: 81.25%] [G loss: 4.483653]\n",
            "8720 [D loss: 0.291162, acc.: 89.84%] [G loss: 4.547482]\n",
            "8740 [D loss: 0.272211, acc.: 89.45%] [G loss: 5.259771]\n",
            "8760 [D loss: 0.300338, acc.: 89.06%] [G loss: 4.994818]\n",
            "8780 [D loss: 0.165917, acc.: 95.70%] [G loss: 4.161804]\n",
            "8800 [D loss: 0.262181, acc.: 89.06%] [G loss: 4.381589]\n",
            "8820 [D loss: 0.366845, acc.: 84.77%] [G loss: 4.678288]\n",
            "8840 [D loss: 0.356838, acc.: 84.38%] [G loss: 4.391065]\n",
            "8860 [D loss: 0.317773, acc.: 86.72%] [G loss: 5.087899]\n",
            "8880 [D loss: 0.301357, acc.: 89.06%] [G loss: 5.207426]\n",
            "8900 [D loss: 0.415138, acc.: 81.25%] [G loss: 4.783024]\n",
            "8920 [D loss: 0.261798, acc.: 89.45%] [G loss: 4.700616]\n",
            "8940 [D loss: 0.290867, acc.: 87.89%] [G loss: 5.362225]\n",
            "8960 [D loss: 0.302861, acc.: 89.45%] [G loss: 5.130871]\n",
            "8980 [D loss: 0.405977, acc.: 82.42%] [G loss: 4.672053]\n",
            "9000 [D loss: 0.301594, acc.: 86.33%] [G loss: 4.924905]\n",
            "9020 [D loss: 0.374966, acc.: 85.16%] [G loss: 4.598363]\n",
            "9040 [D loss: 0.356573, acc.: 83.98%] [G loss: 4.320249]\n",
            "9060 [D loss: 0.291078, acc.: 89.84%] [G loss: 5.025998]\n",
            "9080 [D loss: 0.284955, acc.: 89.06%] [G loss: 4.873401]\n",
            "9100 [D loss: 0.340587, acc.: 84.38%] [G loss: 4.727383]\n",
            "9120 [D loss: 0.409597, acc.: 82.42%] [G loss: 4.274794]\n",
            "9140 [D loss: 0.340841, acc.: 85.94%] [G loss: 4.420088]\n",
            "9160 [D loss: 0.367653, acc.: 87.11%] [G loss: 4.632758]\n",
            "9180 [D loss: 0.343595, acc.: 85.55%] [G loss: 4.362648]\n",
            "9200 [D loss: 0.402040, acc.: 83.98%] [G loss: 4.331769]\n",
            "9220 [D loss: 0.338936, acc.: 85.94%] [G loss: 4.614621]\n",
            "9240 [D loss: 0.314944, acc.: 85.94%] [G loss: 4.928287]\n",
            "9260 [D loss: 0.313025, acc.: 87.11%] [G loss: 4.394022]\n",
            "9280 [D loss: 0.340353, acc.: 87.11%] [G loss: 4.729609]\n",
            "9300 [D loss: 0.388792, acc.: 82.81%] [G loss: 4.618552]\n",
            "9320 [D loss: 0.323104, acc.: 84.38%] [G loss: 4.993255]\n",
            "9340 [D loss: 0.388894, acc.: 85.94%] [G loss: 4.203055]\n",
            "9360 [D loss: 0.432078, acc.: 81.64%] [G loss: 4.226387]\n",
            "9380 [D loss: 0.400364, acc.: 81.25%] [G loss: 4.625824]\n",
            "9400 [D loss: 0.405556, acc.: 83.59%] [G loss: 4.488070]\n",
            "9420 [D loss: 0.323865, acc.: 85.94%] [G loss: 4.650316]\n",
            "9440 [D loss: 0.354810, acc.: 85.55%] [G loss: 4.565337]\n",
            "9460 [D loss: 0.413705, acc.: 81.64%] [G loss: 4.067022]\n",
            "9480 [D loss: 0.342281, acc.: 84.77%] [G loss: 4.250812]\n",
            "9500 [D loss: 0.310325, acc.: 87.50%] [G loss: 4.318416]\n",
            "9520 [D loss: 0.323131, acc.: 88.67%] [G loss: 4.726884]\n",
            "9540 [D loss: 0.360582, acc.: 84.38%] [G loss: 4.409189]\n",
            "9560 [D loss: 0.355458, acc.: 82.81%] [G loss: 4.455582]\n",
            "9580 [D loss: 0.338105, acc.: 85.55%] [G loss: 4.128982]\n",
            "9600 [D loss: 0.347263, acc.: 87.89%] [G loss: 4.534519]\n",
            "9620 [D loss: 0.298550, acc.: 87.89%] [G loss: 4.690908]\n",
            "9640 [D loss: 0.349257, acc.: 84.77%] [G loss: 4.295678]\n",
            "9660 [D loss: 0.279677, acc.: 88.67%] [G loss: 4.817104]\n",
            "9680 [D loss: 0.357021, acc.: 84.77%] [G loss: 4.612180]\n",
            "9700 [D loss: 0.225340, acc.: 91.02%] [G loss: 3.468170]\n",
            "9720 [D loss: 0.408100, acc.: 80.86%] [G loss: 4.110803]\n",
            "9740 [D loss: 0.354475, acc.: 87.89%] [G loss: 4.343664]\n",
            "9760 [D loss: 0.454871, acc.: 81.64%] [G loss: 4.114688]\n",
            "9780 [D loss: 0.265703, acc.: 89.84%] [G loss: 4.728211]\n",
            "9800 [D loss: 0.389367, acc.: 84.38%] [G loss: 4.563790]\n",
            "9820 [D loss: 0.375536, acc.: 83.20%] [G loss: 4.234245]\n",
            "9840 [D loss: 0.290670, acc.: 88.28%] [G loss: 4.791508]\n",
            "9860 [D loss: 0.300152, acc.: 87.89%] [G loss: 4.726147]\n",
            "9880 [D loss: 0.333102, acc.: 87.89%] [G loss: 4.696382]\n",
            "9900 [D loss: 0.356743, acc.: 85.55%] [G loss: 4.687780]\n",
            "9920 [D loss: 0.362884, acc.: 84.38%] [G loss: 4.338201]\n",
            "9940 [D loss: 0.310876, acc.: 86.72%] [G loss: 4.624803]\n",
            "9960 [D loss: 0.325089, acc.: 87.11%] [G loss: 5.075215]\n",
            "9980 [D loss: 0.354865, acc.: 85.55%] [G loss: 4.868052]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKyOoPom5Zz",
        "outputId": "c8d3af2f-3729-4904-f9a0-9cb8c6d9a8f8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 48, 48, 128)  3328        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 100)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          51712       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 2304)      16128       input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1152)         0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2304)         0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_15[0][0]                 \n",
            "                                                                 flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            3969        concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.775621, acc.: 20.31%] [G loss: 1.527493]\n",
            "20 [D loss: 1.136945, acc.: 31.64%] [G loss: 1.106470]\n",
            "40 [D loss: 0.436015, acc.: 49.61%] [G loss: 17.784233]\n",
            "60 [D loss: 0.537491, acc.: 73.83%] [G loss: 4.473651]\n",
            "80 [D loss: 0.834621, acc.: 22.66%] [G loss: 1.869293]\n",
            "100 [D loss: 0.741704, acc.: 39.45%] [G loss: 1.638891]\n",
            "120 [D loss: 0.575083, acc.: 82.81%] [G loss: 2.042169]\n",
            "140 [D loss: 0.455398, acc.: 78.12%] [G loss: 3.064530]\n",
            "160 [D loss: 1.021226, acc.: 2.73%] [G loss: 1.166114]\n",
            "180 [D loss: 0.558926, acc.: 76.17%] [G loss: 2.472870]\n",
            "200 [D loss: 0.770760, acc.: 41.80%] [G loss: 1.451331]\n",
            "220 [D loss: 0.876846, acc.: 49.22%] [G loss: 3.595408]\n",
            "240 [D loss: 0.186598, acc.: 100.00%] [G loss: 7.332498]\n",
            "260 [D loss: 0.682056, acc.: 68.75%] [G loss: 1.890666]\n",
            "280 [D loss: 0.209264, acc.: 95.31%] [G loss: 10.532845]\n",
            "300 [D loss: 0.691736, acc.: 60.55%] [G loss: 1.672787]\n",
            "320 [D loss: 0.672510, acc.: 71.48%] [G loss: 1.888943]\n",
            "340 [D loss: 0.385103, acc.: 89.06%] [G loss: 4.332753]\n",
            "360 [D loss: 0.380074, acc.: 84.77%] [G loss: 4.375402]\n",
            "380 [D loss: 0.762896, acc.: 57.81%] [G loss: 1.722982]\n",
            "400 [D loss: 0.730825, acc.: 46.09%] [G loss: 2.234101]\n",
            "420 [D loss: 0.644904, acc.: 57.81%] [G loss: 2.380552]\n",
            "440 [D loss: 0.686306, acc.: 59.77%] [G loss: 2.222342]\n",
            "460 [D loss: 0.746045, acc.: 54.69%] [G loss: 1.957719]\n",
            "480 [D loss: 0.601959, acc.: 67.58%] [G loss: 2.125723]\n",
            "500 [D loss: 0.650282, acc.: 62.11%] [G loss: 2.268615]\n",
            "520 [D loss: 0.609233, acc.: 67.58%] [G loss: 2.602067]\n",
            "540 [D loss: 0.572285, acc.: 69.53%] [G loss: 2.928072]\n",
            "560 [D loss: 0.618016, acc.: 66.41%] [G loss: 2.138952]\n",
            "580 [D loss: 0.671801, acc.: 58.20%] [G loss: 1.966664]\n",
            "600 [D loss: 0.712011, acc.: 48.83%] [G loss: 1.836228]\n",
            "620 [D loss: 0.744234, acc.: 47.27%] [G loss: 1.852621]\n",
            "640 [D loss: 0.625981, acc.: 65.23%] [G loss: 2.130571]\n",
            "660 [D loss: 0.605573, acc.: 68.75%] [G loss: 2.238631]\n",
            "680 [D loss: 0.745590, acc.: 51.17%] [G loss: 1.863794]\n",
            "700 [D loss: 0.541753, acc.: 73.05%] [G loss: 2.539672]\n",
            "720 [D loss: 0.546152, acc.: 75.78%] [G loss: 2.414199]\n",
            "740 [D loss: 0.617598, acc.: 65.62%] [G loss: 2.347521]\n",
            "760 [D loss: 0.445802, acc.: 83.98%] [G loss: 3.243727]\n",
            "780 [D loss: 0.495237, acc.: 78.12%] [G loss: 2.732445]\n",
            "800 [D loss: 0.560501, acc.: 75.39%] [G loss: 2.522155]\n",
            "820 [D loss: 0.555277, acc.: 67.97%] [G loss: 2.861588]\n",
            "840 [D loss: 0.536461, acc.: 76.95%] [G loss: 2.788723]\n",
            "860 [D loss: 0.578135, acc.: 70.70%] [G loss: 3.652322]\n",
            "880 [D loss: 0.623166, acc.: 65.23%] [G loss: 2.995444]\n",
            "900 [D loss: 0.660175, acc.: 62.89%] [G loss: 2.713311]\n",
            "920 [D loss: 0.448838, acc.: 81.25%] [G loss: 3.552639]\n",
            "940 [D loss: 0.507613, acc.: 76.56%] [G loss: 3.297152]\n",
            "960 [D loss: 0.538144, acc.: 73.05%] [G loss: 3.129484]\n",
            "980 [D loss: 0.657594, acc.: 61.72%] [G loss: 3.054131]\n",
            "1000 [D loss: 0.435557, acc.: 84.77%] [G loss: 3.602971]\n",
            "1020 [D loss: 0.389269, acc.: 85.16%] [G loss: 4.215716]\n",
            "1040 [D loss: 0.441645, acc.: 80.47%] [G loss: 3.759854]\n",
            "1060 [D loss: 0.417450, acc.: 81.64%] [G loss: 3.896385]\n",
            "1080 [D loss: 0.648562, acc.: 64.84%] [G loss: 3.626264]\n",
            "1100 [D loss: 0.402189, acc.: 83.98%] [G loss: 3.678422]\n",
            "1120 [D loss: 0.418486, acc.: 82.81%] [G loss: 3.632628]\n",
            "1140 [D loss: 0.467013, acc.: 76.95%] [G loss: 3.824034]\n",
            "1160 [D loss: 0.570863, acc.: 70.31%] [G loss: 3.044311]\n",
            "1180 [D loss: 0.511243, acc.: 76.95%] [G loss: 3.353204]\n",
            "1200 [D loss: 0.478253, acc.: 78.12%] [G loss: 3.580424]\n",
            "1220 [D loss: 0.470860, acc.: 77.73%] [G loss: 3.401241]\n",
            "1240 [D loss: 0.509738, acc.: 75.00%] [G loss: 3.294276]\n",
            "1260 [D loss: 0.510494, acc.: 78.91%] [G loss: 3.284930]\n",
            "1280 [D loss: 0.589470, acc.: 67.19%] [G loss: 3.138620]\n",
            "1300 [D loss: 0.555125, acc.: 73.05%] [G loss: 2.883266]\n",
            "1320 [D loss: 0.479775, acc.: 79.30%] [G loss: 3.127534]\n",
            "1340 [D loss: 0.498140, acc.: 75.78%] [G loss: 3.167541]\n",
            "1360 [D loss: 0.530504, acc.: 76.56%] [G loss: 2.911742]\n",
            "1380 [D loss: 0.482003, acc.: 77.34%] [G loss: 3.266805]\n",
            "1400 [D loss: 0.470014, acc.: 80.47%] [G loss: 3.325715]\n",
            "1420 [D loss: 0.546466, acc.: 73.44%] [G loss: 3.093244]\n",
            "1440 [D loss: 0.548517, acc.: 73.83%] [G loss: 3.152225]\n",
            "1460 [D loss: 0.533497, acc.: 71.88%] [G loss: 3.297659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1-QlEvelsO",
        "outputId": "d5e6db9b-1b83-4118-fa9f-1173864d140b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      16128       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_73 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_33 (Flatten)            (None, 2304)         0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_32 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4708)         0           input_73[0][0]                   \n",
            "                                                                 flatten_33[0][0]                 \n",
            "                                                                 flatten_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 1024)         4822016     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, 1024)         0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 1024)         0           leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1024)         1049600     dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1024)         0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 1024)         0           leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 1024)         1049600     dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1024)         0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 1024)         0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 1)            1025        dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.849750, acc.: 30.47%] [G loss: 5.367685]\n",
            "20 [D loss: 0.341891, acc.: 91.41%] [G loss: 8.554846]\n",
            "40 [D loss: 0.710111, acc.: 44.14%] [G loss: 2.083147]\n",
            "60 [D loss: 0.715512, acc.: 46.48%] [G loss: 1.738583]\n",
            "80 [D loss: 0.648917, acc.: 56.25%] [G loss: 1.863081]\n",
            "100 [D loss: 0.701115, acc.: 44.92%] [G loss: 1.767049]\n",
            "120 [D loss: 0.732758, acc.: 40.23%] [G loss: 1.580268]\n",
            "140 [D loss: 0.706723, acc.: 48.05%] [G loss: 1.537512]\n",
            "160 [D loss: 0.717456, acc.: 40.23%] [G loss: 1.437199]\n",
            "180 [D loss: 0.710523, acc.: 42.97%] [G loss: 1.439812]\n",
            "200 [D loss: 0.711228, acc.: 45.70%] [G loss: 1.451416]\n",
            "220 [D loss: 0.721137, acc.: 35.55%] [G loss: 1.414149]\n",
            "240 [D loss: 0.714687, acc.: 42.58%] [G loss: 1.432792]\n",
            "260 [D loss: 0.703062, acc.: 46.09%] [G loss: 1.423483]\n",
            "280 [D loss: 0.724828, acc.: 36.72%] [G loss: 1.404998]\n",
            "300 [D loss: 0.716795, acc.: 42.58%] [G loss: 1.396966]\n",
            "320 [D loss: 0.720376, acc.: 42.97%] [G loss: 1.444355]\n",
            "340 [D loss: 0.706069, acc.: 46.88%] [G loss: 1.423882]\n",
            "360 [D loss: 0.709865, acc.: 39.06%] [G loss: 1.404603]\n",
            "380 [D loss: 0.714929, acc.: 43.75%] [G loss: 1.404289]\n",
            "400 [D loss: 0.711042, acc.: 42.97%] [G loss: 1.414505]\n",
            "420 [D loss: 0.717568, acc.: 40.23%] [G loss: 1.403865]\n",
            "440 [D loss: 0.711510, acc.: 40.23%] [G loss: 1.410104]\n",
            "460 [D loss: 0.714272, acc.: 41.41%] [G loss: 1.419285]\n",
            "480 [D loss: 0.708251, acc.: 41.02%] [G loss: 1.413074]\n",
            "500 [D loss: 0.706564, acc.: 40.62%] [G loss: 1.414759]\n",
            "520 [D loss: 0.711570, acc.: 39.84%] [G loss: 1.416901]\n",
            "540 [D loss: 0.707966, acc.: 44.14%] [G loss: 1.396562]\n",
            "560 [D loss: 0.714514, acc.: 40.62%] [G loss: 1.400588]\n",
            "580 [D loss: 0.708786, acc.: 39.06%] [G loss: 1.401240]\n",
            "600 [D loss: 0.715925, acc.: 37.50%] [G loss: 1.401176]\n",
            "620 [D loss: 0.700743, acc.: 42.19%] [G loss: 1.417309]\n",
            "640 [D loss: 0.706829, acc.: 40.23%] [G loss: 1.403668]\n",
            "660 [D loss: 0.707380, acc.: 39.84%] [G loss: 1.389831]\n",
            "680 [D loss: 0.707225, acc.: 39.84%] [G loss: 1.398783]\n",
            "700 [D loss: 0.703080, acc.: 40.62%] [G loss: 1.396986]\n",
            "720 [D loss: 0.718166, acc.: 34.38%] [G loss: 1.389264]\n",
            "740 [D loss: 0.713579, acc.: 40.23%] [G loss: 1.403226]\n",
            "760 [D loss: 0.705897, acc.: 45.31%] [G loss: 1.437489]\n",
            "780 [D loss: 0.707634, acc.: 40.62%] [G loss: 1.401700]\n",
            "800 [D loss: 0.701959, acc.: 39.06%] [G loss: 1.398977]\n",
            "820 [D loss: 0.706529, acc.: 42.19%] [G loss: 1.400484]\n",
            "840 [D loss: 0.700337, acc.: 41.02%] [G loss: 1.400969]\n",
            "860 [D loss: 0.706927, acc.: 38.67%] [G loss: 1.405506]\n",
            "880 [D loss: 0.708806, acc.: 38.28%] [G loss: 1.387069]\n",
            "900 [D loss: 0.706434, acc.: 41.41%] [G loss: 1.398114]\n",
            "920 [D loss: 0.704907, acc.: 35.94%] [G loss: 1.406425]\n",
            "940 [D loss: 0.696471, acc.: 47.66%] [G loss: 1.392628]\n",
            "960 [D loss: 0.705585, acc.: 41.02%] [G loss: 1.396953]\n",
            "980 [D loss: 0.705313, acc.: 39.84%] [G loss: 1.393413]\n",
            "1000 [D loss: 0.693701, acc.: 48.44%] [G loss: 1.410988]\n",
            "1020 [D loss: 0.707825, acc.: 38.28%] [G loss: 1.393575]\n",
            "1040 [D loss: 0.700687, acc.: 44.53%] [G loss: 1.400779]\n",
            "1060 [D loss: 0.703935, acc.: 39.84%] [G loss: 1.387593]\n",
            "1080 [D loss: 0.706629, acc.: 39.84%] [G loss: 1.389481]\n",
            "1100 [D loss: 0.707315, acc.: 37.89%] [G loss: 1.378371]\n",
            "1120 [D loss: 0.707380, acc.: 42.97%] [G loss: 1.383276]\n",
            "1140 [D loss: 0.701792, acc.: 45.31%] [G loss: 1.392983]\n",
            "1160 [D loss: 0.698018, acc.: 41.41%] [G loss: 1.396971]\n",
            "1180 [D loss: 0.710089, acc.: 35.94%] [G loss: 1.389826]\n",
            "1200 [D loss: 0.704007, acc.: 40.23%] [G loss: 1.392146]\n",
            "1220 [D loss: 0.704774, acc.: 42.97%] [G loss: 1.406959]\n",
            "1240 [D loss: 0.699959, acc.: 41.41%] [G loss: 1.412235]\n",
            "1260 [D loss: 0.702670, acc.: 44.53%] [G loss: 1.408520]\n",
            "1280 [D loss: 0.704078, acc.: 41.41%] [G loss: 1.384988]\n",
            "1300 [D loss: 0.707281, acc.: 40.62%] [G loss: 1.399330]\n",
            "1320 [D loss: 0.709640, acc.: 39.06%] [G loss: 1.400736]\n",
            "1340 [D loss: 0.697176, acc.: 46.88%] [G loss: 1.408559]\n",
            "1360 [D loss: 0.690890, acc.: 46.09%] [G loss: 1.407980]\n",
            "1380 [D loss: 0.695711, acc.: 48.44%] [G loss: 1.384939]\n",
            "1400 [D loss: 0.696284, acc.: 41.41%] [G loss: 1.391898]\n",
            "1420 [D loss: 0.702413, acc.: 41.02%] [G loss: 1.391516]\n",
            "1440 [D loss: 0.702305, acc.: 43.75%] [G loss: 1.404577]\n",
            "1460 [D loss: 0.702467, acc.: 40.62%] [G loss: 1.401769]\n",
            "1480 [D loss: 0.700135, acc.: 39.84%] [G loss: 1.391395]\n",
            "1500 [D loss: 0.701792, acc.: 41.80%] [G loss: 1.390833]\n",
            "1520 [D loss: 0.700585, acc.: 42.19%] [G loss: 1.394488]\n",
            "1540 [D loss: 0.704487, acc.: 41.41%] [G loss: 1.397046]\n",
            "1560 [D loss: 0.695279, acc.: 46.48%] [G loss: 1.408600]\n",
            "1580 [D loss: 0.695438, acc.: 48.83%] [G loss: 1.392757]\n",
            "1600 [D loss: 0.704662, acc.: 38.67%] [G loss: 1.390574]\n",
            "1620 [D loss: 0.699533, acc.: 46.09%] [G loss: 1.401004]\n",
            "1640 [D loss: 0.702735, acc.: 43.75%] [G loss: 1.398700]\n",
            "1660 [D loss: 0.697723, acc.: 43.75%] [G loss: 1.401226]\n",
            "1680 [D loss: 0.697773, acc.: 41.80%] [G loss: 1.410201]\n",
            "1700 [D loss: 0.696773, acc.: 43.36%] [G loss: 1.407255]\n",
            "1720 [D loss: 0.701552, acc.: 39.45%] [G loss: 1.395131]\n",
            "1740 [D loss: 0.696195, acc.: 48.44%] [G loss: 1.396674]\n",
            "1760 [D loss: 0.702511, acc.: 43.36%] [G loss: 1.383245]\n",
            "1780 [D loss: 0.704975, acc.: 40.62%] [G loss: 1.399527]\n",
            "1800 [D loss: 0.698895, acc.: 41.41%] [G loss: 1.391627]\n",
            "1820 [D loss: 0.702525, acc.: 37.50%] [G loss: 1.391825]\n",
            "1840 [D loss: 0.698295, acc.: 46.48%] [G loss: 1.401908]\n",
            "1860 [D loss: 0.699764, acc.: 42.97%] [G loss: 1.399934]\n",
            "1880 [D loss: 0.702305, acc.: 41.80%] [G loss: 1.396282]\n",
            "1900 [D loss: 0.705898, acc.: 42.19%] [G loss: 1.384895]\n",
            "1920 [D loss: 0.700531, acc.: 44.92%] [G loss: 1.391573]\n",
            "1940 [D loss: 0.707957, acc.: 41.02%] [G loss: 1.386891]\n",
            "1960 [D loss: 0.700286, acc.: 41.41%] [G loss: 1.396966]\n",
            "1980 [D loss: 0.697184, acc.: 42.19%] [G loss: 1.398432]\n",
            "2000 [D loss: 0.701151, acc.: 46.09%] [G loss: 1.408123]\n",
            "2020 [D loss: 0.707549, acc.: 41.80%] [G loss: 1.400710]\n",
            "2040 [D loss: 0.706370, acc.: 41.80%] [G loss: 1.382454]\n",
            "2060 [D loss: 0.696820, acc.: 44.14%] [G loss: 1.416820]\n",
            "2080 [D loss: 0.704532, acc.: 39.84%] [G loss: 1.388172]\n",
            "2100 [D loss: 0.698797, acc.: 41.80%] [G loss: 1.401481]\n",
            "2120 [D loss: 0.701321, acc.: 42.58%] [G loss: 1.405037]\n",
            "2140 [D loss: 0.700277, acc.: 42.58%] [G loss: 1.388465]\n",
            "2160 [D loss: 0.706183, acc.: 34.77%] [G loss: 1.395349]\n",
            "2180 [D loss: 0.700379, acc.: 44.53%] [G loss: 1.399946]\n",
            "2200 [D loss: 0.706776, acc.: 35.55%] [G loss: 1.397158]\n",
            "2220 [D loss: 0.690010, acc.: 49.61%] [G loss: 1.415059]\n",
            "2240 [D loss: 0.700388, acc.: 45.31%] [G loss: 1.396531]\n",
            "2260 [D loss: 0.698710, acc.: 41.41%] [G loss: 1.387498]\n",
            "2280 [D loss: 0.699097, acc.: 44.53%] [G loss: 1.400677]\n",
            "2300 [D loss: 0.698183, acc.: 41.41%] [G loss: 1.393741]\n",
            "2320 [D loss: 0.700072, acc.: 44.92%] [G loss: 1.387109]\n",
            "2340 [D loss: 0.702045, acc.: 43.75%] [G loss: 1.396456]\n",
            "2360 [D loss: 0.698545, acc.: 42.97%] [G loss: 1.386749]\n",
            "2380 [D loss: 0.701191, acc.: 44.53%] [G loss: 1.390651]\n",
            "2400 [D loss: 0.702897, acc.: 41.02%] [G loss: 1.385156]\n",
            "2420 [D loss: 0.705316, acc.: 41.02%] [G loss: 1.401502]\n",
            "2440 [D loss: 0.706111, acc.: 37.11%] [G loss: 1.388617]\n",
            "2460 [D loss: 0.702543, acc.: 41.02%] [G loss: 1.407373]\n",
            "2480 [D loss: 0.705135, acc.: 44.92%] [G loss: 1.386979]\n",
            "2500 [D loss: 0.699529, acc.: 43.75%] [G loss: 1.433604]\n",
            "2520 [D loss: 0.701719, acc.: 43.36%] [G loss: 1.395393]\n",
            "2540 [D loss: 0.699508, acc.: 42.58%] [G loss: 1.396002]\n",
            "2560 [D loss: 0.703406, acc.: 38.67%] [G loss: 1.390184]\n",
            "2580 [D loss: 0.702974, acc.: 38.67%] [G loss: 1.384311]\n",
            "2600 [D loss: 0.697291, acc.: 44.92%] [G loss: 1.391137]\n",
            "2620 [D loss: 0.698857, acc.: 46.48%] [G loss: 1.397241]\n",
            "2640 [D loss: 0.702192, acc.: 41.02%] [G loss: 1.391943]\n",
            "2660 [D loss: 0.703083, acc.: 35.94%] [G loss: 1.392532]\n",
            "2680 [D loss: 0.704199, acc.: 41.80%] [G loss: 1.383410]\n",
            "2700 [D loss: 0.705399, acc.: 40.62%] [G loss: 1.386579]\n",
            "2720 [D loss: 0.702500, acc.: 42.19%] [G loss: 1.398633]\n",
            "2740 [D loss: 0.701238, acc.: 41.41%] [G loss: 1.387710]\n",
            "2760 [D loss: 0.695434, acc.: 47.66%] [G loss: 1.398785]\n",
            "2780 [D loss: 0.698381, acc.: 46.48%] [G loss: 1.409770]\n",
            "2800 [D loss: 0.703224, acc.: 41.02%] [G loss: 1.398382]\n",
            "2820 [D loss: 0.704637, acc.: 42.97%] [G loss: 1.391324]\n",
            "2840 [D loss: 0.695273, acc.: 46.48%] [G loss: 1.403218]\n",
            "2860 [D loss: 0.703234, acc.: 45.31%] [G loss: 1.395308]\n",
            "2880 [D loss: 0.699889, acc.: 41.80%] [G loss: 1.389093]\n",
            "2900 [D loss: 0.704404, acc.: 42.19%] [G loss: 1.391987]\n",
            "2920 [D loss: 0.700413, acc.: 43.36%] [G loss: 1.394257]\n",
            "2940 [D loss: 0.701555, acc.: 45.31%] [G loss: 1.386916]\n",
            "2960 [D loss: 0.704574, acc.: 41.02%] [G loss: 1.397935]\n",
            "2980 [D loss: 0.701216, acc.: 38.28%] [G loss: 1.397118]\n",
            "3000 [D loss: 0.703859, acc.: 43.75%] [G loss: 1.386564]\n",
            "3020 [D loss: 0.697224, acc.: 47.27%] [G loss: 1.404107]\n",
            "3040 [D loss: 0.700284, acc.: 44.14%] [G loss: 1.408972]\n",
            "3060 [D loss: 0.710935, acc.: 39.45%] [G loss: 1.398036]\n",
            "3080 [D loss: 0.701705, acc.: 43.75%] [G loss: 1.385907]\n",
            "3100 [D loss: 0.703351, acc.: 42.19%] [G loss: 1.400969]\n",
            "3120 [D loss: 0.705543, acc.: 35.55%] [G loss: 1.395355]\n",
            "3140 [D loss: 0.705566, acc.: 41.02%] [G loss: 1.389833]\n",
            "3160 [D loss: 0.694014, acc.: 50.78%] [G loss: 1.399073]\n",
            "3180 [D loss: 0.696958, acc.: 47.27%] [G loss: 1.398360]\n",
            "3200 [D loss: 0.697163, acc.: 45.31%] [G loss: 1.396299]\n",
            "3220 [D loss: 0.704172, acc.: 39.45%] [G loss: 1.401600]\n",
            "3240 [D loss: 0.706146, acc.: 35.55%] [G loss: 1.386641]\n",
            "3260 [D loss: 0.693955, acc.: 46.88%] [G loss: 1.392742]\n",
            "3280 [D loss: 0.694101, acc.: 48.83%] [G loss: 1.400743]\n",
            "3300 [D loss: 0.699886, acc.: 42.58%] [G loss: 1.380824]\n",
            "3320 [D loss: 0.709387, acc.: 47.27%] [G loss: 1.398697]\n",
            "3340 [D loss: 0.699752, acc.: 48.44%] [G loss: 1.394596]\n",
            "3360 [D loss: 0.706852, acc.: 40.62%] [G loss: 1.405188]\n",
            "3380 [D loss: 0.699275, acc.: 44.53%] [G loss: 1.398795]\n",
            "3400 [D loss: 0.696006, acc.: 44.92%] [G loss: 1.421854]\n",
            "3420 [D loss: 0.696535, acc.: 48.05%] [G loss: 1.411503]\n",
            "3440 [D loss: 0.702331, acc.: 43.36%] [G loss: 1.399069]\n",
            "3460 [D loss: 0.705177, acc.: 42.19%] [G loss: 1.388886]\n",
            "3480 [D loss: 0.703387, acc.: 36.33%] [G loss: 1.393602]\n",
            "3500 [D loss: 0.701317, acc.: 38.67%] [G loss: 1.405229]\n",
            "3520 [D loss: 0.700779, acc.: 36.33%] [G loss: 1.394291]\n",
            "3540 [D loss: 0.704368, acc.: 43.75%] [G loss: 1.388223]\n",
            "3560 [D loss: 0.698320, acc.: 44.92%] [G loss: 1.408260]\n",
            "3580 [D loss: 0.699920, acc.: 41.80%] [G loss: 1.396066]\n",
            "3600 [D loss: 0.703215, acc.: 42.97%] [G loss: 1.393963]\n",
            "3620 [D loss: 0.704095, acc.: 42.58%] [G loss: 1.389001]\n",
            "3640 [D loss: 0.696503, acc.: 46.48%] [G loss: 1.391987]\n",
            "3660 [D loss: 0.706459, acc.: 39.45%] [G loss: 1.403823]\n",
            "3680 [D loss: 0.701097, acc.: 45.70%] [G loss: 1.391359]\n",
            "3700 [D loss: 0.700797, acc.: 40.62%] [G loss: 1.401428]\n",
            "3720 [D loss: 0.706842, acc.: 39.45%] [G loss: 1.401338]\n",
            "3740 [D loss: 0.695790, acc.: 48.44%] [G loss: 1.410640]\n",
            "3760 [D loss: 0.700871, acc.: 41.02%] [G loss: 1.389013]\n",
            "3780 [D loss: 0.698214, acc.: 45.31%] [G loss: 1.391579]\n",
            "3800 [D loss: 0.702306, acc.: 41.02%] [G loss: 1.390742]\n",
            "3820 [D loss: 0.699549, acc.: 44.92%] [G loss: 1.388456]\n",
            "3840 [D loss: 0.726881, acc.: 41.02%] [G loss: 1.427503]\n",
            "3860 [D loss: 0.706836, acc.: 40.23%] [G loss: 1.408166]\n",
            "3880 [D loss: 0.708650, acc.: 37.89%] [G loss: 1.388743]\n",
            "3900 [D loss: 0.703004, acc.: 42.19%] [G loss: 1.413317]\n",
            "3920 [D loss: 0.701894, acc.: 43.75%] [G loss: 1.391886]\n",
            "3940 [D loss: 0.695446, acc.: 49.22%] [G loss: 1.405801]\n",
            "3960 [D loss: 0.704857, acc.: 41.02%] [G loss: 1.401155]\n",
            "3980 [D loss: 0.699231, acc.: 42.58%] [G loss: 1.404975]\n",
            "4000 [D loss: 0.699956, acc.: 39.84%] [G loss: 1.397395]\n",
            "4020 [D loss: 0.693868, acc.: 48.83%] [G loss: 1.414370]\n",
            "4040 [D loss: 0.699592, acc.: 46.48%] [G loss: 1.402936]\n",
            "4060 [D loss: 0.697715, acc.: 45.31%] [G loss: 1.424391]\n",
            "4080 [D loss: 0.708574, acc.: 40.23%] [G loss: 1.381411]\n",
            "4100 [D loss: 1.072704, acc.: 84.38%] [G loss: 26.749468]\n",
            "4120 [D loss: 0.717382, acc.: 67.97%] [G loss: 6.265586]\n",
            "4140 [D loss: 1.001233, acc.: 50.00%] [G loss: 3.090046]\n",
            "4160 [D loss: 0.768635, acc.: 51.56%] [G loss: 2.125261]\n",
            "4180 [D loss: 0.740759, acc.: 46.48%] [G loss: 1.783273]\n",
            "4200 [D loss: 0.722959, acc.: 47.66%] [G loss: 1.520224]\n",
            "4220 [D loss: 0.734547, acc.: 45.31%] [G loss: 1.466105]\n",
            "4240 [D loss: 0.688544, acc.: 57.03%] [G loss: 1.615793]\n",
            "4260 [D loss: 0.738717, acc.: 43.75%] [G loss: 1.405132]\n",
            "4280 [D loss: 0.722575, acc.: 42.19%] [G loss: 1.447261]\n",
            "4300 [D loss: 0.690146, acc.: 49.61%] [G loss: 1.476141]\n",
            "4320 [D loss: 0.719582, acc.: 43.75%] [G loss: 1.429009]\n",
            "4340 [D loss: 0.720578, acc.: 45.70%] [G loss: 1.434604]\n",
            "4360 [D loss: 0.706739, acc.: 45.70%] [G loss: 1.414366]\n",
            "4380 [D loss: 0.702524, acc.: 48.44%] [G loss: 1.432420]\n",
            "4400 [D loss: 0.710964, acc.: 48.83%] [G loss: 1.441298]\n",
            "4420 [D loss: 0.711855, acc.: 46.48%] [G loss: 1.427701]\n",
            "4440 [D loss: 0.710840, acc.: 47.66%] [G loss: 1.458087]\n",
            "4460 [D loss: 0.705373, acc.: 44.53%] [G loss: 1.425750]\n",
            "4480 [D loss: 0.704472, acc.: 48.83%] [G loss: 1.433243]\n",
            "4500 [D loss: 0.698071, acc.: 51.17%] [G loss: 1.431295]\n",
            "4520 [D loss: 0.716092, acc.: 41.80%] [G loss: 1.414632]\n",
            "4540 [D loss: 0.709937, acc.: 44.53%] [G loss: 1.402517]\n",
            "4560 [D loss: 0.706339, acc.: 43.75%] [G loss: 1.417696]\n",
            "4580 [D loss: 0.701099, acc.: 49.61%] [G loss: 1.398619]\n",
            "4600 [D loss: 0.705118, acc.: 43.75%] [G loss: 1.420572]\n",
            "4620 [D loss: 0.706447, acc.: 43.75%] [G loss: 1.411010]\n",
            "4640 [D loss: 0.701780, acc.: 48.44%] [G loss: 1.398860]\n",
            "4660 [D loss: 0.705210, acc.: 48.05%] [G loss: 1.383090]\n",
            "4680 [D loss: 0.708879, acc.: 46.09%] [G loss: 1.397547]\n",
            "4700 [D loss: 0.710725, acc.: 42.97%] [G loss: 1.417038]\n",
            "4720 [D loss: 0.704634, acc.: 45.70%] [G loss: 1.393745]\n",
            "4740 [D loss: 0.696728, acc.: 51.56%] [G loss: 1.395010]\n",
            "4760 [D loss: 0.702551, acc.: 48.83%] [G loss: 1.404335]\n",
            "4780 [D loss: 0.699379, acc.: 46.48%] [G loss: 1.415344]\n",
            "4800 [D loss: 0.700585, acc.: 49.22%] [G loss: 1.406814]\n",
            "4820 [D loss: 0.698388, acc.: 46.88%] [G loss: 1.390556]\n",
            "4840 [D loss: 0.694551, acc.: 51.56%] [G loss: 1.385368]\n",
            "4860 [D loss: 0.694786, acc.: 53.12%] [G loss: 1.396678]\n",
            "4880 [D loss: 0.703839, acc.: 44.53%] [G loss: 1.405728]\n",
            "4900 [D loss: 0.705474, acc.: 44.53%] [G loss: 1.391244]\n",
            "4920 [D loss: 0.698867, acc.: 46.09%] [G loss: 1.391430]\n",
            "4940 [D loss: 0.705874, acc.: 39.45%] [G loss: 1.395215]\n",
            "4960 [D loss: 0.697893, acc.: 49.61%] [G loss: 1.396267]\n",
            "4980 [D loss: 0.709537, acc.: 39.84%] [G loss: 1.394031]\n",
            "5000 [D loss: 0.704992, acc.: 42.19%] [G loss: 1.393940]\n",
            "5020 [D loss: 0.702913, acc.: 48.05%] [G loss: 1.390844]\n",
            "5040 [D loss: 0.697565, acc.: 48.83%] [G loss: 1.388696]\n",
            "5060 [D loss: 0.700234, acc.: 42.58%] [G loss: 1.404058]\n",
            "5080 [D loss: 0.703479, acc.: 44.92%] [G loss: 1.406952]\n",
            "5100 [D loss: 0.693664, acc.: 49.22%] [G loss: 1.407932]\n",
            "5120 [D loss: 0.706131, acc.: 42.97%] [G loss: 1.404334]\n",
            "5140 [D loss: 0.700269, acc.: 48.44%] [G loss: 1.391555]\n",
            "5160 [D loss: 0.696768, acc.: 45.31%] [G loss: 1.396595]\n",
            "5180 [D loss: 0.699592, acc.: 46.09%] [G loss: 1.397417]\n",
            "5200 [D loss: 0.695383, acc.: 47.27%] [G loss: 1.387535]\n",
            "5220 [D loss: 0.693621, acc.: 47.27%] [G loss: 1.408859]\n",
            "5240 [D loss: 0.695037, acc.: 47.66%] [G loss: 1.403044]\n",
            "5260 [D loss: 0.700985, acc.: 41.41%] [G loss: 1.395171]\n",
            "5280 [D loss: 0.695701, acc.: 47.66%] [G loss: 1.403005]\n",
            "5300 [D loss: 0.700270, acc.: 42.19%] [G loss: 1.394384]\n",
            "5320 [D loss: 0.698988, acc.: 44.92%] [G loss: 1.401334]\n",
            "5340 [D loss: 0.694417, acc.: 53.12%] [G loss: 1.388224]\n",
            "5360 [D loss: 0.696847, acc.: 50.78%] [G loss: 1.402042]\n",
            "5380 [D loss: 0.699440, acc.: 46.48%] [G loss: 1.400354]\n",
            "5400 [D loss: 0.695333, acc.: 46.88%] [G loss: 1.389589]\n",
            "5420 [D loss: 0.693278, acc.: 52.34%] [G loss: 1.395435]\n",
            "5440 [D loss: 0.697515, acc.: 52.73%] [G loss: 1.403365]\n",
            "5460 [D loss: 0.700039, acc.: 44.92%] [G loss: 1.385313]\n",
            "5480 [D loss: 0.701342, acc.: 41.80%] [G loss: 1.400771]\n",
            "5500 [D loss: 0.691950, acc.: 52.73%] [G loss: 1.405167]\n",
            "5520 [D loss: 0.697928, acc.: 47.66%] [G loss: 1.407117]\n",
            "5540 [D loss: 0.694162, acc.: 50.39%] [G loss: 1.412841]\n",
            "5560 [D loss: 0.696665, acc.: 49.22%] [G loss: 1.398718]\n",
            "5580 [D loss: 0.700371, acc.: 45.31%] [G loss: 1.393484]\n",
            "5600 [D loss: 0.702304, acc.: 42.19%] [G loss: 1.393507]\n",
            "5620 [D loss: 0.694285, acc.: 50.39%] [G loss: 1.394578]\n",
            "5640 [D loss: 0.697163, acc.: 46.88%] [G loss: 1.395253]\n",
            "5660 [D loss: 0.693129, acc.: 50.78%] [G loss: 1.392592]\n",
            "5680 [D loss: 0.696335, acc.: 45.31%] [G loss: 1.386523]\n",
            "5700 [D loss: 0.697125, acc.: 50.00%] [G loss: 1.412799]\n",
            "5720 [D loss: 0.697893, acc.: 47.27%] [G loss: 1.399107]\n",
            "5740 [D loss: 0.695130, acc.: 49.22%] [G loss: 1.400753]\n",
            "5760 [D loss: 0.698657, acc.: 44.14%] [G loss: 1.395851]\n",
            "5780 [D loss: 0.697089, acc.: 47.66%] [G loss: 1.392132]\n",
            "5800 [D loss: 0.700125, acc.: 46.09%] [G loss: 1.401990]\n",
            "5820 [D loss: 0.697709, acc.: 47.27%] [G loss: 1.394505]\n",
            "5840 [D loss: 0.693451, acc.: 50.39%] [G loss: 1.400309]\n",
            "5860 [D loss: 0.697274, acc.: 45.31%] [G loss: 1.386672]\n",
            "5880 [D loss: 0.700695, acc.: 44.53%] [G loss: 1.398656]\n",
            "5900 [D loss: 0.697150, acc.: 47.27%] [G loss: 1.390164]\n",
            "5920 [D loss: 0.696779, acc.: 44.14%] [G loss: 1.396117]\n",
            "5940 [D loss: 0.693782, acc.: 50.00%] [G loss: 1.390036]\n",
            "5960 [D loss: 0.700256, acc.: 51.17%] [G loss: 1.398869]\n",
            "5980 [D loss: 0.696114, acc.: 49.22%] [G loss: 1.395630]\n",
            "6000 [D loss: 0.697736, acc.: 45.70%] [G loss: 1.393245]\n",
            "6020 [D loss: 0.699556, acc.: 43.75%] [G loss: 1.396108]\n",
            "6040 [D loss: 0.693481, acc.: 50.39%] [G loss: 1.397763]\n",
            "6060 [D loss: 0.703812, acc.: 44.14%] [G loss: 1.388477]\n",
            "6080 [D loss: 0.697451, acc.: 43.36%] [G loss: 1.389992]\n",
            "6100 [D loss: 0.692298, acc.: 46.88%] [G loss: 1.387400]\n",
            "6120 [D loss: 0.699595, acc.: 45.70%] [G loss: 1.405494]\n",
            "6140 [D loss: 0.697563, acc.: 45.70%] [G loss: 1.396657]\n",
            "6160 [D loss: 0.695493, acc.: 48.83%] [G loss: 1.383792]\n",
            "6180 [D loss: 0.697858, acc.: 43.75%] [G loss: 1.381215]\n",
            "6200 [D loss: 0.695829, acc.: 48.83%] [G loss: 1.407797]\n",
            "6220 [D loss: 0.717301, acc.: 37.11%] [G loss: 1.387314]\n",
            "6240 [D loss: 0.697440, acc.: 46.88%] [G loss: 1.404925]\n",
            "6260 [D loss: 0.693353, acc.: 48.05%] [G loss: 1.386393]\n",
            "6280 [D loss: 0.691351, acc.: 53.52%] [G loss: 1.411113]\n",
            "6300 [D loss: 0.697193, acc.: 48.44%] [G loss: 1.393546]\n",
            "6320 [D loss: 0.693712, acc.: 48.05%] [G loss: 1.386686]\n",
            "6340 [D loss: 0.701024, acc.: 43.75%] [G loss: 1.396646]\n",
            "6360 [D loss: 0.701343, acc.: 46.09%] [G loss: 1.392262]\n",
            "6380 [D loss: 0.696558, acc.: 48.05%] [G loss: 1.396049]\n",
            "6400 [D loss: 0.699096, acc.: 45.31%] [G loss: 1.396192]\n",
            "6420 [D loss: 0.695062, acc.: 48.44%] [G loss: 1.393358]\n",
            "6440 [D loss: 0.699274, acc.: 41.80%] [G loss: 1.398998]\n",
            "6460 [D loss: 0.698745, acc.: 42.97%] [G loss: 1.413985]\n",
            "6480 [D loss: 0.696595, acc.: 49.22%] [G loss: 1.407619]\n",
            "6500 [D loss: 0.699104, acc.: 44.53%] [G loss: 1.392327]\n",
            "6520 [D loss: 0.696238, acc.: 47.66%] [G loss: 1.400739]\n",
            "6540 [D loss: 0.694688, acc.: 49.61%] [G loss: 1.409534]\n",
            "6560 [D loss: 0.700760, acc.: 41.80%] [G loss: 1.394047]\n",
            "6580 [D loss: 0.690933, acc.: 51.56%] [G loss: 1.394747]\n",
            "6600 [D loss: 0.698814, acc.: 42.58%] [G loss: 1.389279]\n",
            "6620 [D loss: 0.700194, acc.: 44.53%] [G loss: 1.398422]\n",
            "6640 [D loss: 0.698942, acc.: 47.27%] [G loss: 1.386726]\n",
            "6660 [D loss: 0.700201, acc.: 48.05%] [G loss: 1.389932]\n",
            "6680 [D loss: 0.697296, acc.: 46.88%] [G loss: 1.393817]\n",
            "6700 [D loss: 0.700214, acc.: 42.97%] [G loss: 1.383901]\n",
            "6720 [D loss: 0.697019, acc.: 46.88%] [G loss: 1.392673]\n",
            "6740 [D loss: 0.701964, acc.: 48.05%] [G loss: 1.392654]\n",
            "6760 [D loss: 0.696323, acc.: 52.34%] [G loss: 1.397271]\n",
            "6780 [D loss: 0.699229, acc.: 43.75%] [G loss: 1.387532]\n",
            "6800 [D loss: 0.693560, acc.: 50.78%] [G loss: 1.384406]\n",
            "6820 [D loss: 0.699900, acc.: 46.48%] [G loss: 1.396095]\n",
            "6840 [D loss: 0.700351, acc.: 46.09%] [G loss: 1.400817]\n",
            "6860 [D loss: 0.696565, acc.: 50.39%] [G loss: 1.393807]\n",
            "6880 [D loss: 0.694871, acc.: 49.22%] [G loss: 1.389832]\n",
            "6900 [D loss: 0.700529, acc.: 46.88%] [G loss: 1.386945]\n",
            "6920 [D loss: 0.695679, acc.: 47.27%] [G loss: 1.394230]\n",
            "6940 [D loss: 0.697947, acc.: 43.75%] [G loss: 1.383168]\n",
            "6960 [D loss: 0.697765, acc.: 47.27%] [G loss: 1.401471]\n",
            "6980 [D loss: 0.699596, acc.: 42.97%] [G loss: 1.397855]\n",
            "7000 [D loss: 0.701152, acc.: 42.19%] [G loss: 1.395025]\n",
            "7020 [D loss: 0.697597, acc.: 44.53%] [G loss: 1.392945]\n",
            "7040 [D loss: 0.696670, acc.: 44.14%] [G loss: 1.395268]\n",
            "7060 [D loss: 0.701981, acc.: 41.80%] [G loss: 1.394562]\n",
            "7080 [D loss: 0.698433, acc.: 48.83%] [G loss: 1.398969]\n",
            "7100 [D loss: 0.699508, acc.: 44.92%] [G loss: 1.388082]\n",
            "7120 [D loss: 0.698022, acc.: 46.88%] [G loss: 1.393650]\n",
            "7140 [D loss: 0.694677, acc.: 44.53%] [G loss: 1.384647]\n",
            "7160 [D loss: 0.698689, acc.: 47.27%] [G loss: 1.392577]\n",
            "7180 [D loss: 0.698242, acc.: 42.58%] [G loss: 1.391645]\n",
            "7200 [D loss: 0.700687, acc.: 43.75%] [G loss: 1.396249]\n",
            "7220 [D loss: 0.696790, acc.: 44.14%] [G loss: 1.388211]\n",
            "7240 [D loss: 0.692794, acc.: 46.48%] [G loss: 1.397785]\n",
            "7260 [D loss: 0.698412, acc.: 43.36%] [G loss: 1.383541]\n",
            "7280 [D loss: 0.696848, acc.: 45.70%] [G loss: 1.394438]\n",
            "7300 [D loss: 0.694678, acc.: 45.70%] [G loss: 1.395237]\n",
            "7320 [D loss: 0.693550, acc.: 47.66%] [G loss: 1.395023]\n",
            "7340 [D loss: 0.697784, acc.: 46.09%] [G loss: 1.393231]\n",
            "7360 [D loss: 0.699125, acc.: 43.36%] [G loss: 1.386775]\n",
            "7380 [D loss: 0.698780, acc.: 42.97%] [G loss: 1.399514]\n",
            "7400 [D loss: 0.693878, acc.: 49.61%] [G loss: 1.394288]\n",
            "7420 [D loss: 0.694396, acc.: 50.39%] [G loss: 1.387681]\n",
            "7440 [D loss: 0.699487, acc.: 42.58%] [G loss: 1.391767]\n",
            "7460 [D loss: 0.695520, acc.: 47.27%] [G loss: 1.400790]\n",
            "7480 [D loss: 0.696784, acc.: 43.75%] [G loss: 1.389204]\n",
            "7500 [D loss: 0.705090, acc.: 40.23%] [G loss: 1.392731]\n",
            "7520 [D loss: 0.698871, acc.: 41.02%] [G loss: 1.388768]\n",
            "7540 [D loss: 0.697078, acc.: 46.09%] [G loss: 1.396813]\n",
            "7560 [D loss: 0.696970, acc.: 43.36%] [G loss: 1.395193]\n",
            "7580 [D loss: 0.696497, acc.: 46.09%] [G loss: 1.390957]\n",
            "7600 [D loss: 0.702041, acc.: 41.02%] [G loss: 1.395289]\n",
            "7620 [D loss: 0.697056, acc.: 43.75%] [G loss: 1.393222]\n",
            "7640 [D loss: 0.696136, acc.: 47.27%] [G loss: 1.394576]\n",
            "7660 [D loss: 0.694996, acc.: 47.27%] [G loss: 1.393033]\n",
            "7680 [D loss: 0.695782, acc.: 46.48%] [G loss: 1.385630]\n",
            "7700 [D loss: 0.693979, acc.: 50.39%] [G loss: 1.398277]\n",
            "7720 [D loss: 0.702364, acc.: 45.70%] [G loss: 1.391881]\n",
            "7740 [D loss: 0.699747, acc.: 38.67%] [G loss: 1.395062]\n",
            "7760 [D loss: 0.697299, acc.: 44.53%] [G loss: 1.393300]\n",
            "7780 [D loss: 0.697575, acc.: 44.14%] [G loss: 1.388515]\n",
            "7800 [D loss: 0.696327, acc.: 47.27%] [G loss: 1.399309]\n",
            "7820 [D loss: 0.699796, acc.: 39.45%] [G loss: 1.394260]\n",
            "7840 [D loss: 0.697553, acc.: 44.92%] [G loss: 1.389583]\n",
            "7860 [D loss: 0.698274, acc.: 40.62%] [G loss: 1.385870]\n",
            "7880 [D loss: 0.695591, acc.: 50.39%] [G loss: 1.394403]\n",
            "7900 [D loss: 0.697667, acc.: 42.97%] [G loss: 1.399807]\n",
            "7920 [D loss: 0.699197, acc.: 43.36%] [G loss: 1.389332]\n",
            "7940 [D loss: 0.695461, acc.: 43.75%] [G loss: 1.395153]\n",
            "7960 [D loss: 0.693985, acc.: 50.78%] [G loss: 1.396758]\n",
            "7980 [D loss: 0.694616, acc.: 49.61%] [G loss: 1.394727]\n",
            "8000 [D loss: 0.698273, acc.: 47.66%] [G loss: 1.394009]\n",
            "8020 [D loss: 0.697546, acc.: 46.09%] [G loss: 1.393506]\n",
            "8040 [D loss: 0.696341, acc.: 41.80%] [G loss: 1.394150]\n",
            "8060 [D loss: 0.697245, acc.: 48.44%] [G loss: 1.392660]\n",
            "8080 [D loss: 0.694503, acc.: 49.22%] [G loss: 1.399790]\n",
            "8100 [D loss: 0.699422, acc.: 40.23%] [G loss: 1.391258]\n",
            "8120 [D loss: 0.694864, acc.: 47.66%] [G loss: 1.396652]\n",
            "8140 [D loss: 0.694395, acc.: 45.70%] [G loss: 1.396786]\n",
            "8160 [D loss: 0.696896, acc.: 41.02%] [G loss: 1.390924]\n",
            "8180 [D loss: 0.697071, acc.: 45.31%] [G loss: 1.389972]\n",
            "8200 [D loss: 0.698414, acc.: 41.02%] [G loss: 1.395693]\n",
            "8220 [D loss: 0.696968, acc.: 42.97%] [G loss: 1.391768]\n",
            "8240 [D loss: 0.695341, acc.: 49.61%] [G loss: 1.391918]\n",
            "8260 [D loss: 0.696258, acc.: 42.97%] [G loss: 1.389055]\n",
            "8280 [D loss: 0.694510, acc.: 44.92%] [G loss: 1.391949]\n",
            "8300 [D loss: 0.693706, acc.: 51.17%] [G loss: 1.391831]\n",
            "8320 [D loss: 0.692619, acc.: 50.78%] [G loss: 1.394634]\n",
            "8340 [D loss: 0.692181, acc.: 45.31%] [G loss: 1.398439]\n",
            "8360 [D loss: 0.696840, acc.: 50.00%] [G loss: 1.386354]\n",
            "8380 [D loss: 0.696040, acc.: 45.70%] [G loss: 1.388847]\n",
            "8400 [D loss: 0.697098, acc.: 38.28%] [G loss: 1.385808]\n",
            "8420 [D loss: 0.696205, acc.: 48.44%] [G loss: 1.395315]\n",
            "8440 [D loss: 0.696577, acc.: 44.53%] [G loss: 1.395890]\n",
            "8460 [D loss: 0.698952, acc.: 40.23%] [G loss: 1.392299]\n",
            "8480 [D loss: 0.705718, acc.: 39.84%] [G loss: 1.425954]\n",
            "8500 [D loss: 0.692237, acc.: 47.66%] [G loss: 1.392538]\n",
            "8520 [D loss: 0.694271, acc.: 43.36%] [G loss: 1.400621]\n",
            "8540 [D loss: 0.697283, acc.: 43.36%] [G loss: 1.392912]\n",
            "8560 [D loss: 0.693932, acc.: 46.88%] [G loss: 1.389800]\n",
            "8580 [D loss: 0.698320, acc.: 41.02%] [G loss: 1.390708]\n",
            "8600 [D loss: 0.696863, acc.: 44.53%] [G loss: 1.393254]\n",
            "8620 [D loss: 0.693541, acc.: 48.83%] [G loss: 1.392359]\n",
            "8640 [D loss: 0.698448, acc.: 46.09%] [G loss: 1.390194]\n",
            "8660 [D loss: 0.698225, acc.: 43.36%] [G loss: 1.391702]\n",
            "8680 [D loss: 0.696173, acc.: 43.36%] [G loss: 1.393579]\n",
            "8700 [D loss: 0.698311, acc.: 39.84%] [G loss: 1.393775]\n",
            "8720 [D loss: 0.695746, acc.: 42.97%] [G loss: 1.389348]\n",
            "8740 [D loss: 0.697773, acc.: 45.31%] [G loss: 1.388839]\n",
            "8760 [D loss: 0.698051, acc.: 41.02%] [G loss: 1.387564]\n",
            "8780 [D loss: 0.695869, acc.: 41.80%] [G loss: 1.393906]\n",
            "8800 [D loss: 0.695220, acc.: 43.36%] [G loss: 1.384415]\n",
            "8820 [D loss: 0.694940, acc.: 43.36%] [G loss: 1.385797]\n",
            "8840 [D loss: 0.699507, acc.: 44.53%] [G loss: 1.392140]\n",
            "8860 [D loss: 0.698243, acc.: 41.02%] [G loss: 1.383003]\n",
            "8880 [D loss: 0.694200, acc.: 51.17%] [G loss: 1.389239]\n",
            "8900 [D loss: 0.700332, acc.: 39.45%] [G loss: 1.392415]\n",
            "8920 [D loss: 0.694468, acc.: 43.75%] [G loss: 1.398754]\n",
            "8940 [D loss: 0.693868, acc.: 46.09%] [G loss: 1.401220]\n",
            "8960 [D loss: 0.696201, acc.: 46.09%] [G loss: 1.390292]\n",
            "8980 [D loss: 0.700917, acc.: 37.11%] [G loss: 1.384135]\n",
            "9000 [D loss: 0.529419, acc.: 60.16%] [G loss: 3.451092]\n",
            "9020 [D loss: 0.714005, acc.: 34.77%] [G loss: 1.418368]\n",
            "9040 [D loss: 0.708870, acc.: 40.62%] [G loss: 1.395880]\n",
            "9060 [D loss: 0.705619, acc.: 41.80%] [G loss: 1.390461]\n",
            "9080 [D loss: 0.698528, acc.: 45.31%] [G loss: 1.404152]\n",
            "9100 [D loss: 0.699169, acc.: 39.06%] [G loss: 1.402751]\n",
            "9120 [D loss: 0.696621, acc.: 41.41%] [G loss: 1.406112]\n",
            "9140 [D loss: 0.698882, acc.: 44.92%] [G loss: 1.396840]\n",
            "9160 [D loss: 0.696500, acc.: 44.53%] [G loss: 1.399978]\n",
            "9180 [D loss: 0.694397, acc.: 50.39%] [G loss: 1.403586]\n",
            "9200 [D loss: 0.694435, acc.: 49.22%] [G loss: 1.399590]\n",
            "9220 [D loss: 0.696431, acc.: 43.36%] [G loss: 1.405650]\n",
            "9240 [D loss: 0.695898, acc.: 47.27%] [G loss: 1.392597]\n",
            "9260 [D loss: 0.698319, acc.: 42.58%] [G loss: 1.398521]\n",
            "9280 [D loss: 0.700153, acc.: 39.06%] [G loss: 1.388895]\n",
            "9300 [D loss: 0.693785, acc.: 47.27%] [G loss: 1.391394]\n",
            "9320 [D loss: 0.691352, acc.: 50.78%] [G loss: 1.394004]\n",
            "9340 [D loss: 0.694901, acc.: 43.36%] [G loss: 1.396414]\n",
            "9360 [D loss: 0.696790, acc.: 42.58%] [G loss: 1.395674]\n",
            "9380 [D loss: 0.692653, acc.: 48.83%] [G loss: 1.399105]\n",
            "9400 [D loss: 0.697056, acc.: 44.14%] [G loss: 1.392526]\n",
            "9420 [D loss: 0.699672, acc.: 42.97%] [G loss: 1.388630]\n",
            "9440 [D loss: 0.696520, acc.: 47.66%] [G loss: 1.388295]\n",
            "9460 [D loss: 0.692791, acc.: 49.22%] [G loss: 1.395600]\n",
            "9480 [D loss: 0.692895, acc.: 45.70%] [G loss: 1.389935]\n",
            "9500 [D loss: 0.697345, acc.: 44.92%] [G loss: 1.405686]\n",
            "9520 [D loss: 0.695038, acc.: 48.83%] [G loss: 1.398619]\n",
            "9540 [D loss: 0.693577, acc.: 49.22%] [G loss: 1.392874]\n",
            "9560 [D loss: 0.696086, acc.: 44.53%] [G loss: 1.395670]\n",
            "9580 [D loss: 0.694003, acc.: 47.66%] [G loss: 1.405685]\n",
            "9600 [D loss: 0.691519, acc.: 48.44%] [G loss: 1.388005]\n",
            "9620 [D loss: 0.701189, acc.: 41.41%] [G loss: 1.402854]\n",
            "9640 [D loss: 0.692627, acc.: 50.39%] [G loss: 1.407890]\n",
            "9660 [D loss: 0.699078, acc.: 45.31%] [G loss: 1.403761]\n",
            "9680 [D loss: 0.692119, acc.: 46.48%] [G loss: 1.422443]\n",
            "9700 [D loss: 0.694870, acc.: 47.27%] [G loss: 1.402087]\n",
            "9720 [D loss: 0.694653, acc.: 47.66%] [G loss: 1.401800]\n",
            "9740 [D loss: 0.691276, acc.: 53.52%] [G loss: 1.412094]\n",
            "9760 [D loss: 0.697540, acc.: 41.41%] [G loss: 1.401138]\n",
            "9780 [D loss: 0.697595, acc.: 46.09%] [G loss: 1.409874]\n",
            "9800 [D loss: 0.693005, acc.: 46.48%] [G loss: 1.400883]\n",
            "9820 [D loss: 0.692952, acc.: 54.30%] [G loss: 1.406455]\n",
            "9840 [D loss: 0.691773, acc.: 48.05%] [G loss: 1.401276]\n",
            "9860 [D loss: 0.691053, acc.: 51.17%] [G loss: 1.397547]\n",
            "9880 [D loss: 0.687381, acc.: 53.12%] [G loss: 1.407923]\n",
            "9900 [D loss: 0.692056, acc.: 47.27%] [G loss: 1.400826]\n",
            "9920 [D loss: 0.694512, acc.: 43.75%] [G loss: 1.410615]\n",
            "9940 [D loss: 0.693986, acc.: 45.70%] [G loss: 1.404719]\n",
            "9960 [D loss: 0.691524, acc.: 52.73%] [G loss: 1.402085]\n",
            "9980 [D loss: 0.695208, acc.: 48.05%] [G loss: 1.405380]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
