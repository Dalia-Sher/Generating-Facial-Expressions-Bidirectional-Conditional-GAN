{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "BiCoGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\r\n",
        "\r\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\r\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\r\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\r\n",
        "from keras.utils import plot_model\r\n",
        "from keras.models import Sequential, Model, load_model\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras import losses\r\n",
        "from keras.utils import to_categorical\r\n",
        "import keras.backend as K\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "eae67b35-8bc5-4207-b92a-e94164279663"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\r\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed815d97-1ba3-4e54-f35d-a5f2b76d144b"
      },
      "source": [
        "data.Usage.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training       28709\n",
              "PrivateTest     3589\n",
              "PublicTest      3589\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 7\r\n",
        "img_width = 48\r\n",
        "img_height = 48"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "32c8dacd-d952-47ff-b2f3-38d0d3efd7aa"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnh1CQpGKk-t"
      },
      "source": [
        "class BiCoGAN():\r\n",
        "    def __init__(self):\r\n",
        "        # Input shape\r\n",
        "        self.img_rows = 48\r\n",
        "        self.img_cols = 48\r\n",
        "        self.channels = 1\r\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\r\n",
        "        self.num_classes = 7\r\n",
        "        self.latent_dim = 100\r\n",
        "\r\n",
        "        optimizer = Adam(0.0002, 0.5)\r\n",
        "\r\n",
        "        # Build and compile the discriminator\r\n",
        "        self.discriminator = self.build_discriminator()\r\n",
        "        print(self.discriminator.summary())\r\n",
        "        plot_model(self.discriminator, show_shapes=True)\r\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\r\n",
        "            optimizer=optimizer,\r\n",
        "            metrics=['accuracy'])\r\n",
        "\r\n",
        "        # Build the generator\r\n",
        "        self.generator = self.build_generator()\r\n",
        "\r\n",
        "        # Build the encoder\r\n",
        "        self.encoder = self.build_encoder()\r\n",
        "\r\n",
        "        # The generator takes noise and the target label as input\r\n",
        "        # and generates the corresponding digit of that label\r\n",
        "        # noise = Input(shape=(self.latent_dim,))\r\n",
        "        label = Input(shape=(1,))\r\n",
        "        # img = self.generator([noise, label])\r\n",
        "\r\n",
        "        # For the combined model we will only train the generator\r\n",
        "        self.discriminator.trainable = False\r\n",
        "\r\n",
        "        # Generate image from sampled noise\r\n",
        "        z = Input(shape=(self.latent_dim, ))\r\n",
        "        img_ = self.generator([z, label])\r\n",
        "\r\n",
        "        # Encode image\r\n",
        "        img = Input(shape=self.img_shape)\r\n",
        "        z_ = self.encoder(img)\r\n",
        "\r\n",
        "        # Latent -> img is fake, and img -> latent is valid\r\n",
        "        fake = self.discriminator([z, img_, label])\r\n",
        "        valid = self.discriminator([z_, img, label])\r\n",
        "\r\n",
        "        # Set up and compile the combined model\r\n",
        "        # Trains generator to fool the discriminator\r\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\r\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\r\n",
        "            optimizer=optimizer)\r\n",
        "\r\n",
        "    def build_encoder(self):\r\n",
        "        model = Sequential()\r\n",
        "\r\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\r\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\r\n",
        "        model.add(BatchNormalization(momentum=0.9))\r\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\r\n",
        "        model.add(BatchNormalization(momentum=0.9))\r\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\r\n",
        "        model.add(BatchNormalization(momentum=0.9))\r\n",
        "        model.add(Flatten())\r\n",
        "        model.add(Dense(self.latent_dim))\r\n",
        "\r\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\r\n",
        "        # model.add(Dense(512))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(BatchNormalization(momentum=0.8))\r\n",
        "        # model.add(Dense(512))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(BatchNormalization(momentum=0.8))\r\n",
        "        # model.add(Dense(self.latent_dim))\r\n",
        "\r\n",
        "        print('encoder')\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "        img = Input(shape=self.img_shape)\r\n",
        "        z = model(img)\r\n",
        "\r\n",
        "        return Model(img, z)\r\n",
        "\r\n",
        "    def build_generator(self):\r\n",
        "\r\n",
        "        model = Sequential()\r\n",
        "\r\n",
        "        # # foundation for 12x12 image\r\n",
        "        # n_nodes = 128 * 12 * 12\r\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(Reshape((12, 12, 128)))\r\n",
        "        # # upsample to 24x24\r\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # # upsample to 48x48\r\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # # generate\r\n",
        "        # model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\r\n",
        "\r\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\r\n",
        "        model.add(LeakyReLU(alpha=0.2))\r\n",
        "        model.add(BatchNormalization(momentum=0.8))\r\n",
        "        model.add(Dense(512))\r\n",
        "        model.add(LeakyReLU(alpha=0.2))\r\n",
        "        model.add(BatchNormalization(momentum=0.8))\r\n",
        "        model.add(Dense(1024))\r\n",
        "        model.add(LeakyReLU(alpha=0.2))\r\n",
        "        model.add(BatchNormalization(momentum=0.8))\r\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\r\n",
        "        model.add(Reshape(self.img_shape))\r\n",
        "\r\n",
        "        print('generator')\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "        z = Input(shape=(self.latent_dim,))\r\n",
        "        label = Input(shape=(1,), dtype='int32')\r\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\r\n",
        "\r\n",
        "        model_input = multiply([z, label_embedding])\r\n",
        "        img = model(model_input)\r\n",
        "\r\n",
        "        return Model([z, label], img)\r\n",
        "\r\n",
        "    def build_discriminator(self):\r\n",
        "\r\n",
        "        z = Input(shape=(self.latent_dim, ))\r\n",
        "        img = Input(shape=self.img_shape)\r\n",
        "        label = Input(shape=(1,), dtype='int32')\r\n",
        "\r\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\r\n",
        "        flat_img = Flatten()(img)\r\n",
        "\r\n",
        "        d_in = concatenate([z, flat_img, label_embedding])\r\n",
        "\r\n",
        "        model = Dense(1024)(d_in)\r\n",
        "        model = LeakyReLU(alpha=0.2)(model)\r\n",
        "        model = Dropout(0.5)(model)\r\n",
        "        model = Dense(1024)(model)\r\n",
        "        model = LeakyReLU(alpha=0.2)(model)\r\n",
        "        model = Dropout(0.5)(model)\r\n",
        "        model = Dense(1024)(model)\r\n",
        "        model = LeakyReLU(alpha=0.2)(model)\r\n",
        "        model = Dropout(0.5)(model)\r\n",
        "        validity = Dense(1, activation=\"sigmoid\")(model)\r\n",
        "\r\n",
        "        return Model([z, img, label], validity, name='discriminator')\r\n",
        "\r\n",
        "\r\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\r\n",
        "\r\n",
        "        # Load the dataset\r\n",
        "        # (_, y_train), (_, _) = mnist.load_data()\r\n",
        "\r\n",
        "        # Configure input\r\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\r\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\r\n",
        "        y_train = y.to_numpy().reshape(-1, 1)\r\n",
        "        # y_train = y.reshape(-1, 1)\r\n",
        "\r\n",
        "        # Adversarial ground truths\r\n",
        "        valid = np.ones((batch_size, 1))\r\n",
        "        fake = np.zeros((batch_size, 1))\r\n",
        "\r\n",
        "        for epoch in range(epochs):\r\n",
        "\r\n",
        "            # ---------------------\r\n",
        "            #  Train Discriminator\r\n",
        "            # ---------------------\r\n",
        "\r\n",
        "            # Select a random batch of images and encode\r\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\r\n",
        "            imgs, labels = X_train[idx], y_train[idx]\r\n",
        "            z_ = self.encoder.predict(imgs)\r\n",
        "\r\n",
        "            # Sample noise and generate img\r\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\r\n",
        "            imgs_ = self.generator.predict([z, labels])\r\n",
        "\r\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\r\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\r\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\r\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n",
        "\r\n",
        "            # ---------------------\r\n",
        "            #  Train Generator\r\n",
        "            # ---------------------\r\n",
        "\r\n",
        "            # Condition on labels\r\n",
        "            sampled_labels = np.random.randint(0, 7, batch_size).reshape(-1, 1)\r\n",
        "\r\n",
        "            # Train the generator\r\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\r\n",
        "\r\n",
        "            # Plot the progress\r\n",
        "            if epoch%20 == 0:\r\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\r\n",
        "\r\n",
        "            # If at save interval => save generated image samples\r\n",
        "            if epoch % sample_interval == 0:\r\n",
        "                self.sample_images(epoch)\r\n",
        "\r\n",
        "    # def sample_images(self, epoch):\r\n",
        "    #     r, c = 1, 7\r\n",
        "    #     noise = np.random.normal(0, 1, (r * c, 100))\r\n",
        "    #     sampled_labels = np.arange(0, 7).reshape(-1, 1)\r\n",
        "\r\n",
        "    #     gen_imgs = self.generator.predict([noise, sampled_labels])\r\n",
        "    #     print(gen_imgs.shape)\r\n",
        "\r\n",
        "    #     # Rescale images 0 - 1\r\n",
        "    #     gen_imgs = 0.5 * gen_imgs + 0.5\r\n",
        "\r\n",
        "    #     fig, axs = plt.subplots(r, c)\r\n",
        "    #     cnt = 0\r\n",
        "    #     for i in range(r):\r\n",
        "    #         print(i)\r\n",
        "    #         for j in range(c):\r\n",
        "    #             print(j)\r\n",
        "    #             axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\r\n",
        "    #             axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\r\n",
        "    #             axs[i,j].axis('off')\r\n",
        "    #             cnt += 1\r\n",
        "    #     fig.savefig(\"images/%d.png\" % epoch)\r\n",
        "    #     plt.close()\r\n",
        "\r\n",
        "    def sample_images(self, epoch):\r\n",
        "          r, c = 1, 7\r\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\r\n",
        "          sampled_labels = np.arange(0, 7).reshape(-1, 1)\r\n",
        "\r\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\r\n",
        "\r\n",
        "          # Rescale images 0 - 1\r\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\r\n",
        "\r\n",
        "          fig, axs = plt.subplots(r, c)\r\n",
        "          cnt = 0\r\n",
        "          for j in range(c):\r\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\r\n",
        "              axs[j].set_title(\"label: %d\" % sampled_labels[cnt])\r\n",
        "              axs[j].axis('off')\r\n",
        "              cnt += 1\r\n",
        "          fig.savefig(\"images_3/%d.png\" % epoch)\r\n",
        "          plt.close()\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqR5QtObXpaN",
        "outputId": "8f0ea3cd-2e04-4d6e-f64c-94c2e7b4f796"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    bicogan = BiCoGAN()\r\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_66 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_65 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_14 (Embedding)        (None, 1, 2304)      16128       input_66[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_64 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_29 (Flatten)            (None, 2304)         0           input_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_28 (Flatten)            (None, 2304)         0           embedding_14[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 4708)         0           input_64[0][0]                   \n",
            "                                                                 flatten_29[0][0]                 \n",
            "                                                                 flatten_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_63 (Dense)                (None, 1024)         4822016     concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_42 (LeakyReLU)      (None, 1024)         0           dense_63[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 1024)         0           leaky_re_lu_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_64 (Dense)                (None, 1024)         1049600     dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_43 (LeakyReLU)      (None, 1024)         0           dense_64[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 1024)         0           leaky_re_lu_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_65 (Dense)                (None, 1024)         1049600     dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_44 (LeakyReLU)      (None, 1024)         0           dense_65[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 1024)         0           leaky_re_lu_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_66 (Dense)                (None, 1)            1025        dropout_23[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "<IPython.core.display.Image object>\n",
            "generator\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_67 (Dense)             (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_45 (LeakyReLU)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_46 (LeakyReLU)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_47 (LeakyReLU)   (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 2304)              2361600   \n",
            "_________________________________________________________________\n",
            "reshape_7 (Reshape)          (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 3,051,520\n",
            "Trainable params: 3,047,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_28 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_31 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.826911, acc.: 35.94%] [G loss: 4.964511]\n",
            "20 [D loss: 0.002407, acc.: 100.00%] [G loss: 2.393224]\n",
            "40 [D loss: 0.257624, acc.: 87.89%] [G loss: 18.361641]\n",
            "60 [D loss: 0.349599, acc.: 89.06%] [G loss: 23.198723]\n",
            "80 [D loss: 0.118372, acc.: 95.70%] [G loss: 10.267849]\n",
            "100 [D loss: 0.359374, acc.: 91.80%] [G loss: 9.741158]\n",
            "120 [D loss: 0.165015, acc.: 96.09%] [G loss: 14.184084]\n",
            "140 [D loss: 0.450395, acc.: 80.08%] [G loss: 9.385517]\n",
            "160 [D loss: 0.384853, acc.: 83.20%] [G loss: 6.172188]\n",
            "180 [D loss: 0.891755, acc.: 51.56%] [G loss: 4.575292]\n",
            "200 [D loss: 0.830943, acc.: 48.83%] [G loss: 3.175241]\n",
            "220 [D loss: 0.773781, acc.: 53.91%] [G loss: 2.263720]\n",
            "240 [D loss: 0.697401, acc.: 56.64%] [G loss: 2.134512]\n",
            "260 [D loss: 0.655960, acc.: 57.81%] [G loss: 2.172576]\n",
            "280 [D loss: 0.665241, acc.: 58.98%] [G loss: 2.035323]\n",
            "300 [D loss: 0.737145, acc.: 49.61%] [G loss: 1.968781]\n",
            "320 [D loss: 0.713959, acc.: 53.91%] [G loss: 1.870077]\n",
            "340 [D loss: 0.732593, acc.: 53.52%] [G loss: 1.857405]\n",
            "360 [D loss: 0.683003, acc.: 57.03%] [G loss: 1.850654]\n",
            "380 [D loss: 0.650786, acc.: 65.23%] [G loss: 1.859812]\n",
            "400 [D loss: 0.707992, acc.: 52.34%] [G loss: 1.734445]\n",
            "420 [D loss: 0.702201, acc.: 55.47%] [G loss: 1.696256]\n",
            "440 [D loss: 0.705920, acc.: 52.73%] [G loss: 1.682934]\n",
            "460 [D loss: 0.712392, acc.: 52.73%] [G loss: 1.592758]\n",
            "480 [D loss: 0.708825, acc.: 56.25%] [G loss: 1.550822]\n",
            "500 [D loss: 0.720383, acc.: 47.66%] [G loss: 1.564562]\n",
            "520 [D loss: 0.690021, acc.: 56.64%] [G loss: 1.549849]\n",
            "540 [D loss: 0.690475, acc.: 55.86%] [G loss: 1.631913]\n",
            "560 [D loss: 0.731823, acc.: 48.05%] [G loss: 1.560483]\n",
            "580 [D loss: 0.703675, acc.: 49.22%] [G loss: 1.516791]\n",
            "600 [D loss: 0.715014, acc.: 47.66%] [G loss: 1.524696]\n",
            "620 [D loss: 0.698078, acc.: 54.30%] [G loss: 1.576646]\n",
            "640 [D loss: 0.715914, acc.: 50.00%] [G loss: 1.517236]\n",
            "660 [D loss: 0.715075, acc.: 44.53%] [G loss: 1.525688]\n",
            "680 [D loss: 0.727615, acc.: 43.36%] [G loss: 1.521900]\n",
            "700 [D loss: 0.722493, acc.: 43.75%] [G loss: 1.488898]\n",
            "720 [D loss: 0.700770, acc.: 53.12%] [G loss: 1.522385]\n",
            "740 [D loss: 0.723610, acc.: 44.53%] [G loss: 1.479110]\n",
            "760 [D loss: 0.687584, acc.: 53.12%] [G loss: 1.546128]\n",
            "780 [D loss: 0.716565, acc.: 47.66%] [G loss: 1.495386]\n",
            "800 [D loss: 0.719174, acc.: 50.00%] [G loss: 1.479075]\n",
            "820 [D loss: 0.687401, acc.: 52.73%] [G loss: 1.481702]\n",
            "840 [D loss: 0.713528, acc.: 50.78%] [G loss: 1.452140]\n",
            "860 [D loss: 0.707561, acc.: 50.39%] [G loss: 1.461921]\n",
            "880 [D loss: 0.689795, acc.: 51.56%] [G loss: 1.469760]\n",
            "900 [D loss: 0.703063, acc.: 50.39%] [G loss: 1.476413]\n",
            "920 [D loss: 0.697435, acc.: 51.17%] [G loss: 1.483057]\n",
            "940 [D loss: 0.719985, acc.: 44.53%] [G loss: 1.479481]\n",
            "960 [D loss: 0.709782, acc.: 48.44%] [G loss: 1.489974]\n",
            "980 [D loss: 0.689550, acc.: 53.12%] [G loss: 1.523420]\n",
            "1000 [D loss: 0.715200, acc.: 43.75%] [G loss: 1.511798]\n",
            "1020 [D loss: 0.697741, acc.: 51.56%] [G loss: 1.525696]\n",
            "1040 [D loss: 0.683766, acc.: 56.25%] [G loss: 1.517835]\n",
            "1060 [D loss: 0.676490, acc.: 54.69%] [G loss: 1.569828]\n",
            "1080 [D loss: 0.698862, acc.: 58.98%] [G loss: 1.503160]\n",
            "1100 [D loss: 0.708023, acc.: 51.56%] [G loss: 1.555667]\n",
            "1120 [D loss: 0.714231, acc.: 49.22%] [G loss: 1.549054]\n",
            "1140 [D loss: 0.703591, acc.: 51.17%] [G loss: 1.549277]\n",
            "1160 [D loss: 0.697687, acc.: 48.05%] [G loss: 1.495551]\n",
            "1180 [D loss: 0.697923, acc.: 51.56%] [G loss: 1.548787]\n",
            "1200 [D loss: 0.692021, acc.: 53.12%] [G loss: 1.485193]\n",
            "1220 [D loss: 0.692510, acc.: 51.95%] [G loss: 1.484241]\n",
            "1240 [D loss: 0.693861, acc.: 57.03%] [G loss: 1.478392]\n",
            "1260 [D loss: 0.702743, acc.: 48.83%] [G loss: 1.503257]\n",
            "1280 [D loss: 0.699097, acc.: 49.22%] [G loss: 1.518584]\n",
            "1300 [D loss: 0.703972, acc.: 47.66%] [G loss: 1.540632]\n",
            "1320 [D loss: 0.690170, acc.: 52.34%] [G loss: 1.572068]\n",
            "1340 [D loss: 0.704993, acc.: 52.73%] [G loss: 1.605534]\n",
            "1360 [D loss: 0.704760, acc.: 49.22%] [G loss: 1.648466]\n",
            "1380 [D loss: 0.668450, acc.: 57.81%] [G loss: 1.626891]\n",
            "1400 [D loss: 0.698638, acc.: 53.91%] [G loss: 1.648754]\n",
            "1420 [D loss: 0.697768, acc.: 55.47%] [G loss: 1.669004]\n",
            "1440 [D loss: 0.717828, acc.: 52.73%] [G loss: 1.670163]\n",
            "1460 [D loss: 0.716888, acc.: 53.91%] [G loss: 1.659144]\n",
            "1480 [D loss: 0.736139, acc.: 46.48%] [G loss: 1.591816]\n",
            "1500 [D loss: 0.704100, acc.: 55.86%] [G loss: 1.642791]\n",
            "1520 [D loss: 0.707169, acc.: 55.08%] [G loss: 1.670069]\n",
            "1540 [D loss: 0.673204, acc.: 56.25%] [G loss: 1.711027]\n",
            "1560 [D loss: 0.683699, acc.: 59.38%] [G loss: 1.684004]\n",
            "1580 [D loss: 0.713817, acc.: 51.56%] [G loss: 1.684971]\n",
            "1600 [D loss: 0.676656, acc.: 58.59%] [G loss: 1.687842]\n",
            "1620 [D loss: 0.657513, acc.: 62.89%] [G loss: 1.689317]\n",
            "1640 [D loss: 0.690862, acc.: 57.81%] [G loss: 1.752842]\n",
            "1660 [D loss: 0.715049, acc.: 52.73%] [G loss: 1.841855]\n",
            "1680 [D loss: 0.735514, acc.: 52.34%] [G loss: 1.748227]\n",
            "1700 [D loss: 0.693992, acc.: 53.12%] [G loss: 1.638542]\n",
            "1720 [D loss: 0.681715, acc.: 55.08%] [G loss: 1.589844]\n",
            "1740 [D loss: 0.686811, acc.: 54.69%] [G loss: 1.599834]\n",
            "1760 [D loss: 0.684375, acc.: 56.64%] [G loss: 1.581268]\n",
            "1780 [D loss: 0.690847, acc.: 54.69%] [G loss: 1.597430]\n",
            "1800 [D loss: 0.661558, acc.: 62.50%] [G loss: 1.649556]\n",
            "1820 [D loss: 0.685267, acc.: 55.86%] [G loss: 1.583965]\n",
            "1840 [D loss: 0.691688, acc.: 54.69%] [G loss: 1.606926]\n",
            "1860 [D loss: 0.681251, acc.: 60.16%] [G loss: 1.582769]\n",
            "1880 [D loss: 0.695283, acc.: 51.95%] [G loss: 1.620978]\n",
            "1900 [D loss: 0.691100, acc.: 51.56%] [G loss: 1.582235]\n",
            "1920 [D loss: 0.690326, acc.: 53.52%] [G loss: 1.575137]\n",
            "1940 [D loss: 0.719888, acc.: 48.83%] [G loss: 1.541001]\n",
            "1960 [D loss: 0.679855, acc.: 57.03%] [G loss: 1.575166]\n",
            "1980 [D loss: 0.696198, acc.: 52.73%] [G loss: 1.579667]\n",
            "2000 [D loss: 0.703511, acc.: 51.95%] [G loss: 1.580392]\n",
            "2020 [D loss: 0.711651, acc.: 54.30%] [G loss: 1.523192]\n",
            "2040 [D loss: 0.725826, acc.: 44.92%] [G loss: 1.533627]\n",
            "2060 [D loss: 0.685369, acc.: 53.91%] [G loss: 1.543403]\n",
            "2080 [D loss: 0.715533, acc.: 45.70%] [G loss: 1.444267]\n",
            "2100 [D loss: 0.690956, acc.: 50.39%] [G loss: 1.469324]\n",
            "2120 [D loss: 0.689836, acc.: 51.17%] [G loss: 1.462368]\n",
            "2140 [D loss: 0.686056, acc.: 54.30%] [G loss: 1.473098]\n",
            "2160 [D loss: 0.722621, acc.: 41.80%] [G loss: 1.437816]\n",
            "2180 [D loss: 0.719752, acc.: 48.44%] [G loss: 1.480473]\n",
            "2200 [D loss: 0.694097, acc.: 55.47%] [G loss: 1.479482]\n",
            "2220 [D loss: 0.701294, acc.: 50.78%] [G loss: 1.452362]\n",
            "2240 [D loss: 0.674075, acc.: 55.47%] [G loss: 1.541652]\n",
            "2260 [D loss: 0.672772, acc.: 60.55%] [G loss: 1.569842]\n",
            "2280 [D loss: 0.658957, acc.: 62.89%] [G loss: 1.644521]\n",
            "2300 [D loss: 0.646569, acc.: 66.02%] [G loss: 1.828531]\n",
            "2320 [D loss: 0.663298, acc.: 63.28%] [G loss: 1.807408]\n",
            "2340 [D loss: 0.713178, acc.: 51.56%] [G loss: 1.726270]\n",
            "2360 [D loss: 0.678587, acc.: 59.38%] [G loss: 1.735705]\n",
            "2380 [D loss: 0.716215, acc.: 49.61%] [G loss: 1.617467]\n",
            "2400 [D loss: 0.681999, acc.: 52.34%] [G loss: 1.601566]\n",
            "2420 [D loss: 0.705268, acc.: 51.95%] [G loss: 1.610494]\n",
            "2440 [D loss: 0.690812, acc.: 53.12%] [G loss: 1.487444]\n",
            "2460 [D loss: 0.714317, acc.: 47.66%] [G loss: 1.532799]\n",
            "2480 [D loss: 0.679534, acc.: 53.91%] [G loss: 1.534919]\n",
            "2500 [D loss: 0.701028, acc.: 52.34%] [G loss: 1.493049]\n",
            "2520 [D loss: 0.699341, acc.: 51.56%] [G loss: 1.541630]\n",
            "2540 [D loss: 0.711360, acc.: 47.66%] [G loss: 1.510027]\n",
            "2560 [D loss: 0.688058, acc.: 51.95%] [G loss: 1.434506]\n",
            "2580 [D loss: 0.708334, acc.: 47.66%] [G loss: 1.460879]\n",
            "2600 [D loss: 0.719558, acc.: 51.17%] [G loss: 1.472824]\n",
            "2620 [D loss: 0.698286, acc.: 48.05%] [G loss: 1.491403]\n",
            "2640 [D loss: 0.694065, acc.: 50.00%] [G loss: 1.521497]\n",
            "2660 [D loss: 0.701323, acc.: 50.00%] [G loss: 1.434899]\n",
            "2680 [D loss: 0.687258, acc.: 54.30%] [G loss: 1.477643]\n",
            "2700 [D loss: 0.704195, acc.: 44.92%] [G loss: 1.436648]\n",
            "2720 [D loss: 0.705169, acc.: 47.66%] [G loss: 1.470610]\n",
            "2740 [D loss: 0.704505, acc.: 45.70%] [G loss: 1.463992]\n",
            "2760 [D loss: 0.711086, acc.: 45.70%] [G loss: 1.440330]\n",
            "2780 [D loss: 0.700405, acc.: 48.05%] [G loss: 1.462428]\n",
            "2800 [D loss: 0.701339, acc.: 50.00%] [G loss: 1.454344]\n",
            "2820 [D loss: 0.695438, acc.: 51.17%] [G loss: 1.463377]\n",
            "2840 [D loss: 0.699568, acc.: 51.17%] [G loss: 1.466310]\n",
            "2860 [D loss: 0.695856, acc.: 45.70%] [G loss: 1.470275]\n",
            "2880 [D loss: 0.686858, acc.: 57.03%] [G loss: 1.453982]\n",
            "2900 [D loss: 0.706958, acc.: 48.44%] [G loss: 1.445006]\n",
            "2920 [D loss: 0.703128, acc.: 44.14%] [G loss: 1.462609]\n",
            "2940 [D loss: 0.695311, acc.: 51.95%] [G loss: 1.426776]\n",
            "2960 [D loss: 0.695585, acc.: 53.91%] [G loss: 1.438819]\n",
            "2980 [D loss: 0.700135, acc.: 45.70%] [G loss: 1.434319]\n",
            "3000 [D loss: 0.717998, acc.: 38.28%] [G loss: 1.449864]\n",
            "3020 [D loss: 0.698662, acc.: 50.78%] [G loss: 1.445592]\n",
            "3040 [D loss: 0.700014, acc.: 46.88%] [G loss: 1.418758]\n",
            "3060 [D loss: 0.701654, acc.: 50.39%] [G loss: 1.433100]\n",
            "3080 [D loss: 0.704852, acc.: 45.70%] [G loss: 1.431874]\n",
            "3100 [D loss: 0.701811, acc.: 46.48%] [G loss: 1.432201]\n",
            "3120 [D loss: 0.705988, acc.: 45.70%] [G loss: 1.433812]\n",
            "3140 [D loss: 0.701175, acc.: 46.09%] [G loss: 1.428013]\n",
            "3160 [D loss: 0.708816, acc.: 46.48%] [G loss: 1.444151]\n",
            "3180 [D loss: 0.710450, acc.: 42.97%] [G loss: 1.440201]\n",
            "3200 [D loss: 0.694308, acc.: 52.34%] [G loss: 1.429188]\n",
            "3220 [D loss: 0.693397, acc.: 54.30%] [G loss: 1.447537]\n",
            "3240 [D loss: 0.699213, acc.: 45.70%] [G loss: 1.438806]\n",
            "3260 [D loss: 0.701572, acc.: 50.00%] [G loss: 1.410105]\n",
            "3280 [D loss: 0.701563, acc.: 46.48%] [G loss: 1.435767]\n",
            "3300 [D loss: 0.704864, acc.: 43.36%] [G loss: 1.440411]\n",
            "3320 [D loss: 0.705181, acc.: 42.58%] [G loss: 1.433103]\n",
            "3340 [D loss: 0.694720, acc.: 46.48%] [G loss: 1.423905]\n",
            "3360 [D loss: 0.715672, acc.: 44.53%] [G loss: 1.414267]\n",
            "3380 [D loss: 0.694832, acc.: 46.09%] [G loss: 1.416476]\n",
            "3400 [D loss: 0.698049, acc.: 47.66%] [G loss: 1.434649]\n",
            "3420 [D loss: 0.695335, acc.: 50.00%] [G loss: 1.417256]\n",
            "3440 [D loss: 0.753415, acc.: 34.38%] [G loss: 1.401083]\n",
            "3460 [D loss: 0.712904, acc.: 43.36%] [G loss: 1.417919]\n",
            "3480 [D loss: 0.689769, acc.: 52.34%] [G loss: 1.445430]\n",
            "3500 [D loss: 0.691140, acc.: 50.39%] [G loss: 1.448756]\n",
            "3520 [D loss: 0.664984, acc.: 66.02%] [G loss: 1.495172]\n",
            "3540 [D loss: 0.711837, acc.: 48.44%] [G loss: 1.484247]\n",
            "3560 [D loss: 0.690800, acc.: 57.42%] [G loss: 1.489736]\n",
            "3580 [D loss: 0.684825, acc.: 53.91%] [G loss: 1.509409]\n",
            "3600 [D loss: 0.684670, acc.: 53.91%] [G loss: 1.493896]\n",
            "3620 [D loss: 0.694364, acc.: 50.78%] [G loss: 1.478275]\n",
            "3640 [D loss: 0.693833, acc.: 51.17%] [G loss: 1.486874]\n",
            "3660 [D loss: 0.698720, acc.: 49.22%] [G loss: 1.422862]\n",
            "3680 [D loss: 0.696803, acc.: 45.70%] [G loss: 1.430608]\n",
            "3700 [D loss: 0.695005, acc.: 46.88%] [G loss: 1.450449]\n",
            "3720 [D loss: 0.707793, acc.: 48.44%] [G loss: 1.443254]\n",
            "3740 [D loss: 0.710938, acc.: 47.66%] [G loss: 1.453517]\n",
            "3760 [D loss: 0.706155, acc.: 44.14%] [G loss: 1.439053]\n",
            "3780 [D loss: 0.713001, acc.: 40.23%] [G loss: 1.429250]\n",
            "3800 [D loss: 0.701870, acc.: 46.48%] [G loss: 1.448114]\n",
            "3820 [D loss: 0.697337, acc.: 50.00%] [G loss: 1.425606]\n",
            "3840 [D loss: 0.695756, acc.: 50.39%] [G loss: 1.420981]\n",
            "3860 [D loss: 0.702208, acc.: 42.58%] [G loss: 1.427657]\n",
            "3880 [D loss: 0.693512, acc.: 51.95%] [G loss: 1.434269]\n",
            "3900 [D loss: 0.690227, acc.: 50.00%] [G loss: 1.452146]\n",
            "3920 [D loss: 0.703983, acc.: 46.09%] [G loss: 1.434564]\n",
            "3940 [D loss: 0.686231, acc.: 54.69%] [G loss: 1.440786]\n",
            "3960 [D loss: 0.689491, acc.: 53.12%] [G loss: 1.451035]\n",
            "3980 [D loss: 0.693714, acc.: 51.17%] [G loss: 1.443612]\n",
            "4000 [D loss: 0.717142, acc.: 39.06%] [G loss: 1.448965]\n",
            "4020 [D loss: 0.697147, acc.: 49.61%] [G loss: 1.440637]\n",
            "4040 [D loss: 0.698382, acc.: 50.39%] [G loss: 1.444523]\n",
            "4060 [D loss: 0.708603, acc.: 46.09%] [G loss: 1.502112]\n",
            "4080 [D loss: 0.692876, acc.: 51.17%] [G loss: 1.446925]\n",
            "4100 [D loss: 0.701445, acc.: 48.44%] [G loss: 1.415573]\n",
            "4120 [D loss: 0.687019, acc.: 54.69%] [G loss: 1.432256]\n",
            "4140 [D loss: 0.701010, acc.: 48.83%] [G loss: 1.437170]\n",
            "4160 [D loss: 0.745230, acc.: 30.86%] [G loss: 1.416899]\n",
            "4180 [D loss: 0.703995, acc.: 49.61%] [G loss: 1.424776]\n",
            "4200 [D loss: 0.688016, acc.: 58.20%] [G loss: 1.437783]\n",
            "4220 [D loss: 0.700051, acc.: 47.66%] [G loss: 1.419456]\n",
            "4240 [D loss: 0.694701, acc.: 49.61%] [G loss: 1.418283]\n",
            "4260 [D loss: 0.693883, acc.: 49.61%] [G loss: 1.409715]\n",
            "4280 [D loss: 0.693882, acc.: 51.95%] [G loss: 1.408787]\n",
            "4300 [D loss: 0.699959, acc.: 53.12%] [G loss: 1.413220]\n",
            "4320 [D loss: 0.699370, acc.: 45.70%] [G loss: 1.419937]\n",
            "4340 [D loss: 0.745811, acc.: 30.86%] [G loss: 1.417630]\n",
            "4360 [D loss: 0.692830, acc.: 46.88%] [G loss: 1.420617]\n",
            "4380 [D loss: 0.709057, acc.: 39.45%] [G loss: 1.414404]\n",
            "4400 [D loss: 0.697396, acc.: 47.66%] [G loss: 1.411969]\n",
            "4420 [D loss: 0.697021, acc.: 47.66%] [G loss: 1.431252]\n",
            "4440 [D loss: 0.696567, acc.: 48.83%] [G loss: 1.428991]\n",
            "4460 [D loss: 0.696923, acc.: 49.61%] [G loss: 1.415610]\n",
            "4480 [D loss: 0.704595, acc.: 46.09%] [G loss: 1.407055]\n",
            "4500 [D loss: 0.695725, acc.: 51.17%] [G loss: 1.412494]\n",
            "4520 [D loss: 0.695956, acc.: 47.27%] [G loss: 1.439657]\n",
            "4540 [D loss: 0.696433, acc.: 49.22%] [G loss: 1.424017]\n",
            "4560 [D loss: 0.698559, acc.: 47.66%] [G loss: 1.437605]\n",
            "4580 [D loss: 0.694104, acc.: 46.88%] [G loss: 1.417557]\n",
            "4600 [D loss: 0.695957, acc.: 50.78%] [G loss: 1.423108]\n",
            "4620 [D loss: 0.716088, acc.: 35.94%] [G loss: 1.407817]\n",
            "4640 [D loss: 0.702695, acc.: 45.31%] [G loss: 1.412820]\n",
            "4660 [D loss: 0.686024, acc.: 58.20%] [G loss: 1.444611]\n",
            "4680 [D loss: 0.701994, acc.: 45.70%] [G loss: 1.427695]\n",
            "4700 [D loss: 0.696139, acc.: 48.05%] [G loss: 1.403518]\n",
            "4720 [D loss: 0.698449, acc.: 50.78%] [G loss: 1.415624]\n",
            "4740 [D loss: 0.695378, acc.: 50.00%] [G loss: 1.421859]\n",
            "4760 [D loss: 0.696298, acc.: 46.48%] [G loss: 1.419459]\n",
            "4780 [D loss: 0.699858, acc.: 45.31%] [G loss: 1.413932]\n",
            "4800 [D loss: 0.700274, acc.: 46.09%] [G loss: 1.420046]\n",
            "4820 [D loss: 0.692852, acc.: 51.56%] [G loss: 1.421996]\n",
            "4840 [D loss: 0.694121, acc.: 47.66%] [G loss: 1.403414]\n",
            "4860 [D loss: 0.699181, acc.: 48.44%] [G loss: 1.415141]\n",
            "4880 [D loss: 0.706427, acc.: 42.97%] [G loss: 1.404147]\n",
            "4900 [D loss: 0.694180, acc.: 48.83%] [G loss: 1.424622]\n",
            "4920 [D loss: 0.698233, acc.: 45.31%] [G loss: 1.395836]\n",
            "4940 [D loss: 0.698450, acc.: 47.66%] [G loss: 1.420870]\n",
            "4960 [D loss: 0.696847, acc.: 52.34%] [G loss: 1.414490]\n",
            "4980 [D loss: 0.705198, acc.: 46.48%] [G loss: 1.427585]\n",
            "5000 [D loss: 0.697227, acc.: 46.09%] [G loss: 1.426640]\n",
            "5020 [D loss: 0.700426, acc.: 46.88%] [G loss: 1.417464]\n",
            "5040 [D loss: 0.695069, acc.: 44.92%] [G loss: 1.411498]\n",
            "5060 [D loss: 0.704607, acc.: 43.75%] [G loss: 1.431370]\n",
            "5080 [D loss: 0.689293, acc.: 45.70%] [G loss: 1.434803]\n",
            "5100 [D loss: 0.689957, acc.: 51.17%] [G loss: 1.427057]\n",
            "5120 [D loss: 0.692742, acc.: 51.56%] [G loss: 1.419916]\n",
            "5140 [D loss: 0.695085, acc.: 48.05%] [G loss: 1.419271]\n",
            "5160 [D loss: 0.690384, acc.: 53.12%] [G loss: 1.410737]\n",
            "5180 [D loss: 1.012671, acc.: 48.05%] [G loss: 1.522950]\n",
            "5200 [D loss: 0.727283, acc.: 36.72%] [G loss: 1.417305]\n",
            "5220 [D loss: 0.708691, acc.: 41.02%] [G loss: 1.406122]\n",
            "5240 [D loss: 0.706943, acc.: 42.58%] [G loss: 1.391567]\n",
            "5260 [D loss: 0.709624, acc.: 35.94%] [G loss: 1.392547]\n",
            "5280 [D loss: 0.705719, acc.: 41.41%] [G loss: 1.397869]\n",
            "5300 [D loss: 0.694998, acc.: 50.78%] [G loss: 1.391285]\n",
            "5320 [D loss: 0.695255, acc.: 48.83%] [G loss: 1.394795]\n",
            "5340 [D loss: 0.702556, acc.: 46.48%] [G loss: 1.391489]\n",
            "5360 [D loss: 0.702252, acc.: 42.58%] [G loss: 1.398364]\n",
            "5380 [D loss: 0.699052, acc.: 46.09%] [G loss: 1.401921]\n",
            "5400 [D loss: 0.702825, acc.: 40.62%] [G loss: 1.389737]\n",
            "5420 [D loss: 0.699665, acc.: 44.53%] [G loss: 1.398371]\n",
            "5440 [D loss: 0.700660, acc.: 42.97%] [G loss: 1.384677]\n",
            "5460 [D loss: 0.697025, acc.: 44.92%] [G loss: 1.395576]\n",
            "5480 [D loss: 0.700690, acc.: 46.09%] [G loss: 1.380006]\n",
            "5500 [D loss: 0.698395, acc.: 43.36%] [G loss: 1.391323]\n",
            "5520 [D loss: 0.700751, acc.: 42.19%] [G loss: 1.387234]\n",
            "5540 [D loss: 0.699906, acc.: 42.97%] [G loss: 1.417115]\n",
            "5560 [D loss: 0.700181, acc.: 46.48%] [G loss: 1.399609]\n",
            "5580 [D loss: 0.696083, acc.: 44.14%] [G loss: 1.387178]\n",
            "5600 [D loss: 0.701721, acc.: 44.14%] [G loss: 1.398707]\n",
            "5620 [D loss: 0.700271, acc.: 47.66%] [G loss: 1.386191]\n",
            "5640 [D loss: 0.697733, acc.: 47.27%] [G loss: 1.401603]\n",
            "5660 [D loss: 0.699148, acc.: 42.19%] [G loss: 1.418366]\n",
            "5680 [D loss: 0.700481, acc.: 47.27%] [G loss: 1.393268]\n",
            "5700 [D loss: 0.694299, acc.: 46.88%] [G loss: 1.394834]\n",
            "5720 [D loss: 0.701391, acc.: 45.70%] [G loss: 1.392570]\n",
            "5740 [D loss: 0.702263, acc.: 42.97%] [G loss: 1.406956]\n",
            "5760 [D loss: 0.697984, acc.: 43.75%] [G loss: 1.395903]\n",
            "5780 [D loss: 0.693015, acc.: 48.44%] [G loss: 1.387486]\n",
            "5800 [D loss: 0.698194, acc.: 42.97%] [G loss: 1.403596]\n",
            "5820 [D loss: 0.696433, acc.: 48.44%] [G loss: 1.406263]\n",
            "5840 [D loss: 0.694022, acc.: 47.27%] [G loss: 1.410668]\n",
            "5860 [D loss: 0.694351, acc.: 46.09%] [G loss: 1.420547]\n",
            "5880 [D loss: 0.700743, acc.: 41.02%] [G loss: 1.396940]\n",
            "5900 [D loss: 0.695259, acc.: 49.22%] [G loss: 1.408814]\n",
            "5920 [D loss: 0.699233, acc.: 45.31%] [G loss: 1.391367]\n",
            "5940 [D loss: 0.697543, acc.: 49.22%] [G loss: 1.399017]\n",
            "5960 [D loss: 0.694568, acc.: 46.48%] [G loss: 1.400665]\n",
            "5980 [D loss: 0.699636, acc.: 41.80%] [G loss: 1.412460]\n",
            "6000 [D loss: 0.695206, acc.: 46.88%] [G loss: 1.409679]\n",
            "6020 [D loss: 0.689912, acc.: 53.52%] [G loss: 1.415947]\n",
            "6040 [D loss: 0.694409, acc.: 48.83%] [G loss: 1.410059]\n",
            "6060 [D loss: 0.705544, acc.: 41.41%] [G loss: 1.403663]\n",
            "6080 [D loss: 0.690539, acc.: 52.34%] [G loss: 1.400703]\n",
            "6100 [D loss: 0.702954, acc.: 43.36%] [G loss: 1.400158]\n",
            "6120 [D loss: 0.701554, acc.: 42.58%] [G loss: 1.409489]\n",
            "6140 [D loss: 0.688542, acc.: 51.95%] [G loss: 1.399226]\n",
            "6160 [D loss: 0.693335, acc.: 48.83%] [G loss: 1.410598]\n",
            "6180 [D loss: 0.692820, acc.: 48.83%] [G loss: 1.394371]\n",
            "6200 [D loss: 0.699526, acc.: 45.31%] [G loss: 1.412369]\n",
            "6220 [D loss: 0.693934, acc.: 47.66%] [G loss: 1.394455]\n",
            "6240 [D loss: 0.695471, acc.: 45.70%] [G loss: 1.424318]\n",
            "6260 [D loss: 0.697494, acc.: 47.27%] [G loss: 1.399517]\n",
            "6280 [D loss: 0.699151, acc.: 46.88%] [G loss: 1.409764]\n",
            "6300 [D loss: 0.692745, acc.: 49.61%] [G loss: 1.443252]\n",
            "6320 [D loss: 0.702683, acc.: 42.58%] [G loss: 1.434612]\n",
            "6340 [D loss: 0.698161, acc.: 46.48%] [G loss: 1.419926]\n",
            "6360 [D loss: 0.696624, acc.: 48.05%] [G loss: 1.431552]\n",
            "6380 [D loss: 0.699802, acc.: 46.48%] [G loss: 1.409697]\n",
            "6400 [D loss: 0.696284, acc.: 48.05%] [G loss: 1.394385]\n",
            "6420 [D loss: 0.689743, acc.: 52.73%] [G loss: 1.441482]\n",
            "6440 [D loss: 0.701868, acc.: 43.75%] [G loss: 1.399594]\n",
            "6460 [D loss: 0.700493, acc.: 43.75%] [G loss: 1.410112]\n",
            "6480 [D loss: 0.692998, acc.: 54.30%] [G loss: 1.384058]\n",
            "6500 [D loss: 0.700005, acc.: 46.48%] [G loss: 1.403947]\n",
            "6520 [D loss: 0.698148, acc.: 40.23%] [G loss: 1.416374]\n",
            "6540 [D loss: 0.692351, acc.: 48.05%] [G loss: 1.423293]\n",
            "6560 [D loss: 0.702474, acc.: 45.31%] [G loss: 1.411208]\n",
            "6580 [D loss: 0.689364, acc.: 51.17%] [G loss: 1.402667]\n",
            "6600 [D loss: 0.686588, acc.: 50.39%] [G loss: 1.422474]\n",
            "6620 [D loss: 0.689999, acc.: 48.83%] [G loss: 1.426065]\n",
            "6640 [D loss: 0.702135, acc.: 43.75%] [G loss: 1.430835]\n",
            "6660 [D loss: 0.691306, acc.: 55.08%] [G loss: 1.409989]\n",
            "6680 [D loss: 0.697459, acc.: 45.31%] [G loss: 1.412226]\n",
            "6700 [D loss: 0.696601, acc.: 50.00%] [G loss: 1.420211]\n",
            "6720 [D loss: 0.685408, acc.: 55.47%] [G loss: 1.425485]\n",
            "6740 [D loss: 0.708700, acc.: 46.09%] [G loss: 1.413840]\n",
            "6760 [D loss: 0.716200, acc.: 40.23%] [G loss: 1.445971]\n",
            "6780 [D loss: 0.692742, acc.: 50.39%] [G loss: 1.438337]\n",
            "6800 [D loss: 0.697702, acc.: 47.66%] [G loss: 1.407079]\n",
            "6820 [D loss: 0.701104, acc.: 42.19%] [G loss: 1.443135]\n",
            "6840 [D loss: 0.699977, acc.: 42.97%] [G loss: 1.408394]\n",
            "6860 [D loss: 0.695381, acc.: 45.70%] [G loss: 1.408337]\n",
            "6880 [D loss: 0.444099, acc.: 84.38%] [G loss: 6.053907]\n",
            "6900 [D loss: 0.273319, acc.: 91.02%] [G loss: 7.060250]\n",
            "6920 [D loss: 0.323156, acc.: 95.70%] [G loss: 4.163063]\n",
            "6940 [D loss: 1.101694, acc.: 29.30%] [G loss: 1.402190]\n",
            "6960 [D loss: 0.651357, acc.: 59.38%] [G loss: 2.243197]\n",
            "6980 [D loss: 0.699383, acc.: 44.53%] [G loss: 1.564636]\n",
            "7000 [D loss: 0.712263, acc.: 44.14%] [G loss: 1.581691]\n",
            "7020 [D loss: 0.690582, acc.: 51.17%] [G loss: 1.567748]\n",
            "7040 [D loss: 0.686206, acc.: 51.56%] [G loss: 1.491084]\n",
            "7060 [D loss: 0.694190, acc.: 51.95%] [G loss: 1.497639]\n",
            "7080 [D loss: 0.699104, acc.: 49.61%] [G loss: 1.508282]\n",
            "7100 [D loss: 0.700722, acc.: 49.22%] [G loss: 1.539163]\n",
            "7120 [D loss: 0.683961, acc.: 50.00%] [G loss: 1.519552]\n",
            "7140 [D loss: 0.684341, acc.: 55.47%] [G loss: 1.516116]\n",
            "7160 [D loss: 0.697103, acc.: 45.31%] [G loss: 1.461138]\n",
            "7180 [D loss: 0.693808, acc.: 50.78%] [G loss: 1.481620]\n",
            "7200 [D loss: 0.690671, acc.: 50.78%] [G loss: 1.505087]\n",
            "7220 [D loss: 0.687076, acc.: 46.09%] [G loss: 1.495131]\n",
            "7240 [D loss: 0.693192, acc.: 50.00%] [G loss: 1.454266]\n",
            "7260 [D loss: 0.698602, acc.: 47.66%] [G loss: 1.466166]\n",
            "7280 [D loss: 0.695293, acc.: 50.78%] [G loss: 1.505988]\n",
            "7300 [D loss: 0.684979, acc.: 53.91%] [G loss: 1.477982]\n",
            "7320 [D loss: 0.687425, acc.: 54.69%] [G loss: 1.500499]\n",
            "7340 [D loss: 0.686579, acc.: 54.69%] [G loss: 1.470049]\n",
            "7360 [D loss: 0.694102, acc.: 49.22%] [G loss: 1.477323]\n",
            "7380 [D loss: 0.689325, acc.: 51.56%] [G loss: 1.461196]\n",
            "7400 [D loss: 0.689564, acc.: 48.05%] [G loss: 1.472092]\n",
            "7420 [D loss: 0.689597, acc.: 52.73%] [G loss: 1.455076]\n",
            "7440 [D loss: 0.687651, acc.: 51.56%] [G loss: 1.473908]\n",
            "7460 [D loss: 0.694740, acc.: 48.05%] [G loss: 1.498318]\n",
            "7480 [D loss: 0.696078, acc.: 50.39%] [G loss: 1.525023]\n",
            "7500 [D loss: 0.693112, acc.: 49.61%] [G loss: 1.456388]\n",
            "7520 [D loss: 0.695667, acc.: 46.09%] [G loss: 1.407835]\n",
            "7540 [D loss: 0.695401, acc.: 48.05%] [G loss: 1.427727]\n",
            "7560 [D loss: 0.691981, acc.: 50.78%] [G loss: 1.435450]\n",
            "7580 [D loss: 0.692715, acc.: 47.27%] [G loss: 1.432324]\n",
            "7600 [D loss: 0.693979, acc.: 51.17%] [G loss: 1.404536]\n",
            "7620 [D loss: 0.691484, acc.: 48.05%] [G loss: 1.429542]\n",
            "7640 [D loss: 0.694974, acc.: 44.92%] [G loss: 1.411864]\n",
            "7660 [D loss: 0.698339, acc.: 46.48%] [G loss: 1.405938]\n",
            "7680 [D loss: 0.699277, acc.: 46.48%] [G loss: 1.408947]\n",
            "7700 [D loss: 0.699492, acc.: 44.92%] [G loss: 1.405575]\n",
            "7720 [D loss: 0.689510, acc.: 50.78%] [G loss: 1.395054]\n",
            "7740 [D loss: 0.690052, acc.: 52.34%] [G loss: 1.407024]\n",
            "7760 [D loss: 0.694280, acc.: 48.05%] [G loss: 1.420364]\n",
            "7780 [D loss: 0.693998, acc.: 48.44%] [G loss: 1.412959]\n",
            "7800 [D loss: 0.691867, acc.: 52.73%] [G loss: 1.416630]\n",
            "7820 [D loss: 0.692904, acc.: 52.73%] [G loss: 1.407740]\n",
            "7840 [D loss: 0.688603, acc.: 55.86%] [G loss: 1.411860]\n",
            "7860 [D loss: 0.688145, acc.: 55.08%] [G loss: 1.427579]\n",
            "7880 [D loss: 0.688801, acc.: 55.08%] [G loss: 1.424311]\n",
            "7900 [D loss: 0.690764, acc.: 51.95%] [G loss: 1.417383]\n",
            "7920 [D loss: 0.696118, acc.: 50.39%] [G loss: 1.404105]\n",
            "7940 [D loss: 0.694125, acc.: 48.44%] [G loss: 1.402529]\n",
            "7960 [D loss: 0.690083, acc.: 50.78%] [G loss: 1.417923]\n",
            "7980 [D loss: 0.693435, acc.: 49.22%] [G loss: 1.440601]\n",
            "8000 [D loss: 0.688032, acc.: 51.56%] [G loss: 1.424206]\n",
            "8020 [D loss: 0.692617, acc.: 47.66%] [G loss: 1.426255]\n",
            "8040 [D loss: 0.691548, acc.: 54.69%] [G loss: 1.413899]\n",
            "8060 [D loss: 0.696949, acc.: 45.31%] [G loss: 1.422938]\n",
            "8080 [D loss: 0.692273, acc.: 44.14%] [G loss: 1.420765]\n",
            "8100 [D loss: 0.694756, acc.: 49.61%] [G loss: 1.456577]\n",
            "8120 [D loss: 0.694418, acc.: 51.17%] [G loss: 1.438942]\n",
            "8140 [D loss: 0.691277, acc.: 52.34%] [G loss: 1.416148]\n",
            "8160 [D loss: 0.688812, acc.: 53.91%] [G loss: 1.407507]\n",
            "8180 [D loss: 0.692528, acc.: 52.73%] [G loss: 1.431111]\n",
            "8200 [D loss: 0.690510, acc.: 50.00%] [G loss: 1.419295]\n",
            "8220 [D loss: 0.693027, acc.: 51.95%] [G loss: 1.401641]\n",
            "8240 [D loss: 0.688154, acc.: 53.91%] [G loss: 1.422031]\n",
            "8260 [D loss: 0.684049, acc.: 57.42%] [G loss: 1.434079]\n",
            "8280 [D loss: 0.696402, acc.: 48.44%] [G loss: 1.402962]\n",
            "8300 [D loss: 0.681774, acc.: 56.25%] [G loss: 1.430569]\n",
            "8320 [D loss: 0.139754, acc.: 98.44%] [G loss: 6.243649]\n",
            "8340 [D loss: 0.716489, acc.: 52.34%] [G loss: 2.046253]\n",
            "8360 [D loss: 0.765530, acc.: 23.83%] [G loss: 1.374165]\n",
            "8380 [D loss: 0.741326, acc.: 36.33%] [G loss: 1.448000]\n",
            "8400 [D loss: 0.695419, acc.: 48.83%] [G loss: 1.448341]\n",
            "8420 [D loss: 0.714366, acc.: 39.06%] [G loss: 1.419053]\n",
            "8440 [D loss: 0.710955, acc.: 41.41%] [G loss: 1.416771]\n",
            "8460 [D loss: 0.700765, acc.: 42.97%] [G loss: 1.403914]\n",
            "8480 [D loss: 0.701133, acc.: 43.36%] [G loss: 1.414495]\n",
            "8500 [D loss: 0.700658, acc.: 46.88%] [G loss: 1.413072]\n",
            "8520 [D loss: 0.697649, acc.: 47.66%] [G loss: 1.410884]\n",
            "8540 [D loss: 0.694475, acc.: 48.05%] [G loss: 1.406441]\n",
            "8560 [D loss: 0.698797, acc.: 45.31%] [G loss: 1.412277]\n",
            "8580 [D loss: 0.703069, acc.: 44.53%] [G loss: 1.405510]\n",
            "8600 [D loss: 0.699880, acc.: 45.31%] [G loss: 1.404989]\n",
            "8620 [D loss: 0.703012, acc.: 42.58%] [G loss: 1.398409]\n",
            "8640 [D loss: 0.700837, acc.: 41.02%] [G loss: 1.414674]\n",
            "8660 [D loss: 0.697648, acc.: 46.09%] [G loss: 1.405505]\n",
            "8680 [D loss: 0.700969, acc.: 47.66%] [G loss: 1.396464]\n",
            "8700 [D loss: 0.696272, acc.: 46.48%] [G loss: 1.409630]\n",
            "8720 [D loss: 0.692322, acc.: 50.39%] [G loss: 1.414293]\n",
            "8740 [D loss: 0.699485, acc.: 44.92%] [G loss: 1.401961]\n",
            "8760 [D loss: 0.696273, acc.: 48.83%] [G loss: 1.405428]\n",
            "8780 [D loss: 0.693667, acc.: 50.00%] [G loss: 1.402102]\n",
            "8800 [D loss: 0.695137, acc.: 52.34%] [G loss: 1.406978]\n",
            "8820 [D loss: 0.697011, acc.: 44.92%] [G loss: 1.396670]\n",
            "8840 [D loss: 0.693467, acc.: 52.73%] [G loss: 1.402870]\n",
            "8860 [D loss: 0.695329, acc.: 50.00%] [G loss: 1.405411]\n",
            "8880 [D loss: 0.693323, acc.: 51.17%] [G loss: 1.398617]\n",
            "8900 [D loss: 0.697285, acc.: 47.66%] [G loss: 1.400946]\n",
            "8920 [D loss: 0.695924, acc.: 48.44%] [G loss: 1.407973]\n",
            "8940 [D loss: 0.691699, acc.: 48.44%] [G loss: 1.418185]\n",
            "8960 [D loss: 0.695740, acc.: 47.66%] [G loss: 1.420077]\n",
            "8980 [D loss: 0.690593, acc.: 52.34%] [G loss: 1.414288]\n",
            "9000 [D loss: 0.697371, acc.: 46.88%] [G loss: 1.419703]\n",
            "9020 [D loss: 0.693907, acc.: 47.66%] [G loss: 1.414523]\n",
            "9040 [D loss: 0.690701, acc.: 54.69%] [G loss: 1.401510]\n",
            "9060 [D loss: 0.693004, acc.: 50.78%] [G loss: 1.397298]\n",
            "9080 [D loss: 0.691940, acc.: 52.34%] [G loss: 1.416731]\n",
            "9100 [D loss: 0.691173, acc.: 55.86%] [G loss: 1.398205]\n",
            "9120 [D loss: 0.696686, acc.: 48.83%] [G loss: 1.414791]\n",
            "9140 [D loss: 0.693727, acc.: 49.22%] [G loss: 1.391773]\n",
            "9160 [D loss: 0.697954, acc.: 47.66%] [G loss: 1.409909]\n",
            "9180 [D loss: 0.696974, acc.: 48.83%] [G loss: 1.415303]\n",
            "9200 [D loss: 0.692683, acc.: 55.08%] [G loss: 1.426481]\n",
            "9220 [D loss: 0.693954, acc.: 50.78%] [G loss: 1.411644]\n",
            "9240 [D loss: 0.692389, acc.: 53.91%] [G loss: 1.404522]\n",
            "9260 [D loss: 0.697386, acc.: 48.44%] [G loss: 1.403096]\n",
            "9280 [D loss: 0.699856, acc.: 43.75%] [G loss: 1.401332]\n",
            "9300 [D loss: 0.693084, acc.: 48.83%] [G loss: 1.396296]\n",
            "9320 [D loss: 0.701190, acc.: 43.75%] [G loss: 1.418929]\n",
            "9340 [D loss: 0.691749, acc.: 51.95%] [G loss: 1.410571]\n",
            "9360 [D loss: 0.685287, acc.: 57.03%] [G loss: 1.449596]\n",
            "9380 [D loss: 0.687058, acc.: 55.08%] [G loss: 1.433376]\n",
            "9400 [D loss: 0.692221, acc.: 50.78%] [G loss: 1.415294]\n",
            "9420 [D loss: 0.697219, acc.: 44.14%] [G loss: 1.424789]\n",
            "9440 [D loss: 0.693744, acc.: 51.56%] [G loss: 1.396848]\n",
            "9460 [D loss: 0.690543, acc.: 47.66%] [G loss: 1.409210]\n",
            "9480 [D loss: 0.691840, acc.: 48.05%] [G loss: 1.403609]\n",
            "9500 [D loss: 0.689467, acc.: 51.95%] [G loss: 1.412318]\n",
            "9520 [D loss: 0.693292, acc.: 47.66%] [G loss: 1.416318]\n",
            "9540 [D loss: 0.695761, acc.: 48.83%] [G loss: 1.416045]\n",
            "9560 [D loss: 0.690365, acc.: 53.12%] [G loss: 1.407329]\n",
            "9580 [D loss: 0.694444, acc.: 51.56%] [G loss: 1.405172]\n",
            "9600 [D loss: 0.683955, acc.: 57.03%] [G loss: 1.392226]\n",
            "9620 [D loss: 0.693085, acc.: 53.52%] [G loss: 1.406688]\n",
            "9640 [D loss: 0.688358, acc.: 56.64%] [G loss: 1.440224]\n",
            "9660 [D loss: 0.690784, acc.: 55.86%] [G loss: 1.414756]\n",
            "9680 [D loss: 0.460662, acc.: 72.27%] [G loss: 5.750870]\n",
            "9700 [D loss: 0.376775, acc.: 83.59%] [G loss: 8.148158]\n",
            "9720 [D loss: 0.707921, acc.: 53.91%] [G loss: 1.501420]\n",
            "9740 [D loss: 0.696211, acc.: 48.83%] [G loss: 1.446587]\n",
            "9760 [D loss: 0.698908, acc.: 43.36%] [G loss: 1.428715]\n",
            "9780 [D loss: 0.693807, acc.: 46.48%] [G loss: 1.441884]\n",
            "9800 [D loss: 0.701047, acc.: 46.48%] [G loss: 1.399015]\n",
            "9820 [D loss: 0.693996, acc.: 50.00%] [G loss: 1.436534]\n",
            "9840 [D loss: 0.700555, acc.: 47.66%] [G loss: 1.422491]\n",
            "9860 [D loss: 0.694560, acc.: 48.44%] [G loss: 1.434762]\n",
            "9880 [D loss: 0.695178, acc.: 44.53%] [G loss: 1.413695]\n",
            "9900 [D loss: 0.686277, acc.: 54.69%] [G loss: 1.443591]\n",
            "9920 [D loss: 0.692438, acc.: 49.22%] [G loss: 1.428039]\n",
            "9940 [D loss: 0.693166, acc.: 51.56%] [G loss: 1.426500]\n",
            "9960 [D loss: 0.690441, acc.: 53.52%] [G loss: 1.443086]\n",
            "9980 [D loss: 0.710367, acc.: 44.92%] [G loss: 1.437533]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}