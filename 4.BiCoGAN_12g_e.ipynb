{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "6_Bidirectional_Conditional_GAN_12_e.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "373f0c85-6746-4001-cfea-7b4e631ade49"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6db40c6-6ee3-46d0-d72b-bdcaaeee31db"
      },
      "source": [
        "data.Usage.value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training       28709\n",
              "PrivateTest     3589\n",
              "PublicTest      3589\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 7\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "559857ce-758b-4407-fef8-3fc79591eafe"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnh1CQpGKk-t"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 7\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        # noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,))\n",
        "        # img = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        zn = Dense(512, activation='relu')(zn)\n",
        "        zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        # (_, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Configure input\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\n",
        "        y_train = y.to_numpy().reshape(-1, 1)\n",
        "        # y_train = y.reshape(-1, 1)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 7, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    # def sample_images(self, epoch):\n",
        "    #     r, c = 1, 7\n",
        "    #     noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    #     sampled_labels = np.arange(0, 7).reshape(-1, 1)\n",
        "\n",
        "    #     gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "    #     print(gen_imgs.shape)\n",
        "\n",
        "    #     # Rescale images 0 - 1\n",
        "    #     gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    #     fig, axs = plt.subplots(r, c)\n",
        "    #     cnt = 0\n",
        "    #     for i in range(r):\n",
        "    #         print(i)\n",
        "    #         for j in range(c):\n",
        "    #             print(j)\n",
        "    #             axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "    #             axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
        "    #             axs[i,j].axis('off')\n",
        "    #             cnt += 1\n",
        "    #     fig.savefig(\"images/%d.png\" % epoch)\n",
        "    #     plt.close()\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 7\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 7).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"label: %d\" % sampled_labels[cnt])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images_new_encoder/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvRCidGjuuZj",
        "outputId": "62b765b7-c98d-4b4e-86b1-1b5d14ee0db4"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 48, 48, 128)  3328        input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 100)          0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          51712       flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 2304)      16128       input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 2304)         0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            3969        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 64)        1088      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       131200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 256)         524544    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 3, 3, 512)         2097664   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 3,218,980\n",
            "Trainable params: 3,217,188\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.693916, acc.: 48.44%] [G loss: 1.439529]\n",
            "20 [D loss: 0.926208, acc.: 49.22%] [G loss: 1.242952]\n",
            "40 [D loss: 0.643202, acc.: 45.70%] [G loss: 5.251611]\n",
            "60 [D loss: 1.993852, acc.: 41.41%] [G loss: 4.328197]\n",
            "80 [D loss: 0.260403, acc.: 93.75%] [G loss: 4.577056]\n",
            "100 [D loss: 0.597281, acc.: 60.55%] [G loss: 3.123721]\n",
            "120 [D loss: 1.027904, acc.: 8.98%] [G loss: 1.187343]\n",
            "140 [D loss: 0.641382, acc.: 61.33%] [G loss: 3.394920]\n",
            "160 [D loss: 0.675995, acc.: 66.02%] [G loss: 3.111628]\n",
            "180 [D loss: 0.668609, acc.: 40.23%] [G loss: 2.084477]\n",
            "200 [D loss: 0.721104, acc.: 76.95%] [G loss: 3.901381]\n",
            "220 [D loss: 0.962076, acc.: 15.62%] [G loss: 1.523612]\n",
            "240 [D loss: 1.391038, acc.: 28.12%] [G loss: 3.405057]\n",
            "260 [D loss: 0.823425, acc.: 32.81%] [G loss: 1.445563]\n",
            "280 [D loss: 0.570033, acc.: 74.61%] [G loss: 2.738454]\n",
            "300 [D loss: 1.180600, acc.: 21.09%] [G loss: 1.654526]\n",
            "320 [D loss: 0.868864, acc.: 14.06%] [G loss: 1.278434]\n",
            "340 [D loss: 0.668805, acc.: 64.45%] [G loss: 1.779541]\n",
            "360 [D loss: 0.233543, acc.: 93.36%] [G loss: 6.594237]\n",
            "380 [D loss: 0.304224, acc.: 92.19%] [G loss: 7.480969]\n",
            "400 [D loss: 0.596022, acc.: 75.78%] [G loss: 2.630725]\n",
            "420 [D loss: 0.609043, acc.: 66.80%] [G loss: 4.035032]\n",
            "440 [D loss: 0.822233, acc.: 37.89%] [G loss: 1.698026]\n",
            "460 [D loss: 0.504743, acc.: 83.20%] [G loss: 2.973323]\n",
            "480 [D loss: 0.634396, acc.: 68.75%] [G loss: 1.975714]\n",
            "500 [D loss: 0.516075, acc.: 82.81%] [G loss: 3.124448]\n",
            "520 [D loss: 0.614186, acc.: 75.39%] [G loss: 2.953848]\n",
            "540 [D loss: 0.474431, acc.: 76.17%] [G loss: 4.159148]\n",
            "560 [D loss: 0.521632, acc.: 75.39%] [G loss: 3.656991]\n",
            "580 [D loss: 0.619137, acc.: 66.80%] [G loss: 3.438854]\n",
            "600 [D loss: 0.547471, acc.: 70.31%] [G loss: 4.681175]\n",
            "620 [D loss: 0.489510, acc.: 76.17%] [G loss: 3.526897]\n",
            "640 [D loss: 0.601739, acc.: 70.31%] [G loss: 2.845148]\n",
            "660 [D loss: 0.377533, acc.: 85.55%] [G loss: 5.464051]\n",
            "680 [D loss: 0.713686, acc.: 58.59%] [G loss: 2.680807]\n",
            "700 [D loss: 0.628917, acc.: 62.50%] [G loss: 2.769758]\n",
            "720 [D loss: 0.535001, acc.: 77.34%] [G loss: 2.924260]\n",
            "740 [D loss: 0.380170, acc.: 87.50%] [G loss: 3.940012]\n",
            "760 [D loss: 0.586154, acc.: 71.09%] [G loss: 3.850037]\n",
            "780 [D loss: 0.479908, acc.: 79.69%] [G loss: 3.405774]\n",
            "800 [D loss: 0.625523, acc.: 70.31%] [G loss: 2.419383]\n",
            "820 [D loss: 0.663629, acc.: 61.33%] [G loss: 2.238888]\n",
            "840 [D loss: 0.581111, acc.: 69.14%] [G loss: 2.938924]\n",
            "860 [D loss: 0.618610, acc.: 65.62%] [G loss: 2.591014]\n",
            "880 [D loss: 0.594183, acc.: 69.92%] [G loss: 2.883147]\n",
            "900 [D loss: 0.518608, acc.: 75.00%] [G loss: 2.991222]\n",
            "920 [D loss: 0.619111, acc.: 67.58%] [G loss: 2.937140]\n",
            "940 [D loss: 0.495031, acc.: 77.73%] [G loss: 2.941169]\n",
            "960 [D loss: 0.695202, acc.: 63.28%] [G loss: 2.909618]\n",
            "980 [D loss: 0.520030, acc.: 75.39%] [G loss: 2.906405]\n",
            "1000 [D loss: 0.456451, acc.: 80.47%] [G loss: 3.426274]\n",
            "1020 [D loss: 0.465538, acc.: 79.69%] [G loss: 3.503361]\n",
            "1040 [D loss: 0.542151, acc.: 71.48%] [G loss: 3.395344]\n",
            "1060 [D loss: 0.588313, acc.: 71.09%] [G loss: 3.265959]\n",
            "1080 [D loss: 0.597720, acc.: 69.92%] [G loss: 3.100487]\n",
            "1100 [D loss: 0.558154, acc.: 72.27%] [G loss: 3.348246]\n",
            "1120 [D loss: 0.470964, acc.: 78.52%] [G loss: 3.729856]\n",
            "1140 [D loss: 0.550901, acc.: 76.17%] [G loss: 3.069927]\n",
            "1160 [D loss: 0.490871, acc.: 77.34%] [G loss: 3.208707]\n",
            "1180 [D loss: 0.573894, acc.: 69.14%] [G loss: 3.343178]\n",
            "1200 [D loss: 0.524559, acc.: 76.17%] [G loss: 2.842883]\n",
            "1220 [D loss: 0.489987, acc.: 78.12%] [G loss: 3.462937]\n",
            "1240 [D loss: 0.550087, acc.: 70.31%] [G loss: 3.321040]\n",
            "1260 [D loss: 0.517888, acc.: 75.00%] [G loss: 2.944423]\n",
            "1280 [D loss: 0.536488, acc.: 71.48%] [G loss: 3.356628]\n",
            "1300 [D loss: 0.557454, acc.: 72.66%] [G loss: 2.919161]\n",
            "1320 [D loss: 0.640990, acc.: 66.41%] [G loss: 2.819467]\n",
            "1340 [D loss: 0.537703, acc.: 74.22%] [G loss: 3.000007]\n",
            "1360 [D loss: 0.649323, acc.: 60.55%] [G loss: 2.431930]\n",
            "1380 [D loss: 0.558127, acc.: 70.70%] [G loss: 2.962583]\n",
            "1400 [D loss: 0.594632, acc.: 68.36%] [G loss: 2.735722]\n",
            "1420 [D loss: 0.578983, acc.: 70.70%] [G loss: 3.029470]\n",
            "1440 [D loss: 0.543018, acc.: 73.83%] [G loss: 3.016975]\n",
            "1460 [D loss: 0.485536, acc.: 75.39%] [G loss: 3.521998]\n",
            "1480 [D loss: 0.519549, acc.: 71.48%] [G loss: 3.341362]\n",
            "1500 [D loss: 0.520606, acc.: 76.56%] [G loss: 3.043445]\n",
            "1520 [D loss: 0.457733, acc.: 78.12%] [G loss: 3.220740]\n",
            "1540 [D loss: 0.513263, acc.: 76.95%] [G loss: 3.251546]\n",
            "1560 [D loss: 0.512199, acc.: 75.00%] [G loss: 3.187892]\n",
            "1580 [D loss: 0.483176, acc.: 81.25%] [G loss: 3.116382]\n",
            "1600 [D loss: 0.514249, acc.: 73.83%] [G loss: 3.189354]\n",
            "1620 [D loss: 0.470290, acc.: 75.78%] [G loss: 3.277145]\n",
            "1640 [D loss: 0.540384, acc.: 71.88%] [G loss: 3.087031]\n",
            "1660 [D loss: 0.459116, acc.: 81.25%] [G loss: 3.323812]\n",
            "1680 [D loss: 0.572485, acc.: 70.31%] [G loss: 3.105845]\n",
            "1700 [D loss: 0.491970, acc.: 78.12%] [G loss: 3.275803]\n",
            "1720 [D loss: 0.480541, acc.: 78.52%] [G loss: 3.352374]\n",
            "1740 [D loss: 0.468666, acc.: 77.73%] [G loss: 3.270364]\n",
            "1760 [D loss: 0.469253, acc.: 79.30%] [G loss: 3.233434]\n",
            "1780 [D loss: 0.523560, acc.: 75.00%] [G loss: 3.165415]\n",
            "1800 [D loss: 0.504695, acc.: 77.73%] [G loss: 3.219032]\n",
            "1820 [D loss: 0.582437, acc.: 67.97%] [G loss: 3.169259]\n",
            "1840 [D loss: 0.467564, acc.: 78.52%] [G loss: 3.423534]\n",
            "1860 [D loss: 0.503477, acc.: 75.39%] [G loss: 3.383215]\n",
            "1880 [D loss: 0.464996, acc.: 77.73%] [G loss: 3.595153]\n",
            "1900 [D loss: 0.522082, acc.: 75.00%] [G loss: 3.337089]\n",
            "1920 [D loss: 0.473584, acc.: 77.34%] [G loss: 3.393001]\n",
            "1940 [D loss: 0.591504, acc.: 70.70%] [G loss: 3.138098]\n",
            "1960 [D loss: 0.581011, acc.: 71.88%] [G loss: 3.378711]\n",
            "1980 [D loss: 0.535963, acc.: 72.66%] [G loss: 3.289177]\n",
            "2000 [D loss: 0.524791, acc.: 75.00%] [G loss: 3.512549]\n",
            "2020 [D loss: 0.544364, acc.: 72.66%] [G loss: 3.073509]\n",
            "2040 [D loss: 0.495233, acc.: 80.86%] [G loss: 3.064870]\n",
            "2060 [D loss: 0.563794, acc.: 71.48%] [G loss: 2.975322]\n",
            "2080 [D loss: 0.483765, acc.: 76.95%] [G loss: 3.361030]\n",
            "2100 [D loss: 0.495173, acc.: 76.56%] [G loss: 3.202473]\n",
            "2120 [D loss: 0.531498, acc.: 73.44%] [G loss: 3.315490]\n",
            "2140 [D loss: 0.565693, acc.: 71.88%] [G loss: 2.974917]\n",
            "2160 [D loss: 0.595637, acc.: 69.14%] [G loss: 2.886601]\n",
            "2180 [D loss: 0.454674, acc.: 82.81%] [G loss: 3.098834]\n",
            "2200 [D loss: 0.522554, acc.: 73.83%] [G loss: 3.323519]\n",
            "2220 [D loss: 0.674192, acc.: 60.55%] [G loss: 3.238855]\n",
            "2240 [D loss: 0.445171, acc.: 79.69%] [G loss: 3.333067]\n",
            "2260 [D loss: 0.440806, acc.: 80.47%] [G loss: 3.550718]\n",
            "2280 [D loss: 0.608096, acc.: 66.80%] [G loss: 3.088698]\n",
            "2300 [D loss: 0.427181, acc.: 84.38%] [G loss: 3.229176]\n",
            "2320 [D loss: 0.506957, acc.: 75.39%] [G loss: 3.253879]\n",
            "2340 [D loss: 0.452019, acc.: 77.73%] [G loss: 3.661633]\n",
            "2360 [D loss: 0.476690, acc.: 78.12%] [G loss: 3.133325]\n",
            "2380 [D loss: 0.528625, acc.: 73.05%] [G loss: 3.251669]\n",
            "2400 [D loss: 0.438511, acc.: 80.47%] [G loss: 3.565143]\n",
            "2420 [D loss: 0.562744, acc.: 73.05%] [G loss: 3.192548]\n",
            "2440 [D loss: 0.458835, acc.: 78.12%] [G loss: 3.429938]\n",
            "2460 [D loss: 0.562781, acc.: 69.53%] [G loss: 3.295292]\n",
            "2480 [D loss: 0.638190, acc.: 65.23%] [G loss: 2.987185]\n",
            "2500 [D loss: 0.509614, acc.: 78.52%] [G loss: 2.979171]\n",
            "2520 [D loss: 0.478003, acc.: 78.52%] [G loss: 3.402151]\n",
            "2540 [D loss: 0.452733, acc.: 80.47%] [G loss: 3.251888]\n",
            "2560 [D loss: 0.509829, acc.: 78.52%] [G loss: 3.305794]\n",
            "2580 [D loss: 0.560745, acc.: 69.14%] [G loss: 3.083888]\n",
            "2600 [D loss: 0.480696, acc.: 78.12%] [G loss: 3.027272]\n",
            "2620 [D loss: 0.577058, acc.: 69.92%] [G loss: 2.867328]\n",
            "2640 [D loss: 0.496333, acc.: 78.12%] [G loss: 3.309543]\n",
            "2660 [D loss: 0.481093, acc.: 76.95%] [G loss: 3.299796]\n",
            "2680 [D loss: 0.474212, acc.: 77.34%] [G loss: 3.157239]\n",
            "2700 [D loss: 0.460350, acc.: 75.39%] [G loss: 3.306420]\n",
            "2720 [D loss: 0.493545, acc.: 75.39%] [G loss: 2.999612]\n",
            "2740 [D loss: 0.468896, acc.: 79.69%] [G loss: 3.035242]\n",
            "2760 [D loss: 0.493752, acc.: 78.12%] [G loss: 3.237194]\n",
            "2780 [D loss: 0.480837, acc.: 78.52%] [G loss: 3.190739]\n",
            "2800 [D loss: 0.541659, acc.: 73.05%] [G loss: 3.080400]\n",
            "2820 [D loss: 0.506564, acc.: 75.39%] [G loss: 3.177041]\n",
            "2840 [D loss: 0.486019, acc.: 76.17%] [G loss: 3.295734]\n",
            "2860 [D loss: 0.549106, acc.: 74.22%] [G loss: 3.119307]\n",
            "2880 [D loss: 0.463150, acc.: 80.47%] [G loss: 3.635792]\n",
            "2900 [D loss: 0.447036, acc.: 81.64%] [G loss: 3.211789]\n",
            "2920 [D loss: 0.561688, acc.: 72.66%] [G loss: 3.074428]\n",
            "2940 [D loss: 0.541561, acc.: 71.09%] [G loss: 3.083261]\n",
            "2960 [D loss: 0.548057, acc.: 70.31%] [G loss: 3.020555]\n",
            "2980 [D loss: 0.472624, acc.: 78.52%] [G loss: 3.184041]\n",
            "3000 [D loss: 0.507615, acc.: 73.05%] [G loss: 3.153354]\n",
            "3020 [D loss: 0.476967, acc.: 79.30%] [G loss: 3.119408]\n",
            "3040 [D loss: 0.498628, acc.: 75.39%] [G loss: 3.121633]\n",
            "3060 [D loss: 1.006313, acc.: 46.09%] [G loss: 2.964619]\n",
            "3080 [D loss: 0.541502, acc.: 75.39%] [G loss: 2.909225]\n",
            "3100 [D loss: 0.530613, acc.: 75.00%] [G loss: 2.936673]\n",
            "3120 [D loss: 0.435599, acc.: 85.16%] [G loss: 3.338300]\n",
            "3140 [D loss: 0.495252, acc.: 73.05%] [G loss: 3.202557]\n",
            "3160 [D loss: 0.532436, acc.: 75.39%] [G loss: 2.959595]\n",
            "3180 [D loss: 0.549641, acc.: 73.44%] [G loss: 2.980160]\n",
            "3200 [D loss: 0.519097, acc.: 72.66%] [G loss: 3.067997]\n",
            "3220 [D loss: 0.541536, acc.: 70.31%] [G loss: 3.285350]\n",
            "3240 [D loss: 0.487221, acc.: 78.52%] [G loss: 3.050377]\n",
            "3260 [D loss: 0.494254, acc.: 78.52%] [G loss: 3.027379]\n",
            "3280 [D loss: 0.493029, acc.: 76.17%] [G loss: 2.949411]\n",
            "3300 [D loss: 0.528960, acc.: 73.83%] [G loss: 3.056689]\n",
            "3320 [D loss: 0.450802, acc.: 78.91%] [G loss: 3.114955]\n",
            "3340 [D loss: 0.534716, acc.: 71.88%] [G loss: 2.997471]\n",
            "3360 [D loss: 0.585487, acc.: 69.92%] [G loss: 2.702114]\n",
            "3380 [D loss: 0.503655, acc.: 75.00%] [G loss: 3.062495]\n",
            "3400 [D loss: 0.535129, acc.: 73.44%] [G loss: 2.954550]\n",
            "3420 [D loss: 0.507906, acc.: 75.39%] [G loss: 3.162758]\n",
            "3440 [D loss: 0.433785, acc.: 82.81%] [G loss: 3.485695]\n",
            "3460 [D loss: 0.636449, acc.: 61.72%] [G loss: 2.830604]\n",
            "3480 [D loss: 0.533235, acc.: 74.61%] [G loss: 2.882196]\n",
            "3500 [D loss: 0.502772, acc.: 80.86%] [G loss: 2.725880]\n",
            "3520 [D loss: 0.479053, acc.: 77.34%] [G loss: 3.244250]\n",
            "3540 [D loss: 0.517421, acc.: 75.00%] [G loss: 3.074370]\n",
            "3560 [D loss: 0.379306, acc.: 85.55%] [G loss: 3.791992]\n",
            "3580 [D loss: 0.484292, acc.: 77.73%] [G loss: 3.121496]\n",
            "3600 [D loss: 0.403287, acc.: 84.38%] [G loss: 3.301053]\n",
            "3620 [D loss: 0.423815, acc.: 82.42%] [G loss: 3.643307]\n",
            "3640 [D loss: 0.597870, acc.: 68.36%] [G loss: 2.990095]\n",
            "3660 [D loss: 0.396311, acc.: 83.20%] [G loss: 3.381074]\n",
            "3680 [D loss: 0.369973, acc.: 85.94%] [G loss: 3.085876]\n",
            "3700 [D loss: 0.493552, acc.: 78.52%] [G loss: 3.116796]\n",
            "3720 [D loss: 0.518373, acc.: 74.22%] [G loss: 2.981796]\n",
            "3740 [D loss: 0.421795, acc.: 82.42%] [G loss: 3.021959]\n",
            "3760 [D loss: 0.452826, acc.: 80.08%] [G loss: 3.550236]\n",
            "3780 [D loss: 0.524805, acc.: 74.22%] [G loss: 3.042432]\n",
            "3800 [D loss: 0.558152, acc.: 69.14%] [G loss: 3.041130]\n",
            "3820 [D loss: 0.606318, acc.: 68.75%] [G loss: 2.996028]\n",
            "3840 [D loss: 0.551802, acc.: 73.44%] [G loss: 2.732197]\n",
            "3860 [D loss: 0.590952, acc.: 68.36%] [G loss: 2.895323]\n",
            "3880 [D loss: 0.558110, acc.: 67.19%] [G loss: 2.965818]\n",
            "3900 [D loss: 0.552843, acc.: 71.48%] [G loss: 2.946795]\n",
            "3920 [D loss: 0.589827, acc.: 68.75%] [G loss: 2.883342]\n",
            "3940 [D loss: 0.521849, acc.: 74.61%] [G loss: 2.975414]\n",
            "3960 [D loss: 0.565307, acc.: 73.44%] [G loss: 2.859632]\n",
            "3980 [D loss: 0.479380, acc.: 78.91%] [G loss: 3.287780]\n",
            "4000 [D loss: 0.574060, acc.: 72.27%] [G loss: 2.916520]\n",
            "4020 [D loss: 0.499845, acc.: 78.12%] [G loss: 3.180981]\n",
            "4040 [D loss: 0.522010, acc.: 72.66%] [G loss: 2.854155]\n",
            "4060 [D loss: 0.535014, acc.: 75.78%] [G loss: 2.880153]\n",
            "4080 [D loss: 0.490910, acc.: 79.69%] [G loss: 2.844202]\n",
            "4100 [D loss: 0.511144, acc.: 76.56%] [G loss: 2.813448]\n",
            "4120 [D loss: 0.540753, acc.: 72.66%] [G loss: 2.860467]\n",
            "4140 [D loss: 0.492109, acc.: 77.73%] [G loss: 2.902018]\n",
            "4160 [D loss: 0.543693, acc.: 76.17%] [G loss: 2.811043]\n",
            "4180 [D loss: 0.556597, acc.: 74.22%] [G loss: 2.799860]\n",
            "4200 [D loss: 0.491107, acc.: 77.34%] [G loss: 3.049309]\n",
            "4220 [D loss: 0.566915, acc.: 67.97%] [G loss: 2.786958]\n",
            "4240 [D loss: 0.511377, acc.: 73.05%] [G loss: 2.807759]\n",
            "4260 [D loss: 0.485993, acc.: 79.30%] [G loss: 3.053281]\n",
            "4280 [D loss: 0.547532, acc.: 71.09%] [G loss: 2.745451]\n",
            "4300 [D loss: 0.531817, acc.: 73.83%] [G loss: 2.778460]\n",
            "4320 [D loss: 0.515080, acc.: 73.83%] [G loss: 3.092749]\n",
            "4340 [D loss: 0.519388, acc.: 74.61%] [G loss: 2.878280]\n",
            "4360 [D loss: 0.563711, acc.: 70.70%] [G loss: 2.571039]\n",
            "4380 [D loss: 0.537275, acc.: 75.78%] [G loss: 2.878899]\n",
            "4400 [D loss: 0.507742, acc.: 75.39%] [G loss: 3.002395]\n",
            "4420 [D loss: 0.452667, acc.: 77.73%] [G loss: 3.158560]\n",
            "4440 [D loss: 0.535009, acc.: 71.48%] [G loss: 2.805390]\n",
            "4460 [D loss: 0.604689, acc.: 66.41%] [G loss: 2.780441]\n",
            "4480 [D loss: 0.596116, acc.: 69.53%] [G loss: 2.719200]\n",
            "4500 [D loss: 0.546368, acc.: 70.70%] [G loss: 2.872003]\n",
            "4520 [D loss: 0.567871, acc.: 69.92%] [G loss: 2.806176]\n",
            "4540 [D loss: 0.490235, acc.: 76.56%] [G loss: 2.952241]\n",
            "4560 [D loss: 0.509287, acc.: 73.83%] [G loss: 3.069952]\n",
            "4580 [D loss: 0.452986, acc.: 81.25%] [G loss: 3.302000]\n",
            "4600 [D loss: 0.552917, acc.: 73.44%] [G loss: 2.939029]\n",
            "4620 [D loss: 0.579855, acc.: 70.31%] [G loss: 2.737604]\n",
            "4640 [D loss: 0.468751, acc.: 80.08%] [G loss: 3.122018]\n",
            "4660 [D loss: 0.534803, acc.: 72.66%] [G loss: 3.157576]\n",
            "4680 [D loss: 0.527186, acc.: 72.27%] [G loss: 2.727690]\n",
            "4700 [D loss: 0.556374, acc.: 72.66%] [G loss: 2.861473]\n",
            "4720 [D loss: 0.538679, acc.: 72.27%] [G loss: 2.799435]\n",
            "4740 [D loss: 0.526546, acc.: 72.66%] [G loss: 2.959459]\n",
            "4760 [D loss: 0.538520, acc.: 74.22%] [G loss: 3.069341]\n",
            "4780 [D loss: 0.540776, acc.: 71.88%] [G loss: 2.884156]\n",
            "4800 [D loss: 0.559989, acc.: 72.27%] [G loss: 2.822819]\n",
            "4820 [D loss: 0.574402, acc.: 73.44%] [G loss: 2.688658]\n",
            "4840 [D loss: 0.503287, acc.: 77.34%] [G loss: 3.008440]\n",
            "4860 [D loss: 0.492400, acc.: 75.00%] [G loss: 3.222373]\n",
            "4880 [D loss: 0.518746, acc.: 76.56%] [G loss: 3.021993]\n",
            "4900 [D loss: 0.511252, acc.: 76.17%] [G loss: 2.894269]\n",
            "4920 [D loss: 0.610966, acc.: 64.45%] [G loss: 2.783937]\n",
            "4940 [D loss: 0.481271, acc.: 75.78%] [G loss: 3.038539]\n",
            "4960 [D loss: 0.494066, acc.: 75.39%] [G loss: 3.070819]\n",
            "4980 [D loss: 0.486364, acc.: 75.78%] [G loss: 3.004296]\n",
            "5000 [D loss: 0.559784, acc.: 73.05%] [G loss: 2.822780]\n",
            "5020 [D loss: 0.545188, acc.: 70.70%] [G loss: 2.875091]\n",
            "5040 [D loss: 0.503103, acc.: 75.39%] [G loss: 2.835628]\n",
            "5060 [D loss: 0.507247, acc.: 75.00%] [G loss: 3.068230]\n",
            "5080 [D loss: 0.504553, acc.: 76.95%] [G loss: 3.098393]\n",
            "5100 [D loss: 0.516885, acc.: 74.61%] [G loss: 2.984023]\n",
            "5120 [D loss: 0.456125, acc.: 80.47%] [G loss: 2.954108]\n",
            "5140 [D loss: 0.465561, acc.: 78.91%] [G loss: 2.880033]\n",
            "5160 [D loss: 0.499876, acc.: 75.78%] [G loss: 2.781754]\n",
            "5180 [D loss: 0.572131, acc.: 67.58%] [G loss: 2.805935]\n",
            "5200 [D loss: 0.594548, acc.: 69.14%] [G loss: 2.871444]\n",
            "5220 [D loss: 0.502526, acc.: 75.39%] [G loss: 2.936902]\n",
            "5240 [D loss: 0.552074, acc.: 73.05%] [G loss: 2.711816]\n",
            "5260 [D loss: 0.546819, acc.: 73.44%] [G loss: 2.879309]\n",
            "5280 [D loss: 0.527160, acc.: 74.22%] [G loss: 2.761153]\n",
            "5300 [D loss: 0.488149, acc.: 77.73%] [G loss: 2.908770]\n",
            "5320 [D loss: 0.447704, acc.: 78.52%] [G loss: 3.372719]\n",
            "5340 [D loss: 0.554570, acc.: 71.88%] [G loss: 2.825867]\n",
            "5360 [D loss: 0.597181, acc.: 68.36%] [G loss: 2.898433]\n",
            "5380 [D loss: 0.505488, acc.: 74.61%] [G loss: 3.117936]\n",
            "5400 [D loss: 0.554350, acc.: 72.27%] [G loss: 2.995115]\n",
            "5420 [D loss: 0.572300, acc.: 72.27%] [G loss: 2.826276]\n",
            "5440 [D loss: 0.534784, acc.: 71.48%] [G loss: 2.692785]\n",
            "5460 [D loss: 0.561173, acc.: 73.44%] [G loss: 2.843717]\n",
            "5480 [D loss: 0.583402, acc.: 69.92%] [G loss: 2.745281]\n",
            "5500 [D loss: 0.518396, acc.: 75.78%] [G loss: 2.909924]\n",
            "5520 [D loss: 0.487866, acc.: 78.52%] [G loss: 2.848609]\n",
            "5540 [D loss: 0.530024, acc.: 75.39%] [G loss: 3.187000]\n",
            "5560 [D loss: 0.521092, acc.: 72.66%] [G loss: 2.872944]\n",
            "5580 [D loss: 0.491134, acc.: 78.12%] [G loss: 3.216303]\n",
            "5600 [D loss: 0.525197, acc.: 73.05%] [G loss: 3.018693]\n",
            "5620 [D loss: 0.511745, acc.: 74.61%] [G loss: 3.091695]\n",
            "5640 [D loss: 0.477680, acc.: 76.56%] [G loss: 3.006930]\n",
            "5660 [D loss: 0.540405, acc.: 72.66%] [G loss: 2.768055]\n",
            "5680 [D loss: 0.516395, acc.: 75.39%] [G loss: 3.115564]\n",
            "5700 [D loss: 0.557063, acc.: 71.48%] [G loss: 2.895863]\n",
            "5720 [D loss: 0.622919, acc.: 63.28%] [G loss: 2.732181]\n",
            "5740 [D loss: 0.521622, acc.: 74.61%] [G loss: 3.003736]\n",
            "5760 [D loss: 0.488357, acc.: 80.08%] [G loss: 3.067684]\n",
            "5780 [D loss: 0.526062, acc.: 73.05%] [G loss: 2.988684]\n",
            "5800 [D loss: 0.540817, acc.: 71.88%] [G loss: 3.129004]\n",
            "5820 [D loss: 0.492165, acc.: 78.52%] [G loss: 2.952757]\n",
            "5840 [D loss: 0.542312, acc.: 74.22%] [G loss: 3.015333]\n",
            "5860 [D loss: 0.569795, acc.: 68.36%] [G loss: 2.896853]\n",
            "5880 [D loss: 0.509601, acc.: 75.78%] [G loss: 3.026724]\n",
            "5900 [D loss: 0.463253, acc.: 80.08%] [G loss: 3.221033]\n",
            "5920 [D loss: 0.533261, acc.: 71.88%] [G loss: 3.047293]\n",
            "5940 [D loss: 0.518429, acc.: 76.17%] [G loss: 3.110289]\n",
            "5960 [D loss: 0.525352, acc.: 75.00%] [G loss: 2.904836]\n",
            "5980 [D loss: 0.500467, acc.: 76.95%] [G loss: 3.104333]\n",
            "6000 [D loss: 0.479717, acc.: 79.69%] [G loss: 3.012603]\n",
            "6020 [D loss: 0.524559, acc.: 76.17%] [G loss: 3.143943]\n",
            "6040 [D loss: 0.450609, acc.: 85.16%] [G loss: 2.947075]\n",
            "6060 [D loss: 0.502416, acc.: 75.00%] [G loss: 3.144388]\n",
            "6080 [D loss: 0.455811, acc.: 80.08%] [G loss: 2.965949]\n",
            "6100 [D loss: 0.542660, acc.: 73.83%] [G loss: 2.866668]\n",
            "6120 [D loss: 0.510522, acc.: 75.39%] [G loss: 3.087668]\n",
            "6140 [D loss: 0.579569, acc.: 69.92%] [G loss: 2.976321]\n",
            "6160 [D loss: 0.519485, acc.: 76.17%] [G loss: 2.987848]\n",
            "6180 [D loss: 0.619709, acc.: 68.75%] [G loss: 3.000710]\n",
            "6200 [D loss: 0.501729, acc.: 77.34%] [G loss: 2.835332]\n",
            "6220 [D loss: 0.552153, acc.: 72.27%] [G loss: 3.114702]\n",
            "6240 [D loss: 0.537996, acc.: 73.05%] [G loss: 2.885450]\n",
            "6260 [D loss: 0.553925, acc.: 74.22%] [G loss: 2.960182]\n",
            "6280 [D loss: 0.548124, acc.: 71.48%] [G loss: 2.832044]\n",
            "6300 [D loss: 0.544106, acc.: 72.66%] [G loss: 2.956853]\n",
            "6320 [D loss: 0.483776, acc.: 76.95%] [G loss: 3.015981]\n",
            "6340 [D loss: 0.560068, acc.: 75.78%] [G loss: 3.006580]\n",
            "6360 [D loss: 0.509835, acc.: 76.17%] [G loss: 3.323501]\n",
            "6380 [D loss: 0.516386, acc.: 73.05%] [G loss: 2.987305]\n",
            "6400 [D loss: 0.501303, acc.: 77.73%] [G loss: 2.934033]\n",
            "6420 [D loss: 0.559768, acc.: 69.92%] [G loss: 2.745706]\n",
            "6440 [D loss: 0.531929, acc.: 72.27%] [G loss: 2.943164]\n",
            "6460 [D loss: 0.479279, acc.: 79.30%] [G loss: 3.323162]\n",
            "6480 [D loss: 0.507126, acc.: 73.83%] [G loss: 3.064314]\n",
            "6500 [D loss: 0.499595, acc.: 76.17%] [G loss: 3.045979]\n",
            "6520 [D loss: 0.466388, acc.: 78.52%] [G loss: 3.199681]\n",
            "6540 [D loss: 0.496783, acc.: 77.34%] [G loss: 3.155903]\n",
            "6560 [D loss: 0.472121, acc.: 77.34%] [G loss: 3.291791]\n",
            "6580 [D loss: 0.506625, acc.: 75.00%] [G loss: 3.076947]\n",
            "6600 [D loss: 0.478609, acc.: 75.39%] [G loss: 3.081228]\n",
            "6620 [D loss: 0.490557, acc.: 79.30%] [G loss: 2.685616]\n",
            "6640 [D loss: 0.530708, acc.: 69.92%] [G loss: 2.885865]\n",
            "6660 [D loss: 0.528026, acc.: 73.44%] [G loss: 2.996357]\n",
            "6680 [D loss: 0.555051, acc.: 73.05%] [G loss: 2.952509]\n",
            "6700 [D loss: 0.532354, acc.: 72.27%] [G loss: 2.943285]\n",
            "6720 [D loss: 0.526324, acc.: 73.83%] [G loss: 2.870784]\n",
            "6740 [D loss: 0.483997, acc.: 75.39%] [G loss: 3.139986]\n",
            "6760 [D loss: 0.499784, acc.: 74.61%] [G loss: 3.191009]\n",
            "6780 [D loss: 0.536175, acc.: 71.88%] [G loss: 3.034775]\n",
            "6800 [D loss: 0.528065, acc.: 75.39%] [G loss: 2.888131]\n",
            "6820 [D loss: 0.484627, acc.: 76.17%] [G loss: 2.925375]\n",
            "6840 [D loss: 0.542891, acc.: 72.27%] [G loss: 2.811393]\n",
            "6860 [D loss: 0.563601, acc.: 66.80%] [G loss: 2.946019]\n",
            "6880 [D loss: 0.555152, acc.: 71.88%] [G loss: 3.031261]\n",
            "6900 [D loss: 0.491337, acc.: 76.95%] [G loss: 3.340362]\n",
            "6920 [D loss: 0.520117, acc.: 72.66%] [G loss: 3.248701]\n",
            "6940 [D loss: 0.556484, acc.: 75.00%] [G loss: 3.149069]\n",
            "6960 [D loss: 0.503583, acc.: 76.17%] [G loss: 3.349983]\n",
            "6980 [D loss: 0.587353, acc.: 68.36%] [G loss: 3.012030]\n",
            "7000 [D loss: 0.505869, acc.: 76.17%] [G loss: 3.222864]\n",
            "7020 [D loss: 0.456907, acc.: 79.69%] [G loss: 2.831516]\n",
            "7040 [D loss: 0.506041, acc.: 78.91%] [G loss: 3.041790]\n",
            "7060 [D loss: 0.485788, acc.: 79.69%] [G loss: 2.950399]\n",
            "7080 [D loss: 0.554809, acc.: 70.70%] [G loss: 2.760335]\n",
            "7100 [D loss: 0.476375, acc.: 80.08%] [G loss: 3.126918]\n",
            "7120 [D loss: 0.576646, acc.: 70.31%] [G loss: 2.936573]\n",
            "7140 [D loss: 0.496966, acc.: 76.17%] [G loss: 3.066146]\n",
            "7160 [D loss: 0.459904, acc.: 80.08%] [G loss: 2.958298]\n",
            "7180 [D loss: 0.509105, acc.: 73.44%] [G loss: 3.357173]\n",
            "7200 [D loss: 0.537121, acc.: 70.31%] [G loss: 2.920805]\n",
            "7220 [D loss: 0.553679, acc.: 73.44%] [G loss: 3.153750]\n",
            "7240 [D loss: 0.515631, acc.: 73.83%] [G loss: 3.099716]\n",
            "7260 [D loss: 0.475237, acc.: 76.95%] [G loss: 3.014589]\n",
            "7280 [D loss: 0.505411, acc.: 76.17%] [G loss: 3.262653]\n",
            "7300 [D loss: 0.492488, acc.: 75.78%] [G loss: 3.302305]\n",
            "7320 [D loss: 0.503574, acc.: 75.39%] [G loss: 2.937421]\n",
            "7340 [D loss: 0.473884, acc.: 77.34%] [G loss: 3.038538]\n",
            "7360 [D loss: 0.522938, acc.: 74.61%] [G loss: 2.986586]\n",
            "7380 [D loss: 0.514637, acc.: 76.56%] [G loss: 3.060476]\n",
            "7400 [D loss: 0.483458, acc.: 79.30%] [G loss: 3.134975]\n",
            "7420 [D loss: 0.445024, acc.: 79.69%] [G loss: 3.475365]\n",
            "7440 [D loss: 0.562365, acc.: 69.14%] [G loss: 3.102570]\n",
            "7460 [D loss: 0.455936, acc.: 80.47%] [G loss: 3.273061]\n",
            "7480 [D loss: 0.501578, acc.: 76.56%] [G loss: 3.462337]\n",
            "7500 [D loss: 0.504172, acc.: 75.78%] [G loss: 3.190497]\n",
            "7520 [D loss: 0.564285, acc.: 73.44%] [G loss: 2.910695]\n",
            "7540 [D loss: 0.438951, acc.: 80.08%] [G loss: 3.540007]\n",
            "7560 [D loss: 0.484248, acc.: 78.52%] [G loss: 3.210286]\n",
            "7580 [D loss: 0.494577, acc.: 73.83%] [G loss: 2.996080]\n",
            "7600 [D loss: 0.559360, acc.: 73.83%] [G loss: 3.000813]\n",
            "7620 [D loss: 0.547191, acc.: 72.66%] [G loss: 3.177866]\n",
            "7640 [D loss: 0.496395, acc.: 79.69%] [G loss: 2.912389]\n",
            "7660 [D loss: 0.463308, acc.: 80.47%] [G loss: 3.384850]\n",
            "7680 [D loss: 0.464346, acc.: 82.03%] [G loss: 3.042361]\n",
            "7700 [D loss: 0.505597, acc.: 75.78%] [G loss: 3.141961]\n",
            "7720 [D loss: 0.535799, acc.: 74.22%] [G loss: 3.054273]\n",
            "7740 [D loss: 0.479301, acc.: 79.30%] [G loss: 3.171278]\n",
            "7760 [D loss: 0.491005, acc.: 78.91%] [G loss: 3.043944]\n",
            "7780 [D loss: 0.498782, acc.: 77.73%] [G loss: 2.907931]\n",
            "7800 [D loss: 0.565184, acc.: 69.92%] [G loss: 2.978195]\n",
            "7820 [D loss: 0.476672, acc.: 80.08%] [G loss: 3.253633]\n",
            "7840 [D loss: 0.552565, acc.: 73.05%] [G loss: 3.099568]\n",
            "7860 [D loss: 0.524036, acc.: 75.00%] [G loss: 3.110007]\n",
            "7880 [D loss: 0.451619, acc.: 79.30%] [G loss: 3.282053]\n",
            "7900 [D loss: 0.454164, acc.: 75.78%] [G loss: 3.312873]\n",
            "7920 [D loss: 0.556969, acc.: 71.48%] [G loss: 2.930376]\n",
            "7940 [D loss: 0.540750, acc.: 73.44%] [G loss: 3.058594]\n",
            "7960 [D loss: 0.507119, acc.: 72.66%] [G loss: 3.095818]\n",
            "7980 [D loss: 0.496132, acc.: 74.61%] [G loss: 3.233676]\n",
            "8000 [D loss: 0.513082, acc.: 75.78%] [G loss: 2.900083]\n",
            "8020 [D loss: 0.513984, acc.: 74.61%] [G loss: 3.170310]\n",
            "8040 [D loss: 0.528824, acc.: 76.95%] [G loss: 3.105980]\n",
            "8060 [D loss: 0.576709, acc.: 73.05%] [G loss: 2.878433]\n",
            "8080 [D loss: 0.465593, acc.: 81.25%] [G loss: 3.307579]\n",
            "8100 [D loss: 0.499247, acc.: 75.39%] [G loss: 3.221481]\n",
            "8120 [D loss: 0.497306, acc.: 76.17%] [G loss: 3.091539]\n",
            "8140 [D loss: 0.493860, acc.: 73.83%] [G loss: 3.162686]\n",
            "8160 [D loss: 0.547580, acc.: 75.39%] [G loss: 3.019076]\n",
            "8180 [D loss: 0.496238, acc.: 78.12%] [G loss: 3.138309]\n",
            "8200 [D loss: 0.571239, acc.: 73.05%] [G loss: 2.986397]\n",
            "8220 [D loss: 0.551888, acc.: 73.05%] [G loss: 2.864061]\n",
            "8240 [D loss: 0.520425, acc.: 73.05%] [G loss: 3.085619]\n",
            "8260 [D loss: 0.453131, acc.: 79.30%] [G loss: 3.045271]\n",
            "8280 [D loss: 0.486911, acc.: 78.91%] [G loss: 3.333857]\n",
            "8300 [D loss: 0.572274, acc.: 69.92%] [G loss: 2.930419]\n",
            "8320 [D loss: 0.539088, acc.: 70.70%] [G loss: 3.084407]\n",
            "8340 [D loss: 0.478152, acc.: 77.73%] [G loss: 3.227556]\n",
            "8360 [D loss: 0.549221, acc.: 73.05%] [G loss: 3.126567]\n",
            "8380 [D loss: 0.480728, acc.: 76.95%] [G loss: 3.136157]\n",
            "8400 [D loss: 0.458163, acc.: 77.34%] [G loss: 3.242359]\n",
            "8420 [D loss: 0.529831, acc.: 72.66%] [G loss: 2.961609]\n",
            "8440 [D loss: 0.502121, acc.: 77.34%] [G loss: 3.242394]\n",
            "8460 [D loss: 0.476627, acc.: 77.34%] [G loss: 3.071231]\n",
            "8480 [D loss: 0.564400, acc.: 69.92%] [G loss: 2.929000]\n",
            "8500 [D loss: 0.522989, acc.: 73.05%] [G loss: 2.929417]\n",
            "8520 [D loss: 0.557191, acc.: 69.92%] [G loss: 2.906082]\n",
            "8540 [D loss: 0.557073, acc.: 71.48%] [G loss: 3.047668]\n",
            "8560 [D loss: 0.539323, acc.: 71.09%] [G loss: 3.305318]\n",
            "8580 [D loss: 7.463012, acc.: 48.83%] [G loss: 7.924753]\n",
            "8600 [D loss: 0.531623, acc.: 77.73%] [G loss: 6.506050]\n",
            "8620 [D loss: 0.866136, acc.: 50.39%] [G loss: 3.394677]\n",
            "8640 [D loss: 0.875239, acc.: 49.61%] [G loss: 2.056005]\n",
            "8660 [D loss: 1.023522, acc.: 31.25%] [G loss: 1.645721]\n",
            "8680 [D loss: 0.845839, acc.: 37.89%] [G loss: 1.661548]\n",
            "8700 [D loss: 0.787014, acc.: 45.31%] [G loss: 1.775961]\n",
            "8720 [D loss: 0.933965, acc.: 29.69%] [G loss: 1.740819]\n",
            "8740 [D loss: 0.789998, acc.: 39.84%] [G loss: 1.640711]\n",
            "8760 [D loss: 0.812729, acc.: 41.80%] [G loss: 1.549518]\n",
            "8780 [D loss: 0.743959, acc.: 44.92%] [G loss: 1.681257]\n",
            "8800 [D loss: 0.748517, acc.: 44.53%] [G loss: 1.661352]\n",
            "8820 [D loss: 0.670547, acc.: 62.11%] [G loss: 1.724538]\n",
            "8840 [D loss: 0.724227, acc.: 51.17%] [G loss: 1.707003]\n",
            "8860 [D loss: 0.718269, acc.: 51.95%] [G loss: 1.719906]\n",
            "8880 [D loss: 0.745709, acc.: 49.22%] [G loss: 1.745088]\n",
            "8900 [D loss: 0.769577, acc.: 46.48%] [G loss: 1.591969]\n",
            "8920 [D loss: 0.722123, acc.: 51.95%] [G loss: 1.610682]\n",
            "8940 [D loss: 0.728030, acc.: 52.73%] [G loss: 1.566979]\n",
            "8960 [D loss: 0.725230, acc.: 56.25%] [G loss: 1.694845]\n",
            "8980 [D loss: 0.740572, acc.: 50.78%] [G loss: 1.517868]\n",
            "9000 [D loss: 0.732160, acc.: 49.22%] [G loss: 1.537990]\n",
            "9020 [D loss: 0.777355, acc.: 42.58%] [G loss: 1.554515]\n",
            "9040 [D loss: 0.736327, acc.: 47.27%] [G loss: 1.497236]\n",
            "9060 [D loss: 0.729674, acc.: 47.66%] [G loss: 1.529603]\n",
            "9080 [D loss: 0.753085, acc.: 45.31%] [G loss: 1.523955]\n",
            "9100 [D loss: 0.724469, acc.: 51.56%] [G loss: 1.517629]\n",
            "9120 [D loss: 0.735310, acc.: 46.09%] [G loss: 1.529362]\n",
            "9140 [D loss: 0.734467, acc.: 51.17%] [G loss: 1.484455]\n",
            "9160 [D loss: 0.720680, acc.: 53.52%] [G loss: 1.523365]\n",
            "9180 [D loss: 0.706359, acc.: 48.83%] [G loss: 1.620357]\n",
            "9200 [D loss: 0.696181, acc.: 54.69%] [G loss: 1.630766]\n",
            "9220 [D loss: 0.687893, acc.: 58.59%] [G loss: 1.529330]\n",
            "9240 [D loss: 0.688574, acc.: 54.30%] [G loss: 1.559754]\n",
            "9260 [D loss: 0.677343, acc.: 58.20%] [G loss: 1.628137]\n",
            "9280 [D loss: 0.665763, acc.: 62.11%] [G loss: 1.593520]\n",
            "9300 [D loss: 0.712381, acc.: 56.25%] [G loss: 1.634953]\n",
            "9320 [D loss: 0.711610, acc.: 56.25%] [G loss: 1.591491]\n",
            "9340 [D loss: 0.686620, acc.: 54.30%] [G loss: 1.736184]\n",
            "9360 [D loss: 0.649630, acc.: 63.28%] [G loss: 1.785256]\n",
            "9380 [D loss: 0.665745, acc.: 63.28%] [G loss: 1.654246]\n",
            "9400 [D loss: 0.656240, acc.: 64.45%] [G loss: 1.786041]\n",
            "9420 [D loss: 0.700487, acc.: 53.12%] [G loss: 1.773618]\n",
            "9440 [D loss: 0.676812, acc.: 60.55%] [G loss: 1.760797]\n",
            "9460 [D loss: 0.674513, acc.: 60.94%] [G loss: 1.803835]\n",
            "9480 [D loss: 0.657873, acc.: 57.81%] [G loss: 1.813506]\n",
            "9500 [D loss: 0.603765, acc.: 69.14%] [G loss: 2.186348]\n",
            "9520 [D loss: 0.667121, acc.: 64.06%] [G loss: 1.970205]\n",
            "9540 [D loss: 0.673946, acc.: 59.38%] [G loss: 2.090092]\n",
            "9560 [D loss: 0.627041, acc.: 67.19%] [G loss: 2.089412]\n",
            "9580 [D loss: 0.641491, acc.: 66.02%] [G loss: 1.888265]\n",
            "9600 [D loss: 0.633271, acc.: 60.94%] [G loss: 1.995549]\n",
            "9620 [D loss: 0.588569, acc.: 70.70%] [G loss: 2.135814]\n",
            "9640 [D loss: 0.693226, acc.: 57.81%] [G loss: 1.991190]\n",
            "9660 [D loss: 0.612383, acc.: 69.14%] [G loss: 2.126613]\n",
            "9680 [D loss: 0.651903, acc.: 62.11%] [G loss: 1.888608]\n",
            "9700 [D loss: 0.651720, acc.: 62.50%] [G loss: 1.938447]\n",
            "9720 [D loss: 0.642885, acc.: 63.28%] [G loss: 2.194831]\n",
            "9740 [D loss: 0.650085, acc.: 62.11%] [G loss: 1.929576]\n",
            "9760 [D loss: 0.630288, acc.: 65.62%] [G loss: 2.162102]\n",
            "9780 [D loss: 0.652139, acc.: 65.23%] [G loss: 1.961429]\n",
            "9800 [D loss: 0.601074, acc.: 66.80%] [G loss: 2.059206]\n",
            "9820 [D loss: 0.625776, acc.: 68.36%] [G loss: 2.229185]\n",
            "9840 [D loss: 0.638485, acc.: 63.28%] [G loss: 2.031832]\n",
            "9860 [D loss: 0.621448, acc.: 63.67%] [G loss: 2.101439]\n",
            "9880 [D loss: 0.613591, acc.: 67.58%] [G loss: 2.187308]\n",
            "9900 [D loss: 0.657941, acc.: 62.89%] [G loss: 1.936535]\n",
            "9920 [D loss: 0.657170, acc.: 63.28%] [G loss: 1.961619]\n",
            "9940 [D loss: 0.640103, acc.: 66.02%] [G loss: 2.047552]\n",
            "9960 [D loss: 0.597306, acc.: 67.19%] [G loss: 2.156033]\n",
            "9980 [D loss: 0.623659, acc.: 67.19%] [G loss: 2.074342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHqD2NLgijuI",
        "outputId": "fc7889c8-1bb9-44a2-fdc5-d1fd4e048e64"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          51712       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      16128       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3968)         0           dropout[0][0]                    \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            3969        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.752733, acc.: 35.55%] [G loss: 1.347328]\n",
            "20 [D loss: 1.432639, acc.: 33.20%] [G loss: 0.876851]\n",
            "40 [D loss: 1.905391, acc.: 38.28%] [G loss: 7.289034]\n",
            "60 [D loss: 0.291875, acc.: 96.88%] [G loss: 4.017936]\n",
            "80 [D loss: 0.766603, acc.: 30.86%] [G loss: 1.662355]\n",
            "100 [D loss: 0.373798, acc.: 78.91%] [G loss: 1.120935]\n",
            "120 [D loss: 0.527545, acc.: 67.58%] [G loss: 1.481705]\n",
            "140 [D loss: 0.395028, acc.: 93.75%] [G loss: 3.413983]\n",
            "160 [D loss: 0.590546, acc.: 69.92%] [G loss: 2.231776]\n",
            "180 [D loss: 0.104672, acc.: 99.61%] [G loss: 13.495666]\n",
            "200 [D loss: 0.678590, acc.: 67.19%] [G loss: 2.007482]\n",
            "220 [D loss: 0.022444, acc.: 100.00%] [G loss: 11.846992]\n",
            "240 [D loss: 0.456370, acc.: 81.25%] [G loss: 6.360778]\n",
            "260 [D loss: 0.045900, acc.: 98.05%] [G loss: 22.423410]\n",
            "280 [D loss: 0.396580, acc.: 90.62%] [G loss: 4.740822]\n",
            "300 [D loss: 0.226776, acc.: 92.97%] [G loss: 6.761682]\n",
            "320 [D loss: 0.387866, acc.: 85.16%] [G loss: 3.361773]\n",
            "340 [D loss: 0.474465, acc.: 80.47%] [G loss: 3.409347]\n",
            "360 [D loss: 0.332621, acc.: 88.28%] [G loss: 5.434200]\n",
            "380 [D loss: 0.234364, acc.: 92.58%] [G loss: 8.933056]\n",
            "400 [D loss: 0.774828, acc.: 58.20%] [G loss: 2.432592]\n",
            "420 [D loss: 0.880601, acc.: 41.41%] [G loss: 1.931600]\n",
            "440 [D loss: 0.664019, acc.: 60.94%] [G loss: 2.534147]\n",
            "460 [D loss: 0.707272, acc.: 60.16%] [G loss: 2.321575]\n",
            "480 [D loss: 0.677418, acc.: 61.72%] [G loss: 2.606836]\n",
            "500 [D loss: 0.510622, acc.: 76.17%] [G loss: 3.629476]\n",
            "520 [D loss: 0.527010, acc.: 72.27%] [G loss: 4.487833]\n",
            "540 [D loss: 0.622264, acc.: 68.36%] [G loss: 3.006218]\n",
            "560 [D loss: 0.555537, acc.: 70.70%] [G loss: 2.991343]\n",
            "580 [D loss: 0.811010, acc.: 54.30%] [G loss: 2.603267]\n",
            "600 [D loss: 0.378522, acc.: 85.16%] [G loss: 4.575687]\n",
            "620 [D loss: 0.561808, acc.: 76.95%] [G loss: 3.367022]\n",
            "640 [D loss: 0.563004, acc.: 69.53%] [G loss: 3.375570]\n",
            "660 [D loss: 0.355555, acc.: 85.55%] [G loss: 4.490025]\n",
            "680 [D loss: 0.403317, acc.: 83.20%] [G loss: 4.237108]\n",
            "700 [D loss: 0.414125, acc.: 78.12%] [G loss: 4.945202]\n",
            "720 [D loss: 0.599174, acc.: 67.58%] [G loss: 3.590851]\n",
            "740 [D loss: 0.563732, acc.: 73.83%] [G loss: 5.750112]\n",
            "760 [D loss: 0.517263, acc.: 73.83%] [G loss: 4.675909]\n",
            "780 [D loss: 0.406712, acc.: 80.08%] [G loss: 4.368893]\n",
            "800 [D loss: 0.525659, acc.: 74.61%] [G loss: 3.262308]\n",
            "820 [D loss: 0.450634, acc.: 79.69%] [G loss: 4.440208]\n",
            "840 [D loss: 0.400064, acc.: 83.20%] [G loss: 4.277420]\n",
            "860 [D loss: 0.417828, acc.: 85.16%] [G loss: 4.017913]\n",
            "880 [D loss: 0.381078, acc.: 82.81%] [G loss: 4.269731]\n",
            "900 [D loss: 0.460135, acc.: 80.08%] [G loss: 4.469801]\n",
            "920 [D loss: 0.348326, acc.: 85.94%] [G loss: 5.068100]\n",
            "940 [D loss: 0.477602, acc.: 77.73%] [G loss: 4.071817]\n",
            "960 [D loss: 0.517298, acc.: 76.95%] [G loss: 4.511352]\n",
            "980 [D loss: 0.418117, acc.: 82.03%] [G loss: 4.504010]\n",
            "1000 [D loss: 0.432066, acc.: 82.03%] [G loss: 4.317963]\n",
            "1020 [D loss: 0.412435, acc.: 81.25%] [G loss: 4.792113]\n",
            "1040 [D loss: 0.440928, acc.: 80.08%] [G loss: 4.426235]\n",
            "1060 [D loss: 0.456902, acc.: 77.34%] [G loss: 4.890867]\n",
            "1080 [D loss: 0.397134, acc.: 81.25%] [G loss: 4.825299]\n",
            "1100 [D loss: 0.440799, acc.: 81.25%] [G loss: 4.538729]\n",
            "1120 [D loss: 0.474141, acc.: 80.08%] [G loss: 4.492686]\n",
            "1140 [D loss: 0.422514, acc.: 79.30%] [G loss: 4.384820]\n",
            "1160 [D loss: 0.527791, acc.: 73.05%] [G loss: 3.540182]\n",
            "1180 [D loss: 0.319411, acc.: 85.94%] [G loss: 4.561137]\n",
            "1200 [D loss: 0.416233, acc.: 82.42%] [G loss: 4.579088]\n",
            "1220 [D loss: 0.586702, acc.: 70.70%] [G loss: 3.623922]\n",
            "1240 [D loss: 0.538317, acc.: 69.92%] [G loss: 3.847762]\n",
            "1260 [D loss: 0.503507, acc.: 75.00%] [G loss: 3.815890]\n",
            "1280 [D loss: 0.472229, acc.: 78.52%] [G loss: 4.036239]\n",
            "1300 [D loss: 0.465290, acc.: 77.73%] [G loss: 4.010860]\n",
            "1320 [D loss: 0.457167, acc.: 77.73%] [G loss: 3.963623]\n",
            "1340 [D loss: 0.523425, acc.: 71.88%] [G loss: 3.653020]\n",
            "1360 [D loss: 0.527661, acc.: 74.22%] [G loss: 3.617958]\n",
            "1380 [D loss: 0.521302, acc.: 71.09%] [G loss: 4.041906]\n",
            "1400 [D loss: 0.395498, acc.: 80.86%] [G loss: 4.076693]\n",
            "1420 [D loss: 0.476434, acc.: 76.95%] [G loss: 3.863803]\n",
            "1440 [D loss: 0.433841, acc.: 78.12%] [G loss: 4.050649]\n",
            "1460 [D loss: 0.466197, acc.: 76.17%] [G loss: 3.571638]\n",
            "1480 [D loss: 0.416067, acc.: 82.81%] [G loss: 4.014705]\n",
            "1500 [D loss: 0.423379, acc.: 84.77%] [G loss: 3.803331]\n",
            "1520 [D loss: 0.429193, acc.: 82.81%] [G loss: 3.923230]\n",
            "1540 [D loss: 0.411502, acc.: 82.42%] [G loss: 3.667601]\n",
            "1560 [D loss: 0.485881, acc.: 76.17%] [G loss: 3.560176]\n",
            "1580 [D loss: 0.572503, acc.: 71.09%] [G loss: 2.398657]\n",
            "1600 [D loss: 0.552653, acc.: 73.44%] [G loss: 2.713140]\n",
            "1620 [D loss: 0.435745, acc.: 82.42%] [G loss: 3.327998]\n",
            "1640 [D loss: 0.483312, acc.: 78.12%] [G loss: 3.305128]\n",
            "1660 [D loss: 0.465168, acc.: 80.08%] [G loss: 3.923415]\n",
            "1680 [D loss: 0.401300, acc.: 83.20%] [G loss: 4.094516]\n",
            "1700 [D loss: 0.457837, acc.: 80.47%] [G loss: 3.319023]\n",
            "1720 [D loss: 0.388066, acc.: 85.94%] [G loss: 4.231543]\n",
            "1740 [D loss: 0.502505, acc.: 77.34%] [G loss: 3.415478]\n",
            "1760 [D loss: 0.403372, acc.: 81.25%] [G loss: 4.262623]\n",
            "1780 [D loss: 0.494603, acc.: 78.12%] [G loss: 3.743781]\n",
            "1800 [D loss: 0.439535, acc.: 79.30%] [G loss: 3.795562]\n",
            "1820 [D loss: 0.443621, acc.: 79.69%] [G loss: 3.556437]\n",
            "1840 [D loss: 0.468717, acc.: 76.56%] [G loss: 4.146793]\n",
            "1860 [D loss: 0.444593, acc.: 80.08%] [G loss: 3.984486]\n",
            "1880 [D loss: 0.398469, acc.: 86.72%] [G loss: 3.312883]\n",
            "1900 [D loss: 0.335382, acc.: 88.67%] [G loss: 3.622212]\n",
            "1920 [D loss: 0.522007, acc.: 73.83%] [G loss: 3.111658]\n",
            "1940 [D loss: 0.428415, acc.: 81.64%] [G loss: 3.836411]\n",
            "1960 [D loss: 0.542945, acc.: 72.27%] [G loss: 3.546131]\n",
            "1980 [D loss: 0.509607, acc.: 75.78%] [G loss: 3.509056]\n",
            "2000 [D loss: 0.496401, acc.: 73.83%] [G loss: 3.750891]\n",
            "2020 [D loss: 0.491846, acc.: 76.17%] [G loss: 3.389423]\n",
            "2040 [D loss: 0.505777, acc.: 73.05%] [G loss: 3.888064]\n",
            "2060 [D loss: 0.417340, acc.: 83.59%] [G loss: 4.111288]\n",
            "2080 [D loss: 0.483331, acc.: 77.73%] [G loss: 3.642764]\n",
            "2100 [D loss: 0.499221, acc.: 76.56%] [G loss: 3.800180]\n",
            "2120 [D loss: 0.518128, acc.: 74.61%] [G loss: 3.324611]\n",
            "2140 [D loss: 0.501039, acc.: 76.95%] [G loss: 3.636889]\n",
            "2160 [D loss: 0.477647, acc.: 76.95%] [G loss: 4.002950]\n",
            "2180 [D loss: 0.414982, acc.: 81.25%] [G loss: 3.843728]\n",
            "2200 [D loss: 0.462794, acc.: 80.47%] [G loss: 3.555526]\n",
            "2220 [D loss: 0.482910, acc.: 76.95%] [G loss: 3.670928]\n",
            "2240 [D loss: 0.427562, acc.: 82.81%] [G loss: 3.727376]\n",
            "2260 [D loss: 0.469285, acc.: 79.69%] [G loss: 3.815276]\n",
            "2280 [D loss: 0.516036, acc.: 76.17%] [G loss: 3.250696]\n",
            "2300 [D loss: 0.414330, acc.: 81.25%] [G loss: 4.024056]\n",
            "2320 [D loss: 0.522879, acc.: 73.83%] [G loss: 3.810185]\n",
            "2340 [D loss: 0.463483, acc.: 76.56%] [G loss: 3.635508]\n",
            "2360 [D loss: 0.442635, acc.: 80.86%] [G loss: 4.086687]\n",
            "2380 [D loss: 0.420921, acc.: 80.86%] [G loss: 4.410660]\n",
            "2400 [D loss: 0.462574, acc.: 78.12%] [G loss: 3.849672]\n",
            "2420 [D loss: 0.472563, acc.: 77.73%] [G loss: 3.434886]\n",
            "2440 [D loss: 0.468518, acc.: 77.73%] [G loss: 3.766785]\n",
            "2460 [D loss: 0.443613, acc.: 79.69%] [G loss: 3.793796]\n",
            "2480 [D loss: 0.462399, acc.: 80.86%] [G loss: 3.584880]\n",
            "2500 [D loss: 0.435429, acc.: 78.91%] [G loss: 3.798426]\n",
            "2520 [D loss: 0.408284, acc.: 81.64%] [G loss: 3.990993]\n",
            "2540 [D loss: 0.508686, acc.: 77.73%] [G loss: 3.695342]\n",
            "2560 [D loss: 0.561084, acc.: 72.27%] [G loss: 3.482042]\n",
            "2580 [D loss: 0.518138, acc.: 75.39%] [G loss: 3.624019]\n",
            "2600 [D loss: 0.483673, acc.: 77.34%] [G loss: 3.649834]\n",
            "2620 [D loss: 0.443012, acc.: 77.73%] [G loss: 3.859149]\n",
            "2640 [D loss: 0.460065, acc.: 79.69%] [G loss: 3.637911]\n",
            "2660 [D loss: 0.497238, acc.: 73.44%] [G loss: 2.719354]\n",
            "2680 [D loss: 0.462974, acc.: 78.91%] [G loss: 3.357059]\n",
            "2700 [D loss: 0.271192, acc.: 91.02%] [G loss: 4.401017]\n",
            "2720 [D loss: 0.436423, acc.: 81.64%] [G loss: 3.832578]\n",
            "2740 [D loss: 0.441849, acc.: 80.86%] [G loss: 3.565859]\n",
            "2760 [D loss: 0.480852, acc.: 78.12%] [G loss: 3.874752]\n",
            "2780 [D loss: 0.369020, acc.: 86.33%] [G loss: 4.196570]\n",
            "2800 [D loss: 0.381279, acc.: 84.77%] [G loss: 3.819724]\n",
            "2820 [D loss: 0.419094, acc.: 77.73%] [G loss: 3.889711]\n",
            "2840 [D loss: 0.325285, acc.: 90.23%] [G loss: 4.154201]\n",
            "2860 [D loss: 0.504829, acc.: 77.73%] [G loss: 4.114516]\n",
            "2880 [D loss: 0.451909, acc.: 78.91%] [G loss: 3.717267]\n",
            "2900 [D loss: 0.528171, acc.: 75.78%] [G loss: 3.446946]\n",
            "2920 [D loss: 0.523827, acc.: 76.17%] [G loss: 3.492047]\n",
            "2940 [D loss: 0.497402, acc.: 78.52%] [G loss: 3.325841]\n",
            "2960 [D loss: 0.453516, acc.: 77.73%] [G loss: 4.008541]\n",
            "2980 [D loss: 0.548233, acc.: 73.05%] [G loss: 3.588136]\n",
            "3000 [D loss: 0.485728, acc.: 76.56%] [G loss: 4.243700]\n",
            "3020 [D loss: 0.482406, acc.: 75.39%] [G loss: 3.708466]\n",
            "3040 [D loss: 0.458034, acc.: 77.73%] [G loss: 3.737943]\n",
            "3060 [D loss: 0.442686, acc.: 81.64%] [G loss: 4.230360]\n",
            "3080 [D loss: 0.485183, acc.: 76.17%] [G loss: 3.877506]\n",
            "3100 [D loss: 0.505282, acc.: 76.56%] [G loss: 3.544936]\n",
            "3120 [D loss: 0.508117, acc.: 75.00%] [G loss: 3.672837]\n",
            "3140 [D loss: 0.424447, acc.: 79.30%] [G loss: 3.897034]\n",
            "3160 [D loss: 0.507482, acc.: 74.61%] [G loss: 3.543082]\n",
            "3180 [D loss: 0.447269, acc.: 79.30%] [G loss: 3.926054]\n",
            "3200 [D loss: 0.367865, acc.: 86.72%] [G loss: 3.632077]\n",
            "3220 [D loss: 0.430217, acc.: 82.42%] [G loss: 3.622224]\n",
            "3240 [D loss: 0.476053, acc.: 75.39%] [G loss: 4.160157]\n",
            "3260 [D loss: 0.625076, acc.: 66.02%] [G loss: 3.474950]\n",
            "3280 [D loss: 0.466416, acc.: 80.08%] [G loss: 3.718380]\n",
            "3300 [D loss: 0.430422, acc.: 82.81%] [G loss: 3.819004]\n",
            "3320 [D loss: 0.477675, acc.: 76.17%] [G loss: 3.961127]\n",
            "3340 [D loss: 0.486184, acc.: 75.00%] [G loss: 3.664076]\n",
            "3360 [D loss: 0.457290, acc.: 80.08%] [G loss: 3.671846]\n",
            "3380 [D loss: 0.459001, acc.: 79.30%] [G loss: 3.687077]\n",
            "3400 [D loss: 0.466464, acc.: 78.52%] [G loss: 3.875594]\n",
            "3420 [D loss: 0.505238, acc.: 73.44%] [G loss: 3.389238]\n",
            "3440 [D loss: 0.514424, acc.: 74.22%] [G loss: 3.823354]\n",
            "3460 [D loss: 0.540655, acc.: 75.39%] [G loss: 3.394325]\n",
            "3480 [D loss: 0.433547, acc.: 83.20%] [G loss: 3.668978]\n",
            "3500 [D loss: 0.523722, acc.: 73.05%] [G loss: 3.355406]\n",
            "3520 [D loss: 0.501649, acc.: 71.88%] [G loss: 3.814058]\n",
            "3540 [D loss: 0.521573, acc.: 75.39%] [G loss: 3.544188]\n",
            "3560 [D loss: 0.423498, acc.: 81.25%] [G loss: 3.994388]\n",
            "3580 [D loss: 0.429447, acc.: 80.08%] [G loss: 3.895706]\n",
            "3600 [D loss: 0.519879, acc.: 75.78%] [G loss: 3.521474]\n",
            "3620 [D loss: 0.495722, acc.: 74.61%] [G loss: 3.718958]\n",
            "3640 [D loss: 0.475264, acc.: 77.73%] [G loss: 3.428703]\n",
            "3660 [D loss: 0.557349, acc.: 74.22%] [G loss: 3.434443]\n",
            "3680 [D loss: 0.472116, acc.: 79.69%] [G loss: 3.339001]\n",
            "3700 [D loss: 0.521186, acc.: 73.05%] [G loss: 3.558533]\n",
            "3720 [D loss: 0.482316, acc.: 78.52%] [G loss: 3.881656]\n",
            "3740 [D loss: 0.454723, acc.: 77.73%] [G loss: 3.756528]\n",
            "3760 [D loss: 0.461270, acc.: 78.91%] [G loss: 3.713428]\n",
            "3780 [D loss: 0.488031, acc.: 74.22%] [G loss: 4.004941]\n",
            "3800 [D loss: 0.461601, acc.: 80.86%] [G loss: 3.645533]\n",
            "3820 [D loss: 0.468682, acc.: 82.81%] [G loss: 3.761867]\n",
            "3840 [D loss: 0.433677, acc.: 79.69%] [G loss: 4.147599]\n",
            "3860 [D loss: 0.478411, acc.: 76.95%] [G loss: 3.568765]\n",
            "3880 [D loss: 0.438513, acc.: 78.12%] [G loss: 3.530684]\n",
            "3900 [D loss: 0.504138, acc.: 77.34%] [G loss: 3.711124]\n",
            "3920 [D loss: 0.472149, acc.: 75.39%] [G loss: 3.886830]\n",
            "3940 [D loss: 0.498310, acc.: 73.05%] [G loss: 4.427661]\n",
            "3960 [D loss: 0.449070, acc.: 80.86%] [G loss: 3.482347]\n",
            "3980 [D loss: 0.516948, acc.: 74.61%] [G loss: 3.863649]\n",
            "4000 [D loss: 0.513865, acc.: 75.00%] [G loss: 3.793974]\n",
            "4020 [D loss: 0.444508, acc.: 81.25%] [G loss: 4.266806]\n",
            "4040 [D loss: 0.497789, acc.: 77.34%] [G loss: 3.614242]\n",
            "4060 [D loss: 0.465443, acc.: 78.12%] [G loss: 4.087373]\n",
            "4080 [D loss: 0.425959, acc.: 79.69%] [G loss: 4.039562]\n",
            "4100 [D loss: 0.515523, acc.: 75.00%] [G loss: 3.252700]\n",
            "4120 [D loss: 0.458136, acc.: 78.52%] [G loss: 3.856789]\n",
            "4140 [D loss: 0.491378, acc.: 76.17%] [G loss: 3.835379]\n",
            "4160 [D loss: 0.501836, acc.: 76.56%] [G loss: 3.608779]\n",
            "4180 [D loss: 0.481745, acc.: 74.22%] [G loss: 3.765723]\n",
            "4200 [D loss: 0.498319, acc.: 76.56%] [G loss: 3.596745]\n",
            "4220 [D loss: 0.526665, acc.: 75.78%] [G loss: 3.518766]\n",
            "4240 [D loss: 0.462830, acc.: 78.12%] [G loss: 3.782343]\n",
            "4260 [D loss: 0.423817, acc.: 81.25%] [G loss: 3.939545]\n",
            "4280 [D loss: 0.496986, acc.: 75.78%] [G loss: 3.523032]\n",
            "4300 [D loss: 0.452005, acc.: 82.42%] [G loss: 3.960016]\n",
            "4320 [D loss: 0.436469, acc.: 79.30%] [G loss: 3.960890]\n",
            "4340 [D loss: 0.443624, acc.: 80.86%] [G loss: 3.788635]\n",
            "4360 [D loss: 0.543310, acc.: 69.14%] [G loss: 3.577947]\n",
            "4380 [D loss: 0.468227, acc.: 76.56%] [G loss: 3.629298]\n",
            "4400 [D loss: 0.189263, acc.: 96.88%] [G loss: 3.443595]\n",
            "4420 [D loss: 0.726029, acc.: 57.42%] [G loss: 3.713356]\n",
            "4440 [D loss: 0.344009, acc.: 88.67%] [G loss: 3.961944]\n",
            "4460 [D loss: 0.446031, acc.: 79.30%] [G loss: 4.137304]\n",
            "4480 [D loss: 0.516616, acc.: 77.73%] [G loss: 3.512239]\n",
            "4500 [D loss: 0.380178, acc.: 86.72%] [G loss: 4.196905]\n",
            "4520 [D loss: 0.422604, acc.: 79.69%] [G loss: 3.788604]\n",
            "4540 [D loss: 0.479274, acc.: 78.52%] [G loss: 3.584588]\n",
            "4560 [D loss: 0.413861, acc.: 85.55%] [G loss: 3.867668]\n",
            "4580 [D loss: 0.494699, acc.: 78.12%] [G loss: 3.694771]\n",
            "4600 [D loss: 0.413823, acc.: 83.20%] [G loss: 4.174074]\n",
            "4620 [D loss: 0.487150, acc.: 79.30%] [G loss: 3.753357]\n",
            "4640 [D loss: 0.440272, acc.: 80.47%] [G loss: 3.671613]\n",
            "4660 [D loss: 0.551132, acc.: 70.70%] [G loss: 3.537297]\n",
            "4680 [D loss: 0.524763, acc.: 71.09%] [G loss: 3.600065]\n",
            "4700 [D loss: 0.579529, acc.: 69.14%] [G loss: 3.392395]\n",
            "4720 [D loss: 0.470977, acc.: 76.56%] [G loss: 4.113709]\n",
            "4740 [D loss: 0.542851, acc.: 72.66%] [G loss: 3.911129]\n",
            "4760 [D loss: 0.482158, acc.: 77.73%] [G loss: 3.596614]\n",
            "4780 [D loss: 0.498418, acc.: 76.17%] [G loss: 4.112700]\n",
            "4800 [D loss: 0.525242, acc.: 72.66%] [G loss: 3.481728]\n",
            "4820 [D loss: 0.591453, acc.: 67.58%] [G loss: 3.363249]\n",
            "4840 [D loss: 0.476060, acc.: 79.30%] [G loss: 3.458408]\n",
            "4860 [D loss: 0.341452, acc.: 87.50%] [G loss: 4.107739]\n",
            "4880 [D loss: 0.488497, acc.: 76.95%] [G loss: 3.397599]\n",
            "4900 [D loss: 0.525077, acc.: 73.83%] [G loss: 3.541090]\n",
            "4920 [D loss: 0.509522, acc.: 73.05%] [G loss: 3.855235]\n",
            "4940 [D loss: 0.433951, acc.: 82.42%] [G loss: 3.696771]\n",
            "4960 [D loss: 0.492799, acc.: 78.91%] [G loss: 3.531725]\n",
            "4980 [D loss: 0.445815, acc.: 77.34%] [G loss: 3.901674]\n",
            "5000 [D loss: 0.429663, acc.: 82.81%] [G loss: 3.514634]\n",
            "5020 [D loss: 0.486306, acc.: 75.39%] [G loss: 3.495831]\n",
            "5040 [D loss: 0.464214, acc.: 78.12%] [G loss: 3.558017]\n",
            "5060 [D loss: 0.486493, acc.: 79.30%] [G loss: 3.919165]\n",
            "5080 [D loss: 0.448246, acc.: 81.64%] [G loss: 3.824019]\n",
            "5100 [D loss: 0.480360, acc.: 78.12%] [G loss: 3.573201]\n",
            "5120 [D loss: 0.381736, acc.: 85.94%] [G loss: 2.993065]\n",
            "5140 [D loss: 0.481824, acc.: 80.08%] [G loss: 3.681541]\n",
            "5160 [D loss: 0.610477, acc.: 68.75%] [G loss: 3.417373]\n",
            "5180 [D loss: 0.588219, acc.: 68.75%] [G loss: 3.502709]\n",
            "5200 [D loss: 0.450574, acc.: 81.25%] [G loss: 3.678672]\n",
            "5220 [D loss: 0.521284, acc.: 74.22%] [G loss: 4.097604]\n",
            "5240 [D loss: 0.518526, acc.: 71.09%] [G loss: 3.862202]\n",
            "5260 [D loss: 0.532455, acc.: 72.27%] [G loss: 3.428562]\n",
            "5280 [D loss: 0.497156, acc.: 77.73%] [G loss: 3.558437]\n",
            "5300 [D loss: 0.488828, acc.: 74.61%] [G loss: 3.663687]\n",
            "5320 [D loss: 0.521151, acc.: 75.39%] [G loss: 3.782874]\n",
            "5340 [D loss: 0.595837, acc.: 67.97%] [G loss: 3.661939]\n",
            "5360 [D loss: 0.508275, acc.: 75.00%] [G loss: 3.515081]\n",
            "5380 [D loss: 0.431305, acc.: 80.86%] [G loss: 3.877716]\n",
            "5400 [D loss: 0.552448, acc.: 72.66%] [G loss: 4.083392]\n",
            "5420 [D loss: 0.513800, acc.: 72.66%] [G loss: 4.039790]\n",
            "5440 [D loss: 0.489092, acc.: 75.39%] [G loss: 3.634744]\n",
            "5460 [D loss: 0.484842, acc.: 76.56%] [G loss: 3.546544]\n",
            "5480 [D loss: 0.518496, acc.: 75.39%] [G loss: 3.768478]\n",
            "5500 [D loss: 0.484885, acc.: 80.08%] [G loss: 3.634381]\n",
            "5520 [D loss: 0.445176, acc.: 80.08%] [G loss: 3.719345]\n",
            "5540 [D loss: 0.431869, acc.: 78.91%] [G loss: 3.862846]\n",
            "5560 [D loss: 0.507169, acc.: 76.56%] [G loss: 3.437469]\n",
            "5580 [D loss: 0.433952, acc.: 80.47%] [G loss: 3.579319]\n",
            "5600 [D loss: 0.506554, acc.: 75.78%] [G loss: 3.589918]\n",
            "5620 [D loss: 0.437325, acc.: 78.91%] [G loss: 3.749315]\n",
            "5640 [D loss: 0.797127, acc.: 46.88%] [G loss: 2.387320]\n",
            "5660 [D loss: 0.676116, acc.: 56.64%] [G loss: 2.042214]\n",
            "5680 [D loss: 0.686576, acc.: 57.42%] [G loss: 1.747363]\n",
            "5700 [D loss: 0.704206, acc.: 55.08%] [G loss: 2.091849]\n",
            "5720 [D loss: 0.593063, acc.: 69.14%] [G loss: 2.391359]\n",
            "5740 [D loss: 0.633026, acc.: 67.97%] [G loss: 2.419225]\n",
            "5760 [D loss: 0.583389, acc.: 71.09%] [G loss: 2.462202]\n",
            "5780 [D loss: 0.561013, acc.: 71.09%] [G loss: 2.589233]\n",
            "5800 [D loss: 0.555863, acc.: 71.09%] [G loss: 2.744467]\n",
            "5820 [D loss: 0.549635, acc.: 73.05%] [G loss: 2.774135]\n",
            "5840 [D loss: 0.516198, acc.: 76.56%] [G loss: 2.785510]\n",
            "5860 [D loss: 0.553369, acc.: 69.92%] [G loss: 2.662983]\n",
            "5880 [D loss: 0.515624, acc.: 77.34%] [G loss: 2.758574]\n",
            "5900 [D loss: 0.571167, acc.: 70.31%] [G loss: 2.525619]\n",
            "5920 [D loss: 0.511597, acc.: 73.44%] [G loss: 3.023622]\n",
            "5940 [D loss: 0.557180, acc.: 73.05%] [G loss: 2.573212]\n",
            "5960 [D loss: 0.483727, acc.: 78.12%] [G loss: 2.901612]\n",
            "5980 [D loss: 0.422573, acc.: 81.64%] [G loss: 2.521949]\n",
            "6000 [D loss: 0.340817, acc.: 88.28%] [G loss: 2.847933]\n",
            "6020 [D loss: 0.574910, acc.: 66.02%] [G loss: 2.684526]\n",
            "6040 [D loss: 0.617198, acc.: 67.19%] [G loss: 2.566542]\n",
            "6060 [D loss: 0.541393, acc.: 73.83%] [G loss: 2.892648]\n",
            "6080 [D loss: 0.570007, acc.: 67.97%] [G loss: 2.897677]\n",
            "6100 [D loss: 0.465712, acc.: 82.42%] [G loss: 2.953053]\n",
            "6120 [D loss: 0.561253, acc.: 69.92%] [G loss: 2.656998]\n",
            "6140 [D loss: 0.525027, acc.: 73.44%] [G loss: 2.924969]\n",
            "6160 [D loss: 0.506165, acc.: 74.61%] [G loss: 3.047369]\n",
            "6180 [D loss: 0.570842, acc.: 70.70%] [G loss: 2.886923]\n",
            "6200 [D loss: 0.523313, acc.: 74.61%] [G loss: 3.095981]\n",
            "6220 [D loss: 0.553023, acc.: 75.39%] [G loss: 2.863895]\n",
            "6240 [D loss: 0.495030, acc.: 76.95%] [G loss: 2.961315]\n",
            "6260 [D loss: 0.494581, acc.: 76.56%] [G loss: 2.938195]\n",
            "6280 [D loss: 0.553581, acc.: 71.09%] [G loss: 2.862978]\n",
            "6300 [D loss: 0.501845, acc.: 73.83%] [G loss: 3.330834]\n",
            "6320 [D loss: 0.471158, acc.: 79.69%] [G loss: 3.226290]\n",
            "6340 [D loss: 0.563320, acc.: 68.75%] [G loss: 2.820280]\n",
            "6360 [D loss: 0.459432, acc.: 80.08%] [G loss: 3.265758]\n",
            "6380 [D loss: 0.488353, acc.: 78.12%] [G loss: 2.908680]\n",
            "6400 [D loss: 0.544586, acc.: 69.53%] [G loss: 3.092734]\n",
            "6420 [D loss: 0.451343, acc.: 81.64%] [G loss: 3.323593]\n",
            "6440 [D loss: 0.486041, acc.: 77.34%] [G loss: 3.041574]\n",
            "6460 [D loss: 0.505222, acc.: 78.52%] [G loss: 3.086638]\n",
            "6480 [D loss: 0.458745, acc.: 81.64%] [G loss: 3.422199]\n",
            "6500 [D loss: 0.523597, acc.: 73.44%] [G loss: 3.183787]\n",
            "6520 [D loss: 0.515286, acc.: 74.61%] [G loss: 3.094653]\n",
            "6540 [D loss: 0.519395, acc.: 76.56%] [G loss: 2.910190]\n",
            "6560 [D loss: 0.498603, acc.: 75.39%] [G loss: 3.272077]\n",
            "6580 [D loss: 0.538010, acc.: 73.05%] [G loss: 2.980170]\n",
            "6600 [D loss: 0.479126, acc.: 78.12%] [G loss: 3.271513]\n",
            "6620 [D loss: 0.448091, acc.: 76.56%] [G loss: 3.267972]\n",
            "6640 [D loss: 0.513892, acc.: 73.83%] [G loss: 3.201874]\n",
            "6660 [D loss: 0.439094, acc.: 78.12%] [G loss: 3.420353]\n",
            "6680 [D loss: 0.457172, acc.: 79.69%] [G loss: 3.141023]\n",
            "6700 [D loss: 0.529202, acc.: 75.78%] [G loss: 3.171482]\n",
            "6720 [D loss: 0.505626, acc.: 74.61%] [G loss: 3.431247]\n",
            "6740 [D loss: 0.520914, acc.: 75.78%] [G loss: 2.870579]\n",
            "6760 [D loss: 0.500046, acc.: 75.78%] [G loss: 2.974747]\n",
            "6780 [D loss: 0.474796, acc.: 79.69%] [G loss: 3.089014]\n",
            "6800 [D loss: 0.462504, acc.: 78.52%] [G loss: 3.438223]\n",
            "6820 [D loss: 0.469575, acc.: 77.34%] [G loss: 3.192730]\n",
            "6840 [D loss: 0.476575, acc.: 78.52%] [G loss: 3.368754]\n",
            "6860 [D loss: 0.425469, acc.: 81.64%] [G loss: 3.352173]\n",
            "6880 [D loss: 0.480706, acc.: 78.52%] [G loss: 3.264705]\n",
            "6900 [D loss: 0.462931, acc.: 76.17%] [G loss: 3.151577]\n",
            "6920 [D loss: 0.493093, acc.: 75.39%] [G loss: 3.526366]\n",
            "6940 [D loss: 0.458337, acc.: 79.30%] [G loss: 3.597233]\n",
            "6960 [D loss: 0.476043, acc.: 76.95%] [G loss: 3.313582]\n",
            "6980 [D loss: 0.451783, acc.: 78.52%] [G loss: 3.524991]\n",
            "7000 [D loss: 0.462093, acc.: 76.56%] [G loss: 3.501738]\n",
            "7020 [D loss: 0.606861, acc.: 64.84%] [G loss: 3.192286]\n",
            "7040 [D loss: 0.536868, acc.: 72.66%] [G loss: 3.101389]\n",
            "7060 [D loss: 0.516061, acc.: 73.05%] [G loss: 3.110178]\n",
            "7080 [D loss: 0.467272, acc.: 81.64%] [G loss: 3.298069]\n",
            "7100 [D loss: 0.499602, acc.: 73.05%] [G loss: 3.235384]\n",
            "7120 [D loss: 0.438271, acc.: 81.64%] [G loss: 3.389003]\n",
            "7140 [D loss: 0.469151, acc.: 77.34%] [G loss: 3.346164]\n",
            "7160 [D loss: 0.461890, acc.: 82.03%] [G loss: 3.338513]\n",
            "7180 [D loss: 0.526689, acc.: 75.00%] [G loss: 3.269421]\n",
            "7200 [D loss: 0.487674, acc.: 78.52%] [G loss: 3.165553]\n",
            "7220 [D loss: 0.527506, acc.: 73.83%] [G loss: 3.138836]\n",
            "7240 [D loss: 0.455436, acc.: 81.64%] [G loss: 3.618567]\n",
            "7260 [D loss: 0.547784, acc.: 71.48%] [G loss: 2.725751]\n",
            "7280 [D loss: 0.436390, acc.: 82.03%] [G loss: 3.221417]\n",
            "7300 [D loss: 0.517818, acc.: 76.17%] [G loss: 3.325288]\n",
            "7320 [D loss: 0.500613, acc.: 76.95%] [G loss: 3.183635]\n",
            "7340 [D loss: 0.464383, acc.: 78.12%] [G loss: 3.566409]\n",
            "7360 [D loss: 0.505470, acc.: 74.22%] [G loss: 3.271100]\n",
            "7380 [D loss: 0.507191, acc.: 75.00%] [G loss: 3.164970]\n",
            "7400 [D loss: 0.434901, acc.: 82.81%] [G loss: 3.374956]\n",
            "7420 [D loss: 0.502883, acc.: 76.56%] [G loss: 3.109754]\n",
            "7440 [D loss: 0.502089, acc.: 72.66%] [G loss: 3.267833]\n",
            "7460 [D loss: 0.511759, acc.: 79.69%] [G loss: 3.326176]\n",
            "7480 [D loss: 0.517053, acc.: 78.91%] [G loss: 3.163948]\n",
            "7500 [D loss: 0.510618, acc.: 76.17%] [G loss: 3.285439]\n",
            "7520 [D loss: 0.494554, acc.: 74.22%] [G loss: 3.424598]\n",
            "7540 [D loss: 0.517024, acc.: 75.39%] [G loss: 3.080083]\n",
            "7560 [D loss: 0.449773, acc.: 80.08%] [G loss: 3.908043]\n",
            "7580 [D loss: 0.474495, acc.: 78.12%] [G loss: 3.264720]\n",
            "7600 [D loss: 0.489545, acc.: 78.52%] [G loss: 3.126090]\n",
            "7620 [D loss: 0.442534, acc.: 78.12%] [G loss: 3.425619]\n",
            "7640 [D loss: 0.437958, acc.: 82.42%] [G loss: 3.571045]\n",
            "7660 [D loss: 0.451416, acc.: 80.86%] [G loss: 3.273043]\n",
            "7680 [D loss: 0.447762, acc.: 80.47%] [G loss: 3.610585]\n",
            "7700 [D loss: 0.476254, acc.: 76.95%] [G loss: 3.345034]\n",
            "7720 [D loss: 0.475249, acc.: 78.91%] [G loss: 3.366272]\n",
            "7740 [D loss: 0.523420, acc.: 75.39%] [G loss: 3.427686]\n",
            "7760 [D loss: 0.487898, acc.: 78.52%] [G loss: 3.296914]\n",
            "7780 [D loss: 0.490000, acc.: 74.61%] [G loss: 3.262123]\n",
            "7800 [D loss: 0.477617, acc.: 77.34%] [G loss: 3.217216]\n",
            "7820 [D loss: 0.539970, acc.: 73.44%] [G loss: 3.370209]\n",
            "7840 [D loss: 0.476623, acc.: 75.00%] [G loss: 3.489903]\n",
            "7860 [D loss: 0.447548, acc.: 76.95%] [G loss: 3.422470]\n",
            "7880 [D loss: 0.491965, acc.: 76.95%] [G loss: 3.638213]\n",
            "7900 [D loss: 0.479406, acc.: 75.39%] [G loss: 3.356878]\n",
            "7920 [D loss: 0.483008, acc.: 77.73%] [G loss: 3.530294]\n",
            "7940 [D loss: 0.465778, acc.: 78.52%] [G loss: 3.205496]\n",
            "7960 [D loss: 0.476025, acc.: 79.69%] [G loss: 3.230944]\n",
            "7980 [D loss: 0.474112, acc.: 76.95%] [G loss: 3.391557]\n",
            "8000 [D loss: 0.475331, acc.: 80.47%] [G loss: 3.379869]\n",
            "8020 [D loss: 0.473286, acc.: 76.17%] [G loss: 3.559654]\n",
            "8040 [D loss: 0.427171, acc.: 83.20%] [G loss: 3.672556]\n",
            "8060 [D loss: 0.432786, acc.: 81.64%] [G loss: 3.533801]\n",
            "8080 [D loss: 0.452831, acc.: 78.91%] [G loss: 3.427401]\n",
            "8100 [D loss: 0.446472, acc.: 80.47%] [G loss: 3.722816]\n",
            "8120 [D loss: 0.418339, acc.: 81.64%] [G loss: 3.397742]\n",
            "8140 [D loss: 0.465500, acc.: 78.91%] [G loss: 3.384021]\n",
            "8160 [D loss: 0.474344, acc.: 79.30%] [G loss: 3.257665]\n",
            "8180 [D loss: 0.536044, acc.: 75.78%] [G loss: 3.421198]\n",
            "8200 [D loss: 0.414319, acc.: 80.47%] [G loss: 3.402724]\n",
            "8220 [D loss: 0.490274, acc.: 76.95%] [G loss: 3.491111]\n",
            "8240 [D loss: 0.457078, acc.: 79.69%] [G loss: 3.479160]\n",
            "8260 [D loss: 0.428324, acc.: 83.98%] [G loss: 3.360795]\n",
            "8280 [D loss: 0.457949, acc.: 78.91%] [G loss: 3.648529]\n",
            "8300 [D loss: 0.438165, acc.: 83.59%] [G loss: 3.497430]\n",
            "8320 [D loss: 0.452450, acc.: 81.64%] [G loss: 3.373085]\n",
            "8340 [D loss: 0.576814, acc.: 72.66%] [G loss: 3.423584]\n",
            "8360 [D loss: 0.498354, acc.: 75.39%] [G loss: 3.581344]\n",
            "8380 [D loss: 0.465671, acc.: 79.69%] [G loss: 3.514375]\n",
            "8400 [D loss: 0.485220, acc.: 75.00%] [G loss: 3.716040]\n",
            "8420 [D loss: 0.487803, acc.: 79.69%] [G loss: 3.531736]\n",
            "8440 [D loss: 0.463505, acc.: 77.73%] [G loss: 3.596407]\n",
            "8460 [D loss: 0.490870, acc.: 76.56%] [G loss: 3.471682]\n",
            "8480 [D loss: 0.452748, acc.: 79.30%] [G loss: 3.568530]\n",
            "8500 [D loss: 0.416017, acc.: 81.25%] [G loss: 3.613595]\n",
            "8520 [D loss: 0.483586, acc.: 79.30%] [G loss: 3.412146]\n",
            "8540 [D loss: 0.406235, acc.: 82.81%] [G loss: 3.460336]\n",
            "8560 [D loss: 0.310068, acc.: 89.06%] [G loss: 3.745050]\n",
            "8580 [D loss: 0.300549, acc.: 89.84%] [G loss: 4.144867]\n",
            "8600 [D loss: 0.504255, acc.: 77.34%] [G loss: 3.349806]\n",
            "8620 [D loss: 0.462886, acc.: 81.64%] [G loss: 3.462966]\n",
            "8640 [D loss: 0.389471, acc.: 85.16%] [G loss: 3.653942]\n",
            "8660 [D loss: 0.464035, acc.: 76.17%] [G loss: 3.703904]\n",
            "8680 [D loss: 0.478426, acc.: 78.91%] [G loss: 3.602670]\n",
            "8700 [D loss: 0.410570, acc.: 80.47%] [G loss: 3.800275]\n",
            "8720 [D loss: 0.509046, acc.: 76.95%] [G loss: 3.356812]\n",
            "8740 [D loss: 0.484623, acc.: 76.95%] [G loss: 4.101873]\n",
            "8760 [D loss: 0.448074, acc.: 82.42%] [G loss: 3.938914]\n",
            "8780 [D loss: 0.463801, acc.: 78.12%] [G loss: 3.623445]\n",
            "8800 [D loss: 0.414872, acc.: 82.81%] [G loss: 3.599596]\n",
            "8820 [D loss: 0.495461, acc.: 75.78%] [G loss: 3.141862]\n",
            "8840 [D loss: 0.443982, acc.: 79.69%] [G loss: 3.572422]\n",
            "8860 [D loss: 0.484311, acc.: 78.52%] [G loss: 3.510788]\n",
            "8880 [D loss: 0.428973, acc.: 80.86%] [G loss: 3.399287]\n",
            "8900 [D loss: 0.477973, acc.: 75.00%] [G loss: 3.554408]\n",
            "8920 [D loss: 0.485355, acc.: 77.34%] [G loss: 3.808118]\n",
            "8940 [D loss: 2.800898, acc.: 14.45%] [G loss: 27.507885]\n",
            "8960 [D loss: 0.773884, acc.: 52.73%] [G loss: 2.015553]\n",
            "8980 [D loss: 0.761576, acc.: 46.09%] [G loss: 1.679048]\n",
            "9000 [D loss: 0.784883, acc.: 46.09%] [G loss: 1.708151]\n",
            "9020 [D loss: 0.653690, acc.: 65.23%] [G loss: 1.940728]\n",
            "9040 [D loss: 0.677919, acc.: 61.33%] [G loss: 2.007184]\n",
            "9060 [D loss: 0.614786, acc.: 66.02%] [G loss: 2.074200]\n",
            "9080 [D loss: 0.633209, acc.: 67.97%] [G loss: 1.844655]\n",
            "9100 [D loss: 0.588704, acc.: 69.53%] [G loss: 2.320553]\n",
            "9120 [D loss: 0.625063, acc.: 67.97%] [G loss: 1.986333]\n",
            "9140 [D loss: 0.594309, acc.: 67.58%] [G loss: 2.370121]\n",
            "9160 [D loss: 0.607363, acc.: 67.19%] [G loss: 2.275659]\n",
            "9180 [D loss: 0.553753, acc.: 73.05%] [G loss: 2.309159]\n",
            "9200 [D loss: 0.595538, acc.: 68.75%] [G loss: 2.372317]\n",
            "9220 [D loss: 0.548973, acc.: 70.31%] [G loss: 2.489437]\n",
            "9240 [D loss: 0.551011, acc.: 71.48%] [G loss: 2.549460]\n",
            "9260 [D loss: 0.573152, acc.: 69.14%] [G loss: 2.469831]\n",
            "9280 [D loss: 0.572627, acc.: 73.44%] [G loss: 2.307875]\n",
            "9300 [D loss: 0.553323, acc.: 70.31%] [G loss: 2.538066]\n",
            "9320 [D loss: 0.570685, acc.: 71.88%] [G loss: 2.416935]\n",
            "9340 [D loss: 0.532202, acc.: 73.44%] [G loss: 2.449286]\n",
            "9360 [D loss: 0.514219, acc.: 76.56%] [G loss: 2.698652]\n",
            "9380 [D loss: 0.505412, acc.: 77.73%] [G loss: 2.620293]\n",
            "9400 [D loss: 0.549418, acc.: 70.70%] [G loss: 2.638669]\n",
            "9420 [D loss: 0.580951, acc.: 69.14%] [G loss: 2.638317]\n",
            "9440 [D loss: 0.507757, acc.: 75.78%] [G loss: 2.658104]\n",
            "9460 [D loss: 0.518321, acc.: 75.78%] [G loss: 2.717395]\n",
            "9480 [D loss: 0.514472, acc.: 73.05%] [G loss: 2.616160]\n",
            "9500 [D loss: 0.570499, acc.: 73.44%] [G loss: 2.535660]\n",
            "9520 [D loss: 0.568959, acc.: 72.66%] [G loss: 2.286757]\n",
            "9540 [D loss: 0.562937, acc.: 72.66%] [G loss: 2.483506]\n",
            "9560 [D loss: 0.528205, acc.: 73.83%] [G loss: 2.657890]\n",
            "9580 [D loss: 0.503143, acc.: 77.34%] [G loss: 2.848196]\n",
            "9600 [D loss: 0.571531, acc.: 69.92%] [G loss: 2.609770]\n",
            "9620 [D loss: 0.530996, acc.: 74.61%] [G loss: 2.832669]\n",
            "9640 [D loss: 0.535909, acc.: 73.83%] [G loss: 2.864364]\n",
            "9660 [D loss: 0.544892, acc.: 71.48%] [G loss: 2.714231]\n",
            "9680 [D loss: 0.555349, acc.: 73.44%] [G loss: 2.755321]\n",
            "9700 [D loss: 0.489567, acc.: 77.34%] [G loss: 3.041969]\n",
            "9720 [D loss: 0.510935, acc.: 75.78%] [G loss: 2.881957]\n",
            "9740 [D loss: 0.516329, acc.: 75.00%] [G loss: 2.817935]\n",
            "9760 [D loss: 0.556828, acc.: 72.27%] [G loss: 2.688699]\n",
            "9780 [D loss: 0.501733, acc.: 76.17%] [G loss: 2.860299]\n",
            "9800 [D loss: 0.520941, acc.: 72.66%] [G loss: 2.662742]\n",
            "9820 [D loss: 0.569970, acc.: 69.53%] [G loss: 2.823090]\n",
            "9840 [D loss: 0.517138, acc.: 74.61%] [G loss: 2.832041]\n",
            "9860 [D loss: 0.534839, acc.: 72.66%] [G loss: 2.933492]\n",
            "9880 [D loss: 0.549654, acc.: 74.61%] [G loss: 2.858721]\n",
            "9900 [D loss: 0.537882, acc.: 71.09%] [G loss: 2.929425]\n",
            "9920 [D loss: 0.511707, acc.: 78.12%] [G loss: 3.019380]\n",
            "9940 [D loss: 0.488366, acc.: 75.78%] [G loss: 3.096195]\n",
            "9960 [D loss: 0.527950, acc.: 75.00%] [G loss: 2.816784]\n",
            "9980 [D loss: 0.561204, acc.: 70.31%] [G loss: 2.766946]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKyOoPom5Zz",
        "outputId": "c8d3af2f-3729-4904-f9a0-9cb8c6d9a8f8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_28 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 48, 48, 128)  3328        input_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_29 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 100)          0           input_29[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_30 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (None, 512)          51712       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 1, 2304)      16128       input_30[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 512)          0           dense_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 1152)         0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_17 (Flatten)            (None, 2304)         0           embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 3968)         0           dropout_1[0][0]                  \n",
            "                                                                 flatten_15[0][0]                 \n",
            "                                                                 flatten_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (None, 1)            3969        concatenate_3[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,714,049\n",
            "Trainable params: 1,714,049\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_3 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DT (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DT (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_37 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_36 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_37 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.775621, acc.: 20.31%] [G loss: 1.527493]\n",
            "20 [D loss: 1.136945, acc.: 31.64%] [G loss: 1.106470]\n",
            "40 [D loss: 0.436015, acc.: 49.61%] [G loss: 17.784233]\n",
            "60 [D loss: 0.537491, acc.: 73.83%] [G loss: 4.473651]\n",
            "80 [D loss: 0.834621, acc.: 22.66%] [G loss: 1.869293]\n",
            "100 [D loss: 0.741704, acc.: 39.45%] [G loss: 1.638891]\n",
            "120 [D loss: 0.575083, acc.: 82.81%] [G loss: 2.042169]\n",
            "140 [D loss: 0.455398, acc.: 78.12%] [G loss: 3.064530]\n",
            "160 [D loss: 1.021226, acc.: 2.73%] [G loss: 1.166114]\n",
            "180 [D loss: 0.558926, acc.: 76.17%] [G loss: 2.472870]\n",
            "200 [D loss: 0.770760, acc.: 41.80%] [G loss: 1.451331]\n",
            "220 [D loss: 0.876846, acc.: 49.22%] [G loss: 3.595408]\n",
            "240 [D loss: 0.186598, acc.: 100.00%] [G loss: 7.332498]\n",
            "260 [D loss: 0.682056, acc.: 68.75%] [G loss: 1.890666]\n",
            "280 [D loss: 0.209264, acc.: 95.31%] [G loss: 10.532845]\n",
            "300 [D loss: 0.691736, acc.: 60.55%] [G loss: 1.672787]\n",
            "320 [D loss: 0.672510, acc.: 71.48%] [G loss: 1.888943]\n",
            "340 [D loss: 0.385103, acc.: 89.06%] [G loss: 4.332753]\n",
            "360 [D loss: 0.380074, acc.: 84.77%] [G loss: 4.375402]\n",
            "380 [D loss: 0.762896, acc.: 57.81%] [G loss: 1.722982]\n",
            "400 [D loss: 0.730825, acc.: 46.09%] [G loss: 2.234101]\n",
            "420 [D loss: 0.644904, acc.: 57.81%] [G loss: 2.380552]\n",
            "440 [D loss: 0.686306, acc.: 59.77%] [G loss: 2.222342]\n",
            "460 [D loss: 0.746045, acc.: 54.69%] [G loss: 1.957719]\n",
            "480 [D loss: 0.601959, acc.: 67.58%] [G loss: 2.125723]\n",
            "500 [D loss: 0.650282, acc.: 62.11%] [G loss: 2.268615]\n",
            "520 [D loss: 0.609233, acc.: 67.58%] [G loss: 2.602067]\n",
            "540 [D loss: 0.572285, acc.: 69.53%] [G loss: 2.928072]\n",
            "560 [D loss: 0.618016, acc.: 66.41%] [G loss: 2.138952]\n",
            "580 [D loss: 0.671801, acc.: 58.20%] [G loss: 1.966664]\n",
            "600 [D loss: 0.712011, acc.: 48.83%] [G loss: 1.836228]\n",
            "620 [D loss: 0.744234, acc.: 47.27%] [G loss: 1.852621]\n",
            "640 [D loss: 0.625981, acc.: 65.23%] [G loss: 2.130571]\n",
            "660 [D loss: 0.605573, acc.: 68.75%] [G loss: 2.238631]\n",
            "680 [D loss: 0.745590, acc.: 51.17%] [G loss: 1.863794]\n",
            "700 [D loss: 0.541753, acc.: 73.05%] [G loss: 2.539672]\n",
            "720 [D loss: 0.546152, acc.: 75.78%] [G loss: 2.414199]\n",
            "740 [D loss: 0.617598, acc.: 65.62%] [G loss: 2.347521]\n",
            "760 [D loss: 0.445802, acc.: 83.98%] [G loss: 3.243727]\n",
            "780 [D loss: 0.495237, acc.: 78.12%] [G loss: 2.732445]\n",
            "800 [D loss: 0.560501, acc.: 75.39%] [G loss: 2.522155]\n",
            "820 [D loss: 0.555277, acc.: 67.97%] [G loss: 2.861588]\n",
            "840 [D loss: 0.536461, acc.: 76.95%] [G loss: 2.788723]\n",
            "860 [D loss: 0.578135, acc.: 70.70%] [G loss: 3.652322]\n",
            "880 [D loss: 0.623166, acc.: 65.23%] [G loss: 2.995444]\n",
            "900 [D loss: 0.660175, acc.: 62.89%] [G loss: 2.713311]\n",
            "920 [D loss: 0.448838, acc.: 81.25%] [G loss: 3.552639]\n",
            "940 [D loss: 0.507613, acc.: 76.56%] [G loss: 3.297152]\n",
            "960 [D loss: 0.538144, acc.: 73.05%] [G loss: 3.129484]\n",
            "980 [D loss: 0.657594, acc.: 61.72%] [G loss: 3.054131]\n",
            "1000 [D loss: 0.435557, acc.: 84.77%] [G loss: 3.602971]\n",
            "1020 [D loss: 0.389269, acc.: 85.16%] [G loss: 4.215716]\n",
            "1040 [D loss: 0.441645, acc.: 80.47%] [G loss: 3.759854]\n",
            "1060 [D loss: 0.417450, acc.: 81.64%] [G loss: 3.896385]\n",
            "1080 [D loss: 0.648562, acc.: 64.84%] [G loss: 3.626264]\n",
            "1100 [D loss: 0.402189, acc.: 83.98%] [G loss: 3.678422]\n",
            "1120 [D loss: 0.418486, acc.: 82.81%] [G loss: 3.632628]\n",
            "1140 [D loss: 0.467013, acc.: 76.95%] [G loss: 3.824034]\n",
            "1160 [D loss: 0.570863, acc.: 70.31%] [G loss: 3.044311]\n",
            "1180 [D loss: 0.511243, acc.: 76.95%] [G loss: 3.353204]\n",
            "1200 [D loss: 0.478253, acc.: 78.12%] [G loss: 3.580424]\n",
            "1220 [D loss: 0.470860, acc.: 77.73%] [G loss: 3.401241]\n",
            "1240 [D loss: 0.509738, acc.: 75.00%] [G loss: 3.294276]\n",
            "1260 [D loss: 0.510494, acc.: 78.91%] [G loss: 3.284930]\n",
            "1280 [D loss: 0.589470, acc.: 67.19%] [G loss: 3.138620]\n",
            "1300 [D loss: 0.555125, acc.: 73.05%] [G loss: 2.883266]\n",
            "1320 [D loss: 0.479775, acc.: 79.30%] [G loss: 3.127534]\n",
            "1340 [D loss: 0.498140, acc.: 75.78%] [G loss: 3.167541]\n",
            "1360 [D loss: 0.530504, acc.: 76.56%] [G loss: 2.911742]\n",
            "1380 [D loss: 0.482003, acc.: 77.34%] [G loss: 3.266805]\n",
            "1400 [D loss: 0.470014, acc.: 80.47%] [G loss: 3.325715]\n",
            "1420 [D loss: 0.546466, acc.: 73.44%] [G loss: 3.093244]\n",
            "1440 [D loss: 0.548517, acc.: 73.83%] [G loss: 3.152225]\n",
            "1460 [D loss: 0.533497, acc.: 71.88%] [G loss: 3.297659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1-QlEvelsO",
        "outputId": "d5e6db9b-1b83-4118-fa9f-1173864d140b"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      16128       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_73 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_33 (Flatten)            (None, 2304)         0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_32 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4708)         0           input_73[0][0]                   \n",
            "                                                                 flatten_33[0][0]                 \n",
            "                                                                 flatten_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_72 (Dense)                (None, 1024)         4822016     concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_48 (LeakyReLU)      (None, 1024)         0           dense_72[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 1024)         0           leaky_re_lu_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_73 (Dense)                (None, 1024)         1049600     dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_49 (LeakyReLU)      (None, 1024)         0           dense_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 1024)         0           leaky_re_lu_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_74 (Dense)                (None, 1024)         1049600     dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_50 (LeakyReLU)      (None, 1024)         0           dense_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 1024)         0           leaky_re_lu_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_75 (Dense)                (None, 1)            1025        dropout_26[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 6,938,369\n",
            "Trainable params: 6,938,369\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_76 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_51 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_52 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_53 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_33 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_50 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.849750, acc.: 30.47%] [G loss: 5.367685]\n",
            "20 [D loss: 0.341891, acc.: 91.41%] [G loss: 8.554846]\n",
            "40 [D loss: 0.710111, acc.: 44.14%] [G loss: 2.083147]\n",
            "60 [D loss: 0.715512, acc.: 46.48%] [G loss: 1.738583]\n",
            "80 [D loss: 0.648917, acc.: 56.25%] [G loss: 1.863081]\n",
            "100 [D loss: 0.701115, acc.: 44.92%] [G loss: 1.767049]\n",
            "120 [D loss: 0.732758, acc.: 40.23%] [G loss: 1.580268]\n",
            "140 [D loss: 0.706723, acc.: 48.05%] [G loss: 1.537512]\n",
            "160 [D loss: 0.717456, acc.: 40.23%] [G loss: 1.437199]\n",
            "180 [D loss: 0.710523, acc.: 42.97%] [G loss: 1.439812]\n",
            "200 [D loss: 0.711228, acc.: 45.70%] [G loss: 1.451416]\n",
            "220 [D loss: 0.721137, acc.: 35.55%] [G loss: 1.414149]\n",
            "240 [D loss: 0.714687, acc.: 42.58%] [G loss: 1.432792]\n",
            "260 [D loss: 0.703062, acc.: 46.09%] [G loss: 1.423483]\n",
            "280 [D loss: 0.724828, acc.: 36.72%] [G loss: 1.404998]\n",
            "300 [D loss: 0.716795, acc.: 42.58%] [G loss: 1.396966]\n",
            "320 [D loss: 0.720376, acc.: 42.97%] [G loss: 1.444355]\n",
            "340 [D loss: 0.706069, acc.: 46.88%] [G loss: 1.423882]\n",
            "360 [D loss: 0.709865, acc.: 39.06%] [G loss: 1.404603]\n",
            "380 [D loss: 0.714929, acc.: 43.75%] [G loss: 1.404289]\n",
            "400 [D loss: 0.711042, acc.: 42.97%] [G loss: 1.414505]\n",
            "420 [D loss: 0.717568, acc.: 40.23%] [G loss: 1.403865]\n",
            "440 [D loss: 0.711510, acc.: 40.23%] [G loss: 1.410104]\n",
            "460 [D loss: 0.714272, acc.: 41.41%] [G loss: 1.419285]\n",
            "480 [D loss: 0.708251, acc.: 41.02%] [G loss: 1.413074]\n",
            "500 [D loss: 0.706564, acc.: 40.62%] [G loss: 1.414759]\n",
            "520 [D loss: 0.711570, acc.: 39.84%] [G loss: 1.416901]\n",
            "540 [D loss: 0.707966, acc.: 44.14%] [G loss: 1.396562]\n",
            "560 [D loss: 0.714514, acc.: 40.62%] [G loss: 1.400588]\n",
            "580 [D loss: 0.708786, acc.: 39.06%] [G loss: 1.401240]\n",
            "600 [D loss: 0.715925, acc.: 37.50%] [G loss: 1.401176]\n",
            "620 [D loss: 0.700743, acc.: 42.19%] [G loss: 1.417309]\n",
            "640 [D loss: 0.706829, acc.: 40.23%] [G loss: 1.403668]\n",
            "660 [D loss: 0.707380, acc.: 39.84%] [G loss: 1.389831]\n",
            "680 [D loss: 0.707225, acc.: 39.84%] [G loss: 1.398783]\n",
            "700 [D loss: 0.703080, acc.: 40.62%] [G loss: 1.396986]\n",
            "720 [D loss: 0.718166, acc.: 34.38%] [G loss: 1.389264]\n",
            "740 [D loss: 0.713579, acc.: 40.23%] [G loss: 1.403226]\n",
            "760 [D loss: 0.705897, acc.: 45.31%] [G loss: 1.437489]\n",
            "780 [D loss: 0.707634, acc.: 40.62%] [G loss: 1.401700]\n",
            "800 [D loss: 0.701959, acc.: 39.06%] [G loss: 1.398977]\n",
            "820 [D loss: 0.706529, acc.: 42.19%] [G loss: 1.400484]\n",
            "840 [D loss: 0.700337, acc.: 41.02%] [G loss: 1.400969]\n",
            "860 [D loss: 0.706927, acc.: 38.67%] [G loss: 1.405506]\n",
            "880 [D loss: 0.708806, acc.: 38.28%] [G loss: 1.387069]\n",
            "900 [D loss: 0.706434, acc.: 41.41%] [G loss: 1.398114]\n",
            "920 [D loss: 0.704907, acc.: 35.94%] [G loss: 1.406425]\n",
            "940 [D loss: 0.696471, acc.: 47.66%] [G loss: 1.392628]\n",
            "960 [D loss: 0.705585, acc.: 41.02%] [G loss: 1.396953]\n",
            "980 [D loss: 0.705313, acc.: 39.84%] [G loss: 1.393413]\n",
            "1000 [D loss: 0.693701, acc.: 48.44%] [G loss: 1.410988]\n",
            "1020 [D loss: 0.707825, acc.: 38.28%] [G loss: 1.393575]\n",
            "1040 [D loss: 0.700687, acc.: 44.53%] [G loss: 1.400779]\n",
            "1060 [D loss: 0.703935, acc.: 39.84%] [G loss: 1.387593]\n",
            "1080 [D loss: 0.706629, acc.: 39.84%] [G loss: 1.389481]\n",
            "1100 [D loss: 0.707315, acc.: 37.89%] [G loss: 1.378371]\n",
            "1120 [D loss: 0.707380, acc.: 42.97%] [G loss: 1.383276]\n",
            "1140 [D loss: 0.701792, acc.: 45.31%] [G loss: 1.392983]\n",
            "1160 [D loss: 0.698018, acc.: 41.41%] [G loss: 1.396971]\n",
            "1180 [D loss: 0.710089, acc.: 35.94%] [G loss: 1.389826]\n",
            "1200 [D loss: 0.704007, acc.: 40.23%] [G loss: 1.392146]\n",
            "1220 [D loss: 0.704774, acc.: 42.97%] [G loss: 1.406959]\n",
            "1240 [D loss: 0.699959, acc.: 41.41%] [G loss: 1.412235]\n",
            "1260 [D loss: 0.702670, acc.: 44.53%] [G loss: 1.408520]\n",
            "1280 [D loss: 0.704078, acc.: 41.41%] [G loss: 1.384988]\n",
            "1300 [D loss: 0.707281, acc.: 40.62%] [G loss: 1.399330]\n",
            "1320 [D loss: 0.709640, acc.: 39.06%] [G loss: 1.400736]\n",
            "1340 [D loss: 0.697176, acc.: 46.88%] [G loss: 1.408559]\n",
            "1360 [D loss: 0.690890, acc.: 46.09%] [G loss: 1.407980]\n",
            "1380 [D loss: 0.695711, acc.: 48.44%] [G loss: 1.384939]\n",
            "1400 [D loss: 0.696284, acc.: 41.41%] [G loss: 1.391898]\n",
            "1420 [D loss: 0.702413, acc.: 41.02%] [G loss: 1.391516]\n",
            "1440 [D loss: 0.702305, acc.: 43.75%] [G loss: 1.404577]\n",
            "1460 [D loss: 0.702467, acc.: 40.62%] [G loss: 1.401769]\n",
            "1480 [D loss: 0.700135, acc.: 39.84%] [G loss: 1.391395]\n",
            "1500 [D loss: 0.701792, acc.: 41.80%] [G loss: 1.390833]\n",
            "1520 [D loss: 0.700585, acc.: 42.19%] [G loss: 1.394488]\n",
            "1540 [D loss: 0.704487, acc.: 41.41%] [G loss: 1.397046]\n",
            "1560 [D loss: 0.695279, acc.: 46.48%] [G loss: 1.408600]\n",
            "1580 [D loss: 0.695438, acc.: 48.83%] [G loss: 1.392757]\n",
            "1600 [D loss: 0.704662, acc.: 38.67%] [G loss: 1.390574]\n",
            "1620 [D loss: 0.699533, acc.: 46.09%] [G loss: 1.401004]\n",
            "1640 [D loss: 0.702735, acc.: 43.75%] [G loss: 1.398700]\n",
            "1660 [D loss: 0.697723, acc.: 43.75%] [G loss: 1.401226]\n",
            "1680 [D loss: 0.697773, acc.: 41.80%] [G loss: 1.410201]\n",
            "1700 [D loss: 0.696773, acc.: 43.36%] [G loss: 1.407255]\n",
            "1720 [D loss: 0.701552, acc.: 39.45%] [G loss: 1.395131]\n",
            "1740 [D loss: 0.696195, acc.: 48.44%] [G loss: 1.396674]\n",
            "1760 [D loss: 0.702511, acc.: 43.36%] [G loss: 1.383245]\n",
            "1780 [D loss: 0.704975, acc.: 40.62%] [G loss: 1.399527]\n",
            "1800 [D loss: 0.698895, acc.: 41.41%] [G loss: 1.391627]\n",
            "1820 [D loss: 0.702525, acc.: 37.50%] [G loss: 1.391825]\n",
            "1840 [D loss: 0.698295, acc.: 46.48%] [G loss: 1.401908]\n",
            "1860 [D loss: 0.699764, acc.: 42.97%] [G loss: 1.399934]\n",
            "1880 [D loss: 0.702305, acc.: 41.80%] [G loss: 1.396282]\n",
            "1900 [D loss: 0.705898, acc.: 42.19%] [G loss: 1.384895]\n",
            "1920 [D loss: 0.700531, acc.: 44.92%] [G loss: 1.391573]\n",
            "1940 [D loss: 0.707957, acc.: 41.02%] [G loss: 1.386891]\n",
            "1960 [D loss: 0.700286, acc.: 41.41%] [G loss: 1.396966]\n",
            "1980 [D loss: 0.697184, acc.: 42.19%] [G loss: 1.398432]\n",
            "2000 [D loss: 0.701151, acc.: 46.09%] [G loss: 1.408123]\n",
            "2020 [D loss: 0.707549, acc.: 41.80%] [G loss: 1.400710]\n",
            "2040 [D loss: 0.706370, acc.: 41.80%] [G loss: 1.382454]\n",
            "2060 [D loss: 0.696820, acc.: 44.14%] [G loss: 1.416820]\n",
            "2080 [D loss: 0.704532, acc.: 39.84%] [G loss: 1.388172]\n",
            "2100 [D loss: 0.698797, acc.: 41.80%] [G loss: 1.401481]\n",
            "2120 [D loss: 0.701321, acc.: 42.58%] [G loss: 1.405037]\n",
            "2140 [D loss: 0.700277, acc.: 42.58%] [G loss: 1.388465]\n",
            "2160 [D loss: 0.706183, acc.: 34.77%] [G loss: 1.395349]\n",
            "2180 [D loss: 0.700379, acc.: 44.53%] [G loss: 1.399946]\n",
            "2200 [D loss: 0.706776, acc.: 35.55%] [G loss: 1.397158]\n",
            "2220 [D loss: 0.690010, acc.: 49.61%] [G loss: 1.415059]\n",
            "2240 [D loss: 0.700388, acc.: 45.31%] [G loss: 1.396531]\n",
            "2260 [D loss: 0.698710, acc.: 41.41%] [G loss: 1.387498]\n",
            "2280 [D loss: 0.699097, acc.: 44.53%] [G loss: 1.400677]\n",
            "2300 [D loss: 0.698183, acc.: 41.41%] [G loss: 1.393741]\n",
            "2320 [D loss: 0.700072, acc.: 44.92%] [G loss: 1.387109]\n",
            "2340 [D loss: 0.702045, acc.: 43.75%] [G loss: 1.396456]\n",
            "2360 [D loss: 0.698545, acc.: 42.97%] [G loss: 1.386749]\n",
            "2380 [D loss: 0.701191, acc.: 44.53%] [G loss: 1.390651]\n",
            "2400 [D loss: 0.702897, acc.: 41.02%] [G loss: 1.385156]\n",
            "2420 [D loss: 0.705316, acc.: 41.02%] [G loss: 1.401502]\n",
            "2440 [D loss: 0.706111, acc.: 37.11%] [G loss: 1.388617]\n",
            "2460 [D loss: 0.702543, acc.: 41.02%] [G loss: 1.407373]\n",
            "2480 [D loss: 0.705135, acc.: 44.92%] [G loss: 1.386979]\n",
            "2500 [D loss: 0.699529, acc.: 43.75%] [G loss: 1.433604]\n",
            "2520 [D loss: 0.701719, acc.: 43.36%] [G loss: 1.395393]\n",
            "2540 [D loss: 0.699508, acc.: 42.58%] [G loss: 1.396002]\n",
            "2560 [D loss: 0.703406, acc.: 38.67%] [G loss: 1.390184]\n",
            "2580 [D loss: 0.702974, acc.: 38.67%] [G loss: 1.384311]\n",
            "2600 [D loss: 0.697291, acc.: 44.92%] [G loss: 1.391137]\n",
            "2620 [D loss: 0.698857, acc.: 46.48%] [G loss: 1.397241]\n",
            "2640 [D loss: 0.702192, acc.: 41.02%] [G loss: 1.391943]\n",
            "2660 [D loss: 0.703083, acc.: 35.94%] [G loss: 1.392532]\n",
            "2680 [D loss: 0.704199, acc.: 41.80%] [G loss: 1.383410]\n",
            "2700 [D loss: 0.705399, acc.: 40.62%] [G loss: 1.386579]\n",
            "2720 [D loss: 0.702500, acc.: 42.19%] [G loss: 1.398633]\n",
            "2740 [D loss: 0.701238, acc.: 41.41%] [G loss: 1.387710]\n",
            "2760 [D loss: 0.695434, acc.: 47.66%] [G loss: 1.398785]\n",
            "2780 [D loss: 0.698381, acc.: 46.48%] [G loss: 1.409770]\n",
            "2800 [D loss: 0.703224, acc.: 41.02%] [G loss: 1.398382]\n",
            "2820 [D loss: 0.704637, acc.: 42.97%] [G loss: 1.391324]\n",
            "2840 [D loss: 0.695273, acc.: 46.48%] [G loss: 1.403218]\n",
            "2860 [D loss: 0.703234, acc.: 45.31%] [G loss: 1.395308]\n",
            "2880 [D loss: 0.699889, acc.: 41.80%] [G loss: 1.389093]\n",
            "2900 [D loss: 0.704404, acc.: 42.19%] [G loss: 1.391987]\n",
            "2920 [D loss: 0.700413, acc.: 43.36%] [G loss: 1.394257]\n",
            "2940 [D loss: 0.701555, acc.: 45.31%] [G loss: 1.386916]\n",
            "2960 [D loss: 0.704574, acc.: 41.02%] [G loss: 1.397935]\n",
            "2980 [D loss: 0.701216, acc.: 38.28%] [G loss: 1.397118]\n",
            "3000 [D loss: 0.703859, acc.: 43.75%] [G loss: 1.386564]\n",
            "3020 [D loss: 0.697224, acc.: 47.27%] [G loss: 1.404107]\n",
            "3040 [D loss: 0.700284, acc.: 44.14%] [G loss: 1.408972]\n",
            "3060 [D loss: 0.710935, acc.: 39.45%] [G loss: 1.398036]\n",
            "3080 [D loss: 0.701705, acc.: 43.75%] [G loss: 1.385907]\n",
            "3100 [D loss: 0.703351, acc.: 42.19%] [G loss: 1.400969]\n",
            "3120 [D loss: 0.705543, acc.: 35.55%] [G loss: 1.395355]\n",
            "3140 [D loss: 0.705566, acc.: 41.02%] [G loss: 1.389833]\n",
            "3160 [D loss: 0.694014, acc.: 50.78%] [G loss: 1.399073]\n",
            "3180 [D loss: 0.696958, acc.: 47.27%] [G loss: 1.398360]\n",
            "3200 [D loss: 0.697163, acc.: 45.31%] [G loss: 1.396299]\n",
            "3220 [D loss: 0.704172, acc.: 39.45%] [G loss: 1.401600]\n",
            "3240 [D loss: 0.706146, acc.: 35.55%] [G loss: 1.386641]\n",
            "3260 [D loss: 0.693955, acc.: 46.88%] [G loss: 1.392742]\n",
            "3280 [D loss: 0.694101, acc.: 48.83%] [G loss: 1.400743]\n",
            "3300 [D loss: 0.699886, acc.: 42.58%] [G loss: 1.380824]\n",
            "3320 [D loss: 0.709387, acc.: 47.27%] [G loss: 1.398697]\n",
            "3340 [D loss: 0.699752, acc.: 48.44%] [G loss: 1.394596]\n",
            "3360 [D loss: 0.706852, acc.: 40.62%] [G loss: 1.405188]\n",
            "3380 [D loss: 0.699275, acc.: 44.53%] [G loss: 1.398795]\n",
            "3400 [D loss: 0.696006, acc.: 44.92%] [G loss: 1.421854]\n",
            "3420 [D loss: 0.696535, acc.: 48.05%] [G loss: 1.411503]\n",
            "3440 [D loss: 0.702331, acc.: 43.36%] [G loss: 1.399069]\n",
            "3460 [D loss: 0.705177, acc.: 42.19%] [G loss: 1.388886]\n",
            "3480 [D loss: 0.703387, acc.: 36.33%] [G loss: 1.393602]\n",
            "3500 [D loss: 0.701317, acc.: 38.67%] [G loss: 1.405229]\n",
            "3520 [D loss: 0.700779, acc.: 36.33%] [G loss: 1.394291]\n",
            "3540 [D loss: 0.704368, acc.: 43.75%] [G loss: 1.388223]\n",
            "3560 [D loss: 0.698320, acc.: 44.92%] [G loss: 1.408260]\n",
            "3580 [D loss: 0.699920, acc.: 41.80%] [G loss: 1.396066]\n",
            "3600 [D loss: 0.703215, acc.: 42.97%] [G loss: 1.393963]\n",
            "3620 [D loss: 0.704095, acc.: 42.58%] [G loss: 1.389001]\n",
            "3640 [D loss: 0.696503, acc.: 46.48%] [G loss: 1.391987]\n",
            "3660 [D loss: 0.706459, acc.: 39.45%] [G loss: 1.403823]\n",
            "3680 [D loss: 0.701097, acc.: 45.70%] [G loss: 1.391359]\n",
            "3700 [D loss: 0.700797, acc.: 40.62%] [G loss: 1.401428]\n",
            "3720 [D loss: 0.706842, acc.: 39.45%] [G loss: 1.401338]\n",
            "3740 [D loss: 0.695790, acc.: 48.44%] [G loss: 1.410640]\n",
            "3760 [D loss: 0.700871, acc.: 41.02%] [G loss: 1.389013]\n",
            "3780 [D loss: 0.698214, acc.: 45.31%] [G loss: 1.391579]\n",
            "3800 [D loss: 0.702306, acc.: 41.02%] [G loss: 1.390742]\n",
            "3820 [D loss: 0.699549, acc.: 44.92%] [G loss: 1.388456]\n",
            "3840 [D loss: 0.726881, acc.: 41.02%] [G loss: 1.427503]\n",
            "3860 [D loss: 0.706836, acc.: 40.23%] [G loss: 1.408166]\n",
            "3880 [D loss: 0.708650, acc.: 37.89%] [G loss: 1.388743]\n",
            "3900 [D loss: 0.703004, acc.: 42.19%] [G loss: 1.413317]\n",
            "3920 [D loss: 0.701894, acc.: 43.75%] [G loss: 1.391886]\n",
            "3940 [D loss: 0.695446, acc.: 49.22%] [G loss: 1.405801]\n",
            "3960 [D loss: 0.704857, acc.: 41.02%] [G loss: 1.401155]\n",
            "3980 [D loss: 0.699231, acc.: 42.58%] [G loss: 1.404975]\n",
            "4000 [D loss: 0.699956, acc.: 39.84%] [G loss: 1.397395]\n",
            "4020 [D loss: 0.693868, acc.: 48.83%] [G loss: 1.414370]\n",
            "4040 [D loss: 0.699592, acc.: 46.48%] [G loss: 1.402936]\n",
            "4060 [D loss: 0.697715, acc.: 45.31%] [G loss: 1.424391]\n",
            "4080 [D loss: 0.708574, acc.: 40.23%] [G loss: 1.381411]\n",
            "4100 [D loss: 1.072704, acc.: 84.38%] [G loss: 26.749468]\n",
            "4120 [D loss: 0.717382, acc.: 67.97%] [G loss: 6.265586]\n",
            "4140 [D loss: 1.001233, acc.: 50.00%] [G loss: 3.090046]\n",
            "4160 [D loss: 0.768635, acc.: 51.56%] [G loss: 2.125261]\n",
            "4180 [D loss: 0.740759, acc.: 46.48%] [G loss: 1.783273]\n",
            "4200 [D loss: 0.722959, acc.: 47.66%] [G loss: 1.520224]\n",
            "4220 [D loss: 0.734547, acc.: 45.31%] [G loss: 1.466105]\n",
            "4240 [D loss: 0.688544, acc.: 57.03%] [G loss: 1.615793]\n",
            "4260 [D loss: 0.738717, acc.: 43.75%] [G loss: 1.405132]\n",
            "4280 [D loss: 0.722575, acc.: 42.19%] [G loss: 1.447261]\n",
            "4300 [D loss: 0.690146, acc.: 49.61%] [G loss: 1.476141]\n",
            "4320 [D loss: 0.719582, acc.: 43.75%] [G loss: 1.429009]\n",
            "4340 [D loss: 0.720578, acc.: 45.70%] [G loss: 1.434604]\n",
            "4360 [D loss: 0.706739, acc.: 45.70%] [G loss: 1.414366]\n",
            "4380 [D loss: 0.702524, acc.: 48.44%] [G loss: 1.432420]\n",
            "4400 [D loss: 0.710964, acc.: 48.83%] [G loss: 1.441298]\n",
            "4420 [D loss: 0.711855, acc.: 46.48%] [G loss: 1.427701]\n",
            "4440 [D loss: 0.710840, acc.: 47.66%] [G loss: 1.458087]\n",
            "4460 [D loss: 0.705373, acc.: 44.53%] [G loss: 1.425750]\n",
            "4480 [D loss: 0.704472, acc.: 48.83%] [G loss: 1.433243]\n",
            "4500 [D loss: 0.698071, acc.: 51.17%] [G loss: 1.431295]\n",
            "4520 [D loss: 0.716092, acc.: 41.80%] [G loss: 1.414632]\n",
            "4540 [D loss: 0.709937, acc.: 44.53%] [G loss: 1.402517]\n",
            "4560 [D loss: 0.706339, acc.: 43.75%] [G loss: 1.417696]\n",
            "4580 [D loss: 0.701099, acc.: 49.61%] [G loss: 1.398619]\n",
            "4600 [D loss: 0.705118, acc.: 43.75%] [G loss: 1.420572]\n",
            "4620 [D loss: 0.706447, acc.: 43.75%] [G loss: 1.411010]\n",
            "4640 [D loss: 0.701780, acc.: 48.44%] [G loss: 1.398860]\n",
            "4660 [D loss: 0.705210, acc.: 48.05%] [G loss: 1.383090]\n",
            "4680 [D loss: 0.708879, acc.: 46.09%] [G loss: 1.397547]\n",
            "4700 [D loss: 0.710725, acc.: 42.97%] [G loss: 1.417038]\n",
            "4720 [D loss: 0.704634, acc.: 45.70%] [G loss: 1.393745]\n",
            "4740 [D loss: 0.696728, acc.: 51.56%] [G loss: 1.395010]\n",
            "4760 [D loss: 0.702551, acc.: 48.83%] [G loss: 1.404335]\n",
            "4780 [D loss: 0.699379, acc.: 46.48%] [G loss: 1.415344]\n",
            "4800 [D loss: 0.700585, acc.: 49.22%] [G loss: 1.406814]\n",
            "4820 [D loss: 0.698388, acc.: 46.88%] [G loss: 1.390556]\n",
            "4840 [D loss: 0.694551, acc.: 51.56%] [G loss: 1.385368]\n",
            "4860 [D loss: 0.694786, acc.: 53.12%] [G loss: 1.396678]\n",
            "4880 [D loss: 0.703839, acc.: 44.53%] [G loss: 1.405728]\n",
            "4900 [D loss: 0.705474, acc.: 44.53%] [G loss: 1.391244]\n",
            "4920 [D loss: 0.698867, acc.: 46.09%] [G loss: 1.391430]\n",
            "4940 [D loss: 0.705874, acc.: 39.45%] [G loss: 1.395215]\n",
            "4960 [D loss: 0.697893, acc.: 49.61%] [G loss: 1.396267]\n",
            "4980 [D loss: 0.709537, acc.: 39.84%] [G loss: 1.394031]\n",
            "5000 [D loss: 0.704992, acc.: 42.19%] [G loss: 1.393940]\n",
            "5020 [D loss: 0.702913, acc.: 48.05%] [G loss: 1.390844]\n",
            "5040 [D loss: 0.697565, acc.: 48.83%] [G loss: 1.388696]\n",
            "5060 [D loss: 0.700234, acc.: 42.58%] [G loss: 1.404058]\n",
            "5080 [D loss: 0.703479, acc.: 44.92%] [G loss: 1.406952]\n",
            "5100 [D loss: 0.693664, acc.: 49.22%] [G loss: 1.407932]\n",
            "5120 [D loss: 0.706131, acc.: 42.97%] [G loss: 1.404334]\n",
            "5140 [D loss: 0.700269, acc.: 48.44%] [G loss: 1.391555]\n",
            "5160 [D loss: 0.696768, acc.: 45.31%] [G loss: 1.396595]\n",
            "5180 [D loss: 0.699592, acc.: 46.09%] [G loss: 1.397417]\n",
            "5200 [D loss: 0.695383, acc.: 47.27%] [G loss: 1.387535]\n",
            "5220 [D loss: 0.693621, acc.: 47.27%] [G loss: 1.408859]\n",
            "5240 [D loss: 0.695037, acc.: 47.66%] [G loss: 1.403044]\n",
            "5260 [D loss: 0.700985, acc.: 41.41%] [G loss: 1.395171]\n",
            "5280 [D loss: 0.695701, acc.: 47.66%] [G loss: 1.403005]\n",
            "5300 [D loss: 0.700270, acc.: 42.19%] [G loss: 1.394384]\n",
            "5320 [D loss: 0.698988, acc.: 44.92%] [G loss: 1.401334]\n",
            "5340 [D loss: 0.694417, acc.: 53.12%] [G loss: 1.388224]\n",
            "5360 [D loss: 0.696847, acc.: 50.78%] [G loss: 1.402042]\n",
            "5380 [D loss: 0.699440, acc.: 46.48%] [G loss: 1.400354]\n",
            "5400 [D loss: 0.695333, acc.: 46.88%] [G loss: 1.389589]\n",
            "5420 [D loss: 0.693278, acc.: 52.34%] [G loss: 1.395435]\n",
            "5440 [D loss: 0.697515, acc.: 52.73%] [G loss: 1.403365]\n",
            "5460 [D loss: 0.700039, acc.: 44.92%] [G loss: 1.385313]\n",
            "5480 [D loss: 0.701342, acc.: 41.80%] [G loss: 1.400771]\n",
            "5500 [D loss: 0.691950, acc.: 52.73%] [G loss: 1.405167]\n",
            "5520 [D loss: 0.697928, acc.: 47.66%] [G loss: 1.407117]\n",
            "5540 [D loss: 0.694162, acc.: 50.39%] [G loss: 1.412841]\n",
            "5560 [D loss: 0.696665, acc.: 49.22%] [G loss: 1.398718]\n",
            "5580 [D loss: 0.700371, acc.: 45.31%] [G loss: 1.393484]\n",
            "5600 [D loss: 0.702304, acc.: 42.19%] [G loss: 1.393507]\n",
            "5620 [D loss: 0.694285, acc.: 50.39%] [G loss: 1.394578]\n",
            "5640 [D loss: 0.697163, acc.: 46.88%] [G loss: 1.395253]\n",
            "5660 [D loss: 0.693129, acc.: 50.78%] [G loss: 1.392592]\n",
            "5680 [D loss: 0.696335, acc.: 45.31%] [G loss: 1.386523]\n",
            "5700 [D loss: 0.697125, acc.: 50.00%] [G loss: 1.412799]\n",
            "5720 [D loss: 0.697893, acc.: 47.27%] [G loss: 1.399107]\n",
            "5740 [D loss: 0.695130, acc.: 49.22%] [G loss: 1.400753]\n",
            "5760 [D loss: 0.698657, acc.: 44.14%] [G loss: 1.395851]\n",
            "5780 [D loss: 0.697089, acc.: 47.66%] [G loss: 1.392132]\n",
            "5800 [D loss: 0.700125, acc.: 46.09%] [G loss: 1.401990]\n",
            "5820 [D loss: 0.697709, acc.: 47.27%] [G loss: 1.394505]\n",
            "5840 [D loss: 0.693451, acc.: 50.39%] [G loss: 1.400309]\n",
            "5860 [D loss: 0.697274, acc.: 45.31%] [G loss: 1.386672]\n",
            "5880 [D loss: 0.700695, acc.: 44.53%] [G loss: 1.398656]\n",
            "5900 [D loss: 0.697150, acc.: 47.27%] [G loss: 1.390164]\n",
            "5920 [D loss: 0.696779, acc.: 44.14%] [G loss: 1.396117]\n",
            "5940 [D loss: 0.693782, acc.: 50.00%] [G loss: 1.390036]\n",
            "5960 [D loss: 0.700256, acc.: 51.17%] [G loss: 1.398869]\n",
            "5980 [D loss: 0.696114, acc.: 49.22%] [G loss: 1.395630]\n",
            "6000 [D loss: 0.697736, acc.: 45.70%] [G loss: 1.393245]\n",
            "6020 [D loss: 0.699556, acc.: 43.75%] [G loss: 1.396108]\n",
            "6040 [D loss: 0.693481, acc.: 50.39%] [G loss: 1.397763]\n",
            "6060 [D loss: 0.703812, acc.: 44.14%] [G loss: 1.388477]\n",
            "6080 [D loss: 0.697451, acc.: 43.36%] [G loss: 1.389992]\n",
            "6100 [D loss: 0.692298, acc.: 46.88%] [G loss: 1.387400]\n",
            "6120 [D loss: 0.699595, acc.: 45.70%] [G loss: 1.405494]\n",
            "6140 [D loss: 0.697563, acc.: 45.70%] [G loss: 1.396657]\n",
            "6160 [D loss: 0.695493, acc.: 48.83%] [G loss: 1.383792]\n",
            "6180 [D loss: 0.697858, acc.: 43.75%] [G loss: 1.381215]\n",
            "6200 [D loss: 0.695829, acc.: 48.83%] [G loss: 1.407797]\n",
            "6220 [D loss: 0.717301, acc.: 37.11%] [G loss: 1.387314]\n",
            "6240 [D loss: 0.697440, acc.: 46.88%] [G loss: 1.404925]\n",
            "6260 [D loss: 0.693353, acc.: 48.05%] [G loss: 1.386393]\n",
            "6280 [D loss: 0.691351, acc.: 53.52%] [G loss: 1.411113]\n",
            "6300 [D loss: 0.697193, acc.: 48.44%] [G loss: 1.393546]\n",
            "6320 [D loss: 0.693712, acc.: 48.05%] [G loss: 1.386686]\n",
            "6340 [D loss: 0.701024, acc.: 43.75%] [G loss: 1.396646]\n",
            "6360 [D loss: 0.701343, acc.: 46.09%] [G loss: 1.392262]\n",
            "6380 [D loss: 0.696558, acc.: 48.05%] [G loss: 1.396049]\n",
            "6400 [D loss: 0.699096, acc.: 45.31%] [G loss: 1.396192]\n",
            "6420 [D loss: 0.695062, acc.: 48.44%] [G loss: 1.393358]\n",
            "6440 [D loss: 0.699274, acc.: 41.80%] [G loss: 1.398998]\n",
            "6460 [D loss: 0.698745, acc.: 42.97%] [G loss: 1.413985]\n",
            "6480 [D loss: 0.696595, acc.: 49.22%] [G loss: 1.407619]\n",
            "6500 [D loss: 0.699104, acc.: 44.53%] [G loss: 1.392327]\n",
            "6520 [D loss: 0.696238, acc.: 47.66%] [G loss: 1.400739]\n",
            "6540 [D loss: 0.694688, acc.: 49.61%] [G loss: 1.409534]\n",
            "6560 [D loss: 0.700760, acc.: 41.80%] [G loss: 1.394047]\n",
            "6580 [D loss: 0.690933, acc.: 51.56%] [G loss: 1.394747]\n",
            "6600 [D loss: 0.698814, acc.: 42.58%] [G loss: 1.389279]\n",
            "6620 [D loss: 0.700194, acc.: 44.53%] [G loss: 1.398422]\n",
            "6640 [D loss: 0.698942, acc.: 47.27%] [G loss: 1.386726]\n",
            "6660 [D loss: 0.700201, acc.: 48.05%] [G loss: 1.389932]\n",
            "6680 [D loss: 0.697296, acc.: 46.88%] [G loss: 1.393817]\n",
            "6700 [D loss: 0.700214, acc.: 42.97%] [G loss: 1.383901]\n",
            "6720 [D loss: 0.697019, acc.: 46.88%] [G loss: 1.392673]\n",
            "6740 [D loss: 0.701964, acc.: 48.05%] [G loss: 1.392654]\n",
            "6760 [D loss: 0.696323, acc.: 52.34%] [G loss: 1.397271]\n",
            "6780 [D loss: 0.699229, acc.: 43.75%] [G loss: 1.387532]\n",
            "6800 [D loss: 0.693560, acc.: 50.78%] [G loss: 1.384406]\n",
            "6820 [D loss: 0.699900, acc.: 46.48%] [G loss: 1.396095]\n",
            "6840 [D loss: 0.700351, acc.: 46.09%] [G loss: 1.400817]\n",
            "6860 [D loss: 0.696565, acc.: 50.39%] [G loss: 1.393807]\n",
            "6880 [D loss: 0.694871, acc.: 49.22%] [G loss: 1.389832]\n",
            "6900 [D loss: 0.700529, acc.: 46.88%] [G loss: 1.386945]\n",
            "6920 [D loss: 0.695679, acc.: 47.27%] [G loss: 1.394230]\n",
            "6940 [D loss: 0.697947, acc.: 43.75%] [G loss: 1.383168]\n",
            "6960 [D loss: 0.697765, acc.: 47.27%] [G loss: 1.401471]\n",
            "6980 [D loss: 0.699596, acc.: 42.97%] [G loss: 1.397855]\n",
            "7000 [D loss: 0.701152, acc.: 42.19%] [G loss: 1.395025]\n",
            "7020 [D loss: 0.697597, acc.: 44.53%] [G loss: 1.392945]\n",
            "7040 [D loss: 0.696670, acc.: 44.14%] [G loss: 1.395268]\n",
            "7060 [D loss: 0.701981, acc.: 41.80%] [G loss: 1.394562]\n",
            "7080 [D loss: 0.698433, acc.: 48.83%] [G loss: 1.398969]\n",
            "7100 [D loss: 0.699508, acc.: 44.92%] [G loss: 1.388082]\n",
            "7120 [D loss: 0.698022, acc.: 46.88%] [G loss: 1.393650]\n",
            "7140 [D loss: 0.694677, acc.: 44.53%] [G loss: 1.384647]\n",
            "7160 [D loss: 0.698689, acc.: 47.27%] [G loss: 1.392577]\n",
            "7180 [D loss: 0.698242, acc.: 42.58%] [G loss: 1.391645]\n",
            "7200 [D loss: 0.700687, acc.: 43.75%] [G loss: 1.396249]\n",
            "7220 [D loss: 0.696790, acc.: 44.14%] [G loss: 1.388211]\n",
            "7240 [D loss: 0.692794, acc.: 46.48%] [G loss: 1.397785]\n",
            "7260 [D loss: 0.698412, acc.: 43.36%] [G loss: 1.383541]\n",
            "7280 [D loss: 0.696848, acc.: 45.70%] [G loss: 1.394438]\n",
            "7300 [D loss: 0.694678, acc.: 45.70%] [G loss: 1.395237]\n",
            "7320 [D loss: 0.693550, acc.: 47.66%] [G loss: 1.395023]\n",
            "7340 [D loss: 0.697784, acc.: 46.09%] [G loss: 1.393231]\n",
            "7360 [D loss: 0.699125, acc.: 43.36%] [G loss: 1.386775]\n",
            "7380 [D loss: 0.698780, acc.: 42.97%] [G loss: 1.399514]\n",
            "7400 [D loss: 0.693878, acc.: 49.61%] [G loss: 1.394288]\n",
            "7420 [D loss: 0.694396, acc.: 50.39%] [G loss: 1.387681]\n",
            "7440 [D loss: 0.699487, acc.: 42.58%] [G loss: 1.391767]\n",
            "7460 [D loss: 0.695520, acc.: 47.27%] [G loss: 1.400790]\n",
            "7480 [D loss: 0.696784, acc.: 43.75%] [G loss: 1.389204]\n",
            "7500 [D loss: 0.705090, acc.: 40.23%] [G loss: 1.392731]\n",
            "7520 [D loss: 0.698871, acc.: 41.02%] [G loss: 1.388768]\n",
            "7540 [D loss: 0.697078, acc.: 46.09%] [G loss: 1.396813]\n",
            "7560 [D loss: 0.696970, acc.: 43.36%] [G loss: 1.395193]\n",
            "7580 [D loss: 0.696497, acc.: 46.09%] [G loss: 1.390957]\n",
            "7600 [D loss: 0.702041, acc.: 41.02%] [G loss: 1.395289]\n",
            "7620 [D loss: 0.697056, acc.: 43.75%] [G loss: 1.393222]\n",
            "7640 [D loss: 0.696136, acc.: 47.27%] [G loss: 1.394576]\n",
            "7660 [D loss: 0.694996, acc.: 47.27%] [G loss: 1.393033]\n",
            "7680 [D loss: 0.695782, acc.: 46.48%] [G loss: 1.385630]\n",
            "7700 [D loss: 0.693979, acc.: 50.39%] [G loss: 1.398277]\n",
            "7720 [D loss: 0.702364, acc.: 45.70%] [G loss: 1.391881]\n",
            "7740 [D loss: 0.699747, acc.: 38.67%] [G loss: 1.395062]\n",
            "7760 [D loss: 0.697299, acc.: 44.53%] [G loss: 1.393300]\n",
            "7780 [D loss: 0.697575, acc.: 44.14%] [G loss: 1.388515]\n",
            "7800 [D loss: 0.696327, acc.: 47.27%] [G loss: 1.399309]\n",
            "7820 [D loss: 0.699796, acc.: 39.45%] [G loss: 1.394260]\n",
            "7840 [D loss: 0.697553, acc.: 44.92%] [G loss: 1.389583]\n",
            "7860 [D loss: 0.698274, acc.: 40.62%] [G loss: 1.385870]\n",
            "7880 [D loss: 0.695591, acc.: 50.39%] [G loss: 1.394403]\n",
            "7900 [D loss: 0.697667, acc.: 42.97%] [G loss: 1.399807]\n",
            "7920 [D loss: 0.699197, acc.: 43.36%] [G loss: 1.389332]\n",
            "7940 [D loss: 0.695461, acc.: 43.75%] [G loss: 1.395153]\n",
            "7960 [D loss: 0.693985, acc.: 50.78%] [G loss: 1.396758]\n",
            "7980 [D loss: 0.694616, acc.: 49.61%] [G loss: 1.394727]\n",
            "8000 [D loss: 0.698273, acc.: 47.66%] [G loss: 1.394009]\n",
            "8020 [D loss: 0.697546, acc.: 46.09%] [G loss: 1.393506]\n",
            "8040 [D loss: 0.696341, acc.: 41.80%] [G loss: 1.394150]\n",
            "8060 [D loss: 0.697245, acc.: 48.44%] [G loss: 1.392660]\n",
            "8080 [D loss: 0.694503, acc.: 49.22%] [G loss: 1.399790]\n",
            "8100 [D loss: 0.699422, acc.: 40.23%] [G loss: 1.391258]\n",
            "8120 [D loss: 0.694864, acc.: 47.66%] [G loss: 1.396652]\n",
            "8140 [D loss: 0.694395, acc.: 45.70%] [G loss: 1.396786]\n",
            "8160 [D loss: 0.696896, acc.: 41.02%] [G loss: 1.390924]\n",
            "8180 [D loss: 0.697071, acc.: 45.31%] [G loss: 1.389972]\n",
            "8200 [D loss: 0.698414, acc.: 41.02%] [G loss: 1.395693]\n",
            "8220 [D loss: 0.696968, acc.: 42.97%] [G loss: 1.391768]\n",
            "8240 [D loss: 0.695341, acc.: 49.61%] [G loss: 1.391918]\n",
            "8260 [D loss: 0.696258, acc.: 42.97%] [G loss: 1.389055]\n",
            "8280 [D loss: 0.694510, acc.: 44.92%] [G loss: 1.391949]\n",
            "8300 [D loss: 0.693706, acc.: 51.17%] [G loss: 1.391831]\n",
            "8320 [D loss: 0.692619, acc.: 50.78%] [G loss: 1.394634]\n",
            "8340 [D loss: 0.692181, acc.: 45.31%] [G loss: 1.398439]\n",
            "8360 [D loss: 0.696840, acc.: 50.00%] [G loss: 1.386354]\n",
            "8380 [D loss: 0.696040, acc.: 45.70%] [G loss: 1.388847]\n",
            "8400 [D loss: 0.697098, acc.: 38.28%] [G loss: 1.385808]\n",
            "8420 [D loss: 0.696205, acc.: 48.44%] [G loss: 1.395315]\n",
            "8440 [D loss: 0.696577, acc.: 44.53%] [G loss: 1.395890]\n",
            "8460 [D loss: 0.698952, acc.: 40.23%] [G loss: 1.392299]\n",
            "8480 [D loss: 0.705718, acc.: 39.84%] [G loss: 1.425954]\n",
            "8500 [D loss: 0.692237, acc.: 47.66%] [G loss: 1.392538]\n",
            "8520 [D loss: 0.694271, acc.: 43.36%] [G loss: 1.400621]\n",
            "8540 [D loss: 0.697283, acc.: 43.36%] [G loss: 1.392912]\n",
            "8560 [D loss: 0.693932, acc.: 46.88%] [G loss: 1.389800]\n",
            "8580 [D loss: 0.698320, acc.: 41.02%] [G loss: 1.390708]\n",
            "8600 [D loss: 0.696863, acc.: 44.53%] [G loss: 1.393254]\n",
            "8620 [D loss: 0.693541, acc.: 48.83%] [G loss: 1.392359]\n",
            "8640 [D loss: 0.698448, acc.: 46.09%] [G loss: 1.390194]\n",
            "8660 [D loss: 0.698225, acc.: 43.36%] [G loss: 1.391702]\n",
            "8680 [D loss: 0.696173, acc.: 43.36%] [G loss: 1.393579]\n",
            "8700 [D loss: 0.698311, acc.: 39.84%] [G loss: 1.393775]\n",
            "8720 [D loss: 0.695746, acc.: 42.97%] [G loss: 1.389348]\n",
            "8740 [D loss: 0.697773, acc.: 45.31%] [G loss: 1.388839]\n",
            "8760 [D loss: 0.698051, acc.: 41.02%] [G loss: 1.387564]\n",
            "8780 [D loss: 0.695869, acc.: 41.80%] [G loss: 1.393906]\n",
            "8800 [D loss: 0.695220, acc.: 43.36%] [G loss: 1.384415]\n",
            "8820 [D loss: 0.694940, acc.: 43.36%] [G loss: 1.385797]\n",
            "8840 [D loss: 0.699507, acc.: 44.53%] [G loss: 1.392140]\n",
            "8860 [D loss: 0.698243, acc.: 41.02%] [G loss: 1.383003]\n",
            "8880 [D loss: 0.694200, acc.: 51.17%] [G loss: 1.389239]\n",
            "8900 [D loss: 0.700332, acc.: 39.45%] [G loss: 1.392415]\n",
            "8920 [D loss: 0.694468, acc.: 43.75%] [G loss: 1.398754]\n",
            "8940 [D loss: 0.693868, acc.: 46.09%] [G loss: 1.401220]\n",
            "8960 [D loss: 0.696201, acc.: 46.09%] [G loss: 1.390292]\n",
            "8980 [D loss: 0.700917, acc.: 37.11%] [G loss: 1.384135]\n",
            "9000 [D loss: 0.529419, acc.: 60.16%] [G loss: 3.451092]\n",
            "9020 [D loss: 0.714005, acc.: 34.77%] [G loss: 1.418368]\n",
            "9040 [D loss: 0.708870, acc.: 40.62%] [G loss: 1.395880]\n",
            "9060 [D loss: 0.705619, acc.: 41.80%] [G loss: 1.390461]\n",
            "9080 [D loss: 0.698528, acc.: 45.31%] [G loss: 1.404152]\n",
            "9100 [D loss: 0.699169, acc.: 39.06%] [G loss: 1.402751]\n",
            "9120 [D loss: 0.696621, acc.: 41.41%] [G loss: 1.406112]\n",
            "9140 [D loss: 0.698882, acc.: 44.92%] [G loss: 1.396840]\n",
            "9160 [D loss: 0.696500, acc.: 44.53%] [G loss: 1.399978]\n",
            "9180 [D loss: 0.694397, acc.: 50.39%] [G loss: 1.403586]\n",
            "9200 [D loss: 0.694435, acc.: 49.22%] [G loss: 1.399590]\n",
            "9220 [D loss: 0.696431, acc.: 43.36%] [G loss: 1.405650]\n",
            "9240 [D loss: 0.695898, acc.: 47.27%] [G loss: 1.392597]\n",
            "9260 [D loss: 0.698319, acc.: 42.58%] [G loss: 1.398521]\n",
            "9280 [D loss: 0.700153, acc.: 39.06%] [G loss: 1.388895]\n",
            "9300 [D loss: 0.693785, acc.: 47.27%] [G loss: 1.391394]\n",
            "9320 [D loss: 0.691352, acc.: 50.78%] [G loss: 1.394004]\n",
            "9340 [D loss: 0.694901, acc.: 43.36%] [G loss: 1.396414]\n",
            "9360 [D loss: 0.696790, acc.: 42.58%] [G loss: 1.395674]\n",
            "9380 [D loss: 0.692653, acc.: 48.83%] [G loss: 1.399105]\n",
            "9400 [D loss: 0.697056, acc.: 44.14%] [G loss: 1.392526]\n",
            "9420 [D loss: 0.699672, acc.: 42.97%] [G loss: 1.388630]\n",
            "9440 [D loss: 0.696520, acc.: 47.66%] [G loss: 1.388295]\n",
            "9460 [D loss: 0.692791, acc.: 49.22%] [G loss: 1.395600]\n",
            "9480 [D loss: 0.692895, acc.: 45.70%] [G loss: 1.389935]\n",
            "9500 [D loss: 0.697345, acc.: 44.92%] [G loss: 1.405686]\n",
            "9520 [D loss: 0.695038, acc.: 48.83%] [G loss: 1.398619]\n",
            "9540 [D loss: 0.693577, acc.: 49.22%] [G loss: 1.392874]\n",
            "9560 [D loss: 0.696086, acc.: 44.53%] [G loss: 1.395670]\n",
            "9580 [D loss: 0.694003, acc.: 47.66%] [G loss: 1.405685]\n",
            "9600 [D loss: 0.691519, acc.: 48.44%] [G loss: 1.388005]\n",
            "9620 [D loss: 0.701189, acc.: 41.41%] [G loss: 1.402854]\n",
            "9640 [D loss: 0.692627, acc.: 50.39%] [G loss: 1.407890]\n",
            "9660 [D loss: 0.699078, acc.: 45.31%] [G loss: 1.403761]\n",
            "9680 [D loss: 0.692119, acc.: 46.48%] [G loss: 1.422443]\n",
            "9700 [D loss: 0.694870, acc.: 47.27%] [G loss: 1.402087]\n",
            "9720 [D loss: 0.694653, acc.: 47.66%] [G loss: 1.401800]\n",
            "9740 [D loss: 0.691276, acc.: 53.52%] [G loss: 1.412094]\n",
            "9760 [D loss: 0.697540, acc.: 41.41%] [G loss: 1.401138]\n",
            "9780 [D loss: 0.697595, acc.: 46.09%] [G loss: 1.409874]\n",
            "9800 [D loss: 0.693005, acc.: 46.48%] [G loss: 1.400883]\n",
            "9820 [D loss: 0.692952, acc.: 54.30%] [G loss: 1.406455]\n",
            "9840 [D loss: 0.691773, acc.: 48.05%] [G loss: 1.401276]\n",
            "9860 [D loss: 0.691053, acc.: 51.17%] [G loss: 1.397547]\n",
            "9880 [D loss: 0.687381, acc.: 53.12%] [G loss: 1.407923]\n",
            "9900 [D loss: 0.692056, acc.: 47.27%] [G loss: 1.400826]\n",
            "9920 [D loss: 0.694512, acc.: 43.75%] [G loss: 1.410615]\n",
            "9940 [D loss: 0.693986, acc.: 45.70%] [G loss: 1.404719]\n",
            "9960 [D loss: 0.691524, acc.: 52.73%] [G loss: 1.402085]\n",
            "9980 [D loss: 0.695208, acc.: 48.05%] [G loss: 1.405380]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
