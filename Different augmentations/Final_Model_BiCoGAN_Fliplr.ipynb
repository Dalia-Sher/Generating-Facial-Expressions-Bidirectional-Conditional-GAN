{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_Model_BiCoGAN_Fliplr.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS516rLy1LYQ",
        "outputId": "22145318-60dd-46b2-f4d9-1c4b17d9f908"
      },
      "source": [
        "!unzip fer2013.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fer2013.zip\n",
            "  inflating: fer2013.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jF8QfAYPg_71",
        "outputId": "be2a0584-c17a-4ed0-b97f-65ee91335f82"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "6b601e7d-4294-4373-be21-1b561d00aff3"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5xEsrnIB0t"
      },
      "source": [
        "dic = {0:'Angry', 1:'disgust' , 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRPqlqU1IG94"
      },
      "source": [
        "The emotion disgust has too few samples, therefore we won't use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9swxs5P0w",
        "outputId": "4dd180e9-a7ef-43c8-80a9-c78e332b89f9"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSAij3uINfw"
      },
      "source": [
        "We will redefine the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "40bb3697-ed65-4f32-cb5c-d265eb0974cf"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOXTev833cv",
        "outputId": "52023c3a-401f-4e58-b9a8-08c6ad094475"
      },
      "source": [
        "epochs = X_train.shape[0]\n",
        "print(\"number of epochs:\", epochs)\n",
        "\n",
        "X_train_aug = X_train\n",
        "y_train_aug = y_train\n",
        "\n",
        "for k in range(epochs):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  contrast = iaa.Fliplr(p=1.0)\n",
        "  contrast_image = contrast.augment_image(img)\n",
        "  contrasted_images.append(contrast_image)\n",
        "\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train_aug = np.concatenate((X_train_aug, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]\n",
        "  y_train_aug = np.concatenate((y_train_aug, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration:\" , k,\", train shape:\", X_train_aug.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epochs: 35340\n",
            "iteration: 0 , train shape: (35341, 48, 48, 1)\n",
            "iteration: 100 , train shape: (35441, 48, 48, 1)\n",
            "iteration: 200 , train shape: (35541, 48, 48, 1)\n",
            "iteration: 300 , train shape: (35641, 48, 48, 1)\n",
            "iteration: 400 , train shape: (35741, 48, 48, 1)\n",
            "iteration: 500 , train shape: (35841, 48, 48, 1)\n",
            "iteration: 600 , train shape: (35941, 48, 48, 1)\n",
            "iteration: 700 , train shape: (36041, 48, 48, 1)\n",
            "iteration: 800 , train shape: (36141, 48, 48, 1)\n",
            "iteration: 900 , train shape: (36241, 48, 48, 1)\n",
            "iteration: 1000 , train shape: (36341, 48, 48, 1)\n",
            "iteration: 1100 , train shape: (36441, 48, 48, 1)\n",
            "iteration: 1200 , train shape: (36541, 48, 48, 1)\n",
            "iteration: 1300 , train shape: (36641, 48, 48, 1)\n",
            "iteration: 1400 , train shape: (36741, 48, 48, 1)\n",
            "iteration: 1500 , train shape: (36841, 48, 48, 1)\n",
            "iteration: 1600 , train shape: (36941, 48, 48, 1)\n",
            "iteration: 1700 , train shape: (37041, 48, 48, 1)\n",
            "iteration: 1800 , train shape: (37141, 48, 48, 1)\n",
            "iteration: 1900 , train shape: (37241, 48, 48, 1)\n",
            "iteration: 2000 , train shape: (37341, 48, 48, 1)\n",
            "iteration: 2100 , train shape: (37441, 48, 48, 1)\n",
            "iteration: 2200 , train shape: (37541, 48, 48, 1)\n",
            "iteration: 2300 , train shape: (37641, 48, 48, 1)\n",
            "iteration: 2400 , train shape: (37741, 48, 48, 1)\n",
            "iteration: 2500 , train shape: (37841, 48, 48, 1)\n",
            "iteration: 2600 , train shape: (37941, 48, 48, 1)\n",
            "iteration: 2700 , train shape: (38041, 48, 48, 1)\n",
            "iteration: 2800 , train shape: (38141, 48, 48, 1)\n",
            "iteration: 2900 , train shape: (38241, 48, 48, 1)\n",
            "iteration: 3000 , train shape: (38341, 48, 48, 1)\n",
            "iteration: 3100 , train shape: (38441, 48, 48, 1)\n",
            "iteration: 3200 , train shape: (38541, 48, 48, 1)\n",
            "iteration: 3300 , train shape: (38641, 48, 48, 1)\n",
            "iteration: 3400 , train shape: (38741, 48, 48, 1)\n",
            "iteration: 3500 , train shape: (38841, 48, 48, 1)\n",
            "iteration: 3600 , train shape: (38941, 48, 48, 1)\n",
            "iteration: 3700 , train shape: (39041, 48, 48, 1)\n",
            "iteration: 3800 , train shape: (39141, 48, 48, 1)\n",
            "iteration: 3900 , train shape: (39241, 48, 48, 1)\n",
            "iteration: 4000 , train shape: (39341, 48, 48, 1)\n",
            "iteration: 4100 , train shape: (39441, 48, 48, 1)\n",
            "iteration: 4200 , train shape: (39541, 48, 48, 1)\n",
            "iteration: 4300 , train shape: (39641, 48, 48, 1)\n",
            "iteration: 4400 , train shape: (39741, 48, 48, 1)\n",
            "iteration: 4500 , train shape: (39841, 48, 48, 1)\n",
            "iteration: 4600 , train shape: (39941, 48, 48, 1)\n",
            "iteration: 4700 , train shape: (40041, 48, 48, 1)\n",
            "iteration: 4800 , train shape: (40141, 48, 48, 1)\n",
            "iteration: 4900 , train shape: (40241, 48, 48, 1)\n",
            "iteration: 5000 , train shape: (40341, 48, 48, 1)\n",
            "iteration: 5100 , train shape: (40441, 48, 48, 1)\n",
            "iteration: 5200 , train shape: (40541, 48, 48, 1)\n",
            "iteration: 5300 , train shape: (40641, 48, 48, 1)\n",
            "iteration: 5400 , train shape: (40741, 48, 48, 1)\n",
            "iteration: 5500 , train shape: (40841, 48, 48, 1)\n",
            "iteration: 5600 , train shape: (40941, 48, 48, 1)\n",
            "iteration: 5700 , train shape: (41041, 48, 48, 1)\n",
            "iteration: 5800 , train shape: (41141, 48, 48, 1)\n",
            "iteration: 5900 , train shape: (41241, 48, 48, 1)\n",
            "iteration: 6000 , train shape: (41341, 48, 48, 1)\n",
            "iteration: 6100 , train shape: (41441, 48, 48, 1)\n",
            "iteration: 6200 , train shape: (41541, 48, 48, 1)\n",
            "iteration: 6300 , train shape: (41641, 48, 48, 1)\n",
            "iteration: 6400 , train shape: (41741, 48, 48, 1)\n",
            "iteration: 6500 , train shape: (41841, 48, 48, 1)\n",
            "iteration: 6600 , train shape: (41941, 48, 48, 1)\n",
            "iteration: 6700 , train shape: (42041, 48, 48, 1)\n",
            "iteration: 6800 , train shape: (42141, 48, 48, 1)\n",
            "iteration: 6900 , train shape: (42241, 48, 48, 1)\n",
            "iteration: 7000 , train shape: (42341, 48, 48, 1)\n",
            "iteration: 7100 , train shape: (42441, 48, 48, 1)\n",
            "iteration: 7200 , train shape: (42541, 48, 48, 1)\n",
            "iteration: 7300 , train shape: (42641, 48, 48, 1)\n",
            "iteration: 7400 , train shape: (42741, 48, 48, 1)\n",
            "iteration: 7500 , train shape: (42841, 48, 48, 1)\n",
            "iteration: 7600 , train shape: (42941, 48, 48, 1)\n",
            "iteration: 7700 , train shape: (43041, 48, 48, 1)\n",
            "iteration: 7800 , train shape: (43141, 48, 48, 1)\n",
            "iteration: 7900 , train shape: (43241, 48, 48, 1)\n",
            "iteration: 8000 , train shape: (43341, 48, 48, 1)\n",
            "iteration: 8100 , train shape: (43441, 48, 48, 1)\n",
            "iteration: 8200 , train shape: (43541, 48, 48, 1)\n",
            "iteration: 8300 , train shape: (43641, 48, 48, 1)\n",
            "iteration: 8400 , train shape: (43741, 48, 48, 1)\n",
            "iteration: 8500 , train shape: (43841, 48, 48, 1)\n",
            "iteration: 8600 , train shape: (43941, 48, 48, 1)\n",
            "iteration: 8700 , train shape: (44041, 48, 48, 1)\n",
            "iteration: 8800 , train shape: (44141, 48, 48, 1)\n",
            "iteration: 8900 , train shape: (44241, 48, 48, 1)\n",
            "iteration: 9000 , train shape: (44341, 48, 48, 1)\n",
            "iteration: 9100 , train shape: (44441, 48, 48, 1)\n",
            "iteration: 9200 , train shape: (44541, 48, 48, 1)\n",
            "iteration: 9300 , train shape: (44641, 48, 48, 1)\n",
            "iteration: 9400 , train shape: (44741, 48, 48, 1)\n",
            "iteration: 9500 , train shape: (44841, 48, 48, 1)\n",
            "iteration: 9600 , train shape: (44941, 48, 48, 1)\n",
            "iteration: 9700 , train shape: (45041, 48, 48, 1)\n",
            "iteration: 9800 , train shape: (45141, 48, 48, 1)\n",
            "iteration: 9900 , train shape: (45241, 48, 48, 1)\n",
            "iteration: 10000 , train shape: (45341, 48, 48, 1)\n",
            "iteration: 10100 , train shape: (45441, 48, 48, 1)\n",
            "iteration: 10200 , train shape: (45541, 48, 48, 1)\n",
            "iteration: 10300 , train shape: (45641, 48, 48, 1)\n",
            "iteration: 10400 , train shape: (45741, 48, 48, 1)\n",
            "iteration: 10500 , train shape: (45841, 48, 48, 1)\n",
            "iteration: 10600 , train shape: (45941, 48, 48, 1)\n",
            "iteration: 10700 , train shape: (46041, 48, 48, 1)\n",
            "iteration: 10800 , train shape: (46141, 48, 48, 1)\n",
            "iteration: 10900 , train shape: (46241, 48, 48, 1)\n",
            "iteration: 11000 , train shape: (46341, 48, 48, 1)\n",
            "iteration: 11100 , train shape: (46441, 48, 48, 1)\n",
            "iteration: 11200 , train shape: (46541, 48, 48, 1)\n",
            "iteration: 11300 , train shape: (46641, 48, 48, 1)\n",
            "iteration: 11400 , train shape: (46741, 48, 48, 1)\n",
            "iteration: 11500 , train shape: (46841, 48, 48, 1)\n",
            "iteration: 11600 , train shape: (46941, 48, 48, 1)\n",
            "iteration: 11700 , train shape: (47041, 48, 48, 1)\n",
            "iteration: 11800 , train shape: (47141, 48, 48, 1)\n",
            "iteration: 11900 , train shape: (47241, 48, 48, 1)\n",
            "iteration: 12000 , train shape: (47341, 48, 48, 1)\n",
            "iteration: 12100 , train shape: (47441, 48, 48, 1)\n",
            "iteration: 12200 , train shape: (47541, 48, 48, 1)\n",
            "iteration: 12300 , train shape: (47641, 48, 48, 1)\n",
            "iteration: 12400 , train shape: (47741, 48, 48, 1)\n",
            "iteration: 12500 , train shape: (47841, 48, 48, 1)\n",
            "iteration: 12600 , train shape: (47941, 48, 48, 1)\n",
            "iteration: 12700 , train shape: (48041, 48, 48, 1)\n",
            "iteration: 12800 , train shape: (48141, 48, 48, 1)\n",
            "iteration: 12900 , train shape: (48241, 48, 48, 1)\n",
            "iteration: 13000 , train shape: (48341, 48, 48, 1)\n",
            "iteration: 13100 , train shape: (48441, 48, 48, 1)\n",
            "iteration: 13200 , train shape: (48541, 48, 48, 1)\n",
            "iteration: 13300 , train shape: (48641, 48, 48, 1)\n",
            "iteration: 13400 , train shape: (48741, 48, 48, 1)\n",
            "iteration: 13500 , train shape: (48841, 48, 48, 1)\n",
            "iteration: 13600 , train shape: (48941, 48, 48, 1)\n",
            "iteration: 13700 , train shape: (49041, 48, 48, 1)\n",
            "iteration: 13800 , train shape: (49141, 48, 48, 1)\n",
            "iteration: 13900 , train shape: (49241, 48, 48, 1)\n",
            "iteration: 14000 , train shape: (49341, 48, 48, 1)\n",
            "iteration: 14100 , train shape: (49441, 48, 48, 1)\n",
            "iteration: 14200 , train shape: (49541, 48, 48, 1)\n",
            "iteration: 14300 , train shape: (49641, 48, 48, 1)\n",
            "iteration: 14400 , train shape: (49741, 48, 48, 1)\n",
            "iteration: 14500 , train shape: (49841, 48, 48, 1)\n",
            "iteration: 14600 , train shape: (49941, 48, 48, 1)\n",
            "iteration: 14700 , train shape: (50041, 48, 48, 1)\n",
            "iteration: 14800 , train shape: (50141, 48, 48, 1)\n",
            "iteration: 14900 , train shape: (50241, 48, 48, 1)\n",
            "iteration: 15000 , train shape: (50341, 48, 48, 1)\n",
            "iteration: 15100 , train shape: (50441, 48, 48, 1)\n",
            "iteration: 15200 , train shape: (50541, 48, 48, 1)\n",
            "iteration: 15300 , train shape: (50641, 48, 48, 1)\n",
            "iteration: 15400 , train shape: (50741, 48, 48, 1)\n",
            "iteration: 15500 , train shape: (50841, 48, 48, 1)\n",
            "iteration: 15600 , train shape: (50941, 48, 48, 1)\n",
            "iteration: 15700 , train shape: (51041, 48, 48, 1)\n",
            "iteration: 15800 , train shape: (51141, 48, 48, 1)\n",
            "iteration: 15900 , train shape: (51241, 48, 48, 1)\n",
            "iteration: 16000 , train shape: (51341, 48, 48, 1)\n",
            "iteration: 16100 , train shape: (51441, 48, 48, 1)\n",
            "iteration: 16200 , train shape: (51541, 48, 48, 1)\n",
            "iteration: 16300 , train shape: (51641, 48, 48, 1)\n",
            "iteration: 16400 , train shape: (51741, 48, 48, 1)\n",
            "iteration: 16500 , train shape: (51841, 48, 48, 1)\n",
            "iteration: 16600 , train shape: (51941, 48, 48, 1)\n",
            "iteration: 16700 , train shape: (52041, 48, 48, 1)\n",
            "iteration: 16800 , train shape: (52141, 48, 48, 1)\n",
            "iteration: 16900 , train shape: (52241, 48, 48, 1)\n",
            "iteration: 17000 , train shape: (52341, 48, 48, 1)\n",
            "iteration: 17100 , train shape: (52441, 48, 48, 1)\n",
            "iteration: 17200 , train shape: (52541, 48, 48, 1)\n",
            "iteration: 17300 , train shape: (52641, 48, 48, 1)\n",
            "iteration: 17400 , train shape: (52741, 48, 48, 1)\n",
            "iteration: 17500 , train shape: (52841, 48, 48, 1)\n",
            "iteration: 17600 , train shape: (52941, 48, 48, 1)\n",
            "iteration: 17700 , train shape: (53041, 48, 48, 1)\n",
            "iteration: 17800 , train shape: (53141, 48, 48, 1)\n",
            "iteration: 17900 , train shape: (53241, 48, 48, 1)\n",
            "iteration: 18000 , train shape: (53341, 48, 48, 1)\n",
            "iteration: 18100 , train shape: (53441, 48, 48, 1)\n",
            "iteration: 18200 , train shape: (53541, 48, 48, 1)\n",
            "iteration: 18300 , train shape: (53641, 48, 48, 1)\n",
            "iteration: 18400 , train shape: (53741, 48, 48, 1)\n",
            "iteration: 18500 , train shape: (53841, 48, 48, 1)\n",
            "iteration: 18600 , train shape: (53941, 48, 48, 1)\n",
            "iteration: 18700 , train shape: (54041, 48, 48, 1)\n",
            "iteration: 18800 , train shape: (54141, 48, 48, 1)\n",
            "iteration: 18900 , train shape: (54241, 48, 48, 1)\n",
            "iteration: 19000 , train shape: (54341, 48, 48, 1)\n",
            "iteration: 19100 , train shape: (54441, 48, 48, 1)\n",
            "iteration: 19200 , train shape: (54541, 48, 48, 1)\n",
            "iteration: 19300 , train shape: (54641, 48, 48, 1)\n",
            "iteration: 19400 , train shape: (54741, 48, 48, 1)\n",
            "iteration: 19500 , train shape: (54841, 48, 48, 1)\n",
            "iteration: 19600 , train shape: (54941, 48, 48, 1)\n",
            "iteration: 19700 , train shape: (55041, 48, 48, 1)\n",
            "iteration: 19800 , train shape: (55141, 48, 48, 1)\n",
            "iteration: 19900 , train shape: (55241, 48, 48, 1)\n",
            "iteration: 20000 , train shape: (55341, 48, 48, 1)\n",
            "iteration: 20100 , train shape: (55441, 48, 48, 1)\n",
            "iteration: 20200 , train shape: (55541, 48, 48, 1)\n",
            "iteration: 20300 , train shape: (55641, 48, 48, 1)\n",
            "iteration: 20400 , train shape: (55741, 48, 48, 1)\n",
            "iteration: 20500 , train shape: (55841, 48, 48, 1)\n",
            "iteration: 20600 , train shape: (55941, 48, 48, 1)\n",
            "iteration: 20700 , train shape: (56041, 48, 48, 1)\n",
            "iteration: 20800 , train shape: (56141, 48, 48, 1)\n",
            "iteration: 20900 , train shape: (56241, 48, 48, 1)\n",
            "iteration: 21000 , train shape: (56341, 48, 48, 1)\n",
            "iteration: 21100 , train shape: (56441, 48, 48, 1)\n",
            "iteration: 21200 , train shape: (56541, 48, 48, 1)\n",
            "iteration: 21300 , train shape: (56641, 48, 48, 1)\n",
            "iteration: 21400 , train shape: (56741, 48, 48, 1)\n",
            "iteration: 21500 , train shape: (56841, 48, 48, 1)\n",
            "iteration: 21600 , train shape: (56941, 48, 48, 1)\n",
            "iteration: 21700 , train shape: (57041, 48, 48, 1)\n",
            "iteration: 21800 , train shape: (57141, 48, 48, 1)\n",
            "iteration: 21900 , train shape: (57241, 48, 48, 1)\n",
            "iteration: 22000 , train shape: (57341, 48, 48, 1)\n",
            "iteration: 22100 , train shape: (57441, 48, 48, 1)\n",
            "iteration: 22200 , train shape: (57541, 48, 48, 1)\n",
            "iteration: 22300 , train shape: (57641, 48, 48, 1)\n",
            "iteration: 22400 , train shape: (57741, 48, 48, 1)\n",
            "iteration: 22500 , train shape: (57841, 48, 48, 1)\n",
            "iteration: 22600 , train shape: (57941, 48, 48, 1)\n",
            "iteration: 22700 , train shape: (58041, 48, 48, 1)\n",
            "iteration: 22800 , train shape: (58141, 48, 48, 1)\n",
            "iteration: 22900 , train shape: (58241, 48, 48, 1)\n",
            "iteration: 23000 , train shape: (58341, 48, 48, 1)\n",
            "iteration: 23100 , train shape: (58441, 48, 48, 1)\n",
            "iteration: 23200 , train shape: (58541, 48, 48, 1)\n",
            "iteration: 23300 , train shape: (58641, 48, 48, 1)\n",
            "iteration: 23400 , train shape: (58741, 48, 48, 1)\n",
            "iteration: 23500 , train shape: (58841, 48, 48, 1)\n",
            "iteration: 23600 , train shape: (58941, 48, 48, 1)\n",
            "iteration: 23700 , train shape: (59041, 48, 48, 1)\n",
            "iteration: 23800 , train shape: (59141, 48, 48, 1)\n",
            "iteration: 23900 , train shape: (59241, 48, 48, 1)\n",
            "iteration: 24000 , train shape: (59341, 48, 48, 1)\n",
            "iteration: 24100 , train shape: (59441, 48, 48, 1)\n",
            "iteration: 24200 , train shape: (59541, 48, 48, 1)\n",
            "iteration: 24300 , train shape: (59641, 48, 48, 1)\n",
            "iteration: 24400 , train shape: (59741, 48, 48, 1)\n",
            "iteration: 24500 , train shape: (59841, 48, 48, 1)\n",
            "iteration: 24600 , train shape: (59941, 48, 48, 1)\n",
            "iteration: 24700 , train shape: (60041, 48, 48, 1)\n",
            "iteration: 24800 , train shape: (60141, 48, 48, 1)\n",
            "iteration: 24900 , train shape: (60241, 48, 48, 1)\n",
            "iteration: 25000 , train shape: (60341, 48, 48, 1)\n",
            "iteration: 25100 , train shape: (60441, 48, 48, 1)\n",
            "iteration: 25200 , train shape: (60541, 48, 48, 1)\n",
            "iteration: 25300 , train shape: (60641, 48, 48, 1)\n",
            "iteration: 25400 , train shape: (60741, 48, 48, 1)\n",
            "iteration: 25500 , train shape: (60841, 48, 48, 1)\n",
            "iteration: 25600 , train shape: (60941, 48, 48, 1)\n",
            "iteration: 25700 , train shape: (61041, 48, 48, 1)\n",
            "iteration: 25800 , train shape: (61141, 48, 48, 1)\n",
            "iteration: 25900 , train shape: (61241, 48, 48, 1)\n",
            "iteration: 26000 , train shape: (61341, 48, 48, 1)\n",
            "iteration: 26100 , train shape: (61441, 48, 48, 1)\n",
            "iteration: 26200 , train shape: (61541, 48, 48, 1)\n",
            "iteration: 26300 , train shape: (61641, 48, 48, 1)\n",
            "iteration: 26400 , train shape: (61741, 48, 48, 1)\n",
            "iteration: 26500 , train shape: (61841, 48, 48, 1)\n",
            "iteration: 26600 , train shape: (61941, 48, 48, 1)\n",
            "iteration: 26700 , train shape: (62041, 48, 48, 1)\n",
            "iteration: 26800 , train shape: (62141, 48, 48, 1)\n",
            "iteration: 26900 , train shape: (62241, 48, 48, 1)\n",
            "iteration: 27000 , train shape: (62341, 48, 48, 1)\n",
            "iteration: 27100 , train shape: (62441, 48, 48, 1)\n",
            "iteration: 27200 , train shape: (62541, 48, 48, 1)\n",
            "iteration: 27300 , train shape: (62641, 48, 48, 1)\n",
            "iteration: 27400 , train shape: (62741, 48, 48, 1)\n",
            "iteration: 27500 , train shape: (62841, 48, 48, 1)\n",
            "iteration: 27600 , train shape: (62941, 48, 48, 1)\n",
            "iteration: 27700 , train shape: (63041, 48, 48, 1)\n",
            "iteration: 27800 , train shape: (63141, 48, 48, 1)\n",
            "iteration: 27900 , train shape: (63241, 48, 48, 1)\n",
            "iteration: 28000 , train shape: (63341, 48, 48, 1)\n",
            "iteration: 28100 , train shape: (63441, 48, 48, 1)\n",
            "iteration: 28200 , train shape: (63541, 48, 48, 1)\n",
            "iteration: 28300 , train shape: (63641, 48, 48, 1)\n",
            "iteration: 28400 , train shape: (63741, 48, 48, 1)\n",
            "iteration: 28500 , train shape: (63841, 48, 48, 1)\n",
            "iteration: 28600 , train shape: (63941, 48, 48, 1)\n",
            "iteration: 28700 , train shape: (64041, 48, 48, 1)\n",
            "iteration: 28800 , train shape: (64141, 48, 48, 1)\n",
            "iteration: 28900 , train shape: (64241, 48, 48, 1)\n",
            "iteration: 29000 , train shape: (64341, 48, 48, 1)\n",
            "iteration: 29100 , train shape: (64441, 48, 48, 1)\n",
            "iteration: 29200 , train shape: (64541, 48, 48, 1)\n",
            "iteration: 29300 , train shape: (64641, 48, 48, 1)\n",
            "iteration: 29400 , train shape: (64741, 48, 48, 1)\n",
            "iteration: 29500 , train shape: (64841, 48, 48, 1)\n",
            "iteration: 29600 , train shape: (64941, 48, 48, 1)\n",
            "iteration: 29700 , train shape: (65041, 48, 48, 1)\n",
            "iteration: 29800 , train shape: (65141, 48, 48, 1)\n",
            "iteration: 29900 , train shape: (65241, 48, 48, 1)\n",
            "iteration: 30000 , train shape: (65341, 48, 48, 1)\n",
            "iteration: 30100 , train shape: (65441, 48, 48, 1)\n",
            "iteration: 30200 , train shape: (65541, 48, 48, 1)\n",
            "iteration: 30300 , train shape: (65641, 48, 48, 1)\n",
            "iteration: 30400 , train shape: (65741, 48, 48, 1)\n",
            "iteration: 30500 , train shape: (65841, 48, 48, 1)\n",
            "iteration: 30600 , train shape: (65941, 48, 48, 1)\n",
            "iteration: 30700 , train shape: (66041, 48, 48, 1)\n",
            "iteration: 30800 , train shape: (66141, 48, 48, 1)\n",
            "iteration: 30900 , train shape: (66241, 48, 48, 1)\n",
            "iteration: 31000 , train shape: (66341, 48, 48, 1)\n",
            "iteration: 31100 , train shape: (66441, 48, 48, 1)\n",
            "iteration: 31200 , train shape: (66541, 48, 48, 1)\n",
            "iteration: 31300 , train shape: (66641, 48, 48, 1)\n",
            "iteration: 31400 , train shape: (66741, 48, 48, 1)\n",
            "iteration: 31500 , train shape: (66841, 48, 48, 1)\n",
            "iteration: 31600 , train shape: (66941, 48, 48, 1)\n",
            "iteration: 31700 , train shape: (67041, 48, 48, 1)\n",
            "iteration: 31800 , train shape: (67141, 48, 48, 1)\n",
            "iteration: 31900 , train shape: (67241, 48, 48, 1)\n",
            "iteration: 32000 , train shape: (67341, 48, 48, 1)\n",
            "iteration: 32100 , train shape: (67441, 48, 48, 1)\n",
            "iteration: 32200 , train shape: (67541, 48, 48, 1)\n",
            "iteration: 32300 , train shape: (67641, 48, 48, 1)\n",
            "iteration: 32400 , train shape: (67741, 48, 48, 1)\n",
            "iteration: 32500 , train shape: (67841, 48, 48, 1)\n",
            "iteration: 32600 , train shape: (67941, 48, 48, 1)\n",
            "iteration: 32700 , train shape: (68041, 48, 48, 1)\n",
            "iteration: 32800 , train shape: (68141, 48, 48, 1)\n",
            "iteration: 32900 , train shape: (68241, 48, 48, 1)\n",
            "iteration: 33000 , train shape: (68341, 48, 48, 1)\n",
            "iteration: 33100 , train shape: (68441, 48, 48, 1)\n",
            "iteration: 33200 , train shape: (68541, 48, 48, 1)\n",
            "iteration: 33300 , train shape: (68641, 48, 48, 1)\n",
            "iteration: 33400 , train shape: (68741, 48, 48, 1)\n",
            "iteration: 33500 , train shape: (68841, 48, 48, 1)\n",
            "iteration: 33600 , train shape: (68941, 48, 48, 1)\n",
            "iteration: 33700 , train shape: (69041, 48, 48, 1)\n",
            "iteration: 33800 , train shape: (69141, 48, 48, 1)\n",
            "iteration: 33900 , train shape: (69241, 48, 48, 1)\n",
            "iteration: 34000 , train shape: (69341, 48, 48, 1)\n",
            "iteration: 34100 , train shape: (69441, 48, 48, 1)\n",
            "iteration: 34200 , train shape: (69541, 48, 48, 1)\n",
            "iteration: 34300 , train shape: (69641, 48, 48, 1)\n",
            "iteration: 34400 , train shape: (69741, 48, 48, 1)\n",
            "iteration: 34500 , train shape: (69841, 48, 48, 1)\n",
            "iteration: 34600 , train shape: (69941, 48, 48, 1)\n",
            "iteration: 34700 , train shape: (70041, 48, 48, 1)\n",
            "iteration: 34800 , train shape: (70141, 48, 48, 1)\n",
            "iteration: 34900 , train shape: (70241, 48, 48, 1)\n",
            "iteration: 35000 , train shape: (70341, 48, 48, 1)\n",
            "iteration: 35100 , train shape: (70441, 48, 48, 1)\n",
            "iteration: 35200 , train shape: (70541, 48, 48, 1)\n",
            "iteration: 35300 , train shape: (70641, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWsEbPyuvHO8",
        "outputId": "5ed91a16-2828-4083-9bfc-352a1d4a10cf"
      },
      "source": [
        "print(\"X_train_aug shape:\", X_train_aug.shape)\n",
        "print(\"y_train_aug shape:\", y_train_aug.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_aug shape: (70680, 48, 48, 1)\n",
            "y_train_aug shape: (70680, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding image of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "        zn = Flatten()(zi)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train_aug.shape[0], batch_size)\n",
        "            imgs, labels = X_train_aug[idx], y_train_aug[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-_XDn64sD4v",
        "outputId": "f253fbf8-65e8-447d-deca-8fd640910ebd"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18610, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.698123, acc.: 52.73%] [G loss: 1.569996]\n",
            "20 [D loss: 1.263933, acc.: 18.75%] [G loss: 0.707762]\n",
            "40 [D loss: 8.083212, acc.: 0.00%] [G loss: 5.953251]\n",
            "60 [D loss: 1.142002, acc.: 16.02%] [G loss: 2.008784]\n",
            "80 [D loss: 0.725508, acc.: 49.22%] [G loss: 2.943002]\n",
            "100 [D loss: 0.615188, acc.: 81.64%] [G loss: 1.898335]\n",
            "120 [D loss: 0.784398, acc.: 43.75%] [G loss: 1.850640]\n",
            "140 [D loss: 0.741967, acc.: 38.67%] [G loss: 2.574521]\n",
            "160 [D loss: 0.672686, acc.: 50.39%] [G loss: 8.102724]\n",
            "180 [D loss: 0.693462, acc.: 72.66%] [G loss: 2.071857]\n",
            "200 [D loss: 0.487419, acc.: 78.91%] [G loss: 4.945781]\n",
            "220 [D loss: 0.117920, acc.: 95.70%] [G loss: 13.005884]\n",
            "240 [D loss: 0.601519, acc.: 74.22%] [G loss: 2.892894]\n",
            "260 [D loss: 0.435385, acc.: 82.42%] [G loss: 3.964682]\n",
            "280 [D loss: 0.318348, acc.: 93.75%] [G loss: 3.031775]\n",
            "300 [D loss: 0.175127, acc.: 96.88%] [G loss: 9.014080]\n",
            "320 [D loss: 0.312560, acc.: 92.19%] [G loss: 5.428997]\n",
            "340 [D loss: 0.679572, acc.: 61.72%] [G loss: 1.662849]\n",
            "360 [D loss: 0.383923, acc.: 96.48%] [G loss: 2.960269]\n",
            "380 [D loss: 0.378854, acc.: 87.50%] [G loss: 3.836970]\n",
            "400 [D loss: 0.401361, acc.: 86.33%] [G loss: 3.605958]\n",
            "420 [D loss: 0.287415, acc.: 90.23%] [G loss: 5.850658]\n",
            "440 [D loss: 0.275172, acc.: 91.80%] [G loss: 8.770459]\n",
            "460 [D loss: 0.953379, acc.: 45.70%] [G loss: 2.658277]\n",
            "480 [D loss: 0.207923, acc.: 96.09%] [G loss: 10.320415]\n",
            "500 [D loss: 0.285282, acc.: 89.84%] [G loss: 6.584496]\n",
            "520 [D loss: 0.405940, acc.: 83.59%] [G loss: 4.532123]\n",
            "540 [D loss: 0.219961, acc.: 91.80%] [G loss: 7.962637]\n",
            "560 [D loss: 0.585096, acc.: 65.62%] [G loss: 2.158092]\n",
            "580 [D loss: 0.690967, acc.: 57.42%] [G loss: 2.491554]\n",
            "600 [D loss: 0.652280, acc.: 61.72%] [G loss: 2.159258]\n",
            "620 [D loss: 0.455335, acc.: 81.25%] [G loss: 2.994518]\n",
            "640 [D loss: 0.678416, acc.: 61.72%] [G loss: 2.351568]\n",
            "660 [D loss: 0.419375, acc.: 83.20%] [G loss: 2.126704]\n",
            "680 [D loss: 0.379310, acc.: 83.59%] [G loss: 4.090283]\n",
            "700 [D loss: 0.521701, acc.: 74.61%] [G loss: 3.982643]\n",
            "720 [D loss: 0.546809, acc.: 73.44%] [G loss: 3.407717]\n",
            "740 [D loss: 0.599960, acc.: 69.92%] [G loss: 3.273586]\n",
            "760 [D loss: 0.599551, acc.: 67.58%] [G loss: 2.817073]\n",
            "780 [D loss: 0.536312, acc.: 73.44%] [G loss: 3.089310]\n",
            "800 [D loss: 0.478301, acc.: 80.47%] [G loss: 1.987663]\n",
            "820 [D loss: 0.680464, acc.: 62.50%] [G loss: 3.501022]\n",
            "840 [D loss: 0.504435, acc.: 73.83%] [G loss: 2.778534]\n",
            "860 [D loss: 0.487567, acc.: 76.17%] [G loss: 3.628816]\n",
            "880 [D loss: 0.592883, acc.: 69.92%] [G loss: 2.973521]\n",
            "900 [D loss: 0.476875, acc.: 77.73%] [G loss: 4.188207]\n",
            "920 [D loss: 0.480042, acc.: 75.78%] [G loss: 3.643293]\n",
            "940 [D loss: 0.468124, acc.: 80.08%] [G loss: 3.827842]\n",
            "960 [D loss: 0.458231, acc.: 76.17%] [G loss: 3.952977]\n",
            "980 [D loss: 0.551299, acc.: 73.05%] [G loss: 3.428413]\n",
            "1000 [D loss: 0.482479, acc.: 75.39%] [G loss: 3.712141]\n",
            "1020 [D loss: 0.603649, acc.: 68.75%] [G loss: 3.071601]\n",
            "1040 [D loss: 0.489602, acc.: 75.00%] [G loss: 3.598746]\n",
            "1060 [D loss: 0.479387, acc.: 76.95%] [G loss: 4.022535]\n",
            "1080 [D loss: 0.478688, acc.: 78.12%] [G loss: 3.476630]\n",
            "1100 [D loss: 0.440580, acc.: 80.08%] [G loss: 3.584454]\n",
            "1120 [D loss: 0.439327, acc.: 82.81%] [G loss: 3.374869]\n",
            "1140 [D loss: 0.529551, acc.: 75.00%] [G loss: 3.265483]\n",
            "1160 [D loss: 0.427640, acc.: 82.03%] [G loss: 3.709646]\n",
            "1180 [D loss: 0.503113, acc.: 75.39%] [G loss: 3.294135]\n",
            "1200 [D loss: 0.574645, acc.: 71.48%] [G loss: 3.074146]\n",
            "1220 [D loss: 0.458364, acc.: 82.03%] [G loss: 3.435807]\n",
            "1240 [D loss: 0.463601, acc.: 80.47%] [G loss: 3.920688]\n",
            "1260 [D loss: 0.512620, acc.: 76.95%] [G loss: 3.236205]\n",
            "1280 [D loss: 0.444931, acc.: 80.86%] [G loss: 3.455146]\n",
            "1300 [D loss: 0.566487, acc.: 72.66%] [G loss: 3.144912]\n",
            "1320 [D loss: 0.559826, acc.: 68.75%] [G loss: 2.989975]\n",
            "1340 [D loss: 0.462452, acc.: 82.42%] [G loss: 3.430598]\n",
            "1360 [D loss: 0.508403, acc.: 76.56%] [G loss: 3.170016]\n",
            "1380 [D loss: 0.537018, acc.: 72.66%] [G loss: 3.069696]\n",
            "1400 [D loss: 0.451652, acc.: 78.52%] [G loss: 3.591741]\n",
            "1420 [D loss: 0.497999, acc.: 76.17%] [G loss: 3.212858]\n",
            "1440 [D loss: 0.534284, acc.: 69.14%] [G loss: 3.145919]\n",
            "1460 [D loss: 0.528469, acc.: 76.95%] [G loss: 3.087666]\n",
            "1480 [D loss: 0.562374, acc.: 67.97%] [G loss: 2.956449]\n",
            "1500 [D loss: 0.427514, acc.: 80.86%] [G loss: 3.366858]\n",
            "1520 [D loss: 0.544944, acc.: 72.66%] [G loss: 3.095348]\n",
            "1540 [D loss: 0.452349, acc.: 83.59%] [G loss: 3.125990]\n",
            "1560 [D loss: 0.506757, acc.: 74.61%] [G loss: 2.776407]\n",
            "1580 [D loss: 0.341264, acc.: 88.67%] [G loss: 3.873145]\n",
            "1600 [D loss: 0.672496, acc.: 64.84%] [G loss: 3.715500]\n",
            "1620 [D loss: 0.465925, acc.: 78.52%] [G loss: 4.054165]\n",
            "1640 [D loss: 0.498948, acc.: 78.52%] [G loss: 3.946155]\n",
            "1660 [D loss: 0.513535, acc.: 75.39%] [G loss: 3.612877]\n",
            "1680 [D loss: 0.541383, acc.: 73.44%] [G loss: 3.537860]\n",
            "1700 [D loss: 0.463904, acc.: 78.12%] [G loss: 3.929044]\n",
            "1720 [D loss: 0.401421, acc.: 84.77%] [G loss: 4.445213]\n",
            "1740 [D loss: 0.434981, acc.: 80.08%] [G loss: 4.086342]\n",
            "1760 [D loss: 0.417099, acc.: 83.59%] [G loss: 4.015034]\n",
            "1780 [D loss: 0.447514, acc.: 79.30%] [G loss: 4.076079]\n",
            "1800 [D loss: 0.473030, acc.: 77.34%] [G loss: 4.003439]\n",
            "1820 [D loss: 0.420443, acc.: 82.81%] [G loss: 3.916917]\n",
            "1840 [D loss: 0.452470, acc.: 79.69%] [G loss: 4.007185]\n",
            "1860 [D loss: 0.477254, acc.: 75.39%] [G loss: 3.866680]\n",
            "1880 [D loss: 0.474361, acc.: 76.17%] [G loss: 3.691290]\n",
            "1900 [D loss: 0.420208, acc.: 84.38%] [G loss: 3.809149]\n",
            "1920 [D loss: 0.358747, acc.: 86.33%] [G loss: 4.323447]\n",
            "1940 [D loss: 0.397855, acc.: 82.81%] [G loss: 4.046037]\n",
            "1960 [D loss: 0.390669, acc.: 86.33%] [G loss: 4.198030]\n",
            "1980 [D loss: 0.450312, acc.: 78.52%] [G loss: 4.132113]\n",
            "2000 [D loss: 0.480501, acc.: 76.56%] [G loss: 3.750889]\n",
            "2020 [D loss: 0.446163, acc.: 79.69%] [G loss: 3.967647]\n",
            "2040 [D loss: 0.443026, acc.: 80.08%] [G loss: 3.778702]\n",
            "2060 [D loss: 0.580418, acc.: 71.48%] [G loss: 3.292255]\n",
            "2080 [D loss: 0.410142, acc.: 80.08%] [G loss: 3.865235]\n",
            "2100 [D loss: 0.415676, acc.: 83.59%] [G loss: 3.831032]\n",
            "2120 [D loss: 0.501486, acc.: 78.12%] [G loss: 3.332677]\n",
            "2140 [D loss: 0.462204, acc.: 76.95%] [G loss: 3.597321]\n",
            "2160 [D loss: 0.404726, acc.: 84.77%] [G loss: 3.700217]\n",
            "2180 [D loss: 0.352465, acc.: 87.50%] [G loss: 3.832062]\n",
            "2200 [D loss: 0.450129, acc.: 80.86%] [G loss: 3.646698]\n",
            "2220 [D loss: 0.513123, acc.: 77.34%] [G loss: 3.486762]\n",
            "2240 [D loss: 0.519113, acc.: 75.39%] [G loss: 3.332635]\n",
            "2260 [D loss: 0.449563, acc.: 77.73%] [G loss: 3.894568]\n",
            "2280 [D loss: 0.458776, acc.: 82.03%] [G loss: 3.700673]\n",
            "2300 [D loss: 0.451507, acc.: 80.08%] [G loss: 3.436335]\n",
            "2320 [D loss: 0.475417, acc.: 80.08%] [G loss: 3.429093]\n",
            "2340 [D loss: 0.626247, acc.: 65.62%] [G loss: 2.600543]\n",
            "2360 [D loss: 0.780560, acc.: 58.98%] [G loss: 3.931918]\n",
            "2380 [D loss: 0.568962, acc.: 69.92%] [G loss: 3.364694]\n",
            "2400 [D loss: 0.588922, acc.: 72.66%] [G loss: 3.452894]\n",
            "2420 [D loss: 0.449050, acc.: 78.91%] [G loss: 4.120029]\n",
            "2440 [D loss: 0.452183, acc.: 80.08%] [G loss: 4.447839]\n",
            "2460 [D loss: 0.563413, acc.: 74.22%] [G loss: 3.927955]\n",
            "2480 [D loss: 0.633126, acc.: 66.41%] [G loss: 3.438547]\n",
            "2500 [D loss: 0.506304, acc.: 76.56%] [G loss: 3.684210]\n",
            "2520 [D loss: 0.464513, acc.: 77.73%] [G loss: 3.464009]\n",
            "2540 [D loss: 0.421437, acc.: 82.42%] [G loss: 3.959748]\n",
            "2560 [D loss: 0.462370, acc.: 78.91%] [G loss: 3.981234]\n",
            "2580 [D loss: 0.527595, acc.: 73.44%] [G loss: 3.494977]\n",
            "2600 [D loss: 0.506516, acc.: 76.56%] [G loss: 3.278620]\n",
            "2620 [D loss: 0.766729, acc.: 55.86%] [G loss: 3.072804]\n",
            "2640 [D loss: 0.441283, acc.: 80.08%] [G loss: 3.910257]\n",
            "2660 [D loss: 0.387737, acc.: 86.33%] [G loss: 4.019335]\n",
            "2680 [D loss: 0.523847, acc.: 71.48%] [G loss: 3.442432]\n",
            "2700 [D loss: 0.446408, acc.: 78.12%] [G loss: 4.046684]\n",
            "2720 [D loss: 0.581660, acc.: 66.02%] [G loss: 3.342309]\n",
            "2740 [D loss: 0.536022, acc.: 71.09%] [G loss: 3.627283]\n",
            "2760 [D loss: 0.497864, acc.: 79.30%] [G loss: 3.572876]\n",
            "2780 [D loss: 0.418300, acc.: 83.20%] [G loss: 3.506318]\n",
            "2800 [D loss: 0.467255, acc.: 78.52%] [G loss: 3.629444]\n",
            "2820 [D loss: 0.455620, acc.: 76.95%] [G loss: 3.576855]\n",
            "2840 [D loss: 0.483860, acc.: 75.78%] [G loss: 3.666139]\n",
            "2860 [D loss: 0.435561, acc.: 82.03%] [G loss: 3.533092]\n",
            "2880 [D loss: 0.486474, acc.: 77.73%] [G loss: 3.426453]\n",
            "2900 [D loss: 0.512102, acc.: 75.78%] [G loss: 3.613715]\n",
            "2920 [D loss: 0.469138, acc.: 76.95%] [G loss: 3.482748]\n",
            "2940 [D loss: 0.511724, acc.: 72.27%] [G loss: 3.506606]\n",
            "2960 [D loss: 0.428011, acc.: 82.42%] [G loss: 4.111586]\n",
            "2980 [D loss: 0.514031, acc.: 74.61%] [G loss: 3.501810]\n",
            "3000 [D loss: 0.481320, acc.: 76.17%] [G loss: 3.641515]\n",
            "3020 [D loss: 0.524257, acc.: 74.22%] [G loss: 3.328101]\n",
            "3040 [D loss: 0.514309, acc.: 77.73%] [G loss: 3.468811]\n",
            "3060 [D loss: 0.500154, acc.: 72.27%] [G loss: 3.327208]\n",
            "3080 [D loss: 0.469294, acc.: 80.86%] [G loss: 3.325713]\n",
            "3100 [D loss: 0.544748, acc.: 72.27%] [G loss: 3.662484]\n",
            "3120 [D loss: 0.484564, acc.: 80.47%] [G loss: 3.798297]\n",
            "3140 [D loss: 0.510314, acc.: 75.00%] [G loss: 3.421976]\n",
            "3160 [D loss: 0.479859, acc.: 76.95%] [G loss: 3.716269]\n",
            "3180 [D loss: 0.470981, acc.: 80.47%] [G loss: 3.642578]\n",
            "3200 [D loss: 0.502381, acc.: 75.00%] [G loss: 3.488105]\n",
            "3220 [D loss: 0.512196, acc.: 75.39%] [G loss: 3.537628]\n",
            "3240 [D loss: 0.498762, acc.: 76.56%] [G loss: 3.581745]\n",
            "3260 [D loss: 0.519446, acc.: 75.00%] [G loss: 3.161361]\n",
            "3280 [D loss: 0.426337, acc.: 80.47%] [G loss: 3.764720]\n",
            "3300 [D loss: 0.480855, acc.: 79.69%] [G loss: 3.333790]\n",
            "3320 [D loss: 0.539889, acc.: 71.88%] [G loss: 3.375835]\n",
            "3340 [D loss: 0.464987, acc.: 80.08%] [G loss: 3.439950]\n",
            "3360 [D loss: 0.950963, acc.: 49.61%] [G loss: 2.555340]\n",
            "3380 [D loss: 0.716558, acc.: 59.38%] [G loss: 3.928746]\n",
            "3400 [D loss: 0.723144, acc.: 62.11%] [G loss: 2.860509]\n",
            "3420 [D loss: 0.523876, acc.: 76.17%] [G loss: 3.217328]\n",
            "3440 [D loss: 0.771690, acc.: 56.25%] [G loss: 3.617998]\n",
            "3460 [D loss: 0.489635, acc.: 76.56%] [G loss: 4.184454]\n",
            "3480 [D loss: 0.530998, acc.: 69.92%] [G loss: 3.719770]\n",
            "3500 [D loss: 0.637487, acc.: 68.75%] [G loss: 3.528802]\n",
            "3520 [D loss: 0.531977, acc.: 73.05%] [G loss: 3.387388]\n",
            "3540 [D loss: 0.440774, acc.: 76.95%] [G loss: 3.723085]\n",
            "3560 [D loss: 0.601077, acc.: 67.19%] [G loss: 3.301846]\n",
            "3580 [D loss: 0.528401, acc.: 74.22%] [G loss: 3.676950]\n",
            "3600 [D loss: 0.418482, acc.: 81.64%] [G loss: 3.967714]\n",
            "3620 [D loss: 0.475232, acc.: 80.47%] [G loss: 3.461360]\n",
            "3640 [D loss: 0.489689, acc.: 75.00%] [G loss: 3.384803]\n",
            "3660 [D loss: 0.462855, acc.: 79.69%] [G loss: 3.429291]\n",
            "3680 [D loss: 0.547569, acc.: 69.92%] [G loss: 3.043585]\n",
            "3700 [D loss: 0.492991, acc.: 76.95%] [G loss: 3.254397]\n",
            "3720 [D loss: 0.533487, acc.: 71.88%] [G loss: 3.164992]\n",
            "3740 [D loss: 0.486041, acc.: 76.56%] [G loss: 3.583892]\n",
            "3760 [D loss: 0.427751, acc.: 81.25%] [G loss: 3.656375]\n",
            "3780 [D loss: 0.514197, acc.: 74.22%] [G loss: 3.607103]\n",
            "3800 [D loss: 0.491261, acc.: 75.00%] [G loss: 3.558183]\n",
            "3820 [D loss: 0.510493, acc.: 74.61%] [G loss: 3.419613]\n",
            "3840 [D loss: 0.544819, acc.: 69.92%] [G loss: 3.141578]\n",
            "3860 [D loss: 0.521899, acc.: 75.39%] [G loss: 3.673166]\n",
            "3880 [D loss: 0.417001, acc.: 81.25%] [G loss: 3.462361]\n",
            "3900 [D loss: 0.564989, acc.: 71.88%] [G loss: 3.407071]\n",
            "3920 [D loss: 0.421389, acc.: 81.25%] [G loss: 3.734683]\n",
            "3940 [D loss: 0.501728, acc.: 73.83%] [G loss: 3.508801]\n",
            "3960 [D loss: 0.478323, acc.: 76.17%] [G loss: 3.509462]\n",
            "3980 [D loss: 0.474252, acc.: 78.12%] [G loss: 3.537221]\n",
            "4000 [D loss: 0.443369, acc.: 80.86%] [G loss: 3.486463]\n",
            "4020 [D loss: 0.456899, acc.: 80.08%] [G loss: 3.652416]\n",
            "4040 [D loss: 0.496053, acc.: 75.00%] [G loss: 3.605583]\n",
            "4060 [D loss: 0.387500, acc.: 85.55%] [G loss: 3.079588]\n",
            "4080 [D loss: 0.218068, acc.: 94.14%] [G loss: 4.631824]\n",
            "4100 [D loss: 0.664786, acc.: 62.89%] [G loss: 3.376001]\n",
            "4120 [D loss: 0.538692, acc.: 71.88%] [G loss: 3.422977]\n",
            "4140 [D loss: 0.445359, acc.: 77.73%] [G loss: 3.771974]\n",
            "4160 [D loss: 0.512763, acc.: 74.22%] [G loss: 3.485173]\n",
            "4180 [D loss: 0.551911, acc.: 71.88%] [G loss: 3.085567]\n",
            "4200 [D loss: 0.460412, acc.: 77.73%] [G loss: 3.154349]\n",
            "4220 [D loss: 0.488376, acc.: 77.34%] [G loss: 3.114517]\n",
            "4240 [D loss: 0.508170, acc.: 72.66%] [G loss: 3.065428]\n",
            "4260 [D loss: 0.431342, acc.: 80.86%] [G loss: 3.797499]\n",
            "4280 [D loss: 0.216316, acc.: 96.48%] [G loss: 2.150486]\n",
            "4300 [D loss: 0.335892, acc.: 91.02%] [G loss: 2.214457]\n",
            "4320 [D loss: 0.760940, acc.: 62.50%] [G loss: 2.192388]\n",
            "4340 [D loss: 2.669549, acc.: 2.73%] [G loss: 1.544238]\n",
            "4360 [D loss: 1.241601, acc.: 8.98%] [G loss: 1.291179]\n",
            "4380 [D loss: 1.100204, acc.: 11.72%] [G loss: 1.335270]\n",
            "4400 [D loss: 0.862571, acc.: 17.97%] [G loss: 1.324084]\n",
            "4420 [D loss: 1.004455, acc.: 5.86%] [G loss: 1.083890]\n",
            "4440 [D loss: 0.845161, acc.: 27.73%] [G loss: 1.312308]\n",
            "4460 [D loss: 0.785414, acc.: 28.12%] [G loss: 1.375373]\n",
            "4480 [D loss: 0.757255, acc.: 27.73%] [G loss: 1.349194]\n",
            "4500 [D loss: 0.695474, acc.: 51.17%] [G loss: 1.513360]\n",
            "4520 [D loss: 0.723373, acc.: 37.50%] [G loss: 1.413589]\n",
            "4540 [D loss: 0.707505, acc.: 47.66%] [G loss: 1.448584]\n",
            "4560 [D loss: 0.705215, acc.: 43.36%] [G loss: 1.441908]\n",
            "4580 [D loss: 0.707941, acc.: 46.88%] [G loss: 1.455831]\n",
            "4600 [D loss: 0.713227, acc.: 43.36%] [G loss: 1.437184]\n",
            "4620 [D loss: 0.701993, acc.: 48.44%] [G loss: 1.442947]\n",
            "4640 [D loss: 0.721086, acc.: 35.55%] [G loss: 1.419387]\n",
            "4660 [D loss: 0.703838, acc.: 47.27%] [G loss: 1.456243]\n",
            "4680 [D loss: 0.751380, acc.: 34.38%] [G loss: 1.416242]\n",
            "4700 [D loss: 0.654242, acc.: 66.80%] [G loss: 1.588108]\n",
            "4720 [D loss: 0.744526, acc.: 41.02%] [G loss: 1.403747]\n",
            "4740 [D loss: 0.674219, acc.: 58.98%] [G loss: 1.569756]\n",
            "4760 [D loss: 0.697931, acc.: 50.78%] [G loss: 1.519584]\n",
            "4780 [D loss: 0.765257, acc.: 39.45%] [G loss: 1.430552]\n",
            "4800 [D loss: 0.680960, acc.: 55.86%] [G loss: 1.459392]\n",
            "4820 [D loss: 0.783486, acc.: 28.52%] [G loss: 1.314399]\n",
            "4840 [D loss: 0.650880, acc.: 63.28%] [G loss: 1.601916]\n",
            "4860 [D loss: 0.694774, acc.: 54.30%] [G loss: 1.658290]\n",
            "4880 [D loss: 0.646750, acc.: 64.45%] [G loss: 1.686376]\n",
            "4900 [D loss: 0.703938, acc.: 46.09%] [G loss: 1.558483]\n",
            "4920 [D loss: 0.632584, acc.: 64.84%] [G loss: 1.790708]\n",
            "4940 [D loss: 0.642560, acc.: 67.97%] [G loss: 1.712330]\n",
            "4960 [D loss: 0.660787, acc.: 58.98%] [G loss: 1.824744]\n",
            "4980 [D loss: 0.660338, acc.: 60.55%] [G loss: 1.754838]\n",
            "5000 [D loss: 0.610496, acc.: 69.14%] [G loss: 2.003053]\n",
            "5020 [D loss: 0.770366, acc.: 45.31%] [G loss: 1.575341]\n",
            "5040 [D loss: 0.594654, acc.: 70.31%] [G loss: 2.141172]\n",
            "5060 [D loss: 0.697499, acc.: 53.91%] [G loss: 1.760964]\n",
            "5080 [D loss: 0.561359, acc.: 75.39%] [G loss: 1.740086]\n",
            "5100 [D loss: 0.631894, acc.: 66.41%] [G loss: 1.523816]\n",
            "5120 [D loss: 0.661177, acc.: 56.64%] [G loss: 2.223387]\n",
            "5140 [D loss: 0.429934, acc.: 85.16%] [G loss: 1.379668]\n",
            "5160 [D loss: 0.639970, acc.: 64.06%] [G loss: 1.410083]\n",
            "5180 [D loss: 0.821583, acc.: 47.66%] [G loss: 1.916908]\n",
            "5200 [D loss: 0.902237, acc.: 16.41%] [G loss: 1.450187]\n",
            "5220 [D loss: 0.670591, acc.: 60.94%] [G loss: 1.831165]\n",
            "5240 [D loss: 0.603003, acc.: 66.80%] [G loss: 2.502318]\n",
            "5260 [D loss: 0.912780, acc.: 37.50%] [G loss: 1.894787]\n",
            "5280 [D loss: 0.550485, acc.: 75.00%] [G loss: 2.347754]\n",
            "5300 [D loss: 0.693703, acc.: 54.30%] [G loss: 2.197809]\n",
            "5320 [D loss: 0.584149, acc.: 70.31%] [G loss: 2.384203]\n",
            "5340 [D loss: 0.545592, acc.: 73.44%] [G loss: 2.723635]\n",
            "5360 [D loss: 0.631730, acc.: 64.06%] [G loss: 2.372322]\n",
            "5380 [D loss: 0.641852, acc.: 62.89%] [G loss: 2.213933]\n",
            "5400 [D loss: 0.648627, acc.: 63.28%] [G loss: 2.181452]\n",
            "5420 [D loss: 0.545425, acc.: 74.22%] [G loss: 2.664029]\n",
            "5440 [D loss: 0.649104, acc.: 61.72%] [G loss: 2.259522]\n",
            "5460 [D loss: 0.635431, acc.: 64.84%] [G loss: 2.252838]\n",
            "5480 [D loss: 0.544853, acc.: 78.12%] [G loss: 2.513240]\n",
            "5500 [D loss: 0.590810, acc.: 67.97%] [G loss: 2.334726]\n",
            "5520 [D loss: 0.501326, acc.: 79.30%] [G loss: 2.627891]\n",
            "5540 [D loss: 0.659235, acc.: 60.94%] [G loss: 2.238859]\n",
            "5560 [D loss: 0.604653, acc.: 67.97%] [G loss: 2.640415]\n",
            "5580 [D loss: 0.605422, acc.: 68.36%] [G loss: 2.284490]\n",
            "5600 [D loss: 0.640397, acc.: 61.33%] [G loss: 2.363274]\n",
            "5620 [D loss: 0.594688, acc.: 66.80%] [G loss: 2.409270]\n",
            "5640 [D loss: 0.585852, acc.: 68.36%] [G loss: 2.427438]\n",
            "5660 [D loss: 0.559396, acc.: 71.09%] [G loss: 2.725486]\n",
            "5680 [D loss: 0.572074, acc.: 68.36%] [G loss: 2.777449]\n",
            "5700 [D loss: 0.540033, acc.: 75.00%] [G loss: 2.600851]\n",
            "5720 [D loss: 0.663327, acc.: 57.81%] [G loss: 2.464088]\n",
            "5740 [D loss: 0.544323, acc.: 73.05%] [G loss: 2.623084]\n",
            "5760 [D loss: 0.550524, acc.: 73.44%] [G loss: 2.551991]\n",
            "5780 [D loss: 0.634665, acc.: 66.02%] [G loss: 2.379534]\n",
            "5800 [D loss: 0.631160, acc.: 65.23%] [G loss: 2.376253]\n",
            "5820 [D loss: 0.591699, acc.: 66.80%] [G loss: 2.504402]\n",
            "5840 [D loss: 0.541456, acc.: 74.22%] [G loss: 2.515135]\n",
            "5860 [D loss: 0.580887, acc.: 67.19%] [G loss: 2.385214]\n",
            "5880 [D loss: 0.548577, acc.: 75.00%] [G loss: 2.575234]\n",
            "5900 [D loss: 0.600045, acc.: 68.36%] [G loss: 2.435316]\n",
            "5920 [D loss: 0.609084, acc.: 64.45%] [G loss: 2.490205]\n",
            "5940 [D loss: 0.516825, acc.: 74.22%] [G loss: 2.830549]\n",
            "5960 [D loss: 0.507489, acc.: 72.27%] [G loss: 2.936862]\n",
            "5980 [D loss: 0.508474, acc.: 76.95%] [G loss: 2.850677]\n",
            "6000 [D loss: 0.551561, acc.: 71.09%] [G loss: 2.735157]\n",
            "6020 [D loss: 0.530211, acc.: 73.44%] [G loss: 2.690740]\n",
            "6040 [D loss: 0.519841, acc.: 73.44%] [G loss: 1.683487]\n",
            "6060 [D loss: 0.360776, acc.: 89.84%] [G loss: 1.767117]\n",
            "6080 [D loss: 0.559717, acc.: 76.17%] [G loss: 1.596252]\n",
            "6100 [D loss: 1.420134, acc.: 23.83%] [G loss: 2.341280]\n",
            "6120 [D loss: 0.688798, acc.: 62.89%] [G loss: 2.673249]\n",
            "6140 [D loss: 0.613166, acc.: 67.97%] [G loss: 2.786313]\n",
            "6160 [D loss: 0.775006, acc.: 55.47%] [G loss: 2.631429]\n",
            "6180 [D loss: 0.590485, acc.: 66.41%] [G loss: 2.831644]\n",
            "6200 [D loss: 0.606583, acc.: 69.14%] [G loss: 2.841677]\n",
            "6220 [D loss: 0.685542, acc.: 62.50%] [G loss: 2.484056]\n",
            "6240 [D loss: 0.589244, acc.: 68.75%] [G loss: 2.651791]\n",
            "6260 [D loss: 0.596299, acc.: 69.14%] [G loss: 2.926565]\n",
            "6280 [D loss: 0.605719, acc.: 67.19%] [G loss: 2.463937]\n",
            "6300 [D loss: 0.613154, acc.: 67.97%] [G loss: 2.532734]\n",
            "6320 [D loss: 0.575893, acc.: 71.09%] [G loss: 2.583869]\n",
            "6340 [D loss: 0.618640, acc.: 66.80%] [G loss: 2.505311]\n",
            "6360 [D loss: 0.565133, acc.: 67.97%] [G loss: 2.651241]\n",
            "6380 [D loss: 0.589388, acc.: 70.31%] [G loss: 2.699235]\n",
            "6400 [D loss: 0.604599, acc.: 67.58%] [G loss: 2.673958]\n",
            "6420 [D loss: 0.604425, acc.: 67.97%] [G loss: 2.610802]\n",
            "6440 [D loss: 0.540945, acc.: 72.66%] [G loss: 2.855375]\n",
            "6460 [D loss: 0.520012, acc.: 77.73%] [G loss: 2.839428]\n",
            "6480 [D loss: 0.587786, acc.: 69.14%] [G loss: 2.754251]\n",
            "6500 [D loss: 0.552316, acc.: 71.48%] [G loss: 2.776573]\n",
            "6520 [D loss: 0.560774, acc.: 69.53%] [G loss: 2.938713]\n",
            "6540 [D loss: 0.582146, acc.: 69.92%] [G loss: 2.645265]\n",
            "6560 [D loss: 0.595763, acc.: 67.58%] [G loss: 2.825893]\n",
            "6580 [D loss: 0.565110, acc.: 70.31%] [G loss: 2.791239]\n",
            "6600 [D loss: 0.577104, acc.: 71.09%] [G loss: 2.667309]\n",
            "6620 [D loss: 0.507960, acc.: 74.22%] [G loss: 2.650955]\n",
            "6640 [D loss: 0.510032, acc.: 78.91%] [G loss: 2.579726]\n",
            "6660 [D loss: 0.778451, acc.: 55.86%] [G loss: 2.292472]\n",
            "6680 [D loss: 0.742395, acc.: 57.03%] [G loss: 2.427502]\n",
            "6700 [D loss: 0.660973, acc.: 66.80%] [G loss: 1.953712]\n",
            "6720 [D loss: 0.579342, acc.: 69.92%] [G loss: 1.954632]\n",
            "6740 [D loss: 1.712998, acc.: 20.70%] [G loss: 3.058207]\n",
            "6760 [D loss: 1.024750, acc.: 43.36%] [G loss: 2.603333]\n",
            "6780 [D loss: 0.472431, acc.: 78.12%] [G loss: 3.553901]\n",
            "6800 [D loss: 0.786411, acc.: 53.91%] [G loss: 2.694322]\n",
            "6820 [D loss: 0.502941, acc.: 76.56%] [G loss: 3.054918]\n",
            "6840 [D loss: 0.563030, acc.: 71.09%] [G loss: 2.905575]\n",
            "6860 [D loss: 0.590694, acc.: 64.84%] [G loss: 2.620696]\n",
            "6880 [D loss: 0.539756, acc.: 72.66%] [G loss: 2.855295]\n",
            "6900 [D loss: 0.510238, acc.: 75.39%] [G loss: 3.003023]\n",
            "6920 [D loss: 0.512026, acc.: 75.78%] [G loss: 2.860729]\n",
            "6940 [D loss: 0.516477, acc.: 75.78%] [G loss: 2.938999]\n",
            "6960 [D loss: 0.627278, acc.: 64.06%] [G loss: 2.686785]\n",
            "6980 [D loss: 0.587675, acc.: 66.80%] [G loss: 2.732097]\n",
            "7000 [D loss: 0.565177, acc.: 71.09%] [G loss: 2.733571]\n",
            "7020 [D loss: 0.486196, acc.: 79.69%] [G loss: 2.837702]\n",
            "7040 [D loss: 0.565444, acc.: 72.66%] [G loss: 2.641634]\n",
            "7060 [D loss: 0.575956, acc.: 69.14%] [G loss: 2.831891]\n",
            "7080 [D loss: 0.534442, acc.: 71.48%] [G loss: 2.755014]\n",
            "7100 [D loss: 0.595602, acc.: 68.36%] [G loss: 2.696659]\n",
            "7120 [D loss: 0.550349, acc.: 71.09%] [G loss: 2.820442]\n",
            "7140 [D loss: 0.498869, acc.: 78.91%] [G loss: 3.192667]\n",
            "7160 [D loss: 0.565092, acc.: 69.92%] [G loss: 2.750287]\n",
            "7180 [D loss: 0.511149, acc.: 75.39%] [G loss: 2.848207]\n",
            "7200 [D loss: 0.548189, acc.: 69.92%] [G loss: 2.712440]\n",
            "7220 [D loss: 0.517164, acc.: 73.44%] [G loss: 2.753473]\n",
            "7240 [D loss: 0.550010, acc.: 72.27%] [G loss: 2.909049]\n",
            "7260 [D loss: 0.498177, acc.: 76.95%] [G loss: 2.823099]\n",
            "7280 [D loss: 0.535062, acc.: 74.61%] [G loss: 2.875574]\n",
            "7300 [D loss: 0.525111, acc.: 71.88%] [G loss: 2.809255]\n",
            "7320 [D loss: 0.537422, acc.: 73.44%] [G loss: 2.957986]\n",
            "7340 [D loss: 0.560366, acc.: 72.27%] [G loss: 3.051719]\n",
            "7360 [D loss: 0.547419, acc.: 71.88%] [G loss: 3.020343]\n",
            "7380 [D loss: 0.515920, acc.: 76.17%] [G loss: 3.186307]\n",
            "7400 [D loss: 0.544059, acc.: 71.88%] [G loss: 2.975324]\n",
            "7420 [D loss: 0.573259, acc.: 69.53%] [G loss: 2.765231]\n",
            "7440 [D loss: 0.618357, acc.: 65.62%] [G loss: 2.757306]\n",
            "7460 [D loss: 0.544679, acc.: 71.09%] [G loss: 2.808214]\n",
            "7480 [D loss: 0.553417, acc.: 73.05%] [G loss: 2.914511]\n",
            "7500 [D loss: 0.580494, acc.: 72.27%] [G loss: 2.911003]\n",
            "7520 [D loss: 0.501342, acc.: 75.78%] [G loss: 3.125188]\n",
            "7540 [D loss: 0.583662, acc.: 67.58%] [G loss: 2.651641]\n",
            "7560 [D loss: 0.506748, acc.: 75.00%] [G loss: 3.068903]\n",
            "7580 [D loss: 0.538073, acc.: 74.22%] [G loss: 2.885151]\n",
            "7600 [D loss: 0.558086, acc.: 67.97%] [G loss: 3.303321]\n",
            "7620 [D loss: 0.521459, acc.: 72.66%] [G loss: 2.868323]\n",
            "7640 [D loss: 0.551979, acc.: 74.22%] [G loss: 3.047413]\n",
            "7660 [D loss: 0.490527, acc.: 77.34%] [G loss: 2.969046]\n",
            "7680 [D loss: 0.542830, acc.: 74.22%] [G loss: 3.276855]\n",
            "7700 [D loss: 0.529679, acc.: 70.31%] [G loss: 3.197214]\n",
            "7720 [D loss: 0.522588, acc.: 74.61%] [G loss: 2.950979]\n",
            "7740 [D loss: 0.535111, acc.: 71.09%] [G loss: 3.051482]\n",
            "7760 [D loss: 0.495705, acc.: 79.69%] [G loss: 2.980027]\n",
            "7780 [D loss: 0.561965, acc.: 72.27%] [G loss: 3.117782]\n",
            "7800 [D loss: 0.573504, acc.: 68.36%] [G loss: 2.861039]\n",
            "7820 [D loss: 0.473523, acc.: 78.91%] [G loss: 3.123185]\n",
            "7840 [D loss: 0.575520, acc.: 73.44%] [G loss: 3.023972]\n",
            "7860 [D loss: 0.479203, acc.: 77.34%] [G loss: 3.593217]\n",
            "7880 [D loss: 0.544040, acc.: 71.48%] [G loss: 3.240446]\n",
            "7900 [D loss: 0.554345, acc.: 71.88%] [G loss: 3.043216]\n",
            "7920 [D loss: 0.537524, acc.: 74.61%] [G loss: 2.952210]\n",
            "7940 [D loss: 0.509553, acc.: 72.27%] [G loss: 3.165804]\n",
            "7960 [D loss: 0.528373, acc.: 72.27%] [G loss: 3.181046]\n",
            "7980 [D loss: 0.518755, acc.: 73.05%] [G loss: 3.298374]\n",
            "8000 [D loss: 0.578102, acc.: 67.58%] [G loss: 3.085395]\n",
            "8020 [D loss: 0.549702, acc.: 73.83%] [G loss: 3.121454]\n",
            "8040 [D loss: 0.591368, acc.: 66.02%] [G loss: 2.695958]\n",
            "8060 [D loss: 0.552246, acc.: 71.48%] [G loss: 2.814682]\n",
            "8080 [D loss: 0.527140, acc.: 75.00%] [G loss: 2.960243]\n",
            "8100 [D loss: 0.437059, acc.: 83.20%] [G loss: 2.271298]\n",
            "8120 [D loss: 0.224115, acc.: 96.09%] [G loss: 2.329252]\n",
            "8140 [D loss: 0.277735, acc.: 92.58%] [G loss: 2.037361]\n",
            "8160 [D loss: 0.660083, acc.: 66.02%] [G loss: 2.005924]\n",
            "8180 [D loss: 1.592819, acc.: 17.58%] [G loss: 1.760342]\n",
            "8200 [D loss: 1.067518, acc.: 33.20%] [G loss: 2.909509]\n",
            "8220 [D loss: 1.044893, acc.: 34.77%] [G loss: 2.584385]\n",
            "8240 [D loss: 0.689234, acc.: 60.94%] [G loss: 3.306798]\n",
            "8260 [D loss: 0.778539, acc.: 55.86%] [G loss: 2.183171]\n",
            "8280 [D loss: 0.562990, acc.: 71.48%] [G loss: 2.542121]\n",
            "8300 [D loss: 0.742763, acc.: 58.59%] [G loss: 3.514839]\n",
            "8320 [D loss: 0.549733, acc.: 74.61%] [G loss: 3.068052]\n",
            "8340 [D loss: 0.605815, acc.: 67.97%] [G loss: 2.682636]\n",
            "8360 [D loss: 0.596577, acc.: 67.97%] [G loss: 3.174010]\n",
            "8380 [D loss: 0.511432, acc.: 75.39%] [G loss: 3.214810]\n",
            "8400 [D loss: 0.608826, acc.: 64.84%] [G loss: 3.005487]\n",
            "8420 [D loss: 0.489785, acc.: 78.12%] [G loss: 3.206137]\n",
            "8440 [D loss: 0.564367, acc.: 70.31%] [G loss: 2.886457]\n",
            "8460 [D loss: 0.459230, acc.: 80.47%] [G loss: 3.235763]\n",
            "8480 [D loss: 0.528094, acc.: 72.66%] [G loss: 3.110570]\n",
            "8500 [D loss: 0.581084, acc.: 70.31%] [G loss: 3.140874]\n",
            "8520 [D loss: 0.478944, acc.: 76.95%] [G loss: 3.118637]\n",
            "8540 [D loss: 0.539488, acc.: 71.09%] [G loss: 2.992033]\n",
            "8560 [D loss: 0.554723, acc.: 70.70%] [G loss: 2.884771]\n",
            "8580 [D loss: 0.517846, acc.: 77.34%] [G loss: 3.158133]\n",
            "8600 [D loss: 0.569036, acc.: 74.22%] [G loss: 3.063802]\n",
            "8620 [D loss: 0.446647, acc.: 81.25%] [G loss: 3.331660]\n",
            "8640 [D loss: 0.502164, acc.: 75.00%] [G loss: 3.200024]\n",
            "8660 [D loss: 0.477904, acc.: 80.47%] [G loss: 3.644629]\n",
            "8680 [D loss: 0.473034, acc.: 81.25%] [G loss: 3.379457]\n",
            "8700 [D loss: 0.612472, acc.: 66.80%] [G loss: 2.980698]\n",
            "8720 [D loss: 0.537859, acc.: 70.70%] [G loss: 2.864402]\n",
            "8740 [D loss: 0.598940, acc.: 69.53%] [G loss: 2.750215]\n",
            "8760 [D loss: 0.560402, acc.: 74.22%] [G loss: 2.800817]\n",
            "8780 [D loss: 0.557820, acc.: 69.92%] [G loss: 3.221340]\n",
            "8800 [D loss: 0.514047, acc.: 74.22%] [G loss: 3.275917]\n",
            "8820 [D loss: 0.497290, acc.: 77.34%] [G loss: 3.261922]\n",
            "8840 [D loss: 0.587577, acc.: 69.92%] [G loss: 3.026378]\n",
            "8860 [D loss: 0.488223, acc.: 76.17%] [G loss: 3.381924]\n",
            "8880 [D loss: 0.529359, acc.: 75.78%] [G loss: 3.071657]\n",
            "8900 [D loss: 0.557625, acc.: 69.92%] [G loss: 3.106518]\n",
            "8920 [D loss: 0.536380, acc.: 75.00%] [G loss: 2.980546]\n",
            "8940 [D loss: 0.560429, acc.: 69.14%] [G loss: 2.831224]\n",
            "8960 [D loss: 0.582080, acc.: 69.53%] [G loss: 2.799399]\n",
            "8980 [D loss: 0.522207, acc.: 75.00%] [G loss: 3.237023]\n",
            "9000 [D loss: 0.615044, acc.: 70.31%] [G loss: 2.939059]\n",
            "9020 [D loss: 0.516772, acc.: 75.00%] [G loss: 2.999931]\n",
            "9040 [D loss: 0.506777, acc.: 72.66%] [G loss: 3.069099]\n",
            "9060 [D loss: 0.566404, acc.: 69.92%] [G loss: 3.044225]\n",
            "9080 [D loss: 0.567616, acc.: 71.09%] [G loss: 2.911717]\n",
            "9100 [D loss: 0.603060, acc.: 65.62%] [G loss: 2.898261]\n",
            "9120 [D loss: 0.500109, acc.: 76.17%] [G loss: 3.085845]\n",
            "9140 [D loss: 0.452239, acc.: 81.64%] [G loss: 3.444245]\n",
            "9160 [D loss: 0.445263, acc.: 83.59%] [G loss: 3.245305]\n",
            "9180 [D loss: 0.502903, acc.: 75.78%] [G loss: 3.096669]\n",
            "9200 [D loss: 0.496000, acc.: 77.73%] [G loss: 3.100526]\n",
            "9220 [D loss: 0.505429, acc.: 76.56%] [G loss: 3.143410]\n",
            "9240 [D loss: 0.517350, acc.: 76.17%] [G loss: 3.272176]\n",
            "9260 [D loss: 0.524266, acc.: 74.22%] [G loss: 3.061450]\n",
            "9280 [D loss: 0.581010, acc.: 69.92%] [G loss: 2.949874]\n",
            "9300 [D loss: 0.606518, acc.: 69.14%] [G loss: 3.227669]\n",
            "9320 [D loss: 0.543199, acc.: 72.27%] [G loss: 3.133572]\n",
            "9340 [D loss: 0.559276, acc.: 71.48%] [G loss: 2.917003]\n",
            "9360 [D loss: 0.495243, acc.: 77.73%] [G loss: 3.210256]\n",
            "9380 [D loss: 0.509449, acc.: 72.66%] [G loss: 3.034904]\n",
            "9400 [D loss: 0.449236, acc.: 80.86%] [G loss: 3.680955]\n",
            "9420 [D loss: 0.553917, acc.: 69.92%] [G loss: 3.184403]\n",
            "9440 [D loss: 0.514038, acc.: 74.61%] [G loss: 3.248035]\n",
            "9460 [D loss: 0.553462, acc.: 72.66%] [G loss: 3.140285]\n",
            "9480 [D loss: 0.496471, acc.: 75.39%] [G loss: 3.261702]\n",
            "9500 [D loss: 0.467178, acc.: 75.39%] [G loss: 3.315966]\n",
            "9520 [D loss: 0.670597, acc.: 60.94%] [G loss: 2.991915]\n",
            "9540 [D loss: 0.497898, acc.: 78.12%] [G loss: 3.276913]\n",
            "9560 [D loss: 0.480163, acc.: 78.91%] [G loss: 3.441546]\n",
            "9580 [D loss: 0.520574, acc.: 75.78%] [G loss: 3.181209]\n",
            "9600 [D loss: 0.599856, acc.: 71.09%] [G loss: 2.995785]\n",
            "9620 [D loss: 0.453516, acc.: 80.86%] [G loss: 2.735135]\n",
            "9640 [D loss: 0.498114, acc.: 77.73%] [G loss: 2.723907]\n",
            "9660 [D loss: 0.484949, acc.: 80.86%] [G loss: 2.306581]\n",
            "9680 [D loss: 1.215271, acc.: 34.38%] [G loss: 3.628906]\n",
            "9700 [D loss: 1.011733, acc.: 38.28%] [G loss: 2.797241]\n",
            "9720 [D loss: 0.942530, acc.: 42.19%] [G loss: 2.334705]\n",
            "9740 [D loss: 0.745157, acc.: 58.98%] [G loss: 2.739344]\n",
            "9760 [D loss: 0.741862, acc.: 56.64%] [G loss: 2.562932]\n",
            "9780 [D loss: 0.640108, acc.: 65.23%] [G loss: 3.252892]\n",
            "9800 [D loss: 0.379060, acc.: 83.20%] [G loss: 4.309171]\n",
            "9820 [D loss: 0.567707, acc.: 72.66%] [G loss: 3.867734]\n",
            "9840 [D loss: 0.466145, acc.: 79.69%] [G loss: 3.661878]\n",
            "9860 [D loss: 0.615991, acc.: 67.97%] [G loss: 3.270372]\n",
            "9880 [D loss: 0.408268, acc.: 82.03%] [G loss: 3.483874]\n",
            "9900 [D loss: 0.589351, acc.: 71.88%] [G loss: 2.955650]\n",
            "9920 [D loss: 0.480689, acc.: 77.73%] [G loss: 3.439785]\n",
            "9940 [D loss: 0.622816, acc.: 62.89%] [G loss: 2.429463]\n",
            "9960 [D loss: 0.621643, acc.: 66.02%] [G loss: 2.603937]\n",
            "9980 [D loss: 0.604789, acc.: 67.19%] [G loss: 2.482276]\n",
            "10000 [D loss: 0.556056, acc.: 71.48%] [G loss: 2.670682]\n",
            "10020 [D loss: 0.585698, acc.: 70.31%] [G loss: 2.450370]\n",
            "10040 [D loss: 0.600583, acc.: 69.14%] [G loss: 2.449546]\n",
            "10060 [D loss: 0.577659, acc.: 68.36%] [G loss: 2.468548]\n",
            "10080 [D loss: 0.650031, acc.: 64.84%] [G loss: 2.365372]\n",
            "10100 [D loss: 0.549700, acc.: 71.48%] [G loss: 2.543775]\n",
            "10120 [D loss: 0.615218, acc.: 68.36%] [G loss: 2.399592]\n",
            "10140 [D loss: 0.547771, acc.: 74.61%] [G loss: 2.614799]\n",
            "10160 [D loss: 0.634853, acc.: 60.16%] [G loss: 2.167450]\n",
            "10180 [D loss: 0.582990, acc.: 68.36%] [G loss: 2.518771]\n",
            "10200 [D loss: 0.618923, acc.: 65.62%] [G loss: 2.571146]\n",
            "10220 [D loss: 0.674758, acc.: 60.94%] [G loss: 2.287638]\n",
            "10240 [D loss: 0.583485, acc.: 70.31%] [G loss: 2.426923]\n",
            "10260 [D loss: 0.580618, acc.: 68.36%] [G loss: 2.455193]\n",
            "10280 [D loss: 0.581300, acc.: 68.75%] [G loss: 2.437289]\n",
            "10300 [D loss: 0.556167, acc.: 73.05%] [G loss: 2.506979]\n",
            "10320 [D loss: 0.615251, acc.: 68.36%] [G loss: 2.514368]\n",
            "10340 [D loss: 0.548598, acc.: 73.44%] [G loss: 2.387963]\n",
            "10360 [D loss: 0.605066, acc.: 66.41%] [G loss: 2.452006]\n",
            "10380 [D loss: 0.622954, acc.: 66.02%] [G loss: 2.324791]\n",
            "10400 [D loss: 0.543472, acc.: 75.39%] [G loss: 2.526646]\n",
            "10420 [D loss: 0.613763, acc.: 64.84%] [G loss: 2.476712]\n",
            "10440 [D loss: 0.644237, acc.: 63.28%] [G loss: 2.398672]\n",
            "10460 [D loss: 0.553710, acc.: 73.05%] [G loss: 2.021445]\n",
            "10480 [D loss: 0.323192, acc.: 92.97%] [G loss: 2.234848]\n",
            "10500 [D loss: 0.389392, acc.: 87.89%] [G loss: 1.939262]\n",
            "10520 [D loss: 0.690379, acc.: 62.50%] [G loss: 1.679284]\n",
            "10540 [D loss: 0.656508, acc.: 62.50%] [G loss: 1.331096]\n",
            "10560 [D loss: 0.870297, acc.: 50.78%] [G loss: 1.369135]\n",
            "10580 [D loss: 0.736797, acc.: 54.30%] [G loss: 2.502402]\n",
            "10600 [D loss: 0.671439, acc.: 59.77%] [G loss: 2.751114]\n",
            "10620 [D loss: 0.660033, acc.: 62.89%] [G loss: 2.556337]\n",
            "10640 [D loss: 0.504881, acc.: 74.61%] [G loss: 3.088126]\n",
            "10660 [D loss: 0.733440, acc.: 55.86%] [G loss: 2.372828]\n",
            "10680 [D loss: 0.614682, acc.: 64.45%] [G loss: 2.588113]\n",
            "10700 [D loss: 0.541597, acc.: 72.66%] [G loss: 2.492619]\n",
            "10720 [D loss: 0.661740, acc.: 61.72%] [G loss: 2.366631]\n",
            "10740 [D loss: 0.559204, acc.: 71.88%] [G loss: 2.518898]\n",
            "10760 [D loss: 0.594406, acc.: 67.97%] [G loss: 2.366124]\n",
            "10780 [D loss: 0.576653, acc.: 68.36%] [G loss: 2.639050]\n",
            "10800 [D loss: 0.615429, acc.: 66.02%] [G loss: 2.589082]\n",
            "10820 [D loss: 0.550398, acc.: 71.48%] [G loss: 2.708935]\n",
            "10840 [D loss: 0.568395, acc.: 71.88%] [G loss: 2.368248]\n",
            "10860 [D loss: 0.517541, acc.: 72.66%] [G loss: 2.679492]\n",
            "10880 [D loss: 0.580425, acc.: 68.75%] [G loss: 2.452003]\n",
            "10900 [D loss: 0.567043, acc.: 71.09%] [G loss: 2.519935]\n",
            "10920 [D loss: 0.529608, acc.: 74.61%] [G loss: 2.786145]\n",
            "10940 [D loss: 0.563366, acc.: 73.05%] [G loss: 2.722923]\n",
            "10960 [D loss: 0.663168, acc.: 61.33%] [G loss: 2.425000]\n",
            "10980 [D loss: 0.579796, acc.: 70.31%] [G loss: 2.533625]\n",
            "11000 [D loss: 0.539188, acc.: 74.22%] [G loss: 2.628109]\n",
            "11020 [D loss: 0.534964, acc.: 74.22%] [G loss: 2.695542]\n",
            "11040 [D loss: 0.533828, acc.: 75.78%] [G loss: 2.826753]\n",
            "11060 [D loss: 0.586242, acc.: 68.36%] [G loss: 2.467298]\n",
            "11080 [D loss: 0.576589, acc.: 70.31%] [G loss: 2.491124]\n",
            "11100 [D loss: 0.625870, acc.: 64.84%] [G loss: 2.499760]\n",
            "11120 [D loss: 0.555031, acc.: 71.88%] [G loss: 2.741103]\n",
            "11140 [D loss: 0.559574, acc.: 72.27%] [G loss: 2.557565]\n",
            "11160 [D loss: 0.571171, acc.: 69.92%] [G loss: 2.681203]\n",
            "11180 [D loss: 0.520910, acc.: 73.83%] [G loss: 2.698501]\n",
            "11200 [D loss: 0.664147, acc.: 61.72%] [G loss: 2.351937]\n",
            "11220 [D loss: 0.561990, acc.: 71.09%] [G loss: 2.644462]\n",
            "11240 [D loss: 0.551815, acc.: 71.48%] [G loss: 2.656615]\n",
            "11260 [D loss: 0.565082, acc.: 69.92%] [G loss: 2.480448]\n",
            "11280 [D loss: 0.518493, acc.: 75.78%] [G loss: 2.804416]\n",
            "11300 [D loss: 0.516635, acc.: 74.22%] [G loss: 2.573274]\n",
            "11320 [D loss: 0.586935, acc.: 72.66%] [G loss: 2.546521]\n",
            "11340 [D loss: 0.563033, acc.: 73.44%] [G loss: 2.817175]\n",
            "11360 [D loss: 0.595024, acc.: 69.92%] [G loss: 2.552423]\n",
            "11380 [D loss: 0.544317, acc.: 74.22%] [G loss: 2.723155]\n",
            "11400 [D loss: 0.569332, acc.: 71.88%] [G loss: 2.770822]\n",
            "11420 [D loss: 0.566935, acc.: 68.36%] [G loss: 2.727065]\n",
            "11440 [D loss: 0.504451, acc.: 77.34%] [G loss: 2.902306]\n",
            "11460 [D loss: 0.586071, acc.: 68.75%] [G loss: 2.470899]\n",
            "11480 [D loss: 0.596983, acc.: 69.53%] [G loss: 2.620968]\n",
            "11500 [D loss: 0.529309, acc.: 74.61%] [G loss: 2.865886]\n",
            "11520 [D loss: 0.530795, acc.: 72.66%] [G loss: 2.836254]\n",
            "11540 [D loss: 0.532839, acc.: 73.83%] [G loss: 2.548162]\n",
            "11560 [D loss: 0.333188, acc.: 91.41%] [G loss: 3.510848]\n",
            "11580 [D loss: 0.220713, acc.: 98.44%] [G loss: 2.077978]\n",
            "11600 [D loss: 0.492050, acc.: 78.12%] [G loss: 1.902450]\n",
            "11620 [D loss: 0.665812, acc.: 64.06%] [G loss: 1.460553]\n",
            "11640 [D loss: 1.492861, acc.: 25.39%] [G loss: 2.346267]\n",
            "11660 [D loss: 0.740040, acc.: 57.81%] [G loss: 2.857303]\n",
            "11680 [D loss: 0.788963, acc.: 56.25%] [G loss: 3.007245]\n",
            "11700 [D loss: 0.581080, acc.: 74.61%] [G loss: 3.150152]\n",
            "11720 [D loss: 0.492976, acc.: 78.12%] [G loss: 3.229701]\n",
            "11740 [D loss: 0.559554, acc.: 70.70%] [G loss: 2.850891]\n",
            "11760 [D loss: 0.587928, acc.: 68.75%] [G loss: 2.948276]\n",
            "11780 [D loss: 0.594953, acc.: 69.92%] [G loss: 2.744600]\n",
            "11800 [D loss: 0.532782, acc.: 73.05%] [G loss: 2.898841]\n",
            "11820 [D loss: 0.589233, acc.: 69.53%] [G loss: 2.707985]\n",
            "11840 [D loss: 0.587871, acc.: 69.53%] [G loss: 2.777072]\n",
            "11860 [D loss: 0.660606, acc.: 64.45%] [G loss: 2.515131]\n",
            "11880 [D loss: 0.536813, acc.: 73.05%] [G loss: 2.743800]\n",
            "11900 [D loss: 0.526107, acc.: 74.22%] [G loss: 2.927455]\n",
            "11920 [D loss: 0.530879, acc.: 70.70%] [G loss: 2.761687]\n",
            "11940 [D loss: 0.503514, acc.: 78.12%] [G loss: 2.979467]\n",
            "11960 [D loss: 0.611364, acc.: 65.62%] [G loss: 2.699605]\n",
            "11980 [D loss: 0.554563, acc.: 72.27%] [G loss: 2.757446]\n",
            "12000 [D loss: 0.587532, acc.: 70.70%] [G loss: 2.862464]\n",
            "12020 [D loss: 0.556066, acc.: 72.66%] [G loss: 2.837598]\n",
            "12040 [D loss: 0.566438, acc.: 69.92%] [G loss: 2.878350]\n",
            "12060 [D loss: 0.580965, acc.: 71.09%] [G loss: 2.967974]\n",
            "12080 [D loss: 0.529417, acc.: 73.05%] [G loss: 2.811473]\n",
            "12100 [D loss: 0.545707, acc.: 70.70%] [G loss: 2.724492]\n",
            "12120 [D loss: 0.545488, acc.: 73.44%] [G loss: 3.019100]\n",
            "12140 [D loss: 0.549770, acc.: 72.66%] [G loss: 3.055489]\n",
            "12160 [D loss: 0.495905, acc.: 75.78%] [G loss: 2.886868]\n",
            "12180 [D loss: 0.524818, acc.: 73.44%] [G loss: 3.050236]\n",
            "12200 [D loss: 0.564301, acc.: 67.97%] [G loss: 2.907444]\n",
            "12220 [D loss: 0.550411, acc.: 75.39%] [G loss: 2.692843]\n",
            "12240 [D loss: 0.545396, acc.: 72.27%] [G loss: 3.057083]\n",
            "12260 [D loss: 0.541676, acc.: 72.27%] [G loss: 3.069217]\n",
            "12280 [D loss: 0.586155, acc.: 70.70%] [G loss: 2.652617]\n",
            "12300 [D loss: 0.504786, acc.: 77.34%] [G loss: 3.017223]\n",
            "12320 [D loss: 0.533785, acc.: 74.22%] [G loss: 3.075903]\n",
            "12340 [D loss: 0.536593, acc.: 73.44%] [G loss: 3.036658]\n",
            "12360 [D loss: 0.528333, acc.: 74.22%] [G loss: 2.815980]\n",
            "12380 [D loss: 0.555358, acc.: 69.14%] [G loss: 3.019434]\n",
            "12400 [D loss: 0.545240, acc.: 74.22%] [G loss: 3.048965]\n",
            "12420 [D loss: 0.538733, acc.: 69.14%] [G loss: 2.848561]\n",
            "12440 [D loss: 0.541494, acc.: 73.05%] [G loss: 2.815517]\n",
            "12460 [D loss: 0.555186, acc.: 71.09%] [G loss: 2.988440]\n",
            "12480 [D loss: 0.491771, acc.: 78.12%] [G loss: 2.982701]\n",
            "12500 [D loss: 0.566285, acc.: 71.48%] [G loss: 3.079874]\n",
            "12520 [D loss: 0.479686, acc.: 76.95%] [G loss: 3.107335]\n",
            "12540 [D loss: 0.502862, acc.: 77.73%] [G loss: 3.117058]\n",
            "12560 [D loss: 0.575910, acc.: 66.41%] [G loss: 3.175883]\n",
            "12580 [D loss: 0.499637, acc.: 75.00%] [G loss: 3.070124]\n",
            "12600 [D loss: 0.517502, acc.: 74.61%] [G loss: 3.279170]\n",
            "12620 [D loss: 0.514505, acc.: 75.78%] [G loss: 3.074044]\n",
            "12640 [D loss: 0.534963, acc.: 74.22%] [G loss: 3.185006]\n",
            "12660 [D loss: 0.551883, acc.: 69.92%] [G loss: 3.104226]\n",
            "12680 [D loss: 0.519814, acc.: 73.44%] [G loss: 3.266816]\n",
            "12700 [D loss: 0.548990, acc.: 68.75%] [G loss: 2.917577]\n",
            "12720 [D loss: 0.499998, acc.: 76.56%] [G loss: 3.152283]\n",
            "12740 [D loss: 0.493605, acc.: 77.34%] [G loss: 3.274895]\n",
            "12760 [D loss: 0.572212, acc.: 69.14%] [G loss: 2.902751]\n",
            "12780 [D loss: 0.532575, acc.: 74.22%] [G loss: 3.148187]\n",
            "12800 [D loss: 0.482073, acc.: 76.95%] [G loss: 3.406129]\n",
            "12820 [D loss: 0.499271, acc.: 74.22%] [G loss: 3.068045]\n",
            "12840 [D loss: 0.540345, acc.: 72.27%] [G loss: 3.278505]\n",
            "12860 [D loss: 0.497112, acc.: 75.39%] [G loss: 3.205802]\n",
            "12880 [D loss: 0.438892, acc.: 82.42%] [G loss: 3.268667]\n",
            "12900 [D loss: 0.500916, acc.: 77.34%] [G loss: 3.266642]\n",
            "12920 [D loss: 0.598853, acc.: 65.23%] [G loss: 2.983240]\n",
            "12940 [D loss: 0.465349, acc.: 80.08%] [G loss: 3.555751]\n",
            "12960 [D loss: 0.482213, acc.: 79.69%] [G loss: 3.269861]\n",
            "12980 [D loss: 0.550317, acc.: 73.05%] [G loss: 2.995450]\n",
            "13000 [D loss: 0.529175, acc.: 74.61%] [G loss: 3.131307]\n",
            "13020 [D loss: 0.501609, acc.: 76.95%] [G loss: 3.269964]\n",
            "13040 [D loss: 0.521582, acc.: 76.17%] [G loss: 2.988777]\n",
            "13060 [D loss: 0.454039, acc.: 78.91%] [G loss: 3.310795]\n",
            "13080 [D loss: 0.592397, acc.: 67.19%] [G loss: 2.971127]\n",
            "13100 [D loss: 0.384516, acc.: 85.94%] [G loss: 2.207126]\n",
            "13120 [D loss: 0.824649, acc.: 53.52%] [G loss: 2.892508]\n",
            "13140 [D loss: 0.644399, acc.: 63.28%] [G loss: 3.581767]\n",
            "13160 [D loss: 0.663053, acc.: 66.41%] [G loss: 3.271734]\n",
            "13180 [D loss: 0.300534, acc.: 91.41%] [G loss: 2.597092]\n",
            "13200 [D loss: 0.450857, acc.: 78.52%] [G loss: 1.810591]\n",
            "13220 [D loss: 1.413657, acc.: 31.64%] [G loss: 1.967619]\n",
            "13240 [D loss: 1.116148, acc.: 40.62%] [G loss: 3.530558]\n",
            "13260 [D loss: 0.774270, acc.: 59.77%] [G loss: 4.115308]\n",
            "13280 [D loss: 0.541596, acc.: 71.88%] [G loss: 2.770442]\n",
            "13300 [D loss: 0.584729, acc.: 69.53%] [G loss: 2.527215]\n",
            "13320 [D loss: 0.736541, acc.: 56.64%] [G loss: 2.899766]\n",
            "13340 [D loss: 0.415731, acc.: 83.20%] [G loss: 3.844957]\n",
            "13360 [D loss: 0.676180, acc.: 61.33%] [G loss: 3.076269]\n",
            "13380 [D loss: 0.376188, acc.: 86.33%] [G loss: 3.887544]\n",
            "13400 [D loss: 0.522538, acc.: 75.78%] [G loss: 3.710353]\n",
            "13420 [D loss: 0.507183, acc.: 75.78%] [G loss: 3.223972]\n",
            "13440 [D loss: 0.499287, acc.: 75.78%] [G loss: 3.535984]\n",
            "13460 [D loss: 0.432739, acc.: 79.30%] [G loss: 3.596563]\n",
            "13480 [D loss: 0.569806, acc.: 68.36%] [G loss: 3.417174]\n",
            "13500 [D loss: 0.487363, acc.: 78.52%] [G loss: 3.196985]\n",
            "13520 [D loss: 0.503545, acc.: 75.00%] [G loss: 3.234942]\n",
            "13540 [D loss: 0.464496, acc.: 79.69%] [G loss: 3.421081]\n",
            "13560 [D loss: 0.504980, acc.: 76.56%] [G loss: 3.346558]\n",
            "13580 [D loss: 0.573984, acc.: 71.88%] [G loss: 3.217370]\n",
            "13600 [D loss: 0.443885, acc.: 80.86%] [G loss: 3.665757]\n",
            "13620 [D loss: 0.556314, acc.: 73.83%] [G loss: 3.176795]\n",
            "13640 [D loss: 0.551338, acc.: 70.31%] [G loss: 3.315272]\n",
            "13660 [D loss: 0.518853, acc.: 74.22%] [G loss: 3.497323]\n",
            "13680 [D loss: 0.518639, acc.: 73.05%] [G loss: 3.399439]\n",
            "13700 [D loss: 0.472557, acc.: 79.69%] [G loss: 3.593258]\n",
            "13720 [D loss: 0.512192, acc.: 76.17%] [G loss: 3.395924]\n",
            "13740 [D loss: 0.489502, acc.: 74.22%] [G loss: 3.589009]\n",
            "13760 [D loss: 0.440117, acc.: 78.91%] [G loss: 3.619289]\n",
            "13780 [D loss: 0.597121, acc.: 66.80%] [G loss: 3.088511]\n",
            "13800 [D loss: 0.508458, acc.: 75.39%] [G loss: 3.588322]\n",
            "13820 [D loss: 0.452269, acc.: 80.47%] [G loss: 3.549728]\n",
            "13840 [D loss: 0.569538, acc.: 68.75%] [G loss: 3.173690]\n",
            "13860 [D loss: 0.453431, acc.: 80.86%] [G loss: 3.638789]\n",
            "13880 [D loss: 0.538663, acc.: 71.09%] [G loss: 3.256326]\n",
            "13900 [D loss: 0.454311, acc.: 81.64%] [G loss: 3.684909]\n",
            "13920 [D loss: 0.381683, acc.: 85.55%] [G loss: 3.885673]\n",
            "13940 [D loss: 0.538027, acc.: 71.48%] [G loss: 3.509828]\n",
            "13960 [D loss: 0.462696, acc.: 76.56%] [G loss: 3.664763]\n",
            "13980 [D loss: 0.450089, acc.: 77.73%] [G loss: 3.840251]\n",
            "14000 [D loss: 0.542160, acc.: 71.09%] [G loss: 3.434824]\n",
            "14020 [D loss: 0.442598, acc.: 82.81%] [G loss: 3.629861]\n",
            "14040 [D loss: 0.415695, acc.: 82.81%] [G loss: 3.802679]\n",
            "14060 [D loss: 0.538296, acc.: 72.66%] [G loss: 3.364664]\n",
            "14080 [D loss: 0.472608, acc.: 76.95%] [G loss: 3.746163]\n",
            "14100 [D loss: 0.475384, acc.: 76.17%] [G loss: 3.902348]\n",
            "14120 [D loss: 0.500722, acc.: 72.66%] [G loss: 3.433594]\n",
            "14140 [D loss: 0.426336, acc.: 83.20%] [G loss: 3.990406]\n",
            "14160 [D loss: 0.476921, acc.: 78.12%] [G loss: 3.658955]\n",
            "14180 [D loss: 0.538800, acc.: 71.88%] [G loss: 3.508848]\n",
            "14200 [D loss: 0.422263, acc.: 81.64%] [G loss: 2.884894]\n",
            "14220 [D loss: 0.442745, acc.: 82.03%] [G loss: 2.597464]\n",
            "14240 [D loss: 0.583609, acc.: 67.19%] [G loss: 2.584501]\n",
            "14260 [D loss: 0.558727, acc.: 70.70%] [G loss: 4.499975]\n",
            "14280 [D loss: 0.721034, acc.: 61.72%] [G loss: 3.165493]\n",
            "14300 [D loss: 0.463852, acc.: 76.17%] [G loss: 3.978736]\n",
            "14320 [D loss: 0.634867, acc.: 66.80%] [G loss: 3.519205]\n",
            "14340 [D loss: 0.589743, acc.: 71.88%] [G loss: 3.658075]\n",
            "14360 [D loss: 0.456640, acc.: 80.86%] [G loss: 3.866583]\n",
            "14380 [D loss: 0.610524, acc.: 67.19%] [G loss: 3.660098]\n",
            "14400 [D loss: 0.420315, acc.: 83.98%] [G loss: 4.320860]\n",
            "14420 [D loss: 0.541899, acc.: 73.83%] [G loss: 3.342916]\n",
            "14440 [D loss: 0.411559, acc.: 82.81%] [G loss: 3.844027]\n",
            "14460 [D loss: 0.484254, acc.: 77.73%] [G loss: 3.900236]\n",
            "14480 [D loss: 0.543187, acc.: 72.66%] [G loss: 3.653971]\n",
            "14500 [D loss: 0.483191, acc.: 75.00%] [G loss: 3.671592]\n",
            "14520 [D loss: 0.481760, acc.: 79.69%] [G loss: 3.577200]\n",
            "14540 [D loss: 0.477566, acc.: 79.30%] [G loss: 3.997219]\n",
            "14560 [D loss: 0.563881, acc.: 73.83%] [G loss: 3.534985]\n",
            "14580 [D loss: 0.563550, acc.: 71.09%] [G loss: 3.672464]\n",
            "14600 [D loss: 0.466632, acc.: 77.73%] [G loss: 3.561044]\n",
            "14620 [D loss: 0.455796, acc.: 82.42%] [G loss: 3.644745]\n",
            "14640 [D loss: 0.433931, acc.: 81.25%] [G loss: 3.959833]\n",
            "14660 [D loss: 0.451665, acc.: 78.91%] [G loss: 3.834344]\n",
            "14680 [D loss: 0.474238, acc.: 81.64%] [G loss: 3.588170]\n",
            "14700 [D loss: 0.490202, acc.: 76.95%] [G loss: 3.740617]\n",
            "14720 [D loss: 0.456453, acc.: 79.69%] [G loss: 4.030152]\n",
            "14740 [D loss: 0.480814, acc.: 78.12%] [G loss: 3.768783]\n",
            "14760 [D loss: 0.480983, acc.: 76.56%] [G loss: 4.132136]\n",
            "14780 [D loss: 0.523182, acc.: 76.56%] [G loss: 3.436683]\n",
            "14800 [D loss: 0.475291, acc.: 76.17%] [G loss: 3.695091]\n",
            "14820 [D loss: 0.454085, acc.: 77.73%] [G loss: 4.099783]\n",
            "14840 [D loss: 0.430844, acc.: 80.86%] [G loss: 3.766231]\n",
            "14860 [D loss: 0.485859, acc.: 75.78%] [G loss: 3.849741]\n",
            "14880 [D loss: 0.404419, acc.: 82.03%] [G loss: 4.101513]\n",
            "14900 [D loss: 0.455885, acc.: 78.52%] [G loss: 3.883560]\n",
            "14920 [D loss: 0.459468, acc.: 78.91%] [G loss: 3.852487]\n",
            "14940 [D loss: 0.434179, acc.: 80.86%] [G loss: 3.968769]\n",
            "14960 [D loss: 0.515407, acc.: 74.22%] [G loss: 3.701086]\n",
            "14980 [D loss: 0.489031, acc.: 79.30%] [G loss: 3.677482]\n",
            "15000 [D loss: 0.502319, acc.: 74.22%] [G loss: 3.731939]\n",
            "15020 [D loss: 0.391362, acc.: 82.81%] [G loss: 4.371559]\n",
            "15040 [D loss: 0.485064, acc.: 79.30%] [G loss: 3.759503]\n",
            "15060 [D loss: 0.583971, acc.: 70.70%] [G loss: 3.510099]\n",
            "15080 [D loss: 0.451004, acc.: 78.52%] [G loss: 4.045126]\n",
            "15100 [D loss: 0.565561, acc.: 72.66%] [G loss: 3.736769]\n",
            "15120 [D loss: 0.516709, acc.: 76.95%] [G loss: 3.903764]\n",
            "15140 [D loss: 0.454704, acc.: 81.25%] [G loss: 3.696895]\n",
            "15160 [D loss: 0.488619, acc.: 76.17%] [G loss: 3.964455]\n",
            "15180 [D loss: 0.441082, acc.: 81.25%] [G loss: 3.770135]\n",
            "15200 [D loss: 0.450490, acc.: 78.91%] [G loss: 4.328911]\n",
            "15220 [D loss: 0.540409, acc.: 71.09%] [G loss: 3.736720]\n",
            "15240 [D loss: 0.454970, acc.: 78.91%] [G loss: 3.904644]\n",
            "15260 [D loss: 0.464822, acc.: 78.12%] [G loss: 3.991132]\n",
            "15280 [D loss: 0.491833, acc.: 78.12%] [G loss: 3.625486]\n",
            "15300 [D loss: 0.408772, acc.: 79.30%] [G loss: 4.003177]\n",
            "15320 [D loss: 0.487761, acc.: 77.73%] [G loss: 3.620228]\n",
            "15340 [D loss: 0.482656, acc.: 75.78%] [G loss: 3.804876]\n",
            "15360 [D loss: 0.495319, acc.: 76.17%] [G loss: 3.805184]\n",
            "15380 [D loss: 0.541735, acc.: 73.05%] [G loss: 3.818195]\n",
            "15400 [D loss: 0.438087, acc.: 79.30%] [G loss: 4.212593]\n",
            "15420 [D loss: 0.423688, acc.: 80.08%] [G loss: 3.905172]\n",
            "15440 [D loss: 0.458355, acc.: 78.52%] [G loss: 3.736654]\n",
            "15460 [D loss: 0.540937, acc.: 72.66%] [G loss: 3.663964]\n",
            "15480 [D loss: 0.465398, acc.: 78.91%] [G loss: 4.099555]\n",
            "15500 [D loss: 0.458022, acc.: 78.12%] [G loss: 4.256845]\n",
            "15520 [D loss: 0.458442, acc.: 77.73%] [G loss: 3.925918]\n",
            "15540 [D loss: 0.450313, acc.: 79.69%] [G loss: 4.002153]\n",
            "15560 [D loss: 0.521403, acc.: 74.22%] [G loss: 3.884771]\n",
            "15580 [D loss: 0.419243, acc.: 81.64%] [G loss: 3.905304]\n",
            "15600 [D loss: 0.461885, acc.: 81.25%] [G loss: 4.139614]\n",
            "15620 [D loss: 0.478636, acc.: 76.56%] [G loss: 3.804355]\n",
            "15640 [D loss: 0.477927, acc.: 81.64%] [G loss: 3.805360]\n",
            "15660 [D loss: 0.416424, acc.: 84.38%] [G loss: 3.989102]\n",
            "15680 [D loss: 0.521052, acc.: 76.56%] [G loss: 3.689693]\n",
            "15700 [D loss: 0.720543, acc.: 64.45%] [G loss: 3.849956]\n",
            "15720 [D loss: 0.602187, acc.: 67.58%] [G loss: 2.797967]\n",
            "15740 [D loss: 0.498599, acc.: 75.78%] [G loss: 2.803997]\n",
            "15760 [D loss: 0.539980, acc.: 72.27%] [G loss: 4.402431]\n",
            "15780 [D loss: 0.493634, acc.: 75.78%] [G loss: 4.479349]\n",
            "15800 [D loss: 0.414488, acc.: 81.25%] [G loss: 5.051283]\n",
            "15820 [D loss: 0.539799, acc.: 72.66%] [G loss: 4.486456]\n",
            "15840 [D loss: 0.480952, acc.: 78.91%] [G loss: 4.068331]\n",
            "15860 [D loss: 0.634525, acc.: 67.97%] [G loss: 4.109639]\n",
            "15880 [D loss: 0.429478, acc.: 81.64%] [G loss: 4.550463]\n",
            "15900 [D loss: 0.476003, acc.: 75.78%] [G loss: 4.243451]\n",
            "15920 [D loss: 0.462584, acc.: 80.86%] [G loss: 4.182878]\n",
            "15940 [D loss: 0.423144, acc.: 81.64%] [G loss: 4.155975]\n",
            "15960 [D loss: 0.474914, acc.: 76.95%] [G loss: 4.275054]\n",
            "15980 [D loss: 0.531211, acc.: 75.00%] [G loss: 4.177190]\n",
            "16000 [D loss: 0.399299, acc.: 83.20%] [G loss: 4.388530]\n",
            "16020 [D loss: 0.430056, acc.: 79.69%] [G loss: 4.267694]\n",
            "16040 [D loss: 0.445617, acc.: 81.64%] [G loss: 4.243593]\n",
            "16060 [D loss: 0.515812, acc.: 71.09%] [G loss: 4.262698]\n",
            "16080 [D loss: 0.429326, acc.: 83.20%] [G loss: 4.138748]\n",
            "16100 [D loss: 0.472835, acc.: 78.91%] [G loss: 4.119802]\n",
            "16120 [D loss: 0.490513, acc.: 78.52%] [G loss: 3.905108]\n",
            "16140 [D loss: 0.456448, acc.: 78.52%] [G loss: 4.427836]\n",
            "16160 [D loss: 0.397922, acc.: 83.59%] [G loss: 4.428994]\n",
            "16180 [D loss: 0.475421, acc.: 76.95%] [G loss: 4.320546]\n",
            "16200 [D loss: 0.424246, acc.: 81.64%] [G loss: 4.455418]\n",
            "16220 [D loss: 0.482014, acc.: 76.56%] [G loss: 4.338426]\n",
            "16240 [D loss: 0.470966, acc.: 78.91%] [G loss: 3.994521]\n",
            "16260 [D loss: 0.444958, acc.: 80.47%] [G loss: 4.511092]\n",
            "16280 [D loss: 0.440248, acc.: 79.30%] [G loss: 4.426658]\n",
            "16300 [D loss: 0.425781, acc.: 80.86%] [G loss: 4.437068]\n",
            "16320 [D loss: 0.403267, acc.: 82.42%] [G loss: 4.021058]\n",
            "16340 [D loss: 0.488122, acc.: 75.39%] [G loss: 4.032325]\n",
            "16360 [D loss: 0.441855, acc.: 78.52%] [G loss: 4.118490]\n",
            "16380 [D loss: 0.386242, acc.: 83.98%] [G loss: 4.271146]\n",
            "16400 [D loss: 0.364827, acc.: 84.77%] [G loss: 4.757149]\n",
            "16420 [D loss: 0.485870, acc.: 76.95%] [G loss: 4.116982]\n",
            "16440 [D loss: 0.408596, acc.: 83.20%] [G loss: 4.406727]\n",
            "16460 [D loss: 0.465521, acc.: 78.52%] [G loss: 4.182261]\n",
            "16480 [D loss: 0.509464, acc.: 75.78%] [G loss: 3.723023]\n",
            "16500 [D loss: 0.467432, acc.: 76.56%] [G loss: 4.324915]\n",
            "16520 [D loss: 0.408921, acc.: 80.86%] [G loss: 4.414312]\n",
            "16540 [D loss: 0.485388, acc.: 77.34%] [G loss: 4.411251]\n",
            "16560 [D loss: 0.413675, acc.: 83.98%] [G loss: 4.289209]\n",
            "16580 [D loss: 0.411393, acc.: 80.08%] [G loss: 4.511639]\n",
            "16600 [D loss: 0.406510, acc.: 82.03%] [G loss: 4.456290]\n",
            "16620 [D loss: 0.404051, acc.: 82.42%] [G loss: 4.736410]\n",
            "16640 [D loss: 0.442211, acc.: 79.69%] [G loss: 4.396434]\n",
            "16660 [D loss: 0.415371, acc.: 84.38%] [G loss: 4.374785]\n",
            "16680 [D loss: 0.406193, acc.: 81.25%] [G loss: 4.313623]\n",
            "16700 [D loss: 0.469344, acc.: 78.12%] [G loss: 4.092942]\n",
            "16720 [D loss: 0.485858, acc.: 76.17%] [G loss: 4.078609]\n",
            "16740 [D loss: 0.463265, acc.: 77.34%] [G loss: 4.364409]\n",
            "16760 [D loss: 0.422663, acc.: 78.91%] [G loss: 4.587979]\n",
            "16780 [D loss: 0.412487, acc.: 82.03%] [G loss: 3.826546]\n",
            "16800 [D loss: 0.343132, acc.: 84.77%] [G loss: 2.928182]\n",
            "16820 [D loss: 0.506751, acc.: 76.56%] [G loss: 4.185143]\n",
            "16840 [D loss: 0.591540, acc.: 71.88%] [G loss: 3.806387]\n",
            "16860 [D loss: 0.374825, acc.: 87.50%] [G loss: 4.195751]\n",
            "16880 [D loss: 0.429234, acc.: 79.69%] [G loss: 5.476977]\n",
            "16900 [D loss: 0.431906, acc.: 81.64%] [G loss: 4.358109]\n",
            "16920 [D loss: 0.493979, acc.: 75.39%] [G loss: 4.523547]\n",
            "16940 [D loss: 0.474072, acc.: 77.73%] [G loss: 4.466509]\n",
            "16960 [D loss: 0.377644, acc.: 85.16%] [G loss: 4.477683]\n",
            "16980 [D loss: 0.506026, acc.: 77.34%] [G loss: 4.147185]\n",
            "17000 [D loss: 0.434110, acc.: 77.73%] [G loss: 4.127425]\n",
            "17020 [D loss: 0.434086, acc.: 80.86%] [G loss: 4.332287]\n",
            "17040 [D loss: 0.354072, acc.: 86.33%] [G loss: 4.604606]\n",
            "17060 [D loss: 0.416978, acc.: 81.64%] [G loss: 5.026738]\n",
            "17080 [D loss: 0.440173, acc.: 81.25%] [G loss: 4.909269]\n",
            "17100 [D loss: 0.424082, acc.: 80.86%] [G loss: 5.034323]\n",
            "17120 [D loss: 0.453596, acc.: 78.52%] [G loss: 4.367324]\n",
            "17140 [D loss: 0.419764, acc.: 80.08%] [G loss: 4.621465]\n",
            "17160 [D loss: 0.362583, acc.: 85.55%] [G loss: 4.909574]\n",
            "17180 [D loss: 0.441887, acc.: 77.73%] [G loss: 4.209521]\n",
            "17200 [D loss: 0.456420, acc.: 79.69%] [G loss: 4.195680]\n",
            "17220 [D loss: 0.406982, acc.: 81.64%] [G loss: 4.460074]\n",
            "17240 [D loss: 0.459792, acc.: 75.00%] [G loss: 4.422410]\n",
            "17260 [D loss: 0.371589, acc.: 85.16%] [G loss: 4.767183]\n",
            "17280 [D loss: 0.480204, acc.: 76.17%] [G loss: 4.368985]\n",
            "17300 [D loss: 0.396410, acc.: 83.20%] [G loss: 4.400676]\n",
            "17320 [D loss: 0.411081, acc.: 82.81%] [G loss: 4.541252]\n",
            "17340 [D loss: 0.384778, acc.: 85.16%] [G loss: 4.425626]\n",
            "17360 [D loss: 0.393319, acc.: 80.86%] [G loss: 4.705682]\n",
            "17380 [D loss: 0.446198, acc.: 78.91%] [G loss: 4.631048]\n",
            "17400 [D loss: 0.331729, acc.: 86.33%] [G loss: 5.398939]\n",
            "17420 [D loss: 0.363560, acc.: 84.77%] [G loss: 4.635889]\n",
            "17440 [D loss: 0.409731, acc.: 79.69%] [G loss: 4.571927]\n",
            "17460 [D loss: 0.464575, acc.: 78.52%] [G loss: 4.288789]\n",
            "17480 [D loss: 0.367905, acc.: 84.77%] [G loss: 4.492714]\n",
            "17500 [D loss: 0.429890, acc.: 82.42%] [G loss: 4.829526]\n",
            "17520 [D loss: 0.417208, acc.: 82.03%] [G loss: 5.140034]\n",
            "17540 [D loss: 0.412532, acc.: 81.25%] [G loss: 4.494820]\n",
            "17560 [D loss: 0.463614, acc.: 77.34%] [G loss: 4.739565]\n",
            "17580 [D loss: 0.337432, acc.: 87.50%] [G loss: 5.105951]\n",
            "17600 [D loss: 0.359924, acc.: 85.55%] [G loss: 5.135260]\n",
            "17620 [D loss: 0.499598, acc.: 75.00%] [G loss: 4.819900]\n",
            "17640 [D loss: 0.491504, acc.: 75.39%] [G loss: 4.595604]\n",
            "17660 [D loss: 0.487698, acc.: 78.52%] [G loss: 4.734136]\n",
            "17680 [D loss: 0.405176, acc.: 80.08%] [G loss: 4.472852]\n",
            "17700 [D loss: 0.357032, acc.: 84.77%] [G loss: 4.966657]\n",
            "17720 [D loss: 0.446954, acc.: 79.69%] [G loss: 4.711740]\n",
            "17740 [D loss: 0.377306, acc.: 83.98%] [G loss: 4.644702]\n",
            "17760 [D loss: 0.362379, acc.: 83.59%] [G loss: 4.987311]\n",
            "17780 [D loss: 0.346200, acc.: 85.55%] [G loss: 4.534392]\n",
            "17800 [D loss: 0.428114, acc.: 80.47%] [G loss: 3.595450]\n",
            "17820 [D loss: 0.490375, acc.: 76.56%] [G loss: 3.348604]\n",
            "17840 [D loss: 0.343502, acc.: 88.28%] [G loss: 3.642110]\n",
            "17860 [D loss: 0.680281, acc.: 64.45%] [G loss: 5.818378]\n",
            "17880 [D loss: 0.538545, acc.: 71.88%] [G loss: 4.579785]\n",
            "17900 [D loss: 0.495005, acc.: 77.34%] [G loss: 5.278362]\n",
            "17920 [D loss: 0.351269, acc.: 85.94%] [G loss: 5.140663]\n",
            "17940 [D loss: 0.382508, acc.: 81.64%] [G loss: 4.759422]\n",
            "17960 [D loss: 0.356493, acc.: 85.16%] [G loss: 4.803349]\n",
            "17980 [D loss: 0.467578, acc.: 74.61%] [G loss: 4.803333]\n",
            "18000 [D loss: 0.474389, acc.: 78.52%] [G loss: 4.813045]\n",
            "18020 [D loss: 0.386632, acc.: 83.98%] [G loss: 4.829432]\n",
            "18040 [D loss: 0.377409, acc.: 82.03%] [G loss: 5.254076]\n",
            "18060 [D loss: 0.494337, acc.: 77.73%] [G loss: 4.726045]\n",
            "18080 [D loss: 0.311894, acc.: 89.84%] [G loss: 4.990333]\n",
            "18100 [D loss: 0.460257, acc.: 75.78%] [G loss: 4.601112]\n",
            "18120 [D loss: 0.415970, acc.: 82.03%] [G loss: 4.894011]\n",
            "18140 [D loss: 0.334639, acc.: 85.94%] [G loss: 5.408156]\n",
            "18160 [D loss: 0.456791, acc.: 78.91%] [G loss: 4.708696]\n",
            "18180 [D loss: 0.374485, acc.: 85.16%] [G loss: 5.112426]\n",
            "18200 [D loss: 0.468404, acc.: 76.56%] [G loss: 4.710945]\n",
            "18220 [D loss: 0.420331, acc.: 78.52%] [G loss: 4.900059]\n",
            "18240 [D loss: 0.387422, acc.: 84.38%] [G loss: 5.292075]\n",
            "18260 [D loss: 0.368883, acc.: 83.20%] [G loss: 4.797148]\n",
            "18280 [D loss: 0.412658, acc.: 80.86%] [G loss: 4.938805]\n",
            "18300 [D loss: 0.366877, acc.: 85.55%] [G loss: 5.093657]\n",
            "18320 [D loss: 0.382230, acc.: 83.98%] [G loss: 4.847178]\n",
            "18340 [D loss: 0.485637, acc.: 77.34%] [G loss: 4.569876]\n",
            "18360 [D loss: 0.387714, acc.: 83.98%] [G loss: 5.259239]\n",
            "18380 [D loss: 0.403256, acc.: 80.08%] [G loss: 4.935344]\n",
            "18400 [D loss: 0.455326, acc.: 77.34%] [G loss: 4.969047]\n",
            "18420 [D loss: 0.376593, acc.: 86.33%] [G loss: 5.148069]\n",
            "18440 [D loss: 0.398083, acc.: 83.98%] [G loss: 4.862328]\n",
            "18460 [D loss: 0.421704, acc.: 81.25%] [G loss: 4.769836]\n",
            "18480 [D loss: 0.395279, acc.: 82.03%] [G loss: 4.912240]\n",
            "18500 [D loss: 0.450749, acc.: 79.69%] [G loss: 4.662714]\n",
            "18520 [D loss: 0.391527, acc.: 83.98%] [G loss: 4.931089]\n",
            "18540 [D loss: 0.479285, acc.: 77.34%] [G loss: 4.971634]\n",
            "18560 [D loss: 0.367009, acc.: 84.77%] [G loss: 4.754582]\n",
            "18580 [D loss: 0.359060, acc.: 84.38%] [G loss: 5.176250]\n",
            "18600 [D loss: 0.385276, acc.: 81.25%] [G loss: 4.782342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Lof-jm3i0Q",
        "outputId": "bc1768b8-9165-4e50-a02e-b3a784112757"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18600, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.696359, acc.: 62.89%] [G loss: 1.446211]\n",
            "20 [D loss: 1.163523, acc.: 22.27%] [G loss: 0.713011]\n",
            "40 [D loss: 1.220478, acc.: 17.58%] [G loss: 1.237201]\n",
            "60 [D loss: 0.655929, acc.: 73.05%] [G loss: 3.198225]\n",
            "80 [D loss: 0.475856, acc.: 79.69%] [G loss: 4.155136]\n",
            "100 [D loss: 1.030986, acc.: 50.39%] [G loss: 1.380551]\n",
            "120 [D loss: 0.413875, acc.: 85.55%] [G loss: 12.094809]\n",
            "140 [D loss: 0.699246, acc.: 65.62%] [G loss: 1.606556]\n",
            "160 [D loss: 0.506435, acc.: 87.89%] [G loss: 3.299470]\n",
            "180 [D loss: 0.251355, acc.: 90.62%] [G loss: 7.642454]\n",
            "200 [D loss: 1.369268, acc.: 51.56%] [G loss: 1.898375]\n",
            "220 [D loss: 0.571116, acc.: 69.53%] [G loss: 2.657409]\n",
            "240 [D loss: 0.554591, acc.: 73.83%] [G loss: 1.613303]\n",
            "260 [D loss: 0.281924, acc.: 91.41%] [G loss: 3.086654]\n",
            "280 [D loss: 0.360274, acc.: 88.28%] [G loss: 2.700082]\n",
            "300 [D loss: 0.528984, acc.: 80.86%] [G loss: 2.713992]\n",
            "320 [D loss: 0.384624, acc.: 90.23%] [G loss: 1.856140]\n",
            "340 [D loss: 0.645530, acc.: 66.41%] [G loss: 1.617378]\n",
            "360 [D loss: 0.340297, acc.: 90.62%] [G loss: 4.522230]\n",
            "380 [D loss: 0.291186, acc.: 87.89%] [G loss: 5.616865]\n",
            "400 [D loss: 0.681895, acc.: 57.42%] [G loss: 2.202144]\n",
            "420 [D loss: 0.515111, acc.: 78.12%] [G loss: 2.813591]\n",
            "440 [D loss: 0.522725, acc.: 73.83%] [G loss: 3.063996]\n",
            "460 [D loss: 0.501683, acc.: 71.09%] [G loss: 4.516839]\n",
            "480 [D loss: 0.471221, acc.: 81.25%] [G loss: 2.272563]\n",
            "500 [D loss: 0.551517, acc.: 71.09%] [G loss: 3.269455]\n",
            "520 [D loss: 0.550512, acc.: 73.44%] [G loss: 2.471466]\n",
            "540 [D loss: 0.317920, acc.: 88.67%] [G loss: 5.961580]\n",
            "560 [D loss: 0.389808, acc.: 83.20%] [G loss: 4.262944]\n",
            "580 [D loss: 0.460144, acc.: 79.30%] [G loss: 4.905150]\n",
            "600 [D loss: 0.320170, acc.: 87.89%] [G loss: 6.336742]\n",
            "620 [D loss: 0.567837, acc.: 72.27%] [G loss: 3.545739]\n",
            "640 [D loss: 0.653890, acc.: 65.62%] [G loss: 3.399276]\n",
            "660 [D loss: 0.492451, acc.: 76.56%] [G loss: 4.467133]\n",
            "680 [D loss: 0.516664, acc.: 76.95%] [G loss: 4.192909]\n",
            "700 [D loss: 0.449728, acc.: 81.64%] [G loss: 4.639505]\n",
            "720 [D loss: 0.576106, acc.: 73.05%] [G loss: 3.609271]\n",
            "740 [D loss: 0.641553, acc.: 68.36%] [G loss: 4.200828]\n",
            "760 [D loss: 0.583512, acc.: 68.75%] [G loss: 2.751359]\n",
            "780 [D loss: 0.730590, acc.: 60.16%] [G loss: 3.544207]\n",
            "800 [D loss: 0.393353, acc.: 84.77%] [G loss: 5.141732]\n",
            "820 [D loss: 0.442701, acc.: 78.52%] [G loss: 4.386707]\n",
            "840 [D loss: 0.357923, acc.: 87.50%] [G loss: 4.665219]\n",
            "860 [D loss: 0.477971, acc.: 79.30%] [G loss: 4.352232]\n",
            "880 [D loss: 0.562903, acc.: 70.31%] [G loss: 4.327156]\n",
            "900 [D loss: 0.493504, acc.: 75.78%] [G loss: 4.012523]\n",
            "920 [D loss: 0.540231, acc.: 76.95%] [G loss: 4.362349]\n",
            "940 [D loss: 0.455799, acc.: 77.73%] [G loss: 3.636975]\n",
            "960 [D loss: 0.477792, acc.: 80.47%] [G loss: 3.706674]\n",
            "980 [D loss: 0.357467, acc.: 87.11%] [G loss: 4.805582]\n",
            "1000 [D loss: 0.363910, acc.: 85.16%] [G loss: 4.385890]\n",
            "1020 [D loss: 0.395662, acc.: 83.20%] [G loss: 4.251680]\n",
            "1040 [D loss: 0.544617, acc.: 73.83%] [G loss: 3.971273]\n",
            "1060 [D loss: 0.406575, acc.: 83.98%] [G loss: 4.369378]\n",
            "1080 [D loss: 0.433259, acc.: 80.08%] [G loss: 3.938429]\n",
            "1100 [D loss: 0.455807, acc.: 79.69%] [G loss: 3.812581]\n",
            "1120 [D loss: 0.478660, acc.: 77.73%] [G loss: 3.781468]\n",
            "1140 [D loss: 0.463711, acc.: 78.12%] [G loss: 3.752246]\n",
            "1160 [D loss: 0.539984, acc.: 72.27%] [G loss: 3.729447]\n",
            "1180 [D loss: 0.443357, acc.: 78.91%] [G loss: 3.888325]\n",
            "1200 [D loss: 0.485968, acc.: 73.05%] [G loss: 4.196789]\n",
            "1220 [D loss: 0.433135, acc.: 80.86%] [G loss: 3.941662]\n",
            "1240 [D loss: 0.505657, acc.: 75.78%] [G loss: 3.800249]\n",
            "1260 [D loss: 0.448927, acc.: 78.91%] [G loss: 3.956593]\n",
            "1280 [D loss: 0.411726, acc.: 81.25%] [G loss: 4.155158]\n",
            "1300 [D loss: 0.434513, acc.: 79.69%] [G loss: 4.001882]\n",
            "1320 [D loss: 0.455434, acc.: 79.69%] [G loss: 3.718468]\n",
            "1340 [D loss: 0.393134, acc.: 84.77%] [G loss: 4.017004]\n",
            "1360 [D loss: 0.445426, acc.: 78.52%] [G loss: 3.952610]\n",
            "1380 [D loss: 0.397145, acc.: 82.81%] [G loss: 4.576160]\n",
            "1400 [D loss: 0.530536, acc.: 74.22%] [G loss: 4.221869]\n",
            "1420 [D loss: 0.429820, acc.: 83.98%] [G loss: 3.989506]\n",
            "1440 [D loss: 0.497594, acc.: 75.00%] [G loss: 4.096679]\n",
            "1460 [D loss: 0.522586, acc.: 77.34%] [G loss: 4.010116]\n",
            "1480 [D loss: 0.458121, acc.: 82.81%] [G loss: 4.037869]\n",
            "1500 [D loss: 0.534014, acc.: 75.39%] [G loss: 3.837799]\n",
            "1520 [D loss: 0.398399, acc.: 81.64%] [G loss: 4.133334]\n",
            "1540 [D loss: 0.431214, acc.: 81.25%] [G loss: 4.334744]\n",
            "1560 [D loss: 0.783335, acc.: 50.39%] [G loss: 2.173300]\n",
            "1580 [D loss: 0.717588, acc.: 56.64%] [G loss: 2.319828]\n",
            "1600 [D loss: 0.709927, acc.: 57.42%] [G loss: 1.991293]\n",
            "1620 [D loss: 0.638347, acc.: 60.16%] [G loss: 2.269987]\n",
            "1640 [D loss: 0.665323, acc.: 57.03%] [G loss: 2.005648]\n",
            "1660 [D loss: 0.518655, acc.: 75.78%] [G loss: 2.671816]\n",
            "1680 [D loss: 0.523227, acc.: 78.52%] [G loss: 2.755344]\n",
            "1700 [D loss: 0.579249, acc.: 68.75%] [G loss: 2.956488]\n",
            "1720 [D loss: 0.468646, acc.: 78.91%] [G loss: 3.110859]\n",
            "1740 [D loss: 0.447784, acc.: 80.47%] [G loss: 3.289743]\n",
            "1760 [D loss: 0.460743, acc.: 78.52%] [G loss: 3.678903]\n",
            "1780 [D loss: 0.448870, acc.: 83.59%] [G loss: 3.301871]\n",
            "1800 [D loss: 0.490325, acc.: 78.91%] [G loss: 3.015180]\n",
            "1820 [D loss: 0.445600, acc.: 82.03%] [G loss: 3.613938]\n",
            "1840 [D loss: 0.436003, acc.: 83.98%] [G loss: 3.533766]\n",
            "1860 [D loss: 0.498762, acc.: 78.91%] [G loss: 3.809198]\n",
            "1880 [D loss: 0.526313, acc.: 73.44%] [G loss: 3.296524]\n",
            "1900 [D loss: 0.471828, acc.: 78.91%] [G loss: 3.538340]\n",
            "1920 [D loss: 0.561648, acc.: 70.70%] [G loss: 3.407851]\n",
            "1940 [D loss: 0.493469, acc.: 75.78%] [G loss: 3.683469]\n",
            "1960 [D loss: 0.472527, acc.: 78.52%] [G loss: 3.501588]\n",
            "1980 [D loss: 0.497438, acc.: 76.95%] [G loss: 3.714194]\n",
            "2000 [D loss: 0.462745, acc.: 78.91%] [G loss: 3.386624]\n",
            "2020 [D loss: 0.420221, acc.: 81.64%] [G loss: 3.648956]\n",
            "2040 [D loss: 0.493041, acc.: 77.73%] [G loss: 3.207301]\n",
            "2060 [D loss: 0.522109, acc.: 75.39%] [G loss: 3.402582]\n",
            "2080 [D loss: 0.509177, acc.: 76.95%] [G loss: 3.566931]\n",
            "2100 [D loss: 0.463567, acc.: 79.69%] [G loss: 3.492286]\n",
            "2120 [D loss: 0.433554, acc.: 81.25%] [G loss: 3.631058]\n",
            "2140 [D loss: 0.527185, acc.: 72.66%] [G loss: 3.487663]\n",
            "2160 [D loss: 0.471744, acc.: 75.39%] [G loss: 3.367339]\n",
            "2180 [D loss: 0.507224, acc.: 76.95%] [G loss: 3.251434]\n",
            "2200 [D loss: 0.469351, acc.: 77.34%] [G loss: 3.594208]\n",
            "2220 [D loss: 0.457895, acc.: 79.30%] [G loss: 3.720166]\n",
            "2240 [D loss: 0.480834, acc.: 79.30%] [G loss: 3.223764]\n",
            "2260 [D loss: 0.501214, acc.: 74.61%] [G loss: 3.476759]\n",
            "2280 [D loss: 0.558307, acc.: 66.41%] [G loss: 3.187800]\n",
            "2300 [D loss: 0.526974, acc.: 73.44%] [G loss: 3.721486]\n",
            "2320 [D loss: 0.501747, acc.: 75.78%] [G loss: 3.540772]\n",
            "2340 [D loss: 0.422631, acc.: 82.42%] [G loss: 3.702205]\n",
            "2360 [D loss: 0.458724, acc.: 80.47%] [G loss: 3.259301]\n",
            "2380 [D loss: 0.587042, acc.: 65.62%] [G loss: 3.306011]\n",
            "2400 [D loss: 0.434851, acc.: 82.42%] [G loss: 3.875597]\n",
            "2420 [D loss: 0.577661, acc.: 67.58%] [G loss: 3.114182]\n",
            "2440 [D loss: 0.495277, acc.: 75.00%] [G loss: 3.272084]\n",
            "2460 [D loss: 0.508051, acc.: 73.83%] [G loss: 3.589397]\n",
            "2480 [D loss: 0.457798, acc.: 78.12%] [G loss: 3.579377]\n",
            "2500 [D loss: 0.493405, acc.: 76.95%] [G loss: 3.440243]\n",
            "2520 [D loss: 0.561211, acc.: 69.53%] [G loss: 3.333122]\n",
            "2540 [D loss: 0.465456, acc.: 75.78%] [G loss: 2.658821]\n",
            "2560 [D loss: 0.610560, acc.: 66.80%] [G loss: 3.313526]\n",
            "2580 [D loss: 0.819851, acc.: 57.42%] [G loss: 3.394972]\n",
            "2600 [D loss: 0.506641, acc.: 75.00%] [G loss: 3.527428]\n",
            "2620 [D loss: 0.366536, acc.: 85.16%] [G loss: 4.327864]\n",
            "2640 [D loss: 0.430177, acc.: 80.08%] [G loss: 2.379280]\n",
            "2660 [D loss: 0.476316, acc.: 77.73%] [G loss: 1.879889]\n",
            "2680 [D loss: 0.308654, acc.: 90.23%] [G loss: 2.107726]\n",
            "2700 [D loss: 1.276173, acc.: 44.14%] [G loss: 4.422312]\n",
            "2720 [D loss: 0.566286, acc.: 72.66%] [G loss: 2.964157]\n",
            "2740 [D loss: 0.519149, acc.: 70.70%] [G loss: 2.615463]\n",
            "2760 [D loss: 0.394649, acc.: 83.20%] [G loss: 3.842801]\n",
            "2780 [D loss: 0.411506, acc.: 83.20%] [G loss: 3.776368]\n",
            "2800 [D loss: 0.434963, acc.: 80.47%] [G loss: 3.866957]\n",
            "2820 [D loss: 0.446653, acc.: 82.81%] [G loss: 3.701382]\n",
            "2840 [D loss: 0.438237, acc.: 81.64%] [G loss: 3.880510]\n",
            "2860 [D loss: 0.465442, acc.: 80.47%] [G loss: 3.495365]\n",
            "2880 [D loss: 0.498248, acc.: 74.61%] [G loss: 3.903087]\n",
            "2900 [D loss: 0.527697, acc.: 71.88%] [G loss: 3.249183]\n",
            "2920 [D loss: 0.548208, acc.: 70.70%] [G loss: 3.584967]\n",
            "2940 [D loss: 0.543540, acc.: 74.22%] [G loss: 3.689894]\n",
            "2960 [D loss: 0.522666, acc.: 75.00%] [G loss: 3.657451]\n",
            "2980 [D loss: 0.451316, acc.: 80.47%] [G loss: 3.589772]\n",
            "3000 [D loss: 0.456579, acc.: 79.30%] [G loss: 3.440430]\n",
            "3020 [D loss: 0.385899, acc.: 85.55%] [G loss: 3.883299]\n",
            "3040 [D loss: 0.470351, acc.: 79.69%] [G loss: 3.780955]\n",
            "3060 [D loss: 0.503027, acc.: 76.56%] [G loss: 3.554591]\n",
            "3080 [D loss: 0.496454, acc.: 75.00%] [G loss: 3.324713]\n",
            "3100 [D loss: 0.398832, acc.: 81.64%] [G loss: 4.244615]\n",
            "3120 [D loss: 0.568626, acc.: 71.88%] [G loss: 3.400843]\n",
            "3140 [D loss: 0.426111, acc.: 78.52%] [G loss: 3.812358]\n",
            "3160 [D loss: 0.574710, acc.: 67.19%] [G loss: 3.353335]\n",
            "3180 [D loss: 0.498938, acc.: 77.73%] [G loss: 3.650208]\n",
            "3200 [D loss: 0.365601, acc.: 85.94%] [G loss: 4.111368]\n",
            "3220 [D loss: 0.516018, acc.: 75.39%] [G loss: 3.455418]\n",
            "3240 [D loss: 0.494219, acc.: 72.27%] [G loss: 3.612473]\n",
            "3260 [D loss: 0.440146, acc.: 79.69%] [G loss: 3.942535]\n",
            "3280 [D loss: 0.531865, acc.: 73.83%] [G loss: 3.394681]\n",
            "3300 [D loss: 0.489271, acc.: 77.73%] [G loss: 3.377133]\n",
            "3320 [D loss: 0.551878, acc.: 73.44%] [G loss: 3.622541]\n",
            "3340 [D loss: 0.465189, acc.: 81.64%] [G loss: 3.586190]\n",
            "3360 [D loss: 0.521616, acc.: 75.78%] [G loss: 3.424371]\n",
            "3380 [D loss: 0.406729, acc.: 82.03%] [G loss: 3.889325]\n",
            "3400 [D loss: 0.542310, acc.: 74.61%] [G loss: 4.021432]\n",
            "3420 [D loss: 0.621952, acc.: 66.41%] [G loss: 3.588639]\n",
            "3440 [D loss: 0.561079, acc.: 71.48%] [G loss: 3.718172]\n",
            "3460 [D loss: 0.478455, acc.: 75.78%] [G loss: 3.916556]\n",
            "3480 [D loss: 0.453478, acc.: 77.34%] [G loss: 4.023107]\n",
            "3500 [D loss: 0.453086, acc.: 78.12%] [G loss: 3.654559]\n",
            "3520 [D loss: 0.478267, acc.: 78.52%] [G loss: 3.676326]\n",
            "3540 [D loss: 0.514600, acc.: 73.05%] [G loss: 3.569495]\n",
            "3560 [D loss: 0.584826, acc.: 68.36%] [G loss: 3.276380]\n",
            "3580 [D loss: 0.449984, acc.: 80.08%] [G loss: 3.572367]\n",
            "3600 [D loss: 0.457138, acc.: 80.08%] [G loss: 3.839814]\n",
            "3620 [D loss: 0.534657, acc.: 71.88%] [G loss: 3.494092]\n",
            "3640 [D loss: 0.434491, acc.: 82.03%] [G loss: 4.018171]\n",
            "3660 [D loss: 0.595732, acc.: 67.97%] [G loss: 3.356070]\n",
            "3680 [D loss: 0.438684, acc.: 81.25%] [G loss: 3.594168]\n",
            "3700 [D loss: 0.463088, acc.: 77.34%] [G loss: 4.194972]\n",
            "3720 [D loss: 0.484385, acc.: 76.95%] [G loss: 3.512795]\n",
            "3740 [D loss: 0.482548, acc.: 75.00%] [G loss: 3.865248]\n",
            "3760 [D loss: 0.487501, acc.: 78.12%] [G loss: 3.512489]\n",
            "3780 [D loss: 0.475157, acc.: 78.91%] [G loss: 3.973012]\n",
            "3800 [D loss: 0.479466, acc.: 77.73%] [G loss: 4.028281]\n",
            "3820 [D loss: 0.477754, acc.: 75.78%] [G loss: 4.027597]\n",
            "3840 [D loss: 0.501783, acc.: 75.78%] [G loss: 3.664977]\n",
            "3860 [D loss: 0.468632, acc.: 78.91%] [G loss: 3.862001]\n",
            "3880 [D loss: 0.446175, acc.: 79.69%] [G loss: 4.066327]\n",
            "3900 [D loss: 0.508537, acc.: 77.34%] [G loss: 3.726521]\n",
            "3920 [D loss: 0.416865, acc.: 79.69%] [G loss: 4.074302]\n",
            "3940 [D loss: 0.548299, acc.: 72.27%] [G loss: 3.360701]\n",
            "3960 [D loss: 0.462850, acc.: 80.86%] [G loss: 3.864856]\n",
            "3980 [D loss: 0.456271, acc.: 80.08%] [G loss: 3.770685]\n",
            "4000 [D loss: 0.523453, acc.: 74.22%] [G loss: 3.741747]\n",
            "4020 [D loss: 0.413136, acc.: 80.47%] [G loss: 3.889089]\n",
            "4040 [D loss: 0.380443, acc.: 85.55%] [G loss: 3.371760]\n",
            "4060 [D loss: 0.518200, acc.: 75.39%] [G loss: 3.529043]\n",
            "4080 [D loss: 0.524756, acc.: 75.78%] [G loss: 4.188472]\n",
            "4100 [D loss: 0.431759, acc.: 80.86%] [G loss: 3.948927]\n",
            "4120 [D loss: 0.462378, acc.: 76.95%] [G loss: 4.372771]\n",
            "4140 [D loss: 0.410177, acc.: 82.03%] [G loss: 4.271484]\n",
            "4160 [D loss: 0.438372, acc.: 78.91%] [G loss: 4.332627]\n",
            "4180 [D loss: 0.406245, acc.: 83.98%] [G loss: 4.045570]\n",
            "4200 [D loss: 0.425405, acc.: 79.69%] [G loss: 3.941288]\n",
            "4220 [D loss: 0.458968, acc.: 78.52%] [G loss: 4.050042]\n",
            "4240 [D loss: 0.456712, acc.: 75.78%] [G loss: 4.253774]\n",
            "4260 [D loss: 0.447605, acc.: 80.08%] [G loss: 4.088403]\n",
            "4280 [D loss: 0.493892, acc.: 74.22%] [G loss: 3.997761]\n",
            "4300 [D loss: 0.506475, acc.: 73.44%] [G loss: 3.891933]\n",
            "4320 [D loss: 0.523100, acc.: 73.44%] [G loss: 3.609283]\n",
            "4340 [D loss: 0.476631, acc.: 74.22%] [G loss: 3.693956]\n",
            "4360 [D loss: 0.450906, acc.: 79.30%] [G loss: 4.125407]\n",
            "4380 [D loss: 0.401704, acc.: 83.59%] [G loss: 4.032945]\n",
            "4400 [D loss: 0.366981, acc.: 85.16%] [G loss: 4.965491]\n",
            "4420 [D loss: 0.483119, acc.: 78.12%] [G loss: 3.959276]\n",
            "4440 [D loss: 0.478138, acc.: 78.12%] [G loss: 4.065999]\n",
            "4460 [D loss: 0.513172, acc.: 75.00%] [G loss: 3.918742]\n",
            "4480 [D loss: 0.440384, acc.: 77.34%] [G loss: 3.804497]\n",
            "4500 [D loss: 0.424071, acc.: 80.08%] [G loss: 4.051526]\n",
            "4520 [D loss: 0.391699, acc.: 83.20%] [G loss: 4.338980]\n",
            "4540 [D loss: 0.429648, acc.: 77.34%] [G loss: 4.165473]\n",
            "4560 [D loss: 0.521977, acc.: 72.27%] [G loss: 3.860038]\n",
            "4580 [D loss: 0.494420, acc.: 75.78%] [G loss: 3.667827]\n",
            "4600 [D loss: 0.526068, acc.: 71.09%] [G loss: 3.937126]\n",
            "4620 [D loss: 0.466164, acc.: 77.73%] [G loss: 4.031190]\n",
            "4640 [D loss: 0.500973, acc.: 76.17%] [G loss: 4.000288]\n",
            "4660 [D loss: 0.462947, acc.: 79.30%] [G loss: 3.940690]\n",
            "4680 [D loss: 0.445987, acc.: 80.08%] [G loss: 3.975265]\n",
            "4700 [D loss: 0.338693, acc.: 88.67%] [G loss: 3.881504]\n",
            "4720 [D loss: 1.364135, acc.: 22.27%] [G loss: 3.498910]\n",
            "4740 [D loss: 1.220002, acc.: 24.61%] [G loss: 1.440901]\n",
            "4760 [D loss: 0.815321, acc.: 50.78%] [G loss: 2.027054]\n",
            "4780 [D loss: 0.527013, acc.: 73.44%] [G loss: 3.342511]\n",
            "4800 [D loss: 0.490507, acc.: 75.78%] [G loss: 3.929365]\n",
            "4820 [D loss: 0.469592, acc.: 76.56%] [G loss: 4.080887]\n",
            "4840 [D loss: 0.501878, acc.: 76.95%] [G loss: 3.811781]\n",
            "4860 [D loss: 0.512156, acc.: 77.34%] [G loss: 3.159840]\n",
            "4880 [D loss: 0.456744, acc.: 76.56%] [G loss: 3.744370]\n",
            "4900 [D loss: 0.418101, acc.: 83.20%] [G loss: 3.878281]\n",
            "4920 [D loss: 0.450059, acc.: 80.08%] [G loss: 3.872850]\n",
            "4940 [D loss: 0.494588, acc.: 76.95%] [G loss: 3.634113]\n",
            "4960 [D loss: 0.496361, acc.: 78.52%] [G loss: 3.841659]\n",
            "4980 [D loss: 0.537523, acc.: 75.78%] [G loss: 3.448776]\n",
            "5000 [D loss: 0.387303, acc.: 84.38%] [G loss: 4.449680]\n",
            "5020 [D loss: 0.446595, acc.: 81.64%] [G loss: 4.179013]\n",
            "5040 [D loss: 0.491357, acc.: 78.91%] [G loss: 4.075773]\n",
            "5060 [D loss: 0.494061, acc.: 77.34%] [G loss: 3.908288]\n",
            "5080 [D loss: 0.449637, acc.: 80.08%] [G loss: 4.150727]\n",
            "5100 [D loss: 0.534417, acc.: 70.70%] [G loss: 3.892555]\n",
            "5120 [D loss: 0.506463, acc.: 74.61%] [G loss: 3.522201]\n",
            "5140 [D loss: 0.540387, acc.: 73.44%] [G loss: 3.527501]\n",
            "5160 [D loss: 0.462005, acc.: 77.34%] [G loss: 4.123527]\n",
            "5180 [D loss: 0.484270, acc.: 77.34%] [G loss: 3.705617]\n",
            "5200 [D loss: 0.485644, acc.: 73.83%] [G loss: 4.230354]\n",
            "5220 [D loss: 0.521139, acc.: 74.22%] [G loss: 3.942693]\n",
            "5240 [D loss: 0.535371, acc.: 73.05%] [G loss: 3.471163]\n",
            "5260 [D loss: 0.445176, acc.: 79.69%] [G loss: 4.029202]\n",
            "5280 [D loss: 0.477804, acc.: 79.69%] [G loss: 3.866130]\n",
            "5300 [D loss: 0.550279, acc.: 73.44%] [G loss: 3.744251]\n",
            "5320 [D loss: 0.526031, acc.: 73.44%] [G loss: 3.403838]\n",
            "5340 [D loss: 0.461962, acc.: 79.69%] [G loss: 4.350829]\n",
            "5360 [D loss: 0.510082, acc.: 73.83%] [G loss: 3.866315]\n",
            "5380 [D loss: 0.499755, acc.: 74.61%] [G loss: 3.524013]\n",
            "5400 [D loss: 0.433077, acc.: 81.64%] [G loss: 4.295393]\n",
            "5420 [D loss: 0.462055, acc.: 78.91%] [G loss: 3.972794]\n",
            "5440 [D loss: 0.476090, acc.: 76.17%] [G loss: 3.947177]\n",
            "5460 [D loss: 0.386328, acc.: 86.33%] [G loss: 4.168908]\n",
            "5480 [D loss: 0.473253, acc.: 80.86%] [G loss: 4.079563]\n",
            "5500 [D loss: 0.472191, acc.: 79.30%] [G loss: 3.799051]\n",
            "5520 [D loss: 0.411991, acc.: 81.25%] [G loss: 4.349048]\n",
            "5540 [D loss: 0.474183, acc.: 79.69%] [G loss: 4.105576]\n",
            "5560 [D loss: 0.484537, acc.: 75.78%] [G loss: 4.006234]\n",
            "5580 [D loss: 0.518917, acc.: 76.56%] [G loss: 3.760057]\n",
            "5600 [D loss: 0.451715, acc.: 81.25%] [G loss: 4.220232]\n",
            "5620 [D loss: 0.433375, acc.: 80.08%] [G loss: 3.920801]\n",
            "5640 [D loss: 0.430463, acc.: 79.69%] [G loss: 4.275650]\n",
            "5660 [D loss: 0.524736, acc.: 73.44%] [G loss: 4.272923]\n",
            "5680 [D loss: 0.545726, acc.: 70.70%] [G loss: 3.449869]\n",
            "5700 [D loss: 0.431793, acc.: 76.95%] [G loss: 4.387297]\n",
            "5720 [D loss: 0.416544, acc.: 81.64%] [G loss: 4.229410]\n",
            "5740 [D loss: 0.480941, acc.: 76.95%] [G loss: 3.962344]\n",
            "5760 [D loss: 0.467794, acc.: 77.34%] [G loss: 3.821721]\n",
            "5780 [D loss: 0.448771, acc.: 77.34%] [G loss: 4.016366]\n",
            "5800 [D loss: 0.432905, acc.: 82.42%] [G loss: 4.255185]\n",
            "5820 [D loss: 0.465397, acc.: 78.91%] [G loss: 4.086635]\n",
            "5840 [D loss: 0.489000, acc.: 74.22%] [G loss: 3.801020]\n",
            "5860 [D loss: 0.433365, acc.: 82.42%] [G loss: 4.001574]\n",
            "5880 [D loss: 0.570723, acc.: 67.58%] [G loss: 3.340290]\n",
            "5900 [D loss: 1.184468, acc.: 40.62%] [G loss: 2.755370]\n",
            "5920 [D loss: 0.661499, acc.: 58.98%] [G loss: 1.248495]\n",
            "5940 [D loss: 1.434140, acc.: 19.14%] [G loss: 1.353012]\n",
            "5960 [D loss: 0.558359, acc.: 73.83%] [G loss: 3.883487]\n",
            "5980 [D loss: 0.449113, acc.: 78.52%] [G loss: 3.760363]\n",
            "6000 [D loss: 0.452505, acc.: 80.47%] [G loss: 3.661014]\n",
            "6020 [D loss: 0.531718, acc.: 71.48%] [G loss: 3.289155]\n",
            "6040 [D loss: 0.391921, acc.: 82.03%] [G loss: 4.357686]\n",
            "6060 [D loss: 0.558357, acc.: 72.66%] [G loss: 3.410869]\n",
            "6080 [D loss: 0.529585, acc.: 76.17%] [G loss: 3.552475]\n",
            "6100 [D loss: 0.621368, acc.: 67.19%] [G loss: 3.159965]\n",
            "6120 [D loss: 0.435864, acc.: 80.86%] [G loss: 3.791369]\n",
            "6140 [D loss: 0.514782, acc.: 73.44%] [G loss: 3.359994]\n",
            "6160 [D loss: 0.437265, acc.: 80.86%] [G loss: 3.959708]\n",
            "6180 [D loss: 0.517861, acc.: 75.39%] [G loss: 3.651121]\n",
            "6200 [D loss: 0.518135, acc.: 73.44%] [G loss: 3.404040]\n",
            "6220 [D loss: 0.458233, acc.: 76.95%] [G loss: 3.610273]\n",
            "6240 [D loss: 0.453167, acc.: 80.08%] [G loss: 3.671097]\n",
            "6260 [D loss: 0.403220, acc.: 84.38%] [G loss: 3.931660]\n",
            "6280 [D loss: 0.447648, acc.: 78.91%] [G loss: 4.014430]\n",
            "6300 [D loss: 0.404328, acc.: 81.64%] [G loss: 4.651325]\n",
            "6320 [D loss: 0.416422, acc.: 82.03%] [G loss: 3.755052]\n",
            "6340 [D loss: 0.489167, acc.: 76.95%] [G loss: 3.427245]\n",
            "6360 [D loss: 0.523445, acc.: 74.61%] [G loss: 3.770679]\n",
            "6380 [D loss: 0.446362, acc.: 80.08%] [G loss: 3.598990]\n",
            "6400 [D loss: 0.492297, acc.: 76.17%] [G loss: 3.748757]\n",
            "6420 [D loss: 0.558397, acc.: 68.36%] [G loss: 3.754236]\n",
            "6440 [D loss: 0.453882, acc.: 77.73%] [G loss: 3.590938]\n",
            "6460 [D loss: 0.549618, acc.: 72.66%] [G loss: 3.753695]\n",
            "6480 [D loss: 0.430891, acc.: 80.08%] [G loss: 3.991786]\n",
            "6500 [D loss: 0.519057, acc.: 75.00%] [G loss: 3.353537]\n",
            "6520 [D loss: 0.437584, acc.: 80.08%] [G loss: 4.426554]\n",
            "6540 [D loss: 0.499923, acc.: 75.78%] [G loss: 3.499173]\n",
            "6560 [D loss: 0.466319, acc.: 80.08%] [G loss: 3.744201]\n",
            "6580 [D loss: 0.470756, acc.: 76.56%] [G loss: 3.738044]\n",
            "6600 [D loss: 0.395009, acc.: 85.94%] [G loss: 4.261152]\n",
            "6620 [D loss: 0.443802, acc.: 78.91%] [G loss: 4.273849]\n",
            "6640 [D loss: 0.475945, acc.: 79.30%] [G loss: 3.712089]\n",
            "6660 [D loss: 0.489327, acc.: 78.52%] [G loss: 3.837448]\n",
            "6680 [D loss: 0.517320, acc.: 75.78%] [G loss: 3.994417]\n",
            "6700 [D loss: 0.470912, acc.: 75.78%] [G loss: 4.399851]\n",
            "6720 [D loss: 0.431276, acc.: 83.59%] [G loss: 3.888953]\n",
            "6740 [D loss: 0.464994, acc.: 78.52%] [G loss: 3.740701]\n",
            "6760 [D loss: 0.534988, acc.: 72.27%] [G loss: 3.620614]\n",
            "6780 [D loss: 0.400307, acc.: 84.38%] [G loss: 4.062611]\n",
            "6800 [D loss: 0.480052, acc.: 76.17%] [G loss: 3.882315]\n",
            "6820 [D loss: 0.395697, acc.: 84.38%] [G loss: 4.012372]\n",
            "6840 [D loss: 0.414366, acc.: 80.08%] [G loss: 4.492537]\n",
            "6860 [D loss: 0.515453, acc.: 75.78%] [G loss: 3.840596]\n",
            "6880 [D loss: 0.392044, acc.: 82.81%] [G loss: 4.691299]\n",
            "6900 [D loss: 0.430791, acc.: 79.30%] [G loss: 4.203332]\n",
            "6920 [D loss: 0.481881, acc.: 76.95%] [G loss: 3.824632]\n",
            "6940 [D loss: 0.490377, acc.: 76.17%] [G loss: 3.669713]\n",
            "6960 [D loss: 0.401364, acc.: 83.59%] [G loss: 4.638701]\n",
            "6980 [D loss: 0.428792, acc.: 82.42%] [G loss: 3.883505]\n",
            "7000 [D loss: 0.440642, acc.: 79.30%] [G loss: 3.723340]\n",
            "7020 [D loss: 0.602773, acc.: 69.14%] [G loss: 4.041219]\n",
            "7040 [D loss: 0.472020, acc.: 76.17%] [G loss: 3.515181]\n",
            "7060 [D loss: 0.545482, acc.: 73.05%] [G loss: 4.035499]\n",
            "7080 [D loss: 0.414555, acc.: 83.20%] [G loss: 4.098302]\n",
            "7100 [D loss: 0.529623, acc.: 72.27%] [G loss: 4.301728]\n",
            "7120 [D loss: 0.555657, acc.: 70.70%] [G loss: 3.897317]\n",
            "7140 [D loss: 0.435829, acc.: 80.86%] [G loss: 4.367137]\n",
            "7160 [D loss: 0.549072, acc.: 71.48%] [G loss: 3.487244]\n",
            "7180 [D loss: 0.571417, acc.: 67.97%] [G loss: 3.973169]\n",
            "7200 [D loss: 0.449170, acc.: 78.91%] [G loss: 4.194701]\n",
            "7220 [D loss: 0.421336, acc.: 81.64%] [G loss: 3.949815]\n",
            "7240 [D loss: 0.443558, acc.: 78.91%] [G loss: 4.082131]\n",
            "7260 [D loss: 0.442217, acc.: 79.69%] [G loss: 4.423971]\n",
            "7280 [D loss: 0.496390, acc.: 78.52%] [G loss: 4.187441]\n",
            "7300 [D loss: 0.430028, acc.: 78.52%] [G loss: 4.230793]\n",
            "7320 [D loss: 0.600982, acc.: 66.41%] [G loss: 3.600776]\n",
            "7340 [D loss: 0.417268, acc.: 81.25%] [G loss: 4.087482]\n",
            "7360 [D loss: 0.539738, acc.: 74.61%] [G loss: 3.834679]\n",
            "7380 [D loss: 0.479893, acc.: 78.12%] [G loss: 4.084527]\n",
            "7400 [D loss: 0.485716, acc.: 77.34%] [G loss: 4.698217]\n",
            "7420 [D loss: 0.405280, acc.: 80.47%] [G loss: 4.788971]\n",
            "7440 [D loss: 0.439409, acc.: 80.47%] [G loss: 4.280563]\n",
            "7460 [D loss: 0.432841, acc.: 79.69%] [G loss: 4.161861]\n",
            "7480 [D loss: 0.416406, acc.: 81.25%] [G loss: 4.827913]\n",
            "7500 [D loss: 0.447153, acc.: 82.03%] [G loss: 3.923577]\n",
            "7520 [D loss: 0.417991, acc.: 80.86%] [G loss: 3.985697]\n",
            "7540 [D loss: 0.401134, acc.: 84.38%] [G loss: 4.309304]\n",
            "7560 [D loss: 0.465119, acc.: 76.56%] [G loss: 4.041121]\n",
            "7580 [D loss: 0.483935, acc.: 76.17%] [G loss: 4.425219]\n",
            "7600 [D loss: 0.467907, acc.: 78.91%] [G loss: 4.081348]\n",
            "7620 [D loss: 0.434769, acc.: 80.08%] [G loss: 4.078183]\n",
            "7640 [D loss: 0.425896, acc.: 78.52%] [G loss: 4.359330]\n",
            "7660 [D loss: 0.396057, acc.: 82.81%] [G loss: 4.607359]\n",
            "7680 [D loss: 0.461577, acc.: 77.73%] [G loss: 4.207151]\n",
            "7700 [D loss: 0.447097, acc.: 76.17%] [G loss: 3.983339]\n",
            "7720 [D loss: 0.395722, acc.: 82.81%] [G loss: 4.307073]\n",
            "7740 [D loss: 0.440221, acc.: 78.52%] [G loss: 4.509144]\n",
            "7760 [D loss: 0.441964, acc.: 80.08%] [G loss: 4.438655]\n",
            "7780 [D loss: 0.383546, acc.: 83.98%] [G loss: 4.573860]\n",
            "7800 [D loss: 0.436608, acc.: 80.47%] [G loss: 4.189570]\n",
            "7820 [D loss: 0.485452, acc.: 78.52%] [G loss: 4.388052]\n",
            "7840 [D loss: 0.468393, acc.: 77.73%] [G loss: 4.009923]\n",
            "7860 [D loss: 0.523841, acc.: 74.22%] [G loss: 4.318661]\n",
            "7880 [D loss: 0.581120, acc.: 73.44%] [G loss: 3.588532]\n",
            "7900 [D loss: 0.627213, acc.: 67.19%] [G loss: 3.830516]\n",
            "7920 [D loss: 0.662497, acc.: 67.58%] [G loss: 3.500309]\n",
            "7940 [D loss: 0.521467, acc.: 75.78%] [G loss: 4.443075]\n",
            "7960 [D loss: 0.448294, acc.: 80.47%] [G loss: 4.385406]\n",
            "7980 [D loss: 0.525751, acc.: 72.27%] [G loss: 3.909022]\n",
            "8000 [D loss: 0.405081, acc.: 82.81%] [G loss: 3.010660]\n",
            "8020 [D loss: 0.814304, acc.: 57.03%] [G loss: 4.047432]\n",
            "8040 [D loss: 0.761669, acc.: 61.72%] [G loss: 4.456038]\n",
            "8060 [D loss: 0.829897, acc.: 60.16%] [G loss: 4.032511]\n",
            "8080 [D loss: 0.471897, acc.: 77.34%] [G loss: 4.889156]\n",
            "8100 [D loss: 0.364450, acc.: 85.16%] [G loss: 4.754349]\n",
            "8120 [D loss: 0.493843, acc.: 76.95%] [G loss: 4.203110]\n",
            "8140 [D loss: 0.560520, acc.: 73.83%] [G loss: 4.330490]\n",
            "8160 [D loss: 0.503012, acc.: 75.39%] [G loss: 3.978580]\n",
            "8180 [D loss: 0.403843, acc.: 83.59%] [G loss: 4.379108]\n",
            "8200 [D loss: 0.395414, acc.: 81.64%] [G loss: 4.853371]\n",
            "8220 [D loss: 0.434186, acc.: 80.08%] [G loss: 4.434687]\n",
            "8240 [D loss: 0.466415, acc.: 77.34%] [G loss: 4.071208]\n",
            "8260 [D loss: 0.385557, acc.: 82.03%] [G loss: 4.877956]\n",
            "8280 [D loss: 0.343655, acc.: 85.55%] [G loss: 5.068187]\n",
            "8300 [D loss: 0.391632, acc.: 83.20%] [G loss: 4.461371]\n",
            "8320 [D loss: 0.450425, acc.: 79.30%] [G loss: 4.721270]\n",
            "8340 [D loss: 0.435197, acc.: 76.95%] [G loss: 4.621767]\n",
            "8360 [D loss: 0.464482, acc.: 76.56%] [G loss: 4.313802]\n",
            "8380 [D loss: 0.464122, acc.: 81.25%] [G loss: 4.049200]\n",
            "8400 [D loss: 0.413482, acc.: 82.81%] [G loss: 4.400159]\n",
            "8420 [D loss: 0.450674, acc.: 77.34%] [G loss: 4.579131]\n",
            "8440 [D loss: 0.419680, acc.: 82.03%] [G loss: 5.217090]\n",
            "8460 [D loss: 0.483819, acc.: 76.17%] [G loss: 4.116592]\n",
            "8480 [D loss: 0.521834, acc.: 75.39%] [G loss: 4.632746]\n",
            "8500 [D loss: 0.444417, acc.: 79.30%] [G loss: 4.122975]\n",
            "8520 [D loss: 0.467254, acc.: 76.17%] [G loss: 4.956676]\n",
            "8540 [D loss: 0.454999, acc.: 81.25%] [G loss: 4.244261]\n",
            "8560 [D loss: 0.445828, acc.: 80.86%] [G loss: 4.550985]\n",
            "8580 [D loss: 0.515280, acc.: 76.56%] [G loss: 4.022831]\n",
            "8600 [D loss: 0.414122, acc.: 81.25%] [G loss: 4.489841]\n",
            "8620 [D loss: 0.445019, acc.: 80.08%] [G loss: 4.639054]\n",
            "8640 [D loss: 0.433097, acc.: 80.08%] [G loss: 4.542360]\n",
            "8660 [D loss: 0.508373, acc.: 75.78%] [G loss: 4.007074]\n",
            "8680 [D loss: 0.494769, acc.: 78.52%] [G loss: 4.564508]\n",
            "8700 [D loss: 0.461846, acc.: 78.91%] [G loss: 4.268891]\n",
            "8720 [D loss: 0.558022, acc.: 70.31%] [G loss: 4.238678]\n",
            "8740 [D loss: 0.403040, acc.: 79.69%] [G loss: 4.628141]\n",
            "8760 [D loss: 0.450536, acc.: 79.30%] [G loss: 4.304743]\n",
            "8780 [D loss: 0.474493, acc.: 78.91%] [G loss: 4.660646]\n",
            "8800 [D loss: 0.416110, acc.: 80.86%] [G loss: 4.808954]\n",
            "8820 [D loss: 0.419097, acc.: 81.64%] [G loss: 4.747538]\n",
            "8840 [D loss: 0.523655, acc.: 74.22%] [G loss: 4.743878]\n",
            "8860 [D loss: 0.418037, acc.: 82.42%] [G loss: 4.118615]\n",
            "8880 [D loss: 0.424758, acc.: 79.69%] [G loss: 4.643620]\n",
            "8900 [D loss: 0.415731, acc.: 83.59%] [G loss: 4.486989]\n",
            "8920 [D loss: 0.442579, acc.: 82.81%] [G loss: 4.787256]\n",
            "8940 [D loss: 0.378622, acc.: 84.77%] [G loss: 4.496308]\n",
            "8960 [D loss: 0.372041, acc.: 83.59%] [G loss: 4.886470]\n",
            "8980 [D loss: 0.446799, acc.: 82.03%] [G loss: 4.024635]\n",
            "9000 [D loss: 0.406316, acc.: 83.98%] [G loss: 4.558497]\n",
            "9020 [D loss: 0.429719, acc.: 83.98%] [G loss: 4.531271]\n",
            "9040 [D loss: 0.460592, acc.: 76.95%] [G loss: 4.689012]\n",
            "9060 [D loss: 0.357934, acc.: 85.94%] [G loss: 4.852410]\n",
            "9080 [D loss: 0.388284, acc.: 83.59%] [G loss: 4.941038]\n",
            "9100 [D loss: 0.435654, acc.: 80.47%] [G loss: 4.733607]\n",
            "9120 [D loss: 0.362489, acc.: 84.38%] [G loss: 4.873433]\n",
            "9140 [D loss: 0.408110, acc.: 83.20%] [G loss: 4.610844]\n",
            "9160 [D loss: 0.429552, acc.: 79.69%] [G loss: 4.707217]\n",
            "9180 [D loss: 0.391391, acc.: 83.98%] [G loss: 4.713454]\n",
            "9200 [D loss: 0.421143, acc.: 79.69%] [G loss: 4.861547]\n",
            "9220 [D loss: 0.454530, acc.: 77.73%] [G loss: 4.625570]\n",
            "9240 [D loss: 0.399664, acc.: 82.42%] [G loss: 5.000984]\n",
            "9260 [D loss: 0.430951, acc.: 80.08%] [G loss: 4.620400]\n",
            "9280 [D loss: 0.430116, acc.: 79.30%] [G loss: 4.291290]\n",
            "9300 [D loss: 0.428269, acc.: 80.08%] [G loss: 4.625419]\n",
            "9320 [D loss: 0.442564, acc.: 81.25%] [G loss: 4.962641]\n",
            "9340 [D loss: 0.422421, acc.: 81.64%] [G loss: 5.114931]\n",
            "9360 [D loss: 0.430364, acc.: 78.91%] [G loss: 4.474069]\n",
            "9380 [D loss: 0.399250, acc.: 83.20%] [G loss: 5.020226]\n",
            "9400 [D loss: 0.503349, acc.: 74.61%] [G loss: 4.808851]\n",
            "9420 [D loss: 0.329813, acc.: 86.33%] [G loss: 5.074733]\n",
            "9440 [D loss: 0.436401, acc.: 78.12%] [G loss: 4.595596]\n",
            "9460 [D loss: 0.496773, acc.: 78.91%] [G loss: 4.817913]\n",
            "9480 [D loss: 0.497857, acc.: 76.95%] [G loss: 5.215463]\n",
            "9500 [D loss: 0.463829, acc.: 76.56%] [G loss: 4.363512]\n",
            "9520 [D loss: 0.434687, acc.: 78.91%] [G loss: 4.770395]\n",
            "9540 [D loss: 0.496067, acc.: 78.12%] [G loss: 3.881543]\n",
            "9560 [D loss: 0.213889, acc.: 91.41%] [G loss: 3.149117]\n",
            "9580 [D loss: 0.734178, acc.: 64.45%] [G loss: 2.862748]\n",
            "9600 [D loss: 0.818585, acc.: 60.16%] [G loss: 6.183168]\n",
            "9620 [D loss: 0.551982, acc.: 74.61%] [G loss: 4.424582]\n",
            "9640 [D loss: 0.323390, acc.: 88.28%] [G loss: 3.019742]\n",
            "9660 [D loss: 0.525289, acc.: 71.48%] [G loss: 3.331948]\n",
            "9680 [D loss: 0.628522, acc.: 67.58%] [G loss: 4.980312]\n",
            "9700 [D loss: 0.536022, acc.: 74.61%] [G loss: 4.950047]\n",
            "9720 [D loss: 0.396292, acc.: 80.86%] [G loss: 4.004656]\n",
            "9740 [D loss: 0.527533, acc.: 75.78%] [G loss: 4.494216]\n",
            "9760 [D loss: 0.402482, acc.: 82.03%] [G loss: 5.241381]\n",
            "9780 [D loss: 0.353052, acc.: 86.72%] [G loss: 4.668776]\n",
            "9800 [D loss: 0.456102, acc.: 78.52%] [G loss: 4.356057]\n",
            "9820 [D loss: 0.378511, acc.: 83.59%] [G loss: 5.101445]\n",
            "9840 [D loss: 0.415848, acc.: 81.25%] [G loss: 4.853883]\n",
            "9860 [D loss: 0.412939, acc.: 80.47%] [G loss: 4.987749]\n",
            "9880 [D loss: 0.361143, acc.: 85.55%] [G loss: 5.019882]\n",
            "9900 [D loss: 0.509136, acc.: 73.05%] [G loss: 4.587637]\n",
            "9920 [D loss: 0.474729, acc.: 74.22%] [G loss: 4.705740]\n",
            "9940 [D loss: 0.403932, acc.: 81.64%] [G loss: 4.743962]\n",
            "9960 [D loss: 0.443180, acc.: 78.12%] [G loss: 4.487846]\n",
            "9980 [D loss: 0.418630, acc.: 79.69%] [G loss: 4.729626]\n",
            "10000 [D loss: 0.446188, acc.: 78.91%] [G loss: 4.882220]\n",
            "10020 [D loss: 0.410152, acc.: 82.81%] [G loss: 5.015415]\n",
            "10040 [D loss: 0.575711, acc.: 68.75%] [G loss: 4.078319]\n",
            "10060 [D loss: 0.401933, acc.: 80.86%] [G loss: 4.899767]\n",
            "10080 [D loss: 0.486373, acc.: 76.95%] [G loss: 4.203418]\n",
            "10100 [D loss: 0.448979, acc.: 79.30%] [G loss: 4.505579]\n",
            "10120 [D loss: 0.464840, acc.: 76.56%] [G loss: 4.587682]\n",
            "10140 [D loss: 0.419798, acc.: 79.30%] [G loss: 4.897750]\n",
            "10160 [D loss: 0.375199, acc.: 83.20%] [G loss: 4.840167]\n",
            "10180 [D loss: 0.358824, acc.: 85.55%] [G loss: 4.893845]\n",
            "10200 [D loss: 0.384754, acc.: 83.98%] [G loss: 5.320091]\n",
            "10220 [D loss: 0.370766, acc.: 83.59%] [G loss: 4.855222]\n",
            "10240 [D loss: 0.485331, acc.: 75.00%] [G loss: 4.786329]\n",
            "10260 [D loss: 0.372941, acc.: 82.03%] [G loss: 4.966037]\n",
            "10280 [D loss: 0.422915, acc.: 80.86%] [G loss: 4.900920]\n",
            "10300 [D loss: 0.495204, acc.: 77.34%] [G loss: 5.107511]\n",
            "10320 [D loss: 0.411564, acc.: 81.64%] [G loss: 5.302939]\n",
            "10340 [D loss: 0.463300, acc.: 77.34%] [G loss: 5.056416]\n",
            "10360 [D loss: 0.366472, acc.: 84.77%] [G loss: 4.528131]\n",
            "10380 [D loss: 0.433938, acc.: 79.69%] [G loss: 4.815724]\n",
            "10400 [D loss: 0.425014, acc.: 80.47%] [G loss: 4.701516]\n",
            "10420 [D loss: 0.451267, acc.: 80.86%] [G loss: 4.619641]\n",
            "10440 [D loss: 0.332426, acc.: 87.11%] [G loss: 4.934545]\n",
            "10460 [D loss: 0.369458, acc.: 83.59%] [G loss: 4.915126]\n",
            "10480 [D loss: 0.396546, acc.: 83.20%] [G loss: 5.143405]\n",
            "10500 [D loss: 0.342190, acc.: 85.55%] [G loss: 4.603179]\n",
            "10520 [D loss: 0.467440, acc.: 78.91%] [G loss: 4.477571]\n",
            "10540 [D loss: 0.436679, acc.: 73.83%] [G loss: 5.084624]\n",
            "10560 [D loss: 0.385656, acc.: 82.81%] [G loss: 5.269370]\n",
            "10580 [D loss: 0.494068, acc.: 77.34%] [G loss: 5.014127]\n",
            "10600 [D loss: 0.394993, acc.: 81.25%] [G loss: 5.241515]\n",
            "10620 [D loss: 0.396303, acc.: 80.86%] [G loss: 5.528327]\n",
            "10640 [D loss: 0.268321, acc.: 89.84%] [G loss: 3.594240]\n",
            "10660 [D loss: 0.333500, acc.: 84.38%] [G loss: 3.331997]\n",
            "10680 [D loss: 0.836885, acc.: 58.20%] [G loss: 4.790165]\n",
            "10700 [D loss: 0.451292, acc.: 78.91%] [G loss: 5.634530]\n",
            "10720 [D loss: 0.523128, acc.: 75.00%] [G loss: 5.629357]\n",
            "10740 [D loss: 0.393909, acc.: 81.64%] [G loss: 5.403531]\n",
            "10760 [D loss: 0.495837, acc.: 76.56%] [G loss: 5.855182]\n",
            "10780 [D loss: 0.390965, acc.: 84.38%] [G loss: 5.348322]\n",
            "10800 [D loss: 0.451935, acc.: 79.30%] [G loss: 5.160943]\n",
            "10820 [D loss: 0.355188, acc.: 87.11%] [G loss: 4.911385]\n",
            "10840 [D loss: 0.496621, acc.: 76.56%] [G loss: 4.953232]\n",
            "10860 [D loss: 0.451013, acc.: 79.69%] [G loss: 4.963518]\n",
            "10880 [D loss: 0.440517, acc.: 75.39%] [G loss: 5.434465]\n",
            "10900 [D loss: 0.489761, acc.: 77.73%] [G loss: 5.425671]\n",
            "10920 [D loss: 0.419537, acc.: 81.64%] [G loss: 5.636090]\n",
            "10940 [D loss: 0.404790, acc.: 82.81%] [G loss: 5.259549]\n",
            "10960 [D loss: 0.350341, acc.: 85.16%] [G loss: 5.319713]\n",
            "10980 [D loss: 0.359147, acc.: 83.59%] [G loss: 4.913852]\n",
            "11000 [D loss: 0.399719, acc.: 82.81%] [G loss: 5.288548]\n",
            "11020 [D loss: 0.399698, acc.: 82.03%] [G loss: 5.430717]\n",
            "11040 [D loss: 0.419493, acc.: 80.08%] [G loss: 5.009459]\n",
            "11060 [D loss: 0.464421, acc.: 78.91%] [G loss: 4.775235]\n",
            "11080 [D loss: 0.344406, acc.: 85.94%] [G loss: 5.122595]\n",
            "11100 [D loss: 0.347601, acc.: 84.77%] [G loss: 4.798695]\n",
            "11120 [D loss: 0.492728, acc.: 75.78%] [G loss: 4.605649]\n",
            "11140 [D loss: 0.417832, acc.: 80.86%] [G loss: 5.458483]\n",
            "11160 [D loss: 0.422043, acc.: 82.81%] [G loss: 5.109190]\n",
            "11180 [D loss: 0.340653, acc.: 86.33%] [G loss: 5.689024]\n",
            "11200 [D loss: 0.389694, acc.: 83.98%] [G loss: 5.411591]\n",
            "11220 [D loss: 0.406294, acc.: 76.95%] [G loss: 4.929880]\n",
            "11240 [D loss: 0.382116, acc.: 84.77%] [G loss: 5.208610]\n",
            "11260 [D loss: 0.386659, acc.: 83.98%] [G loss: 5.481782]\n",
            "11280 [D loss: 0.366539, acc.: 85.16%] [G loss: 5.337728]\n",
            "11300 [D loss: 0.457340, acc.: 78.52%] [G loss: 4.559757]\n",
            "11320 [D loss: 0.454996, acc.: 78.12%] [G loss: 5.450555]\n",
            "11340 [D loss: 0.399445, acc.: 82.42%] [G loss: 4.878756]\n",
            "11360 [D loss: 0.438293, acc.: 80.47%] [G loss: 5.147255]\n",
            "11380 [D loss: 0.472171, acc.: 77.34%] [G loss: 4.640181]\n",
            "11400 [D loss: 0.421914, acc.: 80.08%] [G loss: 4.849022]\n",
            "11420 [D loss: 0.444233, acc.: 79.69%] [G loss: 4.905552]\n",
            "11440 [D loss: 0.413176, acc.: 81.25%] [G loss: 5.142303]\n",
            "11460 [D loss: 0.431507, acc.: 81.25%] [G loss: 4.816772]\n",
            "11480 [D loss: 0.383659, acc.: 83.20%] [G loss: 5.196069]\n",
            "11500 [D loss: 0.385614, acc.: 82.81%] [G loss: 5.332002]\n",
            "11520 [D loss: 0.393623, acc.: 80.47%] [G loss: 5.251717]\n",
            "11540 [D loss: 0.440175, acc.: 79.30%] [G loss: 4.752868]\n",
            "11560 [D loss: 0.448988, acc.: 80.08%] [G loss: 5.215461]\n",
            "11580 [D loss: 0.425406, acc.: 81.64%] [G loss: 5.187115]\n",
            "11600 [D loss: 0.393310, acc.: 81.25%] [G loss: 5.125435]\n",
            "11620 [D loss: 0.371746, acc.: 83.98%] [G loss: 5.713981]\n",
            "11640 [D loss: 0.370914, acc.: 83.59%] [G loss: 5.438190]\n",
            "11660 [D loss: 0.404276, acc.: 80.08%] [G loss: 5.812816]\n",
            "11680 [D loss: 0.311055, acc.: 87.89%] [G loss: 6.133957]\n",
            "11700 [D loss: 0.377688, acc.: 84.77%] [G loss: 5.967576]\n",
            "11720 [D loss: 0.404799, acc.: 79.69%] [G loss: 5.185723]\n",
            "11740 [D loss: 0.358628, acc.: 83.20%] [G loss: 4.971561]\n",
            "11760 [D loss: 0.355555, acc.: 83.98%] [G loss: 5.480636]\n",
            "11780 [D loss: 0.350601, acc.: 85.55%] [G loss: 4.888783]\n",
            "11800 [D loss: 0.497967, acc.: 75.78%] [G loss: 5.226287]\n",
            "11820 [D loss: 0.428572, acc.: 78.91%] [G loss: 5.246504]\n",
            "11840 [D loss: 0.427988, acc.: 78.52%] [G loss: 5.202655]\n",
            "11860 [D loss: 0.361948, acc.: 85.16%] [G loss: 5.613132]\n",
            "11880 [D loss: 0.421719, acc.: 81.64%] [G loss: 4.929526]\n",
            "11900 [D loss: 0.442316, acc.: 82.03%] [G loss: 5.717480]\n",
            "11920 [D loss: 0.377654, acc.: 80.47%] [G loss: 5.767615]\n",
            "11940 [D loss: 0.360441, acc.: 85.16%] [G loss: 4.312012]\n",
            "11960 [D loss: 0.300574, acc.: 88.28%] [G loss: 3.245686]\n",
            "11980 [D loss: 0.933560, acc.: 55.47%] [G loss: 4.099139]\n",
            "12000 [D loss: 0.567492, acc.: 74.22%] [G loss: 5.601203]\n",
            "12020 [D loss: 0.394550, acc.: 83.59%] [G loss: 5.870492]\n",
            "12040 [D loss: 0.384195, acc.: 83.98%] [G loss: 5.649734]\n",
            "12060 [D loss: 0.433226, acc.: 78.91%] [G loss: 5.412020]\n",
            "12080 [D loss: 0.384923, acc.: 83.20%] [G loss: 5.922874]\n",
            "12100 [D loss: 0.380173, acc.: 82.03%] [G loss: 5.267431]\n",
            "12120 [D loss: 0.354983, acc.: 86.72%] [G loss: 5.448306]\n",
            "12140 [D loss: 0.466459, acc.: 77.73%] [G loss: 5.710581]\n",
            "12160 [D loss: 0.262427, acc.: 91.41%] [G loss: 6.133436]\n",
            "12180 [D loss: 0.414443, acc.: 78.52%] [G loss: 5.536724]\n",
            "12200 [D loss: 0.372253, acc.: 84.38%] [G loss: 6.058190]\n",
            "12220 [D loss: 0.347257, acc.: 85.55%] [G loss: 5.480070]\n",
            "12240 [D loss: 0.367087, acc.: 84.38%] [G loss: 6.272826]\n",
            "12260 [D loss: 0.391568, acc.: 79.30%] [G loss: 5.323415]\n",
            "12280 [D loss: 0.417134, acc.: 78.91%] [G loss: 5.522684]\n",
            "12300 [D loss: 0.383903, acc.: 81.25%] [G loss: 5.519178]\n",
            "12320 [D loss: 0.378010, acc.: 83.59%] [G loss: 5.978211]\n",
            "12340 [D loss: 0.326033, acc.: 86.33%] [G loss: 5.958590]\n",
            "12360 [D loss: 0.329661, acc.: 87.11%] [G loss: 5.882007]\n",
            "12380 [D loss: 0.371883, acc.: 85.16%] [G loss: 5.497624]\n",
            "12400 [D loss: 0.403750, acc.: 80.86%] [G loss: 5.336827]\n",
            "12420 [D loss: 0.358533, acc.: 85.16%] [G loss: 5.611672]\n",
            "12440 [D loss: 0.404064, acc.: 81.25%] [G loss: 5.417833]\n",
            "12460 [D loss: 0.457135, acc.: 77.73%] [G loss: 5.015707]\n",
            "12480 [D loss: 0.370300, acc.: 83.98%] [G loss: 5.379239]\n",
            "12500 [D loss: 0.376555, acc.: 85.16%] [G loss: 5.889815]\n",
            "12520 [D loss: 0.426866, acc.: 79.30%] [G loss: 5.560868]\n",
            "12540 [D loss: 0.398566, acc.: 81.64%] [G loss: 5.532619]\n",
            "12560 [D loss: 0.331112, acc.: 85.16%] [G loss: 5.518989]\n",
            "12580 [D loss: 0.414125, acc.: 82.42%] [G loss: 6.373175]\n",
            "12600 [D loss: 0.398754, acc.: 81.64%] [G loss: 5.113152]\n",
            "12620 [D loss: 0.441586, acc.: 77.34%] [G loss: 5.798169]\n",
            "12640 [D loss: 0.422790, acc.: 80.47%] [G loss: 5.325742]\n",
            "12660 [D loss: 0.356213, acc.: 85.94%] [G loss: 6.112675]\n",
            "12680 [D loss: 0.409566, acc.: 78.91%] [G loss: 5.686810]\n",
            "12700 [D loss: 0.375890, acc.: 82.81%] [G loss: 5.608773]\n",
            "12720 [D loss: 0.309722, acc.: 87.11%] [G loss: 6.142492]\n",
            "12740 [D loss: 0.316013, acc.: 89.45%] [G loss: 5.183302]\n",
            "12760 [D loss: 0.332229, acc.: 85.94%] [G loss: 5.554205]\n",
            "12780 [D loss: 0.473701, acc.: 78.12%] [G loss: 5.508623]\n",
            "12800 [D loss: 0.326893, acc.: 85.94%] [G loss: 5.961319]\n",
            "12820 [D loss: 0.379821, acc.: 82.03%] [G loss: 5.709311]\n",
            "12840 [D loss: 0.348136, acc.: 86.72%] [G loss: 5.733764]\n",
            "12860 [D loss: 0.365921, acc.: 85.94%] [G loss: 5.267403]\n",
            "12880 [D loss: 0.473818, acc.: 78.12%] [G loss: 5.015100]\n",
            "12900 [D loss: 0.388453, acc.: 83.59%] [G loss: 5.632430]\n",
            "12920 [D loss: 0.412578, acc.: 80.86%] [G loss: 5.292470]\n",
            "12940 [D loss: 0.420454, acc.: 79.69%] [G loss: 6.090324]\n",
            "12960 [D loss: 0.330676, acc.: 84.38%] [G loss: 6.274776]\n",
            "12980 [D loss: 0.325952, acc.: 87.89%] [G loss: 6.080885]\n",
            "13000 [D loss: 0.392209, acc.: 82.03%] [G loss: 6.153457]\n",
            "13020 [D loss: 0.291795, acc.: 89.45%] [G loss: 5.745456]\n",
            "13040 [D loss: 0.434288, acc.: 80.47%] [G loss: 5.643303]\n",
            "13060 [D loss: 0.468479, acc.: 76.95%] [G loss: 5.241950]\n",
            "13080 [D loss: 0.336457, acc.: 85.94%] [G loss: 6.132161]\n",
            "13100 [D loss: 0.366932, acc.: 84.77%] [G loss: 6.261245]\n",
            "13120 [D loss: 0.327160, acc.: 86.72%] [G loss: 6.042880]\n",
            "13140 [D loss: 0.347647, acc.: 84.77%] [G loss: 6.094627]\n",
            "13160 [D loss: 0.414867, acc.: 79.69%] [G loss: 5.992234]\n",
            "13180 [D loss: 0.314972, acc.: 87.50%] [G loss: 5.822639]\n",
            "13200 [D loss: 0.347329, acc.: 84.77%] [G loss: 5.449367]\n",
            "13220 [D loss: 0.371441, acc.: 83.59%] [G loss: 5.533103]\n",
            "13240 [D loss: 0.395764, acc.: 80.08%] [G loss: 5.804195]\n",
            "13260 [D loss: 0.339982, acc.: 86.33%] [G loss: 6.107881]\n",
            "13280 [D loss: 0.396547, acc.: 82.03%] [G loss: 5.205791]\n",
            "13300 [D loss: 0.264270, acc.: 91.02%] [G loss: 5.423467]\n",
            "13320 [D loss: 0.719425, acc.: 67.97%] [G loss: 4.895162]\n",
            "13340 [D loss: 0.291886, acc.: 90.23%] [G loss: 5.449629]\n",
            "13360 [D loss: 0.339581, acc.: 84.38%] [G loss: 6.392636]\n",
            "13380 [D loss: 0.471879, acc.: 76.56%] [G loss: 5.679469]\n",
            "13400 [D loss: 0.461517, acc.: 79.30%] [G loss: 5.820807]\n",
            "13420 [D loss: 0.346495, acc.: 82.42%] [G loss: 5.653154]\n",
            "13440 [D loss: 0.389049, acc.: 83.59%] [G loss: 5.567899]\n",
            "13460 [D loss: 0.269496, acc.: 88.28%] [G loss: 6.195388]\n",
            "13480 [D loss: 0.359417, acc.: 83.98%] [G loss: 5.761148]\n",
            "13500 [D loss: 0.426787, acc.: 79.69%] [G loss: 6.347794]\n",
            "13520 [D loss: 0.347586, acc.: 86.33%] [G loss: 5.982882]\n",
            "13540 [D loss: 0.339921, acc.: 85.94%] [G loss: 6.056694]\n",
            "13560 [D loss: 0.439902, acc.: 83.20%] [G loss: 5.242191]\n",
            "13580 [D loss: 0.372562, acc.: 85.94%] [G loss: 7.309321]\n",
            "13600 [D loss: 0.351827, acc.: 86.72%] [G loss: 5.647311]\n",
            "13620 [D loss: 0.402840, acc.: 82.03%] [G loss: 5.528349]\n",
            "13640 [D loss: 0.426404, acc.: 82.03%] [G loss: 6.629462]\n",
            "13660 [D loss: 0.355599, acc.: 82.42%] [G loss: 6.098036]\n",
            "13680 [D loss: 0.294297, acc.: 89.06%] [G loss: 6.015219]\n",
            "13700 [D loss: 0.353335, acc.: 85.16%] [G loss: 5.596107]\n",
            "13720 [D loss: 0.375260, acc.: 82.81%] [G loss: 6.989185]\n",
            "13740 [D loss: 0.457202, acc.: 77.34%] [G loss: 6.157535]\n",
            "13760 [D loss: 0.300078, acc.: 87.89%] [G loss: 6.093851]\n",
            "13780 [D loss: 0.300677, acc.: 87.50%] [G loss: 6.556529]\n",
            "13800 [D loss: 0.379395, acc.: 83.59%] [G loss: 6.249478]\n",
            "13820 [D loss: 0.412470, acc.: 84.38%] [G loss: 5.569220]\n",
            "13840 [D loss: 0.405519, acc.: 83.98%] [G loss: 5.970872]\n",
            "13860 [D loss: 0.345835, acc.: 83.98%] [G loss: 5.997483]\n",
            "13880 [D loss: 0.357748, acc.: 86.33%] [G loss: 5.690675]\n",
            "13900 [D loss: 0.309004, acc.: 86.72%] [G loss: 6.941115]\n",
            "13920 [D loss: 0.293935, acc.: 88.67%] [G loss: 6.642017]\n",
            "13940 [D loss: 0.403417, acc.: 81.25%] [G loss: 5.416062]\n",
            "13960 [D loss: 0.312857, acc.: 85.94%] [G loss: 6.378864]\n",
            "13980 [D loss: 0.417658, acc.: 83.59%] [G loss: 5.542074]\n",
            "14000 [D loss: 0.284846, acc.: 88.28%] [G loss: 6.869186]\n",
            "14020 [D loss: 0.323633, acc.: 89.45%] [G loss: 5.428090]\n",
            "14040 [D loss: 0.418161, acc.: 81.25%] [G loss: 6.109464]\n",
            "14060 [D loss: 0.261743, acc.: 90.23%] [G loss: 4.353913]\n",
            "14080 [D loss: 0.413301, acc.: 82.03%] [G loss: 5.114247]\n",
            "14100 [D loss: 0.461376, acc.: 78.12%] [G loss: 7.534500]\n",
            "14120 [D loss: 0.287895, acc.: 90.62%] [G loss: 7.368559]\n",
            "14140 [D loss: 0.398067, acc.: 80.47%] [G loss: 7.183949]\n",
            "14160 [D loss: 0.332274, acc.: 87.11%] [G loss: 6.280947]\n",
            "14180 [D loss: 0.438608, acc.: 82.42%] [G loss: 6.828174]\n",
            "14200 [D loss: 0.255044, acc.: 92.97%] [G loss: 7.202744]\n",
            "14220 [D loss: 0.342124, acc.: 84.77%] [G loss: 6.193193]\n",
            "14240 [D loss: 0.335056, acc.: 84.77%] [G loss: 6.546109]\n",
            "14260 [D loss: 0.361926, acc.: 84.38%] [G loss: 6.236453]\n",
            "14280 [D loss: 0.253380, acc.: 90.62%] [G loss: 6.150144]\n",
            "14300 [D loss: 0.305694, acc.: 88.28%] [G loss: 7.141849]\n",
            "14320 [D loss: 0.306029, acc.: 90.23%] [G loss: 6.022050]\n",
            "14340 [D loss: 0.453507, acc.: 77.73%] [G loss: 5.446638]\n",
            "14360 [D loss: 0.301949, acc.: 85.16%] [G loss: 7.303663]\n",
            "14380 [D loss: 0.432122, acc.: 78.91%] [G loss: 5.774291]\n",
            "14400 [D loss: 0.314446, acc.: 83.98%] [G loss: 6.725939]\n",
            "14420 [D loss: 0.340659, acc.: 83.59%] [G loss: 6.708808]\n",
            "14440 [D loss: 0.320345, acc.: 87.89%] [G loss: 6.367020]\n",
            "14460 [D loss: 0.317644, acc.: 87.11%] [G loss: 5.732747]\n",
            "14480 [D loss: 0.308173, acc.: 85.16%] [G loss: 7.330625]\n",
            "14500 [D loss: 0.338685, acc.: 83.98%] [G loss: 6.672957]\n",
            "14520 [D loss: 0.416461, acc.: 79.69%] [G loss: 6.934550]\n",
            "14540 [D loss: 0.378538, acc.: 83.20%] [G loss: 6.125237]\n",
            "14560 [D loss: 0.312952, acc.: 87.11%] [G loss: 6.900836]\n",
            "14580 [D loss: 0.259133, acc.: 90.23%] [G loss: 6.650978]\n",
            "14600 [D loss: 0.484129, acc.: 76.95%] [G loss: 6.304809]\n",
            "14620 [D loss: 0.245665, acc.: 90.23%] [G loss: 7.485578]\n",
            "14640 [D loss: 0.416954, acc.: 82.81%] [G loss: 6.322415]\n",
            "14660 [D loss: 0.389211, acc.: 82.42%] [G loss: 6.685702]\n",
            "14680 [D loss: 0.399046, acc.: 82.42%] [G loss: 5.862173]\n",
            "14700 [D loss: 0.358573, acc.: 84.77%] [G loss: 6.465609]\n",
            "14720 [D loss: 0.312454, acc.: 86.33%] [G loss: 6.456112]\n",
            "14740 [D loss: 0.373351, acc.: 85.55%] [G loss: 6.195218]\n",
            "14760 [D loss: 0.359301, acc.: 84.38%] [G loss: 5.836758]\n",
            "14780 [D loss: 0.333058, acc.: 84.77%] [G loss: 6.907163]\n",
            "14800 [D loss: 0.313566, acc.: 87.11%] [G loss: 6.633384]\n",
            "14820 [D loss: 0.332800, acc.: 86.33%] [G loss: 6.308480]\n",
            "14840 [D loss: 0.328745, acc.: 87.89%] [G loss: 6.065392]\n",
            "14860 [D loss: 0.435673, acc.: 80.47%] [G loss: 5.545542]\n",
            "14880 [D loss: 0.294503, acc.: 87.50%] [G loss: 6.635432]\n",
            "14900 [D loss: 0.372896, acc.: 81.25%] [G loss: 6.899356]\n",
            "14920 [D loss: 0.350821, acc.: 86.33%] [G loss: 6.513855]\n",
            "14940 [D loss: 0.364285, acc.: 85.55%] [G loss: 6.406915]\n",
            "14960 [D loss: 0.301735, acc.: 87.11%] [G loss: 6.646298]\n",
            "14980 [D loss: 0.326279, acc.: 85.55%] [G loss: 7.544748]\n",
            "15000 [D loss: 0.348254, acc.: 86.72%] [G loss: 7.088714]\n",
            "15020 [D loss: 0.387221, acc.: 85.94%] [G loss: 6.802312]\n",
            "15040 [D loss: 0.335573, acc.: 89.45%] [G loss: 6.871318]\n",
            "15060 [D loss: 0.275146, acc.: 89.45%] [G loss: 6.345190]\n",
            "15080 [D loss: 0.349685, acc.: 85.55%] [G loss: 6.747897]\n",
            "15100 [D loss: 0.349725, acc.: 84.38%] [G loss: 6.227100]\n",
            "15120 [D loss: 0.319280, acc.: 82.03%] [G loss: 6.758388]\n",
            "15140 [D loss: 0.374680, acc.: 83.59%] [G loss: 6.620772]\n",
            "15160 [D loss: 0.256075, acc.: 90.62%] [G loss: 7.424584]\n",
            "15180 [D loss: 0.422042, acc.: 81.64%] [G loss: 6.296021]\n",
            "15200 [D loss: 0.287753, acc.: 87.50%] [G loss: 6.628824]\n",
            "15220 [D loss: 0.269249, acc.: 89.06%] [G loss: 6.370145]\n",
            "15240 [D loss: 0.342222, acc.: 81.25%] [G loss: 6.275812]\n",
            "15260 [D loss: 0.275980, acc.: 88.28%] [G loss: 6.026658]\n",
            "15280 [D loss: 0.309238, acc.: 86.72%] [G loss: 6.660358]\n",
            "15300 [D loss: 0.416287, acc.: 82.03%] [G loss: 6.574394]\n",
            "15320 [D loss: 0.347983, acc.: 85.16%] [G loss: 6.745453]\n",
            "15340 [D loss: 0.277011, acc.: 88.67%] [G loss: 6.273462]\n",
            "15360 [D loss: 0.300505, acc.: 86.33%] [G loss: 6.810410]\n",
            "15380 [D loss: 0.236990, acc.: 91.41%] [G loss: 6.813428]\n",
            "15400 [D loss: 0.294236, acc.: 87.11%] [G loss: 6.232323]\n",
            "15420 [D loss: 0.320071, acc.: 85.55%] [G loss: 7.068229]\n",
            "15440 [D loss: 0.326696, acc.: 85.55%] [G loss: 6.362178]\n",
            "15460 [D loss: 0.286238, acc.: 89.84%] [G loss: 7.057376]\n",
            "15480 [D loss: 0.385547, acc.: 82.81%] [G loss: 6.808632]\n",
            "15500 [D loss: 0.293000, acc.: 89.45%] [G loss: 7.433755]\n",
            "15520 [D loss: 0.265109, acc.: 88.28%] [G loss: 6.874872]\n",
            "15540 [D loss: 0.446864, acc.: 80.08%] [G loss: 6.599197]\n",
            "15560 [D loss: 0.357087, acc.: 84.38%] [G loss: 6.868115]\n",
            "15580 [D loss: 0.298627, acc.: 88.28%] [G loss: 6.548272]\n",
            "15600 [D loss: 0.396041, acc.: 82.81%] [G loss: 6.100401]\n",
            "15620 [D loss: 0.354617, acc.: 83.59%] [G loss: 6.442180]\n",
            "15640 [D loss: 0.339523, acc.: 87.89%] [G loss: 6.502493]\n",
            "15660 [D loss: 0.317409, acc.: 85.55%] [G loss: 6.452634]\n",
            "15680 [D loss: 0.304161, acc.: 89.06%] [G loss: 7.551525]\n",
            "15700 [D loss: 0.360077, acc.: 83.98%] [G loss: 7.104665]\n",
            "15720 [D loss: 0.347308, acc.: 85.55%] [G loss: 6.943678]\n",
            "15740 [D loss: 0.341177, acc.: 88.28%] [G loss: 7.737453]\n",
            "15760 [D loss: 0.346869, acc.: 85.16%] [G loss: 6.774923]\n",
            "15780 [D loss: 0.370005, acc.: 83.59%] [G loss: 6.184312]\n",
            "15800 [D loss: 0.306859, acc.: 85.94%] [G loss: 6.943725]\n",
            "15820 [D loss: 0.371402, acc.: 82.42%] [G loss: 6.984959]\n",
            "15840 [D loss: 0.407651, acc.: 82.42%] [G loss: 6.595233]\n",
            "15860 [D loss: 0.333899, acc.: 86.72%] [G loss: 6.623397]\n",
            "15880 [D loss: 0.403159, acc.: 81.64%] [G loss: 6.446514]\n",
            "15900 [D loss: 0.266128, acc.: 89.84%] [G loss: 7.233557]\n",
            "15920 [D loss: 0.263096, acc.: 89.45%] [G loss: 4.663024]\n",
            "15940 [D loss: 1.489011, acc.: 39.06%] [G loss: 4.228615]\n",
            "15960 [D loss: 1.446993, acc.: 35.16%] [G loss: 3.576192]\n",
            "15980 [D loss: 0.543816, acc.: 75.39%] [G loss: 5.351245]\n",
            "16000 [D loss: 0.423456, acc.: 84.77%] [G loss: 6.389330]\n",
            "16020 [D loss: 0.372977, acc.: 84.77%] [G loss: 6.285897]\n",
            "16040 [D loss: 0.321273, acc.: 85.55%] [G loss: 6.668163]\n",
            "16060 [D loss: 0.209253, acc.: 92.19%] [G loss: 7.846203]\n",
            "16080 [D loss: 0.418959, acc.: 81.64%] [G loss: 6.478230]\n",
            "16100 [D loss: 0.290355, acc.: 88.67%] [G loss: 7.444997]\n",
            "16120 [D loss: 0.451434, acc.: 80.86%] [G loss: 6.301401]\n",
            "16140 [D loss: 0.328394, acc.: 87.11%] [G loss: 6.021435]\n",
            "16160 [D loss: 0.292134, acc.: 90.23%] [G loss: 7.098208]\n",
            "16180 [D loss: 0.317309, acc.: 83.98%] [G loss: 6.525447]\n",
            "16200 [D loss: 0.270237, acc.: 90.23%] [G loss: 7.434779]\n",
            "16220 [D loss: 0.336311, acc.: 87.50%] [G loss: 6.991700]\n",
            "16240 [D loss: 0.241565, acc.: 89.45%] [G loss: 7.805356]\n",
            "16260 [D loss: 0.346332, acc.: 87.11%] [G loss: 6.385931]\n",
            "16280 [D loss: 0.253002, acc.: 89.84%] [G loss: 6.423689]\n",
            "16300 [D loss: 0.338440, acc.: 86.33%] [G loss: 6.435522]\n",
            "16320 [D loss: 0.328373, acc.: 85.94%] [G loss: 6.446262]\n",
            "16340 [D loss: 0.297951, acc.: 87.89%] [G loss: 6.664164]\n",
            "16360 [D loss: 0.294918, acc.: 89.06%] [G loss: 7.363577]\n",
            "16380 [D loss: 0.332329, acc.: 87.11%] [G loss: 6.964708]\n",
            "16400 [D loss: 0.354879, acc.: 87.11%] [G loss: 6.648660]\n",
            "16420 [D loss: 0.372095, acc.: 83.20%] [G loss: 6.716496]\n",
            "16440 [D loss: 0.276707, acc.: 89.84%] [G loss: 7.070796]\n",
            "16460 [D loss: 0.391506, acc.: 82.81%] [G loss: 7.090064]\n",
            "16480 [D loss: 0.267094, acc.: 89.45%] [G loss: 7.872658]\n",
            "16500 [D loss: 0.249378, acc.: 91.80%] [G loss: 6.298358]\n",
            "16520 [D loss: 0.318036, acc.: 84.77%] [G loss: 6.626436]\n",
            "16540 [D loss: 0.329129, acc.: 86.33%] [G loss: 6.417892]\n",
            "16560 [D loss: 0.331748, acc.: 87.11%] [G loss: 7.110525]\n",
            "16580 [D loss: 0.328703, acc.: 89.06%] [G loss: 7.814747]\n",
            "16600 [D loss: 0.306871, acc.: 86.33%] [G loss: 7.267191]\n",
            "16620 [D loss: 0.303580, acc.: 88.28%] [G loss: 6.823442]\n",
            "16640 [D loss: 0.335926, acc.: 85.55%] [G loss: 7.444491]\n",
            "16660 [D loss: 0.261764, acc.: 89.06%] [G loss: 7.264169]\n",
            "16680 [D loss: 0.357545, acc.: 86.33%] [G loss: 6.316022]\n",
            "16700 [D loss: 0.326072, acc.: 85.55%] [G loss: 7.283370]\n",
            "16720 [D loss: 0.365972, acc.: 84.38%] [G loss: 6.610703]\n",
            "16740 [D loss: 0.364077, acc.: 84.38%] [G loss: 6.763168]\n",
            "16760 [D loss: 0.375916, acc.: 83.20%] [G loss: 6.244235]\n",
            "16780 [D loss: 0.245386, acc.: 91.80%] [G loss: 7.290134]\n",
            "16800 [D loss: 0.264105, acc.: 87.89%] [G loss: 7.299521]\n",
            "16820 [D loss: 0.389101, acc.: 82.42%] [G loss: 7.155339]\n",
            "16840 [D loss: 0.299101, acc.: 88.67%] [G loss: 7.336498]\n",
            "16860 [D loss: 0.326876, acc.: 87.50%] [G loss: 7.627811]\n",
            "16880 [D loss: 0.306326, acc.: 87.50%] [G loss: 7.200567]\n",
            "16900 [D loss: 0.277840, acc.: 89.45%] [G loss: 7.295238]\n",
            "16920 [D loss: 0.271619, acc.: 88.28%] [G loss: 7.527098]\n",
            "16940 [D loss: 0.447956, acc.: 80.47%] [G loss: 6.888499]\n",
            "16960 [D loss: 0.260203, acc.: 87.11%] [G loss: 7.612825]\n",
            "16980 [D loss: 0.407541, acc.: 80.08%] [G loss: 6.146461]\n",
            "17000 [D loss: 0.313712, acc.: 86.33%] [G loss: 7.252116]\n",
            "17020 [D loss: 0.322383, acc.: 87.11%] [G loss: 7.116953]\n",
            "17040 [D loss: 0.350891, acc.: 85.94%] [G loss: 5.778086]\n",
            "17060 [D loss: 0.279776, acc.: 88.67%] [G loss: 7.233074]\n",
            "17080 [D loss: 0.374751, acc.: 84.77%] [G loss: 7.191188]\n",
            "17100 [D loss: 0.382637, acc.: 83.59%] [G loss: 6.913104]\n",
            "17120 [D loss: 0.345611, acc.: 83.59%] [G loss: 7.210632]\n",
            "17140 [D loss: 0.303632, acc.: 87.11%] [G loss: 7.321196]\n",
            "17160 [D loss: 0.277408, acc.: 89.84%] [G loss: 6.634642]\n",
            "17180 [D loss: 0.217449, acc.: 91.41%] [G loss: 7.326530]\n",
            "17200 [D loss: 0.312978, acc.: 87.11%] [G loss: 7.096844]\n",
            "17220 [D loss: 0.369925, acc.: 82.03%] [G loss: 6.189439]\n",
            "17240 [D loss: 0.230820, acc.: 91.02%] [G loss: 7.687977]\n",
            "17260 [D loss: 0.307437, acc.: 89.06%] [G loss: 6.319919]\n",
            "17280 [D loss: 0.423093, acc.: 81.64%] [G loss: 6.406345]\n",
            "17300 [D loss: 0.370508, acc.: 85.16%] [G loss: 6.583936]\n",
            "17320 [D loss: 0.355609, acc.: 86.72%] [G loss: 7.090985]\n",
            "17340 [D loss: 0.260083, acc.: 90.23%] [G loss: 6.982968]\n",
            "17360 [D loss: 0.300784, acc.: 85.55%] [G loss: 7.141093]\n",
            "17380 [D loss: 0.281773, acc.: 89.84%] [G loss: 7.430314]\n",
            "17400 [D loss: 0.288613, acc.: 86.33%] [G loss: 7.634693]\n",
            "17420 [D loss: 0.227713, acc.: 91.41%] [G loss: 7.522998]\n",
            "17440 [D loss: 0.289925, acc.: 87.50%] [G loss: 8.030455]\n",
            "17460 [D loss: 0.272390, acc.: 87.89%] [G loss: 7.644981]\n",
            "17480 [D loss: 0.373550, acc.: 82.42%] [G loss: 8.563738]\n",
            "17500 [D loss: 0.213133, acc.: 91.80%] [G loss: 8.143873]\n",
            "17520 [D loss: 0.286524, acc.: 87.50%] [G loss: 6.820018]\n",
            "17540 [D loss: 0.307152, acc.: 87.50%] [G loss: 7.559629]\n",
            "17560 [D loss: 0.275419, acc.: 89.45%] [G loss: 6.507249]\n",
            "17580 [D loss: 0.417028, acc.: 82.03%] [G loss: 6.285392]\n",
            "17600 [D loss: 0.270422, acc.: 91.02%] [G loss: 7.784558]\n",
            "17620 [D loss: 0.383629, acc.: 81.25%] [G loss: 7.122539]\n",
            "17640 [D loss: 0.221368, acc.: 92.19%] [G loss: 7.324263]\n",
            "17660 [D loss: 0.310597, acc.: 87.11%] [G loss: 7.151425]\n",
            "17680 [D loss: 0.276211, acc.: 89.45%] [G loss: 8.139668]\n",
            "17700 [D loss: 0.257451, acc.: 90.23%] [G loss: 7.262533]\n",
            "17720 [D loss: 0.346159, acc.: 85.55%] [G loss: 7.142326]\n",
            "17740 [D loss: 0.249752, acc.: 91.41%] [G loss: 7.826004]\n",
            "17760 [D loss: 0.368095, acc.: 86.33%] [G loss: 7.388266]\n",
            "17780 [D loss: 0.299206, acc.: 87.50%] [G loss: 7.970586]\n",
            "17800 [D loss: 0.253099, acc.: 89.84%] [G loss: 7.824533]\n",
            "17820 [D loss: 0.304311, acc.: 86.33%] [G loss: 7.776467]\n",
            "17840 [D loss: 0.355614, acc.: 82.03%] [G loss: 7.248128]\n",
            "17860 [D loss: 0.318689, acc.: 87.11%] [G loss: 7.832550]\n",
            "17880 [D loss: 0.318094, acc.: 85.16%] [G loss: 7.641703]\n",
            "17900 [D loss: 0.183013, acc.: 92.19%] [G loss: 7.510130]\n",
            "17920 [D loss: 0.324595, acc.: 88.28%] [G loss: 7.682969]\n",
            "17940 [D loss: 0.315460, acc.: 85.55%] [G loss: 7.079403]\n",
            "17960 [D loss: 0.300372, acc.: 87.11%] [G loss: 6.539782]\n",
            "17980 [D loss: 0.247787, acc.: 88.67%] [G loss: 9.782124]\n",
            "18000 [D loss: 0.290877, acc.: 87.89%] [G loss: 7.606002]\n",
            "18020 [D loss: 0.395400, acc.: 82.81%] [G loss: 6.972595]\n",
            "18040 [D loss: 0.292838, acc.: 86.72%] [G loss: 8.986767]\n",
            "18060 [D loss: 0.351471, acc.: 86.33%] [G loss: 7.717530]\n",
            "18080 [D loss: 0.335142, acc.: 85.94%] [G loss: 7.192846]\n",
            "18100 [D loss: 0.298280, acc.: 87.89%] [G loss: 7.545443]\n",
            "18120 [D loss: 0.311399, acc.: 87.11%] [G loss: 8.306311]\n",
            "18140 [D loss: 0.280586, acc.: 89.45%] [G loss: 8.706306]\n",
            "18160 [D loss: 0.349888, acc.: 87.89%] [G loss: 7.504266]\n",
            "18180 [D loss: 0.205998, acc.: 92.19%] [G loss: 8.018051]\n",
            "18200 [D loss: 0.257358, acc.: 90.23%] [G loss: 7.690401]\n",
            "18220 [D loss: 0.202032, acc.: 93.36%] [G loss: 9.167788]\n",
            "18240 [D loss: 0.188980, acc.: 93.36%] [G loss: 7.757931]\n",
            "18260 [D loss: 0.374448, acc.: 85.55%] [G loss: 6.454613]\n",
            "18280 [D loss: 0.320915, acc.: 83.98%] [G loss: 7.460946]\n",
            "18300 [D loss: 0.328598, acc.: 87.11%] [G loss: 7.405629]\n",
            "18320 [D loss: 0.283725, acc.: 88.28%] [G loss: 7.592457]\n",
            "18340 [D loss: 0.331086, acc.: 85.16%] [G loss: 7.657226]\n",
            "18360 [D loss: 0.205221, acc.: 92.97%] [G loss: 8.101967]\n",
            "18380 [D loss: 0.309239, acc.: 88.28%] [G loss: 7.487924]\n",
            "18400 [D loss: 0.260994, acc.: 89.84%] [G loss: 7.585829]\n",
            "18420 [D loss: 0.298824, acc.: 85.94%] [G loss: 7.283183]\n",
            "18440 [D loss: 0.306284, acc.: 87.50%] [G loss: 7.763386]\n",
            "18460 [D loss: 0.257985, acc.: 90.23%] [G loss: 7.063719]\n",
            "18480 [D loss: 0.298385, acc.: 87.50%] [G loss: 7.911516]\n",
            "18500 [D loss: 0.383202, acc.: 83.20%] [G loss: 8.218309]\n",
            "18520 [D loss: 0.391838, acc.: 82.03%] [G loss: 7.459663]\n",
            "18540 [D loss: 0.347757, acc.: 85.55%] [G loss: 7.919562]\n",
            "18560 [D loss: 0.312514, acc.: 87.89%] [G loss: 6.460115]\n",
            "18580 [D loss: 0.180717, acc.: 93.36%] [G loss: 4.792183]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}