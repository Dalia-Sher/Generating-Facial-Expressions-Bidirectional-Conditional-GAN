{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_Model_BiCoGAN_Rotate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS516rLy1LYQ",
        "outputId": "8c18b575-bec3-4d60-e0d2-b8d3a2f90ce8"
      },
      "source": [
        "!unzip fer2013.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fer2013.zip\n",
            "  inflating: fer2013.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jF8QfAYPg_71",
        "outputId": "6c5151a5-6474-4f95-db9f-93919d4700f6"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "223ade4c-e5bd-4b45-fedd-65dcee9525d0"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5xEsrnIB0t"
      },
      "source": [
        "dic = {0:'Angry', 1:'disgust' , 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRPqlqU1IG94"
      },
      "source": [
        "The emotion disgust has too few samples, therefore we won't use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9swxs5P0w",
        "outputId": "54b88770-534f-466e-d627-6387b76a8bf8"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSAij3uINfw"
      },
      "source": [
        "We will redefine the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "d94d4014-3c4d-4e0a-a687-2375966da8a5"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOXTev833cv",
        "outputId": "51298a20-badb-4cbe-b0c2-6b1ff8e899c1"
      },
      "source": [
        "epochs = X_train.shape[0]\n",
        "print(\"number of epochs:\", epochs)\n",
        "\n",
        "X_train_aug = X_train\n",
        "y_train_aug = y_train\n",
        "\n",
        "for k in range(epochs):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  # contrast = iaa.GammaContrast(gamma=2.0)\n",
        "  contrast = iaa.Affine(rotate=(-50, 30))\n",
        "  contrast_image = contrast.augment_image(img)\n",
        "  contrasted_images.append(contrast_image)\n",
        "\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train_aug = np.concatenate((X_train_aug, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]\n",
        "  y_train_aug = np.concatenate((y_train_aug, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration:\" , k,\", train shape:\", X_train_aug.shape)\n",
        "    # print (\"iteration: %d, train shape: %s\" % (k, X_train_aug.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epochs: 35340\n",
            "iteration: 0 , train shape: (35341, 48, 48, 1)\n",
            "iteration: 100 , train shape: (35441, 48, 48, 1)\n",
            "iteration: 200 , train shape: (35541, 48, 48, 1)\n",
            "iteration: 300 , train shape: (35641, 48, 48, 1)\n",
            "iteration: 400 , train shape: (35741, 48, 48, 1)\n",
            "iteration: 500 , train shape: (35841, 48, 48, 1)\n",
            "iteration: 600 , train shape: (35941, 48, 48, 1)\n",
            "iteration: 700 , train shape: (36041, 48, 48, 1)\n",
            "iteration: 800 , train shape: (36141, 48, 48, 1)\n",
            "iteration: 900 , train shape: (36241, 48, 48, 1)\n",
            "iteration: 1000 , train shape: (36341, 48, 48, 1)\n",
            "iteration: 1100 , train shape: (36441, 48, 48, 1)\n",
            "iteration: 1200 , train shape: (36541, 48, 48, 1)\n",
            "iteration: 1300 , train shape: (36641, 48, 48, 1)\n",
            "iteration: 1400 , train shape: (36741, 48, 48, 1)\n",
            "iteration: 1500 , train shape: (36841, 48, 48, 1)\n",
            "iteration: 1600 , train shape: (36941, 48, 48, 1)\n",
            "iteration: 1700 , train shape: (37041, 48, 48, 1)\n",
            "iteration: 1800 , train shape: (37141, 48, 48, 1)\n",
            "iteration: 1900 , train shape: (37241, 48, 48, 1)\n",
            "iteration: 2000 , train shape: (37341, 48, 48, 1)\n",
            "iteration: 2100 , train shape: (37441, 48, 48, 1)\n",
            "iteration: 2200 , train shape: (37541, 48, 48, 1)\n",
            "iteration: 2300 , train shape: (37641, 48, 48, 1)\n",
            "iteration: 2400 , train shape: (37741, 48, 48, 1)\n",
            "iteration: 2500 , train shape: (37841, 48, 48, 1)\n",
            "iteration: 2600 , train shape: (37941, 48, 48, 1)\n",
            "iteration: 2700 , train shape: (38041, 48, 48, 1)\n",
            "iteration: 2800 , train shape: (38141, 48, 48, 1)\n",
            "iteration: 2900 , train shape: (38241, 48, 48, 1)\n",
            "iteration: 3000 , train shape: (38341, 48, 48, 1)\n",
            "iteration: 3100 , train shape: (38441, 48, 48, 1)\n",
            "iteration: 3200 , train shape: (38541, 48, 48, 1)\n",
            "iteration: 3300 , train shape: (38641, 48, 48, 1)\n",
            "iteration: 3400 , train shape: (38741, 48, 48, 1)\n",
            "iteration: 3500 , train shape: (38841, 48, 48, 1)\n",
            "iteration: 3600 , train shape: (38941, 48, 48, 1)\n",
            "iteration: 3700 , train shape: (39041, 48, 48, 1)\n",
            "iteration: 3800 , train shape: (39141, 48, 48, 1)\n",
            "iteration: 3900 , train shape: (39241, 48, 48, 1)\n",
            "iteration: 4000 , train shape: (39341, 48, 48, 1)\n",
            "iteration: 4100 , train shape: (39441, 48, 48, 1)\n",
            "iteration: 4200 , train shape: (39541, 48, 48, 1)\n",
            "iteration: 4300 , train shape: (39641, 48, 48, 1)\n",
            "iteration: 4400 , train shape: (39741, 48, 48, 1)\n",
            "iteration: 4500 , train shape: (39841, 48, 48, 1)\n",
            "iteration: 4600 , train shape: (39941, 48, 48, 1)\n",
            "iteration: 4700 , train shape: (40041, 48, 48, 1)\n",
            "iteration: 4800 , train shape: (40141, 48, 48, 1)\n",
            "iteration: 4900 , train shape: (40241, 48, 48, 1)\n",
            "iteration: 5000 , train shape: (40341, 48, 48, 1)\n",
            "iteration: 5100 , train shape: (40441, 48, 48, 1)\n",
            "iteration: 5200 , train shape: (40541, 48, 48, 1)\n",
            "iteration: 5300 , train shape: (40641, 48, 48, 1)\n",
            "iteration: 5400 , train shape: (40741, 48, 48, 1)\n",
            "iteration: 5500 , train shape: (40841, 48, 48, 1)\n",
            "iteration: 5600 , train shape: (40941, 48, 48, 1)\n",
            "iteration: 5700 , train shape: (41041, 48, 48, 1)\n",
            "iteration: 5800 , train shape: (41141, 48, 48, 1)\n",
            "iteration: 5900 , train shape: (41241, 48, 48, 1)\n",
            "iteration: 6000 , train shape: (41341, 48, 48, 1)\n",
            "iteration: 6100 , train shape: (41441, 48, 48, 1)\n",
            "iteration: 6200 , train shape: (41541, 48, 48, 1)\n",
            "iteration: 6300 , train shape: (41641, 48, 48, 1)\n",
            "iteration: 6400 , train shape: (41741, 48, 48, 1)\n",
            "iteration: 6500 , train shape: (41841, 48, 48, 1)\n",
            "iteration: 6600 , train shape: (41941, 48, 48, 1)\n",
            "iteration: 6700 , train shape: (42041, 48, 48, 1)\n",
            "iteration: 6800 , train shape: (42141, 48, 48, 1)\n",
            "iteration: 6900 , train shape: (42241, 48, 48, 1)\n",
            "iteration: 7000 , train shape: (42341, 48, 48, 1)\n",
            "iteration: 7100 , train shape: (42441, 48, 48, 1)\n",
            "iteration: 7200 , train shape: (42541, 48, 48, 1)\n",
            "iteration: 7300 , train shape: (42641, 48, 48, 1)\n",
            "iteration: 7400 , train shape: (42741, 48, 48, 1)\n",
            "iteration: 7500 , train shape: (42841, 48, 48, 1)\n",
            "iteration: 7600 , train shape: (42941, 48, 48, 1)\n",
            "iteration: 7700 , train shape: (43041, 48, 48, 1)\n",
            "iteration: 7800 , train shape: (43141, 48, 48, 1)\n",
            "iteration: 7900 , train shape: (43241, 48, 48, 1)\n",
            "iteration: 8000 , train shape: (43341, 48, 48, 1)\n",
            "iteration: 8100 , train shape: (43441, 48, 48, 1)\n",
            "iteration: 8200 , train shape: (43541, 48, 48, 1)\n",
            "iteration: 8300 , train shape: (43641, 48, 48, 1)\n",
            "iteration: 8400 , train shape: (43741, 48, 48, 1)\n",
            "iteration: 8500 , train shape: (43841, 48, 48, 1)\n",
            "iteration: 8600 , train shape: (43941, 48, 48, 1)\n",
            "iteration: 8700 , train shape: (44041, 48, 48, 1)\n",
            "iteration: 8800 , train shape: (44141, 48, 48, 1)\n",
            "iteration: 8900 , train shape: (44241, 48, 48, 1)\n",
            "iteration: 9000 , train shape: (44341, 48, 48, 1)\n",
            "iteration: 9100 , train shape: (44441, 48, 48, 1)\n",
            "iteration: 9200 , train shape: (44541, 48, 48, 1)\n",
            "iteration: 9300 , train shape: (44641, 48, 48, 1)\n",
            "iteration: 9400 , train shape: (44741, 48, 48, 1)\n",
            "iteration: 9500 , train shape: (44841, 48, 48, 1)\n",
            "iteration: 9600 , train shape: (44941, 48, 48, 1)\n",
            "iteration: 9700 , train shape: (45041, 48, 48, 1)\n",
            "iteration: 9800 , train shape: (45141, 48, 48, 1)\n",
            "iteration: 9900 , train shape: (45241, 48, 48, 1)\n",
            "iteration: 10000 , train shape: (45341, 48, 48, 1)\n",
            "iteration: 10100 , train shape: (45441, 48, 48, 1)\n",
            "iteration: 10200 , train shape: (45541, 48, 48, 1)\n",
            "iteration: 10300 , train shape: (45641, 48, 48, 1)\n",
            "iteration: 10400 , train shape: (45741, 48, 48, 1)\n",
            "iteration: 10500 , train shape: (45841, 48, 48, 1)\n",
            "iteration: 10600 , train shape: (45941, 48, 48, 1)\n",
            "iteration: 10700 , train shape: (46041, 48, 48, 1)\n",
            "iteration: 10800 , train shape: (46141, 48, 48, 1)\n",
            "iteration: 10900 , train shape: (46241, 48, 48, 1)\n",
            "iteration: 11000 , train shape: (46341, 48, 48, 1)\n",
            "iteration: 11100 , train shape: (46441, 48, 48, 1)\n",
            "iteration: 11200 , train shape: (46541, 48, 48, 1)\n",
            "iteration: 11300 , train shape: (46641, 48, 48, 1)\n",
            "iteration: 11400 , train shape: (46741, 48, 48, 1)\n",
            "iteration: 11500 , train shape: (46841, 48, 48, 1)\n",
            "iteration: 11600 , train shape: (46941, 48, 48, 1)\n",
            "iteration: 11700 , train shape: (47041, 48, 48, 1)\n",
            "iteration: 11800 , train shape: (47141, 48, 48, 1)\n",
            "iteration: 11900 , train shape: (47241, 48, 48, 1)\n",
            "iteration: 12000 , train shape: (47341, 48, 48, 1)\n",
            "iteration: 12100 , train shape: (47441, 48, 48, 1)\n",
            "iteration: 12200 , train shape: (47541, 48, 48, 1)\n",
            "iteration: 12300 , train shape: (47641, 48, 48, 1)\n",
            "iteration: 12400 , train shape: (47741, 48, 48, 1)\n",
            "iteration: 12500 , train shape: (47841, 48, 48, 1)\n",
            "iteration: 12600 , train shape: (47941, 48, 48, 1)\n",
            "iteration: 12700 , train shape: (48041, 48, 48, 1)\n",
            "iteration: 12800 , train shape: (48141, 48, 48, 1)\n",
            "iteration: 12900 , train shape: (48241, 48, 48, 1)\n",
            "iteration: 13000 , train shape: (48341, 48, 48, 1)\n",
            "iteration: 13100 , train shape: (48441, 48, 48, 1)\n",
            "iteration: 13200 , train shape: (48541, 48, 48, 1)\n",
            "iteration: 13300 , train shape: (48641, 48, 48, 1)\n",
            "iteration: 13400 , train shape: (48741, 48, 48, 1)\n",
            "iteration: 13500 , train shape: (48841, 48, 48, 1)\n",
            "iteration: 13600 , train shape: (48941, 48, 48, 1)\n",
            "iteration: 13700 , train shape: (49041, 48, 48, 1)\n",
            "iteration: 13800 , train shape: (49141, 48, 48, 1)\n",
            "iteration: 13900 , train shape: (49241, 48, 48, 1)\n",
            "iteration: 14000 , train shape: (49341, 48, 48, 1)\n",
            "iteration: 14100 , train shape: (49441, 48, 48, 1)\n",
            "iteration: 14200 , train shape: (49541, 48, 48, 1)\n",
            "iteration: 14300 , train shape: (49641, 48, 48, 1)\n",
            "iteration: 14400 , train shape: (49741, 48, 48, 1)\n",
            "iteration: 14500 , train shape: (49841, 48, 48, 1)\n",
            "iteration: 14600 , train shape: (49941, 48, 48, 1)\n",
            "iteration: 14700 , train shape: (50041, 48, 48, 1)\n",
            "iteration: 14800 , train shape: (50141, 48, 48, 1)\n",
            "iteration: 14900 , train shape: (50241, 48, 48, 1)\n",
            "iteration: 15000 , train shape: (50341, 48, 48, 1)\n",
            "iteration: 15100 , train shape: (50441, 48, 48, 1)\n",
            "iteration: 15200 , train shape: (50541, 48, 48, 1)\n",
            "iteration: 15300 , train shape: (50641, 48, 48, 1)\n",
            "iteration: 15400 , train shape: (50741, 48, 48, 1)\n",
            "iteration: 15500 , train shape: (50841, 48, 48, 1)\n",
            "iteration: 15600 , train shape: (50941, 48, 48, 1)\n",
            "iteration: 15700 , train shape: (51041, 48, 48, 1)\n",
            "iteration: 15800 , train shape: (51141, 48, 48, 1)\n",
            "iteration: 15900 , train shape: (51241, 48, 48, 1)\n",
            "iteration: 16000 , train shape: (51341, 48, 48, 1)\n",
            "iteration: 16100 , train shape: (51441, 48, 48, 1)\n",
            "iteration: 16200 , train shape: (51541, 48, 48, 1)\n",
            "iteration: 16300 , train shape: (51641, 48, 48, 1)\n",
            "iteration: 16400 , train shape: (51741, 48, 48, 1)\n",
            "iteration: 16500 , train shape: (51841, 48, 48, 1)\n",
            "iteration: 16600 , train shape: (51941, 48, 48, 1)\n",
            "iteration: 16700 , train shape: (52041, 48, 48, 1)\n",
            "iteration: 16800 , train shape: (52141, 48, 48, 1)\n",
            "iteration: 16900 , train shape: (52241, 48, 48, 1)\n",
            "iteration: 17000 , train shape: (52341, 48, 48, 1)\n",
            "iteration: 17100 , train shape: (52441, 48, 48, 1)\n",
            "iteration: 17200 , train shape: (52541, 48, 48, 1)\n",
            "iteration: 17300 , train shape: (52641, 48, 48, 1)\n",
            "iteration: 17400 , train shape: (52741, 48, 48, 1)\n",
            "iteration: 17500 , train shape: (52841, 48, 48, 1)\n",
            "iteration: 17600 , train shape: (52941, 48, 48, 1)\n",
            "iteration: 17700 , train shape: (53041, 48, 48, 1)\n",
            "iteration: 17800 , train shape: (53141, 48, 48, 1)\n",
            "iteration: 17900 , train shape: (53241, 48, 48, 1)\n",
            "iteration: 18000 , train shape: (53341, 48, 48, 1)\n",
            "iteration: 18100 , train shape: (53441, 48, 48, 1)\n",
            "iteration: 18200 , train shape: (53541, 48, 48, 1)\n",
            "iteration: 18300 , train shape: (53641, 48, 48, 1)\n",
            "iteration: 18400 , train shape: (53741, 48, 48, 1)\n",
            "iteration: 18500 , train shape: (53841, 48, 48, 1)\n",
            "iteration: 18600 , train shape: (53941, 48, 48, 1)\n",
            "iteration: 18700 , train shape: (54041, 48, 48, 1)\n",
            "iteration: 18800 , train shape: (54141, 48, 48, 1)\n",
            "iteration: 18900 , train shape: (54241, 48, 48, 1)\n",
            "iteration: 19000 , train shape: (54341, 48, 48, 1)\n",
            "iteration: 19100 , train shape: (54441, 48, 48, 1)\n",
            "iteration: 19200 , train shape: (54541, 48, 48, 1)\n",
            "iteration: 19300 , train shape: (54641, 48, 48, 1)\n",
            "iteration: 19400 , train shape: (54741, 48, 48, 1)\n",
            "iteration: 19500 , train shape: (54841, 48, 48, 1)\n",
            "iteration: 19600 , train shape: (54941, 48, 48, 1)\n",
            "iteration: 19700 , train shape: (55041, 48, 48, 1)\n",
            "iteration: 19800 , train shape: (55141, 48, 48, 1)\n",
            "iteration: 19900 , train shape: (55241, 48, 48, 1)\n",
            "iteration: 20000 , train shape: (55341, 48, 48, 1)\n",
            "iteration: 20100 , train shape: (55441, 48, 48, 1)\n",
            "iteration: 20200 , train shape: (55541, 48, 48, 1)\n",
            "iteration: 20300 , train shape: (55641, 48, 48, 1)\n",
            "iteration: 20400 , train shape: (55741, 48, 48, 1)\n",
            "iteration: 20500 , train shape: (55841, 48, 48, 1)\n",
            "iteration: 20600 , train shape: (55941, 48, 48, 1)\n",
            "iteration: 20700 , train shape: (56041, 48, 48, 1)\n",
            "iteration: 20800 , train shape: (56141, 48, 48, 1)\n",
            "iteration: 20900 , train shape: (56241, 48, 48, 1)\n",
            "iteration: 21000 , train shape: (56341, 48, 48, 1)\n",
            "iteration: 21100 , train shape: (56441, 48, 48, 1)\n",
            "iteration: 21200 , train shape: (56541, 48, 48, 1)\n",
            "iteration: 21300 , train shape: (56641, 48, 48, 1)\n",
            "iteration: 21400 , train shape: (56741, 48, 48, 1)\n",
            "iteration: 21500 , train shape: (56841, 48, 48, 1)\n",
            "iteration: 21600 , train shape: (56941, 48, 48, 1)\n",
            "iteration: 21700 , train shape: (57041, 48, 48, 1)\n",
            "iteration: 21800 , train shape: (57141, 48, 48, 1)\n",
            "iteration: 21900 , train shape: (57241, 48, 48, 1)\n",
            "iteration: 22000 , train shape: (57341, 48, 48, 1)\n",
            "iteration: 22100 , train shape: (57441, 48, 48, 1)\n",
            "iteration: 22200 , train shape: (57541, 48, 48, 1)\n",
            "iteration: 22300 , train shape: (57641, 48, 48, 1)\n",
            "iteration: 22400 , train shape: (57741, 48, 48, 1)\n",
            "iteration: 22500 , train shape: (57841, 48, 48, 1)\n",
            "iteration: 22600 , train shape: (57941, 48, 48, 1)\n",
            "iteration: 22700 , train shape: (58041, 48, 48, 1)\n",
            "iteration: 22800 , train shape: (58141, 48, 48, 1)\n",
            "iteration: 22900 , train shape: (58241, 48, 48, 1)\n",
            "iteration: 23000 , train shape: (58341, 48, 48, 1)\n",
            "iteration: 23100 , train shape: (58441, 48, 48, 1)\n",
            "iteration: 23200 , train shape: (58541, 48, 48, 1)\n",
            "iteration: 23300 , train shape: (58641, 48, 48, 1)\n",
            "iteration: 23400 , train shape: (58741, 48, 48, 1)\n",
            "iteration: 23500 , train shape: (58841, 48, 48, 1)\n",
            "iteration: 23600 , train shape: (58941, 48, 48, 1)\n",
            "iteration: 23700 , train shape: (59041, 48, 48, 1)\n",
            "iteration: 23800 , train shape: (59141, 48, 48, 1)\n",
            "iteration: 23900 , train shape: (59241, 48, 48, 1)\n",
            "iteration: 24000 , train shape: (59341, 48, 48, 1)\n",
            "iteration: 24100 , train shape: (59441, 48, 48, 1)\n",
            "iteration: 24200 , train shape: (59541, 48, 48, 1)\n",
            "iteration: 24300 , train shape: (59641, 48, 48, 1)\n",
            "iteration: 24400 , train shape: (59741, 48, 48, 1)\n",
            "iteration: 24500 , train shape: (59841, 48, 48, 1)\n",
            "iteration: 24600 , train shape: (59941, 48, 48, 1)\n",
            "iteration: 24700 , train shape: (60041, 48, 48, 1)\n",
            "iteration: 24800 , train shape: (60141, 48, 48, 1)\n",
            "iteration: 24900 , train shape: (60241, 48, 48, 1)\n",
            "iteration: 25000 , train shape: (60341, 48, 48, 1)\n",
            "iteration: 25100 , train shape: (60441, 48, 48, 1)\n",
            "iteration: 25200 , train shape: (60541, 48, 48, 1)\n",
            "iteration: 25300 , train shape: (60641, 48, 48, 1)\n",
            "iteration: 25400 , train shape: (60741, 48, 48, 1)\n",
            "iteration: 25500 , train shape: (60841, 48, 48, 1)\n",
            "iteration: 25600 , train shape: (60941, 48, 48, 1)\n",
            "iteration: 25700 , train shape: (61041, 48, 48, 1)\n",
            "iteration: 25800 , train shape: (61141, 48, 48, 1)\n",
            "iteration: 25900 , train shape: (61241, 48, 48, 1)\n",
            "iteration: 26000 , train shape: (61341, 48, 48, 1)\n",
            "iteration: 26100 , train shape: (61441, 48, 48, 1)\n",
            "iteration: 26200 , train shape: (61541, 48, 48, 1)\n",
            "iteration: 26300 , train shape: (61641, 48, 48, 1)\n",
            "iteration: 26400 , train shape: (61741, 48, 48, 1)\n",
            "iteration: 26500 , train shape: (61841, 48, 48, 1)\n",
            "iteration: 26600 , train shape: (61941, 48, 48, 1)\n",
            "iteration: 26700 , train shape: (62041, 48, 48, 1)\n",
            "iteration: 26800 , train shape: (62141, 48, 48, 1)\n",
            "iteration: 26900 , train shape: (62241, 48, 48, 1)\n",
            "iteration: 27000 , train shape: (62341, 48, 48, 1)\n",
            "iteration: 27100 , train shape: (62441, 48, 48, 1)\n",
            "iteration: 27200 , train shape: (62541, 48, 48, 1)\n",
            "iteration: 27300 , train shape: (62641, 48, 48, 1)\n",
            "iteration: 27400 , train shape: (62741, 48, 48, 1)\n",
            "iteration: 27500 , train shape: (62841, 48, 48, 1)\n",
            "iteration: 27600 , train shape: (62941, 48, 48, 1)\n",
            "iteration: 27700 , train shape: (63041, 48, 48, 1)\n",
            "iteration: 27800 , train shape: (63141, 48, 48, 1)\n",
            "iteration: 27900 , train shape: (63241, 48, 48, 1)\n",
            "iteration: 28000 , train shape: (63341, 48, 48, 1)\n",
            "iteration: 28100 , train shape: (63441, 48, 48, 1)\n",
            "iteration: 28200 , train shape: (63541, 48, 48, 1)\n",
            "iteration: 28300 , train shape: (63641, 48, 48, 1)\n",
            "iteration: 28400 , train shape: (63741, 48, 48, 1)\n",
            "iteration: 28500 , train shape: (63841, 48, 48, 1)\n",
            "iteration: 28600 , train shape: (63941, 48, 48, 1)\n",
            "iteration: 28700 , train shape: (64041, 48, 48, 1)\n",
            "iteration: 28800 , train shape: (64141, 48, 48, 1)\n",
            "iteration: 28900 , train shape: (64241, 48, 48, 1)\n",
            "iteration: 29000 , train shape: (64341, 48, 48, 1)\n",
            "iteration: 29100 , train shape: (64441, 48, 48, 1)\n",
            "iteration: 29200 , train shape: (64541, 48, 48, 1)\n",
            "iteration: 29300 , train shape: (64641, 48, 48, 1)\n",
            "iteration: 29400 , train shape: (64741, 48, 48, 1)\n",
            "iteration: 29500 , train shape: (64841, 48, 48, 1)\n",
            "iteration: 29600 , train shape: (64941, 48, 48, 1)\n",
            "iteration: 29700 , train shape: (65041, 48, 48, 1)\n",
            "iteration: 29800 , train shape: (65141, 48, 48, 1)\n",
            "iteration: 29900 , train shape: (65241, 48, 48, 1)\n",
            "iteration: 30000 , train shape: (65341, 48, 48, 1)\n",
            "iteration: 30100 , train shape: (65441, 48, 48, 1)\n",
            "iteration: 30200 , train shape: (65541, 48, 48, 1)\n",
            "iteration: 30300 , train shape: (65641, 48, 48, 1)\n",
            "iteration: 30400 , train shape: (65741, 48, 48, 1)\n",
            "iteration: 30500 , train shape: (65841, 48, 48, 1)\n",
            "iteration: 30600 , train shape: (65941, 48, 48, 1)\n",
            "iteration: 30700 , train shape: (66041, 48, 48, 1)\n",
            "iteration: 30800 , train shape: (66141, 48, 48, 1)\n",
            "iteration: 30900 , train shape: (66241, 48, 48, 1)\n",
            "iteration: 31000 , train shape: (66341, 48, 48, 1)\n",
            "iteration: 31100 , train shape: (66441, 48, 48, 1)\n",
            "iteration: 31200 , train shape: (66541, 48, 48, 1)\n",
            "iteration: 31300 , train shape: (66641, 48, 48, 1)\n",
            "iteration: 31400 , train shape: (66741, 48, 48, 1)\n",
            "iteration: 31500 , train shape: (66841, 48, 48, 1)\n",
            "iteration: 31600 , train shape: (66941, 48, 48, 1)\n",
            "iteration: 31700 , train shape: (67041, 48, 48, 1)\n",
            "iteration: 31800 , train shape: (67141, 48, 48, 1)\n",
            "iteration: 31900 , train shape: (67241, 48, 48, 1)\n",
            "iteration: 32000 , train shape: (67341, 48, 48, 1)\n",
            "iteration: 32100 , train shape: (67441, 48, 48, 1)\n",
            "iteration: 32200 , train shape: (67541, 48, 48, 1)\n",
            "iteration: 32300 , train shape: (67641, 48, 48, 1)\n",
            "iteration: 32400 , train shape: (67741, 48, 48, 1)\n",
            "iteration: 32500 , train shape: (67841, 48, 48, 1)\n",
            "iteration: 32600 , train shape: (67941, 48, 48, 1)\n",
            "iteration: 32700 , train shape: (68041, 48, 48, 1)\n",
            "iteration: 32800 , train shape: (68141, 48, 48, 1)\n",
            "iteration: 32900 , train shape: (68241, 48, 48, 1)\n",
            "iteration: 33000 , train shape: (68341, 48, 48, 1)\n",
            "iteration: 33100 , train shape: (68441, 48, 48, 1)\n",
            "iteration: 33200 , train shape: (68541, 48, 48, 1)\n",
            "iteration: 33300 , train shape: (68641, 48, 48, 1)\n",
            "iteration: 33400 , train shape: (68741, 48, 48, 1)\n",
            "iteration: 33500 , train shape: (68841, 48, 48, 1)\n",
            "iteration: 33600 , train shape: (68941, 48, 48, 1)\n",
            "iteration: 33700 , train shape: (69041, 48, 48, 1)\n",
            "iteration: 33800 , train shape: (69141, 48, 48, 1)\n",
            "iteration: 33900 , train shape: (69241, 48, 48, 1)\n",
            "iteration: 34000 , train shape: (69341, 48, 48, 1)\n",
            "iteration: 34100 , train shape: (69441, 48, 48, 1)\n",
            "iteration: 34200 , train shape: (69541, 48, 48, 1)\n",
            "iteration: 34300 , train shape: (69641, 48, 48, 1)\n",
            "iteration: 34400 , train shape: (69741, 48, 48, 1)\n",
            "iteration: 34500 , train shape: (69841, 48, 48, 1)\n",
            "iteration: 34600 , train shape: (69941, 48, 48, 1)\n",
            "iteration: 34700 , train shape: (70041, 48, 48, 1)\n",
            "iteration: 34800 , train shape: (70141, 48, 48, 1)\n",
            "iteration: 34900 , train shape: (70241, 48, 48, 1)\n",
            "iteration: 35000 , train shape: (70341, 48, 48, 1)\n",
            "iteration: 35100 , train shape: (70441, 48, 48, 1)\n",
            "iteration: 35200 , train shape: (70541, 48, 48, 1)\n",
            "iteration: 35300 , train shape: (70641, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWsEbPyuvHO8",
        "outputId": "a4e33e2f-207f-49df-f1bb-f106d73d1c86"
      },
      "source": [
        "print(\"X_train_aug shape:\", X_train_aug.shape)\n",
        "print(\"y_train_aug shape:\", y_train_aug.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_aug shape: (70680, 48, 48, 1)\n",
            "y_train_aug shape: (70680, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train_aug.shape[0], batch_size)\n",
        "            imgs, labels = X_train_aug[idx], y_train_aug[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5GK1G4XQFj0",
        "outputId": "375e1a6b-57ae-4b79-9a18-4cc7afff82b2"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18610, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.697734, acc.: 50.00%] [G loss: 1.548923]\n",
            "20 [D loss: 1.283567, acc.: 19.53%] [G loss: 0.690001]\n",
            "40 [D loss: 2.370992, acc.: 3.52%] [G loss: 0.346126]\n",
            "60 [D loss: 0.987706, acc.: 29.69%] [G loss: 2.129796]\n",
            "80 [D loss: 0.550528, acc.: 87.11%] [G loss: 2.348300]\n",
            "100 [D loss: 0.287683, acc.: 96.88%] [G loss: 5.466212]\n",
            "120 [D loss: 0.580996, acc.: 81.64%] [G loss: 2.423259]\n",
            "140 [D loss: 0.302043, acc.: 95.31%] [G loss: 5.984384]\n",
            "160 [D loss: 0.155746, acc.: 100.00%] [G loss: 10.819426]\n",
            "180 [D loss: 0.367775, acc.: 95.31%] [G loss: 2.426296]\n",
            "200 [D loss: 0.537494, acc.: 84.38%] [G loss: 2.797187]\n",
            "220 [D loss: 0.467921, acc.: 85.94%] [G loss: 3.371032]\n",
            "240 [D loss: 0.412285, acc.: 86.33%] [G loss: 8.723379]\n",
            "260 [D loss: 0.570384, acc.: 78.91%] [G loss: 9.821998]\n",
            "280 [D loss: 0.472113, acc.: 83.20%] [G loss: 7.590880]\n",
            "300 [D loss: 0.199239, acc.: 100.00%] [G loss: 4.834557]\n",
            "320 [D loss: 0.563243, acc.: 76.17%] [G loss: 1.970145]\n",
            "340 [D loss: 0.635771, acc.: 66.02%] [G loss: 4.888510]\n",
            "360 [D loss: 0.669577, acc.: 61.33%] [G loss: 3.266885]\n",
            "380 [D loss: 0.425280, acc.: 83.59%] [G loss: 4.607930]\n",
            "400 [D loss: 0.303387, acc.: 89.84%] [G loss: 6.486661]\n",
            "420 [D loss: 0.432550, acc.: 75.00%] [G loss: 5.026457]\n",
            "440 [D loss: 0.491357, acc.: 77.73%] [G loss: 3.678199]\n",
            "460 [D loss: 0.331628, acc.: 87.89%] [G loss: 7.215898]\n",
            "480 [D loss: 0.446885, acc.: 78.91%] [G loss: 3.215711]\n",
            "500 [D loss: 0.874747, acc.: 51.56%] [G loss: 2.661733]\n",
            "520 [D loss: 0.547995, acc.: 75.78%] [G loss: 2.667494]\n",
            "540 [D loss: 0.472161, acc.: 76.56%] [G loss: 5.073229]\n",
            "560 [D loss: 0.440211, acc.: 81.25%] [G loss: 4.619638]\n",
            "580 [D loss: 0.403183, acc.: 82.81%] [G loss: 4.032103]\n",
            "600 [D loss: 0.402924, acc.: 83.98%] [G loss: 4.363013]\n",
            "620 [D loss: 0.316150, acc.: 90.62%] [G loss: 5.591088]\n",
            "640 [D loss: 0.443816, acc.: 77.34%] [G loss: 5.096770]\n",
            "660 [D loss: 0.382921, acc.: 82.42%] [G loss: 4.725486]\n",
            "680 [D loss: 0.305171, acc.: 89.06%] [G loss: 5.561421]\n",
            "700 [D loss: 0.566649, acc.: 74.22%] [G loss: 4.644314]\n",
            "720 [D loss: 0.317839, acc.: 86.72%] [G loss: 5.075389]\n",
            "740 [D loss: 0.383289, acc.: 82.81%] [G loss: 5.910224]\n",
            "760 [D loss: 0.397150, acc.: 83.98%] [G loss: 4.600727]\n",
            "780 [D loss: 0.399550, acc.: 83.59%] [G loss: 4.323882]\n",
            "800 [D loss: 0.603567, acc.: 68.36%] [G loss: 3.549349]\n",
            "820 [D loss: 0.369187, acc.: 87.89%] [G loss: 5.090309]\n",
            "840 [D loss: 0.485960, acc.: 77.73%] [G loss: 3.929976]\n",
            "860 [D loss: 0.506736, acc.: 74.61%] [G loss: 3.556047]\n",
            "880 [D loss: 0.488734, acc.: 75.39%] [G loss: 4.151247]\n",
            "900 [D loss: 0.439328, acc.: 79.69%] [G loss: 4.121727]\n",
            "920 [D loss: 0.460314, acc.: 78.91%] [G loss: 4.488844]\n",
            "940 [D loss: 0.355157, acc.: 85.94%] [G loss: 4.991023]\n",
            "960 [D loss: 0.367390, acc.: 86.33%] [G loss: 4.364448]\n",
            "980 [D loss: 0.375600, acc.: 84.38%] [G loss: 5.141340]\n",
            "1000 [D loss: 0.439586, acc.: 80.86%] [G loss: 5.107465]\n",
            "1020 [D loss: 0.487774, acc.: 78.91%] [G loss: 4.101192]\n",
            "1040 [D loss: 0.451948, acc.: 81.64%] [G loss: 3.875418]\n",
            "1060 [D loss: 0.340956, acc.: 87.50%] [G loss: 4.402813]\n",
            "1080 [D loss: 0.713245, acc.: 66.02%] [G loss: 2.777739]\n",
            "1100 [D loss: 0.511776, acc.: 77.73%] [G loss: 4.883790]\n",
            "1120 [D loss: 0.619944, acc.: 70.70%] [G loss: 6.818527]\n",
            "1140 [D loss: 0.585232, acc.: 71.48%] [G loss: 3.569280]\n",
            "1160 [D loss: 0.461428, acc.: 80.86%] [G loss: 4.755805]\n",
            "1180 [D loss: 0.354441, acc.: 85.55%] [G loss: 5.195897]\n",
            "1200 [D loss: 0.482741, acc.: 74.61%] [G loss: 4.555103]\n",
            "1220 [D loss: 0.384313, acc.: 82.81%] [G loss: 5.355527]\n",
            "1240 [D loss: 0.437631, acc.: 79.69%] [G loss: 4.505162]\n",
            "1260 [D loss: 0.425393, acc.: 82.42%] [G loss: 4.496139]\n",
            "1280 [D loss: 0.456941, acc.: 79.30%] [G loss: 4.622098]\n",
            "1300 [D loss: 0.458642, acc.: 82.03%] [G loss: 4.754512]\n",
            "1320 [D loss: 0.322877, acc.: 87.11%] [G loss: 4.978501]\n",
            "1340 [D loss: 0.364252, acc.: 85.16%] [G loss: 4.832194]\n",
            "1360 [D loss: 0.397720, acc.: 83.98%] [G loss: 4.799105]\n",
            "1380 [D loss: 0.528576, acc.: 73.83%] [G loss: 4.439099]\n",
            "1400 [D loss: 0.380465, acc.: 83.20%] [G loss: 4.763906]\n",
            "1420 [D loss: 0.480374, acc.: 76.95%] [G loss: 4.809299]\n",
            "1440 [D loss: 0.381147, acc.: 83.59%] [G loss: 4.743246]\n",
            "1460 [D loss: 0.407120, acc.: 81.64%] [G loss: 5.103311]\n",
            "1480 [D loss: 0.327421, acc.: 87.89%] [G loss: 5.182826]\n",
            "1500 [D loss: 0.519813, acc.: 73.83%] [G loss: 4.778469]\n",
            "1520 [D loss: 0.401356, acc.: 83.98%] [G loss: 5.265177]\n",
            "1540 [D loss: 0.310523, acc.: 87.89%] [G loss: 5.560895]\n",
            "1560 [D loss: 0.308102, acc.: 89.06%] [G loss: 5.434203]\n",
            "1580 [D loss: 0.367080, acc.: 82.42%] [G loss: 5.364250]\n",
            "1600 [D loss: 0.391171, acc.: 84.38%] [G loss: 5.087455]\n",
            "1620 [D loss: 0.370807, acc.: 87.50%] [G loss: 5.019573]\n",
            "1640 [D loss: 0.492876, acc.: 75.00%] [G loss: 5.438135]\n",
            "1660 [D loss: 0.329173, acc.: 86.33%] [G loss: 5.586573]\n",
            "1680 [D loss: 0.944571, acc.: 32.03%] [G loss: 1.893117]\n",
            "1700 [D loss: 0.828764, acc.: 42.58%] [G loss: 1.879541]\n",
            "1720 [D loss: 1.258865, acc.: 23.05%] [G loss: 1.362733]\n",
            "1740 [D loss: 1.067567, acc.: 44.14%] [G loss: 2.293820]\n",
            "1760 [D loss: 0.748579, acc.: 56.25%] [G loss: 2.094138]\n",
            "1780 [D loss: 0.652264, acc.: 58.98%] [G loss: 2.682230]\n",
            "1800 [D loss: 0.739860, acc.: 51.56%] [G loss: 2.095659]\n",
            "1820 [D loss: 0.711825, acc.: 57.42%] [G loss: 2.259857]\n",
            "1840 [D loss: 0.457131, acc.: 80.86%] [G loss: 2.908075]\n",
            "1860 [D loss: 0.511279, acc.: 76.56%] [G loss: 3.069392]\n",
            "1880 [D loss: 0.514684, acc.: 75.78%] [G loss: 2.816759]\n",
            "1900 [D loss: 0.488220, acc.: 75.78%] [G loss: 3.432525]\n",
            "1920 [D loss: 0.436070, acc.: 80.08%] [G loss: 3.625695]\n",
            "1940 [D loss: 0.494063, acc.: 78.91%] [G loss: 3.232696]\n",
            "1960 [D loss: 0.477095, acc.: 75.78%] [G loss: 3.326528]\n",
            "1980 [D loss: 0.453704, acc.: 78.12%] [G loss: 3.350381]\n",
            "2000 [D loss: 0.480866, acc.: 78.12%] [G loss: 3.419010]\n",
            "2020 [D loss: 0.432569, acc.: 80.86%] [G loss: 3.743415]\n",
            "2040 [D loss: 0.555450, acc.: 70.70%] [G loss: 3.517704]\n",
            "2060 [D loss: 0.471216, acc.: 78.91%] [G loss: 3.377952]\n",
            "2080 [D loss: 0.428153, acc.: 80.86%] [G loss: 3.581824]\n",
            "2100 [D loss: 0.433776, acc.: 82.81%] [G loss: 3.646061]\n",
            "2120 [D loss: 0.561952, acc.: 70.31%] [G loss: 3.116197]\n",
            "2140 [D loss: 0.551355, acc.: 72.66%] [G loss: 3.169238]\n",
            "2160 [D loss: 0.535215, acc.: 71.09%] [G loss: 3.197329]\n",
            "2180 [D loss: 0.423093, acc.: 84.38%] [G loss: 3.960432]\n",
            "2200 [D loss: 0.457667, acc.: 79.69%] [G loss: 3.564749]\n",
            "2220 [D loss: 0.455304, acc.: 79.69%] [G loss: 3.692254]\n",
            "2240 [D loss: 0.504200, acc.: 74.22%] [G loss: 3.373568]\n",
            "2260 [D loss: 0.503347, acc.: 73.83%] [G loss: 3.564409]\n",
            "2280 [D loss: 0.516136, acc.: 76.17%] [G loss: 3.158265]\n",
            "2300 [D loss: 0.508968, acc.: 74.61%] [G loss: 3.492503]\n",
            "2320 [D loss: 0.449827, acc.: 79.69%] [G loss: 3.584324]\n",
            "2340 [D loss: 0.472384, acc.: 75.78%] [G loss: 3.426366]\n",
            "2360 [D loss: 0.467924, acc.: 79.69%] [G loss: 3.271040]\n",
            "2380 [D loss: 0.470863, acc.: 78.52%] [G loss: 3.445466]\n",
            "2400 [D loss: 0.447529, acc.: 79.30%] [G loss: 4.296255]\n",
            "2420 [D loss: 0.526838, acc.: 74.22%] [G loss: 3.293097]\n",
            "2440 [D loss: 0.423475, acc.: 82.42%] [G loss: 3.810988]\n",
            "2460 [D loss: 0.506425, acc.: 75.00%] [G loss: 3.315372]\n",
            "2480 [D loss: 0.478954, acc.: 76.56%] [G loss: 3.384745]\n",
            "2500 [D loss: 0.482185, acc.: 76.95%] [G loss: 3.746778]\n",
            "2520 [D loss: 0.501636, acc.: 74.22%] [G loss: 3.359197]\n",
            "2540 [D loss: 0.522430, acc.: 73.83%] [G loss: 3.245861]\n",
            "2560 [D loss: 0.421893, acc.: 82.03%] [G loss: 3.512280]\n",
            "2580 [D loss: 0.508853, acc.: 76.56%] [G loss: 3.308928]\n",
            "2600 [D loss: 0.453895, acc.: 79.69%] [G loss: 3.313801]\n",
            "2620 [D loss: 0.519211, acc.: 71.09%] [G loss: 3.530001]\n",
            "2640 [D loss: 0.508408, acc.: 74.22%] [G loss: 3.442446]\n",
            "2660 [D loss: 0.552412, acc.: 74.22%] [G loss: 3.156322]\n",
            "2680 [D loss: 0.523135, acc.: 77.34%] [G loss: 3.112247]\n",
            "2700 [D loss: 0.100918, acc.: 100.00%] [G loss: 2.733624]\n",
            "2720 [D loss: 0.684347, acc.: 66.80%] [G loss: 4.140007]\n",
            "2740 [D loss: 0.518320, acc.: 75.39%] [G loss: 3.469811]\n",
            "2760 [D loss: 0.645828, acc.: 66.02%] [G loss: 3.389630]\n",
            "2780 [D loss: 0.594194, acc.: 69.14%] [G loss: 3.326114]\n",
            "2800 [D loss: 0.588912, acc.: 68.36%] [G loss: 3.256595]\n",
            "2820 [D loss: 0.720678, acc.: 63.28%] [G loss: 2.964676]\n",
            "2840 [D loss: 0.574253, acc.: 70.31%] [G loss: 3.223165]\n",
            "2860 [D loss: 0.440992, acc.: 81.25%] [G loss: 4.035916]\n",
            "2880 [D loss: 0.426368, acc.: 83.59%] [G loss: 3.844495]\n",
            "2900 [D loss: 0.517088, acc.: 74.22%] [G loss: 3.363805]\n",
            "2920 [D loss: 0.496143, acc.: 76.17%] [G loss: 3.759048]\n",
            "2940 [D loss: 0.556142, acc.: 71.88%] [G loss: 3.201587]\n",
            "2960 [D loss: 0.455992, acc.: 78.91%] [G loss: 3.349144]\n",
            "2980 [D loss: 0.511823, acc.: 71.88%] [G loss: 3.645873]\n",
            "3000 [D loss: 0.428369, acc.: 80.86%] [G loss: 3.555359]\n",
            "3020 [D loss: 0.529200, acc.: 74.61%] [G loss: 3.409205]\n",
            "3040 [D loss: 0.444395, acc.: 80.47%] [G loss: 3.776263]\n",
            "3060 [D loss: 0.550268, acc.: 68.36%] [G loss: 3.244672]\n",
            "3080 [D loss: 0.469686, acc.: 80.86%] [G loss: 3.496766]\n",
            "3100 [D loss: 0.507880, acc.: 74.22%] [G loss: 3.574321]\n",
            "3120 [D loss: 0.510236, acc.: 73.44%] [G loss: 3.616273]\n",
            "3140 [D loss: 0.510715, acc.: 73.05%] [G loss: 3.335616]\n",
            "3160 [D loss: 0.494050, acc.: 75.78%] [G loss: 3.686509]\n",
            "3180 [D loss: 0.452132, acc.: 78.91%] [G loss: 3.633582]\n",
            "3200 [D loss: 0.504937, acc.: 76.17%] [G loss: 3.402104]\n",
            "3220 [D loss: 0.455439, acc.: 79.30%] [G loss: 3.432268]\n",
            "3240 [D loss: 0.536027, acc.: 74.61%] [G loss: 3.224663]\n",
            "3260 [D loss: 0.502821, acc.: 76.56%] [G loss: 3.773190]\n",
            "3280 [D loss: 0.453662, acc.: 81.25%] [G loss: 3.684907]\n",
            "3300 [D loss: 0.514792, acc.: 75.78%] [G loss: 3.454263]\n",
            "3320 [D loss: 0.459563, acc.: 79.30%] [G loss: 3.477675]\n",
            "3340 [D loss: 0.491437, acc.: 75.39%] [G loss: 3.668483]\n",
            "3360 [D loss: 0.476594, acc.: 76.56%] [G loss: 3.738759]\n",
            "3380 [D loss: 0.472505, acc.: 76.17%] [G loss: 3.391647]\n",
            "3400 [D loss: 0.495019, acc.: 74.22%] [G loss: 3.545323]\n",
            "3420 [D loss: 0.463614, acc.: 79.30%] [G loss: 3.423369]\n",
            "3440 [D loss: 0.566349, acc.: 71.09%] [G loss: 3.640427]\n",
            "3460 [D loss: 0.405743, acc.: 82.81%] [G loss: 3.980531]\n",
            "3480 [D loss: 0.495932, acc.: 73.44%] [G loss: 3.382259]\n",
            "3500 [D loss: 0.480842, acc.: 78.52%] [G loss: 3.656076]\n",
            "3520 [D loss: 0.446968, acc.: 82.03%] [G loss: 2.998627]\n",
            "3540 [D loss: 0.481081, acc.: 78.91%] [G loss: 2.454690]\n",
            "3560 [D loss: 0.625561, acc.: 67.58%] [G loss: 3.496007]\n",
            "3580 [D loss: 0.355422, acc.: 86.72%] [G loss: 2.075427]\n",
            "3600 [D loss: 1.132537, acc.: 46.09%] [G loss: 6.242689]\n",
            "3620 [D loss: 0.852353, acc.: 52.73%] [G loss: 3.295447]\n",
            "3640 [D loss: 0.761839, acc.: 60.16%] [G loss: 3.023181]\n",
            "3660 [D loss: 0.537146, acc.: 73.44%] [G loss: 3.567561]\n",
            "3680 [D loss: 0.460148, acc.: 76.56%] [G loss: 3.718468]\n",
            "3700 [D loss: 0.465492, acc.: 79.69%] [G loss: 3.680309]\n",
            "3720 [D loss: 0.572838, acc.: 72.66%] [G loss: 3.029124]\n",
            "3740 [D loss: 0.449182, acc.: 81.25%] [G loss: 3.396227]\n",
            "3760 [D loss: 0.531787, acc.: 74.22%] [G loss: 3.896007]\n",
            "3780 [D loss: 0.402380, acc.: 83.59%] [G loss: 3.827296]\n",
            "3800 [D loss: 0.497091, acc.: 75.00%] [G loss: 3.501072]\n",
            "3820 [D loss: 0.537309, acc.: 71.48%] [G loss: 3.724484]\n",
            "3840 [D loss: 0.464498, acc.: 77.34%] [G loss: 3.491478]\n",
            "3860 [D loss: 0.556809, acc.: 70.70%] [G loss: 3.145914]\n",
            "3880 [D loss: 0.527980, acc.: 72.27%] [G loss: 3.216409]\n",
            "3900 [D loss: 0.482107, acc.: 78.12%] [G loss: 3.398397]\n",
            "3920 [D loss: 0.534799, acc.: 72.66%] [G loss: 3.566749]\n",
            "3940 [D loss: 0.526083, acc.: 75.78%] [G loss: 3.612576]\n",
            "3960 [D loss: 0.494894, acc.: 75.78%] [G loss: 3.665792]\n",
            "3980 [D loss: 0.567238, acc.: 69.92%] [G loss: 3.270954]\n",
            "4000 [D loss: 0.534141, acc.: 75.78%] [G loss: 3.461102]\n",
            "4020 [D loss: 0.422572, acc.: 82.03%] [G loss: 3.758084]\n",
            "4040 [D loss: 0.615441, acc.: 65.23%] [G loss: 3.996419]\n",
            "4060 [D loss: 0.426236, acc.: 83.98%] [G loss: 3.611062]\n",
            "4080 [D loss: 0.455729, acc.: 81.64%] [G loss: 3.689302]\n",
            "4100 [D loss: 0.472855, acc.: 78.12%] [G loss: 4.227590]\n",
            "4120 [D loss: 0.477154, acc.: 76.17%] [G loss: 3.646786]\n",
            "4140 [D loss: 0.470903, acc.: 78.52%] [G loss: 3.343230]\n",
            "4160 [D loss: 0.499992, acc.: 76.56%] [G loss: 3.808669]\n",
            "4180 [D loss: 0.573197, acc.: 68.36%] [G loss: 3.087667]\n",
            "4200 [D loss: 0.572262, acc.: 70.70%] [G loss: 3.229471]\n",
            "4220 [D loss: 0.475743, acc.: 79.30%] [G loss: 3.223854]\n",
            "4240 [D loss: 0.549080, acc.: 71.88%] [G loss: 3.309688]\n",
            "4260 [D loss: 0.523776, acc.: 72.27%] [G loss: 3.667341]\n",
            "4280 [D loss: 0.517518, acc.: 75.78%] [G loss: 3.456147]\n",
            "4300 [D loss: 0.516220, acc.: 75.78%] [G loss: 3.548264]\n",
            "4320 [D loss: 0.537759, acc.: 74.22%] [G loss: 3.276555]\n",
            "4340 [D loss: 0.448061, acc.: 82.81%] [G loss: 3.862332]\n",
            "4360 [D loss: 0.534462, acc.: 71.48%] [G loss: 3.369606]\n",
            "4380 [D loss: 0.472341, acc.: 79.69%] [G loss: 4.080543]\n",
            "4400 [D loss: 0.527055, acc.: 75.78%] [G loss: 3.297324]\n",
            "4420 [D loss: 0.413857, acc.: 81.64%] [G loss: 2.920808]\n",
            "4440 [D loss: 0.495487, acc.: 76.56%] [G loss: 2.432773]\n",
            "4460 [D loss: 0.500638, acc.: 75.78%] [G loss: 4.187169]\n",
            "4480 [D loss: 0.675420, acc.: 62.50%] [G loss: 3.152438]\n",
            "4500 [D loss: 0.542215, acc.: 73.44%] [G loss: 2.615937]\n",
            "4520 [D loss: 0.687424, acc.: 62.50%] [G loss: 2.919950]\n",
            "4540 [D loss: 0.590397, acc.: 72.66%] [G loss: 3.248223]\n",
            "4560 [D loss: 0.530487, acc.: 73.05%] [G loss: 3.689385]\n",
            "4580 [D loss: 0.497694, acc.: 76.17%] [G loss: 3.658328]\n",
            "4600 [D loss: 0.457465, acc.: 76.95%] [G loss: 4.051321]\n",
            "4620 [D loss: 0.532280, acc.: 72.66%] [G loss: 3.711565]\n",
            "4640 [D loss: 0.502974, acc.: 75.00%] [G loss: 3.307100]\n",
            "4660 [D loss: 0.444164, acc.: 76.56%] [G loss: 3.814842]\n",
            "4680 [D loss: 0.503858, acc.: 73.44%] [G loss: 4.105756]\n",
            "4700 [D loss: 0.423502, acc.: 82.03%] [G loss: 4.230035]\n",
            "4720 [D loss: 0.487716, acc.: 78.12%] [G loss: 4.001184]\n",
            "4740 [D loss: 0.440137, acc.: 81.25%] [G loss: 3.954447]\n",
            "4760 [D loss: 0.505463, acc.: 77.34%] [G loss: 3.584741]\n",
            "4780 [D loss: 0.509048, acc.: 78.12%] [G loss: 3.701793]\n",
            "4800 [D loss: 0.457744, acc.: 76.95%] [G loss: 3.743336]\n",
            "4820 [D loss: 0.463812, acc.: 77.73%] [G loss: 3.744205]\n",
            "4840 [D loss: 0.477626, acc.: 78.91%] [G loss: 4.305583]\n",
            "4860 [D loss: 0.447743, acc.: 80.47%] [G loss: 3.718383]\n",
            "4880 [D loss: 0.480188, acc.: 76.95%] [G loss: 3.912814]\n",
            "4900 [D loss: 0.504011, acc.: 75.78%] [G loss: 3.471519]\n",
            "4920 [D loss: 0.575343, acc.: 69.53%] [G loss: 3.227261]\n",
            "4940 [D loss: 0.501152, acc.: 76.56%] [G loss: 4.012784]\n",
            "4960 [D loss: 0.514858, acc.: 74.22%] [G loss: 3.467636]\n",
            "4980 [D loss: 0.525343, acc.: 72.66%] [G loss: 3.843647]\n",
            "5000 [D loss: 0.481125, acc.: 76.17%] [G loss: 4.030941]\n",
            "5020 [D loss: 0.434252, acc.: 79.30%] [G loss: 4.326940]\n",
            "5040 [D loss: 0.456225, acc.: 80.47%] [G loss: 3.827529]\n",
            "5060 [D loss: 0.506042, acc.: 76.17%] [G loss: 3.701025]\n",
            "5080 [D loss: 0.459967, acc.: 78.12%] [G loss: 3.622523]\n",
            "5100 [D loss: 0.503113, acc.: 73.05%] [G loss: 3.513437]\n",
            "5120 [D loss: 0.516006, acc.: 74.61%] [G loss: 3.467560]\n",
            "5140 [D loss: 0.619706, acc.: 68.75%] [G loss: 3.212530]\n",
            "5160 [D loss: 0.505233, acc.: 75.00%] [G loss: 3.973931]\n",
            "5180 [D loss: 0.451515, acc.: 80.47%] [G loss: 3.842525]\n",
            "5200 [D loss: 0.440018, acc.: 80.47%] [G loss: 3.994718]\n",
            "5220 [D loss: 0.378717, acc.: 83.98%] [G loss: 4.496457]\n",
            "5240 [D loss: 0.468685, acc.: 75.00%] [G loss: 3.803149]\n",
            "5260 [D loss: 0.427807, acc.: 79.30%] [G loss: 4.605141]\n",
            "5280 [D loss: 0.499129, acc.: 77.34%] [G loss: 3.659524]\n",
            "5300 [D loss: 0.490984, acc.: 75.78%] [G loss: 3.690510]\n",
            "5320 [D loss: 0.480250, acc.: 78.12%] [G loss: 3.966791]\n",
            "5340 [D loss: 0.547702, acc.: 73.05%] [G loss: 3.771778]\n",
            "5360 [D loss: 0.415959, acc.: 80.08%] [G loss: 3.934993]\n",
            "5380 [D loss: 0.453692, acc.: 78.52%] [G loss: 4.027442]\n",
            "5400 [D loss: 0.429513, acc.: 81.64%] [G loss: 3.980330]\n",
            "5420 [D loss: 0.479276, acc.: 78.12%] [G loss: 3.894027]\n",
            "5440 [D loss: 0.473995, acc.: 75.39%] [G loss: 3.721940]\n",
            "5460 [D loss: 0.483771, acc.: 76.56%] [G loss: 4.463526]\n",
            "5480 [D loss: 0.480065, acc.: 73.83%] [G loss: 3.607657]\n",
            "5500 [D loss: 0.454511, acc.: 78.52%] [G loss: 4.024220]\n",
            "5520 [D loss: 0.470284, acc.: 80.08%] [G loss: 3.512620]\n",
            "5540 [D loss: 0.395133, acc.: 82.42%] [G loss: 4.149706]\n",
            "5560 [D loss: 0.543488, acc.: 73.83%] [G loss: 3.588698]\n",
            "5580 [D loss: 0.448839, acc.: 82.03%] [G loss: 4.387211]\n",
            "5600 [D loss: 0.518189, acc.: 73.44%] [G loss: 3.527008]\n",
            "5620 [D loss: 0.470819, acc.: 75.39%] [G loss: 3.790100]\n",
            "5640 [D loss: 0.447626, acc.: 77.73%] [G loss: 4.293848]\n",
            "5660 [D loss: 0.432315, acc.: 80.47%] [G loss: 3.989090]\n",
            "5680 [D loss: 0.426290, acc.: 82.03%] [G loss: 2.793005]\n",
            "5700 [D loss: 2.207249, acc.: 51.17%] [G loss: 2.510913]\n",
            "5720 [D loss: 0.890363, acc.: 47.66%] [G loss: 2.394496]\n",
            "5740 [D loss: 1.468785, acc.: 7.42%] [G loss: 1.167918]\n",
            "5760 [D loss: 1.450161, acc.: 11.72%] [G loss: 1.538831]\n",
            "5780 [D loss: 0.859439, acc.: 37.89%] [G loss: 1.914453]\n",
            "5800 [D loss: 0.789167, acc.: 43.36%] [G loss: 1.770433]\n",
            "5820 [D loss: 0.812536, acc.: 35.55%] [G loss: 1.679628]\n",
            "5840 [D loss: 0.727261, acc.: 43.75%] [G loss: 1.742784]\n",
            "5860 [D loss: 0.724215, acc.: 48.83%] [G loss: 1.705127]\n",
            "5880 [D loss: 0.717047, acc.: 51.17%] [G loss: 1.687216]\n",
            "5900 [D loss: 0.674634, acc.: 55.47%] [G loss: 1.762131]\n",
            "5920 [D loss: 0.749673, acc.: 46.88%] [G loss: 1.660499]\n",
            "5940 [D loss: 0.647804, acc.: 59.38%] [G loss: 1.817858]\n",
            "5960 [D loss: 0.678007, acc.: 52.73%] [G loss: 1.724212]\n",
            "5980 [D loss: 0.709896, acc.: 50.39%] [G loss: 1.661507]\n",
            "6000 [D loss: 0.692642, acc.: 53.91%] [G loss: 1.751378]\n",
            "6020 [D loss: 0.646538, acc.: 60.55%] [G loss: 1.853058]\n",
            "6040 [D loss: 0.659250, acc.: 58.98%] [G loss: 1.812984]\n",
            "6060 [D loss: 0.624580, acc.: 65.23%] [G loss: 1.941048]\n",
            "6080 [D loss: 0.675796, acc.: 58.59%] [G loss: 1.865242]\n",
            "6100 [D loss: 0.634822, acc.: 62.50%] [G loss: 2.000556]\n",
            "6120 [D loss: 0.600608, acc.: 66.80%] [G loss: 2.074232]\n",
            "6140 [D loss: 0.674619, acc.: 55.47%] [G loss: 1.967027]\n",
            "6160 [D loss: 0.578840, acc.: 69.92%] [G loss: 2.261443]\n",
            "6180 [D loss: 0.626125, acc.: 63.67%] [G loss: 1.994754]\n",
            "6200 [D loss: 0.587909, acc.: 71.09%] [G loss: 2.480986]\n",
            "6220 [D loss: 0.555298, acc.: 71.88%] [G loss: 2.316145]\n",
            "6240 [D loss: 0.615778, acc.: 64.45%] [G loss: 2.239664]\n",
            "6260 [D loss: 0.543502, acc.: 71.48%] [G loss: 2.621497]\n",
            "6280 [D loss: 0.589744, acc.: 65.62%] [G loss: 2.525754]\n",
            "6300 [D loss: 0.541360, acc.: 73.83%] [G loss: 2.526038]\n",
            "6320 [D loss: 0.613266, acc.: 68.36%] [G loss: 2.453434]\n",
            "6340 [D loss: 0.543727, acc.: 75.39%] [G loss: 2.692166]\n",
            "6360 [D loss: 0.602008, acc.: 68.75%] [G loss: 2.278121]\n",
            "6380 [D loss: 0.555948, acc.: 72.66%] [G loss: 2.318192]\n",
            "6400 [D loss: 0.612383, acc.: 67.19%] [G loss: 1.554608]\n",
            "6420 [D loss: 0.586285, acc.: 66.80%] [G loss: 2.738179]\n",
            "6440 [D loss: 0.552766, acc.: 72.66%] [G loss: 2.699981]\n",
            "6460 [D loss: 0.492398, acc.: 77.34%] [G loss: 2.935956]\n",
            "6480 [D loss: 0.616308, acc.: 66.41%] [G loss: 2.558005]\n",
            "6500 [D loss: 0.485973, acc.: 78.91%] [G loss: 2.645407]\n",
            "6520 [D loss: 0.539793, acc.: 73.83%] [G loss: 2.890632]\n",
            "6540 [D loss: 0.659466, acc.: 61.33%] [G loss: 2.512663]\n",
            "6560 [D loss: 0.666681, acc.: 64.84%] [G loss: 2.366490]\n",
            "6580 [D loss: 0.666193, acc.: 65.23%] [G loss: 2.291872]\n",
            "6600 [D loss: 0.539834, acc.: 75.00%] [G loss: 2.656496]\n",
            "6620 [D loss: 0.616544, acc.: 68.36%] [G loss: 2.266561]\n",
            "6640 [D loss: 0.564527, acc.: 70.31%] [G loss: 2.685023]\n",
            "6660 [D loss: 0.719756, acc.: 55.86%] [G loss: 2.559064]\n",
            "6680 [D loss: 0.425134, acc.: 83.20%] [G loss: 3.299241]\n",
            "6700 [D loss: 0.566576, acc.: 71.09%] [G loss: 2.864050]\n",
            "6720 [D loss: 0.520879, acc.: 75.39%] [G loss: 2.850467]\n",
            "6740 [D loss: 0.574445, acc.: 71.09%] [G loss: 2.864272]\n",
            "6760 [D loss: 0.513954, acc.: 74.22%] [G loss: 2.783944]\n",
            "6780 [D loss: 0.598973, acc.: 64.06%] [G loss: 2.258404]\n",
            "6800 [D loss: 0.503996, acc.: 75.78%] [G loss: 2.931831]\n",
            "6820 [D loss: 0.560656, acc.: 72.66%] [G loss: 2.543737]\n",
            "6840 [D loss: 0.579364, acc.: 67.58%] [G loss: 2.680717]\n",
            "6860 [D loss: 0.592209, acc.: 67.97%] [G loss: 2.568994]\n",
            "6880 [D loss: 0.476462, acc.: 78.52%] [G loss: 3.308953]\n",
            "6900 [D loss: 0.491477, acc.: 76.56%] [G loss: 3.107606]\n",
            "6920 [D loss: 0.603352, acc.: 66.41%] [G loss: 2.407060]\n",
            "6940 [D loss: 0.508704, acc.: 75.78%] [G loss: 2.792399]\n",
            "6960 [D loss: 0.619265, acc.: 63.28%] [G loss: 2.595794]\n",
            "6980 [D loss: 0.546569, acc.: 71.88%] [G loss: 2.717826]\n",
            "7000 [D loss: 0.519249, acc.: 74.22%] [G loss: 2.745413]\n",
            "7020 [D loss: 0.535907, acc.: 73.44%] [G loss: 2.893984]\n",
            "7040 [D loss: 0.544246, acc.: 72.27%] [G loss: 2.717388]\n",
            "7060 [D loss: 0.574369, acc.: 71.48%] [G loss: 2.617791]\n",
            "7080 [D loss: 0.655684, acc.: 64.06%] [G loss: 2.364134]\n",
            "7100 [D loss: 0.501127, acc.: 75.78%] [G loss: 2.358667]\n",
            "7120 [D loss: 0.654142, acc.: 64.45%] [G loss: 2.025026]\n",
            "7140 [D loss: 0.698913, acc.: 60.94%] [G loss: 3.084947]\n",
            "7160 [D loss: 0.524721, acc.: 74.22%] [G loss: 2.958238]\n",
            "7180 [D loss: 0.530202, acc.: 70.70%] [G loss: 3.014191]\n",
            "7200 [D loss: 0.564068, acc.: 68.75%] [G loss: 3.054159]\n",
            "7220 [D loss: 0.614838, acc.: 65.62%] [G loss: 2.528241]\n",
            "7240 [D loss: 0.510842, acc.: 74.22%] [G loss: 2.996816]\n",
            "7260 [D loss: 0.531445, acc.: 74.61%] [G loss: 2.838899]\n",
            "7280 [D loss: 0.604262, acc.: 66.41%] [G loss: 3.443517]\n",
            "7300 [D loss: 0.560710, acc.: 72.27%] [G loss: 2.935805]\n",
            "7320 [D loss: 0.527353, acc.: 75.00%] [G loss: 3.108568]\n",
            "7340 [D loss: 0.538279, acc.: 74.22%] [G loss: 3.205568]\n",
            "7360 [D loss: 0.477303, acc.: 78.52%] [G loss: 3.084794]\n",
            "7380 [D loss: 0.543085, acc.: 74.61%] [G loss: 3.306096]\n",
            "7400 [D loss: 0.519393, acc.: 72.27%] [G loss: 3.020657]\n",
            "7420 [D loss: 0.611241, acc.: 68.36%] [G loss: 3.073063]\n",
            "7440 [D loss: 0.483314, acc.: 78.12%] [G loss: 3.009058]\n",
            "7460 [D loss: 0.551428, acc.: 69.53%] [G loss: 2.891180]\n",
            "7480 [D loss: 0.532314, acc.: 74.22%] [G loss: 3.077636]\n",
            "7500 [D loss: 0.564114, acc.: 69.53%] [G loss: 2.814384]\n",
            "7520 [D loss: 0.558492, acc.: 69.92%] [G loss: 2.855614]\n",
            "7540 [D loss: 0.469333, acc.: 78.91%] [G loss: 3.338792]\n",
            "7560 [D loss: 0.552564, acc.: 73.44%] [G loss: 2.959739]\n",
            "7580 [D loss: 0.513720, acc.: 73.83%] [G loss: 3.097597]\n",
            "7600 [D loss: 0.526523, acc.: 74.61%] [G loss: 3.113131]\n",
            "7620 [D loss: 0.460566, acc.: 82.03%] [G loss: 3.473390]\n",
            "7640 [D loss: 0.577159, acc.: 69.92%] [G loss: 2.809493]\n",
            "7660 [D loss: 0.536551, acc.: 71.88%] [G loss: 3.031549]\n",
            "7680 [D loss: 0.494169, acc.: 74.61%] [G loss: 3.166071]\n",
            "7700 [D loss: 0.552801, acc.: 71.88%] [G loss: 3.075470]\n",
            "7720 [D loss: 0.515393, acc.: 73.83%] [G loss: 2.946174]\n",
            "7740 [D loss: 0.542951, acc.: 73.05%] [G loss: 2.973525]\n",
            "7760 [D loss: 0.560996, acc.: 66.80%] [G loss: 3.133804]\n",
            "7780 [D loss: 0.576913, acc.: 67.19%] [G loss: 3.261157]\n",
            "7800 [D loss: 0.490806, acc.: 77.73%] [G loss: 3.107666]\n",
            "7820 [D loss: 0.601816, acc.: 66.80%] [G loss: 3.131498]\n",
            "7840 [D loss: 0.601322, acc.: 68.36%] [G loss: 2.914399]\n",
            "7860 [D loss: 0.393776, acc.: 85.94%] [G loss: 3.810173]\n",
            "7880 [D loss: 0.539179, acc.: 73.05%] [G loss: 3.050454]\n",
            "7900 [D loss: 0.564541, acc.: 68.36%] [G loss: 2.811281]\n",
            "7920 [D loss: 0.512038, acc.: 76.56%] [G loss: 3.384752]\n",
            "7940 [D loss: 0.528520, acc.: 75.39%] [G loss: 3.058740]\n",
            "7960 [D loss: 0.476770, acc.: 78.91%] [G loss: 3.817729]\n",
            "7980 [D loss: 0.562692, acc.: 70.31%] [G loss: 3.133332]\n",
            "8000 [D loss: 0.456061, acc.: 78.52%] [G loss: 3.677467]\n",
            "8020 [D loss: 0.474892, acc.: 77.73%] [G loss: 3.270413]\n",
            "8040 [D loss: 0.493595, acc.: 75.78%] [G loss: 3.122141]\n",
            "8060 [D loss: 0.509606, acc.: 78.12%] [G loss: 3.501596]\n",
            "8080 [D loss: 0.508403, acc.: 74.61%] [G loss: 3.703897]\n",
            "8100 [D loss: 0.529810, acc.: 71.48%] [G loss: 3.353333]\n",
            "8120 [D loss: 0.601277, acc.: 68.75%] [G loss: 2.646151]\n",
            "8140 [D loss: 0.452751, acc.: 81.25%] [G loss: 3.720129]\n",
            "8160 [D loss: 0.539832, acc.: 72.66%] [G loss: 3.087159]\n",
            "8180 [D loss: 0.606804, acc.: 69.14%] [G loss: 2.422060]\n",
            "8200 [D loss: 0.534556, acc.: 74.61%] [G loss: 3.082531]\n",
            "8220 [D loss: 0.538023, acc.: 70.70%] [G loss: 3.340661]\n",
            "8240 [D loss: 0.492537, acc.: 77.73%] [G loss: 3.844815]\n",
            "8260 [D loss: 0.411749, acc.: 82.81%] [G loss: 3.480125]\n",
            "8280 [D loss: 0.940486, acc.: 55.47%] [G loss: 3.223578]\n",
            "8300 [D loss: 0.548028, acc.: 75.00%] [G loss: 3.819713]\n",
            "8320 [D loss: 0.515257, acc.: 73.44%] [G loss: 3.616592]\n",
            "8340 [D loss: 0.664571, acc.: 63.28%] [G loss: 3.550297]\n",
            "8360 [D loss: 0.519128, acc.: 75.00%] [G loss: 3.199698]\n",
            "8380 [D loss: 0.519188, acc.: 71.88%] [G loss: 3.943111]\n",
            "8400 [D loss: 0.544212, acc.: 73.83%] [G loss: 3.652937]\n",
            "8420 [D loss: 0.509741, acc.: 76.17%] [G loss: 3.312895]\n",
            "8440 [D loss: 0.532589, acc.: 73.44%] [G loss: 3.447786]\n",
            "8460 [D loss: 0.446288, acc.: 78.91%] [G loss: 3.881862]\n",
            "8480 [D loss: 0.508311, acc.: 74.61%] [G loss: 3.625345]\n",
            "8500 [D loss: 0.527258, acc.: 75.39%] [G loss: 3.593431]\n",
            "8520 [D loss: 0.524729, acc.: 74.61%] [G loss: 3.855374]\n",
            "8540 [D loss: 0.463034, acc.: 81.64%] [G loss: 4.089058]\n",
            "8560 [D loss: 0.455465, acc.: 81.64%] [G loss: 3.766096]\n",
            "8580 [D loss: 0.522683, acc.: 73.83%] [G loss: 3.539115]\n",
            "8600 [D loss: 0.465690, acc.: 78.52%] [G loss: 3.614044]\n",
            "8620 [D loss: 0.605127, acc.: 68.75%] [G loss: 4.298413]\n",
            "8640 [D loss: 0.456947, acc.: 78.52%] [G loss: 3.663829]\n",
            "8660 [D loss: 0.482393, acc.: 78.91%] [G loss: 3.555803]\n",
            "8680 [D loss: 0.428580, acc.: 78.91%] [G loss: 3.733907]\n",
            "8700 [D loss: 0.426939, acc.: 79.30%] [G loss: 3.874112]\n",
            "8720 [D loss: 0.498035, acc.: 78.52%] [G loss: 3.787046]\n",
            "8740 [D loss: 0.464896, acc.: 79.69%] [G loss: 3.898816]\n",
            "8760 [D loss: 0.527833, acc.: 71.48%] [G loss: 3.209577]\n",
            "8780 [D loss: 0.491621, acc.: 74.61%] [G loss: 3.790970]\n",
            "8800 [D loss: 0.550980, acc.: 71.48%] [G loss: 3.509937]\n",
            "8820 [D loss: 0.456275, acc.: 78.91%] [G loss: 4.045055]\n",
            "8840 [D loss: 0.507856, acc.: 76.56%] [G loss: 3.662587]\n",
            "8860 [D loss: 0.390004, acc.: 82.42%] [G loss: 4.061714]\n",
            "8880 [D loss: 0.524180, acc.: 71.88%] [G loss: 3.148600]\n",
            "8900 [D loss: 0.457224, acc.: 78.12%] [G loss: 4.187587]\n",
            "8920 [D loss: 0.478076, acc.: 76.17%] [G loss: 3.557046]\n",
            "8940 [D loss: 0.501007, acc.: 76.56%] [G loss: 3.237127]\n",
            "8960 [D loss: 0.450852, acc.: 76.56%] [G loss: 3.847581]\n",
            "8980 [D loss: 0.482136, acc.: 76.95%] [G loss: 3.828495]\n",
            "9000 [D loss: 0.483315, acc.: 77.73%] [G loss: 3.563182]\n",
            "9020 [D loss: 0.479470, acc.: 78.12%] [G loss: 3.863338]\n",
            "9040 [D loss: 0.498906, acc.: 75.00%] [G loss: 3.712154]\n",
            "9060 [D loss: 0.442242, acc.: 78.52%] [G loss: 3.790497]\n",
            "9080 [D loss: 0.429331, acc.: 79.69%] [G loss: 4.446510]\n",
            "9100 [D loss: 0.565088, acc.: 71.88%] [G loss: 4.066839]\n",
            "9120 [D loss: 0.456388, acc.: 76.17%] [G loss: 3.947429]\n",
            "9140 [D loss: 0.224321, acc.: 95.70%] [G loss: 2.727302]\n",
            "9160 [D loss: 0.514238, acc.: 74.61%] [G loss: 2.329399]\n",
            "9180 [D loss: 0.871650, acc.: 60.16%] [G loss: 3.043980]\n",
            "9200 [D loss: 0.776210, acc.: 57.42%] [G loss: 4.362148]\n",
            "9220 [D loss: 0.628662, acc.: 66.41%] [G loss: 4.018559]\n",
            "9240 [D loss: 0.516453, acc.: 73.44%] [G loss: 2.971127]\n",
            "9260 [D loss: 0.540568, acc.: 72.27%] [G loss: 3.620670]\n",
            "9280 [D loss: 0.649822, acc.: 65.62%] [G loss: 3.435280]\n",
            "9300 [D loss: 0.441114, acc.: 79.30%] [G loss: 4.046820]\n",
            "9320 [D loss: 0.497509, acc.: 77.73%] [G loss: 4.214865]\n",
            "9340 [D loss: 0.389078, acc.: 85.55%] [G loss: 4.025277]\n",
            "9360 [D loss: 0.493124, acc.: 77.73%] [G loss: 3.453268]\n",
            "9380 [D loss: 0.476461, acc.: 74.61%] [G loss: 4.490609]\n",
            "9400 [D loss: 0.413329, acc.: 83.59%] [G loss: 4.375767]\n",
            "9420 [D loss: 0.399560, acc.: 83.98%] [G loss: 4.047416]\n",
            "9440 [D loss: 0.381665, acc.: 82.42%] [G loss: 4.083551]\n",
            "9460 [D loss: 0.502971, acc.: 76.17%] [G loss: 4.125199]\n",
            "9480 [D loss: 0.454179, acc.: 78.91%] [G loss: 4.496197]\n",
            "9500 [D loss: 0.487502, acc.: 76.56%] [G loss: 3.927767]\n",
            "9520 [D loss: 0.422071, acc.: 81.25%] [G loss: 5.491996]\n",
            "9540 [D loss: 0.431645, acc.: 77.34%] [G loss: 4.132703]\n",
            "9560 [D loss: 0.396311, acc.: 83.20%] [G loss: 4.374956]\n",
            "9580 [D loss: 0.536423, acc.: 76.95%] [G loss: 4.501300]\n",
            "9600 [D loss: 0.448086, acc.: 80.86%] [G loss: 4.934124]\n",
            "9620 [D loss: 0.540144, acc.: 74.61%] [G loss: 3.629860]\n",
            "9640 [D loss: 0.452234, acc.: 77.73%] [G loss: 4.320711]\n",
            "9660 [D loss: 0.460401, acc.: 77.73%] [G loss: 3.738194]\n",
            "9680 [D loss: 0.506675, acc.: 77.73%] [G loss: 3.835501]\n",
            "9700 [D loss: 0.517266, acc.: 76.56%] [G loss: 3.803913]\n",
            "9720 [D loss: 0.437481, acc.: 79.30%] [G loss: 4.429732]\n",
            "9740 [D loss: 0.493100, acc.: 73.44%] [G loss: 3.662147]\n",
            "9760 [D loss: 0.518395, acc.: 74.22%] [G loss: 3.721194]\n",
            "9780 [D loss: 0.434756, acc.: 79.30%] [G loss: 4.141724]\n",
            "9800 [D loss: 0.522091, acc.: 75.00%] [G loss: 3.850022]\n",
            "9820 [D loss: 0.423437, acc.: 83.20%] [G loss: 3.978816]\n",
            "9840 [D loss: 0.442913, acc.: 80.08%] [G loss: 4.246624]\n",
            "9860 [D loss: 0.496024, acc.: 77.34%] [G loss: 3.927477]\n",
            "9880 [D loss: 0.414169, acc.: 83.20%] [G loss: 4.419979]\n",
            "9900 [D loss: 0.391203, acc.: 83.20%] [G loss: 4.763879]\n",
            "9920 [D loss: 0.396183, acc.: 83.98%] [G loss: 5.133637]\n",
            "9940 [D loss: 0.469115, acc.: 76.95%] [G loss: 4.142194]\n",
            "9960 [D loss: 0.415274, acc.: 82.42%] [G loss: 4.512157]\n",
            "9980 [D loss: 0.468658, acc.: 78.91%] [G loss: 4.131038]\n",
            "10000 [D loss: 0.480509, acc.: 75.78%] [G loss: 4.417917]\n",
            "10020 [D loss: 0.427307, acc.: 81.25%] [G loss: 4.545678]\n",
            "10040 [D loss: 0.423147, acc.: 79.30%] [G loss: 4.361867]\n",
            "10060 [D loss: 0.409040, acc.: 81.25%] [G loss: 4.538980]\n",
            "10080 [D loss: 0.417475, acc.: 82.42%] [G loss: 4.885777]\n",
            "10100 [D loss: 0.413718, acc.: 81.64%] [G loss: 4.395040]\n",
            "10120 [D loss: 0.405602, acc.: 85.16%] [G loss: 4.592649]\n",
            "10140 [D loss: 0.470736, acc.: 77.34%] [G loss: 4.451930]\n",
            "10160 [D loss: 0.446971, acc.: 80.86%] [G loss: 4.488653]\n",
            "10180 [D loss: 0.517890, acc.: 73.44%] [G loss: 3.972299]\n",
            "10200 [D loss: 0.433135, acc.: 80.08%] [G loss: 4.868363]\n",
            "10220 [D loss: 0.405819, acc.: 81.64%] [G loss: 5.360374]\n",
            "10240 [D loss: 0.465814, acc.: 77.73%] [G loss: 4.680752]\n",
            "10260 [D loss: 0.377442, acc.: 85.94%] [G loss: 5.106811]\n",
            "10280 [D loss: 0.389483, acc.: 83.59%] [G loss: 4.461942]\n",
            "10300 [D loss: 0.444025, acc.: 80.86%] [G loss: 3.971976]\n",
            "10320 [D loss: 0.471580, acc.: 76.17%] [G loss: 4.099287]\n",
            "10340 [D loss: 0.429200, acc.: 80.08%] [G loss: 4.341433]\n",
            "10360 [D loss: 0.438833, acc.: 80.08%] [G loss: 4.613157]\n",
            "10380 [D loss: 0.407779, acc.: 81.25%] [G loss: 4.593510]\n",
            "10400 [D loss: 0.472456, acc.: 75.78%] [G loss: 4.690992]\n",
            "10420 [D loss: 0.424786, acc.: 78.52%] [G loss: 4.429993]\n",
            "10440 [D loss: 0.433918, acc.: 79.30%] [G loss: 5.351582]\n",
            "10460 [D loss: 0.389466, acc.: 84.38%] [G loss: 4.900419]\n",
            "10480 [D loss: 0.476296, acc.: 77.73%] [G loss: 4.144945]\n",
            "10500 [D loss: 0.372542, acc.: 82.81%] [G loss: 4.750437]\n",
            "10520 [D loss: 0.428574, acc.: 76.56%] [G loss: 4.643574]\n",
            "10540 [D loss: 0.385839, acc.: 82.03%] [G loss: 5.665579]\n",
            "10560 [D loss: 0.381208, acc.: 84.38%] [G loss: 5.065922]\n",
            "10580 [D loss: 0.366320, acc.: 85.94%] [G loss: 4.952451]\n",
            "10600 [D loss: 0.465536, acc.: 78.12%] [G loss: 4.041990]\n",
            "10620 [D loss: 0.464505, acc.: 78.91%] [G loss: 5.046002]\n",
            "10640 [D loss: 0.420928, acc.: 78.91%] [G loss: 4.517241]\n",
            "10660 [D loss: 0.373695, acc.: 83.20%] [G loss: 4.985516]\n",
            "10680 [D loss: 0.392993, acc.: 82.03%] [G loss: 5.522408]\n",
            "10700 [D loss: 0.474309, acc.: 75.39%] [G loss: 4.651552]\n",
            "10720 [D loss: 0.301103, acc.: 89.06%] [G loss: 5.894167]\n",
            "10740 [D loss: 0.522380, acc.: 74.22%] [G loss: 3.943722]\n",
            "10760 [D loss: 0.430893, acc.: 80.86%] [G loss: 5.749470]\n",
            "10780 [D loss: 0.452720, acc.: 79.30%] [G loss: 4.658351]\n",
            "10800 [D loss: 0.369547, acc.: 85.94%] [G loss: 4.903233]\n",
            "10820 [D loss: 0.307435, acc.: 89.45%] [G loss: 4.788550]\n",
            "10840 [D loss: 0.395560, acc.: 85.55%] [G loss: 4.581063]\n",
            "10860 [D loss: 0.406973, acc.: 81.64%] [G loss: 4.903319]\n",
            "10880 [D loss: 0.438280, acc.: 82.03%] [G loss: 5.476682]\n",
            "10900 [D loss: 0.303441, acc.: 86.33%] [G loss: 5.845872]\n",
            "10920 [D loss: 0.415224, acc.: 79.69%] [G loss: 5.062531]\n",
            "10940 [D loss: 0.535252, acc.: 71.48%] [G loss: 4.207927]\n",
            "10960 [D loss: 0.374779, acc.: 83.98%] [G loss: 5.370421]\n",
            "10980 [D loss: 0.382357, acc.: 83.20%] [G loss: 5.781460]\n",
            "11000 [D loss: 0.448238, acc.: 78.12%] [G loss: 5.054698]\n",
            "11020 [D loss: 0.387821, acc.: 85.94%] [G loss: 5.198973]\n",
            "11040 [D loss: 0.436145, acc.: 82.03%] [G loss: 5.038472]\n",
            "11060 [D loss: 0.416187, acc.: 82.42%] [G loss: 5.216393]\n",
            "11080 [D loss: 0.350778, acc.: 83.20%] [G loss: 6.382454]\n",
            "11100 [D loss: 0.461337, acc.: 76.17%] [G loss: 4.582994]\n",
            "11120 [D loss: 0.461307, acc.: 76.17%] [G loss: 5.078738]\n",
            "11140 [D loss: 0.472069, acc.: 77.34%] [G loss: 5.307887]\n",
            "11160 [D loss: 0.385924, acc.: 81.25%] [G loss: 5.072592]\n",
            "11180 [D loss: 0.453665, acc.: 78.12%] [G loss: 3.867723]\n",
            "11200 [D loss: 0.411545, acc.: 79.69%] [G loss: 5.442444]\n",
            "11220 [D loss: 0.506945, acc.: 76.17%] [G loss: 4.555628]\n",
            "11240 [D loss: 0.417099, acc.: 80.86%] [G loss: 5.663673]\n",
            "11260 [D loss: 0.430634, acc.: 78.52%] [G loss: 4.674031]\n",
            "11280 [D loss: 0.389138, acc.: 82.42%] [G loss: 5.361537]\n",
            "11300 [D loss: 0.330585, acc.: 86.72%] [G loss: 5.852851]\n",
            "11320 [D loss: 0.425282, acc.: 79.69%] [G loss: 4.755063]\n",
            "11340 [D loss: 0.378982, acc.: 83.98%] [G loss: 4.737522]\n",
            "11360 [D loss: 0.507136, acc.: 76.95%] [G loss: 3.623380]\n",
            "11380 [D loss: 0.626232, acc.: 66.80%] [G loss: 4.662861]\n",
            "11400 [D loss: 1.133503, acc.: 46.48%] [G loss: 4.546093]\n",
            "11420 [D loss: 0.373260, acc.: 86.33%] [G loss: 4.825460]\n",
            "11440 [D loss: 0.442230, acc.: 79.30%] [G loss: 5.522822]\n",
            "11460 [D loss: 0.568578, acc.: 73.83%] [G loss: 5.044984]\n",
            "11480 [D loss: 0.374084, acc.: 85.55%] [G loss: 5.270505]\n",
            "11500 [D loss: 0.416414, acc.: 82.42%] [G loss: 5.188753]\n",
            "11520 [D loss: 0.345965, acc.: 84.38%] [G loss: 5.671137]\n",
            "11540 [D loss: 0.442863, acc.: 78.91%] [G loss: 4.488462]\n",
            "11560 [D loss: 0.376493, acc.: 81.64%] [G loss: 5.503234]\n",
            "11580 [D loss: 0.436889, acc.: 77.73%] [G loss: 5.303383]\n",
            "11600 [D loss: 0.438417, acc.: 78.12%] [G loss: 5.647703]\n",
            "11620 [D loss: 0.405082, acc.: 84.77%] [G loss: 5.148907]\n",
            "11640 [D loss: 0.452751, acc.: 78.12%] [G loss: 4.839200]\n",
            "11660 [D loss: 0.291551, acc.: 89.84%] [G loss: 6.294246]\n",
            "11680 [D loss: 0.411082, acc.: 82.03%] [G loss: 5.257402]\n",
            "11700 [D loss: 0.355526, acc.: 85.94%] [G loss: 6.420026]\n",
            "11720 [D loss: 0.374519, acc.: 83.98%] [G loss: 5.483109]\n",
            "11740 [D loss: 0.363886, acc.: 83.98%] [G loss: 5.891066]\n",
            "11760 [D loss: 0.436085, acc.: 80.08%] [G loss: 4.948734]\n",
            "11780 [D loss: 0.401146, acc.: 81.64%] [G loss: 6.478646]\n",
            "11800 [D loss: 0.464595, acc.: 76.56%] [G loss: 4.332670]\n",
            "11820 [D loss: 0.389637, acc.: 82.03%] [G loss: 5.066226]\n",
            "11840 [D loss: 0.409810, acc.: 81.25%] [G loss: 5.758197]\n",
            "11860 [D loss: 0.409055, acc.: 79.30%] [G loss: 5.624437]\n",
            "11880 [D loss: 0.407202, acc.: 81.25%] [G loss: 5.540837]\n",
            "11900 [D loss: 0.323256, acc.: 85.94%] [G loss: 6.531792]\n",
            "11920 [D loss: 0.402282, acc.: 79.30%] [G loss: 5.154116]\n",
            "11940 [D loss: 0.381702, acc.: 82.42%] [G loss: 5.725841]\n",
            "11960 [D loss: 0.447378, acc.: 80.08%] [G loss: 4.610763]\n",
            "11980 [D loss: 0.418832, acc.: 82.03%] [G loss: 5.939856]\n",
            "12000 [D loss: 0.365062, acc.: 85.55%] [G loss: 6.356049]\n",
            "12020 [D loss: 0.389279, acc.: 83.98%] [G loss: 6.471623]\n",
            "12040 [D loss: 0.495431, acc.: 75.78%] [G loss: 5.624032]\n",
            "12060 [D loss: 0.393177, acc.: 80.47%] [G loss: 5.288004]\n",
            "12080 [D loss: 0.450757, acc.: 78.91%] [G loss: 5.839144]\n",
            "12100 [D loss: 0.378466, acc.: 85.16%] [G loss: 4.962793]\n",
            "12120 [D loss: 0.336541, acc.: 87.11%] [G loss: 5.552405]\n",
            "12140 [D loss: 0.419386, acc.: 79.30%] [G loss: 5.672557]\n",
            "12160 [D loss: 0.443064, acc.: 81.25%] [G loss: 5.947913]\n",
            "12180 [D loss: 0.382328, acc.: 82.03%] [G loss: 5.788836]\n",
            "12200 [D loss: 0.385936, acc.: 82.81%] [G loss: 6.424562]\n",
            "12220 [D loss: 0.340806, acc.: 85.94%] [G loss: 5.365752]\n",
            "12240 [D loss: 0.345306, acc.: 84.77%] [G loss: 6.800056]\n",
            "12260 [D loss: 0.419575, acc.: 78.12%] [G loss: 6.508112]\n",
            "12280 [D loss: 0.407743, acc.: 81.25%] [G loss: 5.750739]\n",
            "12300 [D loss: 0.409340, acc.: 82.42%] [G loss: 6.750494]\n",
            "12320 [D loss: 0.409737, acc.: 82.81%] [G loss: 5.590847]\n",
            "12340 [D loss: 0.339864, acc.: 87.11%] [G loss: 5.648168]\n",
            "12360 [D loss: 0.372595, acc.: 83.20%] [G loss: 5.675991]\n",
            "12380 [D loss: 0.364662, acc.: 85.55%] [G loss: 6.779284]\n",
            "12400 [D loss: 0.388856, acc.: 83.98%] [G loss: 6.216835]\n",
            "12420 [D loss: 0.321575, acc.: 87.11%] [G loss: 6.621998]\n",
            "12440 [D loss: 0.397192, acc.: 82.42%] [G loss: 6.729762]\n",
            "12460 [D loss: 0.427845, acc.: 79.69%] [G loss: 6.141900]\n",
            "12480 [D loss: 0.367813, acc.: 82.81%] [G loss: 7.445224]\n",
            "12500 [D loss: 0.380947, acc.: 81.64%] [G loss: 5.942633]\n",
            "12520 [D loss: 0.376841, acc.: 83.20%] [G loss: 5.752484]\n",
            "12540 [D loss: 0.370536, acc.: 82.81%] [G loss: 6.746759]\n",
            "12560 [D loss: 0.470350, acc.: 78.52%] [G loss: 5.339250]\n",
            "12580 [D loss: 0.289880, acc.: 89.06%] [G loss: 6.479836]\n",
            "12600 [D loss: 0.365032, acc.: 85.94%] [G loss: 5.672522]\n",
            "12620 [D loss: 0.351850, acc.: 82.42%] [G loss: 4.352086]\n",
            "12640 [D loss: 0.683722, acc.: 64.84%] [G loss: 5.618680]\n",
            "12660 [D loss: 0.457083, acc.: 76.56%] [G loss: 6.393121]\n",
            "12680 [D loss: 0.522405, acc.: 73.05%] [G loss: 4.663572]\n",
            "12700 [D loss: 0.389079, acc.: 81.64%] [G loss: 6.187271]\n",
            "12720 [D loss: 0.523042, acc.: 74.22%] [G loss: 4.541053]\n",
            "12740 [D loss: 0.478962, acc.: 80.47%] [G loss: 6.646634]\n",
            "12760 [D loss: 0.504086, acc.: 75.78%] [G loss: 7.247461]\n",
            "12780 [D loss: 0.378936, acc.: 82.03%] [G loss: 6.673873]\n",
            "12800 [D loss: 0.456275, acc.: 79.30%] [G loss: 6.499812]\n",
            "12820 [D loss: 0.458760, acc.: 78.12%] [G loss: 6.433017]\n",
            "12840 [D loss: 0.504854, acc.: 75.78%] [G loss: 5.556077]\n",
            "12860 [D loss: 0.305015, acc.: 89.84%] [G loss: 7.165704]\n",
            "12880 [D loss: 0.582848, acc.: 73.44%] [G loss: 6.641410]\n",
            "12900 [D loss: 0.338949, acc.: 85.94%] [G loss: 6.092625]\n",
            "12920 [D loss: 0.448654, acc.: 78.52%] [G loss: 6.151815]\n",
            "12940 [D loss: 0.337942, acc.: 85.94%] [G loss: 6.129220]\n",
            "12960 [D loss: 0.397479, acc.: 82.03%] [G loss: 6.232902]\n",
            "12980 [D loss: 0.408416, acc.: 80.86%] [G loss: 5.979030]\n",
            "13000 [D loss: 0.399241, acc.: 82.03%] [G loss: 6.418961]\n",
            "13020 [D loss: 0.399939, acc.: 80.08%] [G loss: 6.700551]\n",
            "13040 [D loss: 0.468362, acc.: 76.56%] [G loss: 6.743195]\n",
            "13060 [D loss: 0.293335, acc.: 86.72%] [G loss: 6.344500]\n",
            "13080 [D loss: 0.437123, acc.: 78.12%] [G loss: 7.040424]\n",
            "13100 [D loss: 0.288071, acc.: 88.28%] [G loss: 6.492424]\n",
            "13120 [D loss: 0.425319, acc.: 81.64%] [G loss: 6.124841]\n",
            "13140 [D loss: 0.292755, acc.: 91.41%] [G loss: 6.687903]\n",
            "13160 [D loss: 0.352722, acc.: 85.94%] [G loss: 6.273176]\n",
            "13180 [D loss: 0.337344, acc.: 85.94%] [G loss: 6.699172]\n",
            "13200 [D loss: 0.323198, acc.: 87.89%] [G loss: 8.013114]\n",
            "13220 [D loss: 0.431302, acc.: 81.25%] [G loss: 6.660610]\n",
            "13240 [D loss: 0.366438, acc.: 83.98%] [G loss: 6.233786]\n",
            "13260 [D loss: 0.329588, acc.: 85.94%] [G loss: 6.976315]\n",
            "13280 [D loss: 0.313852, acc.: 88.67%] [G loss: 6.665234]\n",
            "13300 [D loss: 0.410497, acc.: 83.20%] [G loss: 7.304500]\n",
            "13320 [D loss: 0.341655, acc.: 85.94%] [G loss: 6.611860]\n",
            "13340 [D loss: 0.293667, acc.: 89.84%] [G loss: 6.912965]\n",
            "13360 [D loss: 0.283053, acc.: 89.06%] [G loss: 7.007438]\n",
            "13380 [D loss: 0.276903, acc.: 87.89%] [G loss: 8.564660]\n",
            "13400 [D loss: 0.343979, acc.: 83.59%] [G loss: 6.494528]\n",
            "13420 [D loss: 0.442427, acc.: 76.95%] [G loss: 6.566976]\n",
            "13440 [D loss: 0.346784, acc.: 82.81%] [G loss: 6.495490]\n",
            "13460 [D loss: 0.272926, acc.: 90.62%] [G loss: 8.090980]\n",
            "13480 [D loss: 0.408167, acc.: 80.08%] [G loss: 7.268408]\n",
            "13500 [D loss: 0.300460, acc.: 87.89%] [G loss: 7.444194]\n",
            "13520 [D loss: 0.438080, acc.: 78.12%] [G loss: 6.644561]\n",
            "13540 [D loss: 0.306309, acc.: 85.94%] [G loss: 7.230372]\n",
            "13560 [D loss: 0.360662, acc.: 83.59%] [G loss: 7.258842]\n",
            "13580 [D loss: 0.345779, acc.: 86.72%] [G loss: 6.609086]\n",
            "13600 [D loss: 0.362381, acc.: 84.77%] [G loss: 7.241385]\n",
            "13620 [D loss: 0.378200, acc.: 85.16%] [G loss: 6.917225]\n",
            "13640 [D loss: 0.307635, acc.: 85.55%] [G loss: 6.928971]\n",
            "13660 [D loss: 0.371656, acc.: 82.81%] [G loss: 6.515791]\n",
            "13680 [D loss: 0.263649, acc.: 89.45%] [G loss: 8.307802]\n",
            "13700 [D loss: 0.281728, acc.: 91.80%] [G loss: 7.154031]\n",
            "13720 [D loss: 0.408420, acc.: 81.64%] [G loss: 6.553104]\n",
            "13740 [D loss: 0.341414, acc.: 87.89%] [G loss: 6.294973]\n",
            "13760 [D loss: 0.327865, acc.: 86.72%] [G loss: 7.374726]\n",
            "13780 [D loss: 0.375843, acc.: 82.42%] [G loss: 6.988334]\n",
            "13800 [D loss: 0.387603, acc.: 82.81%] [G loss: 7.736346]\n",
            "13820 [D loss: 0.380372, acc.: 82.42%] [G loss: 7.188281]\n",
            "13840 [D loss: 0.347847, acc.: 86.72%] [G loss: 6.954853]\n",
            "13860 [D loss: 0.323044, acc.: 85.16%] [G loss: 7.535838]\n",
            "13880 [D loss: 0.446596, acc.: 77.34%] [G loss: 6.652277]\n",
            "13900 [D loss: 0.343810, acc.: 83.20%] [G loss: 6.366400]\n",
            "13920 [D loss: 0.404110, acc.: 80.86%] [G loss: 7.588097]\n",
            "13940 [D loss: 0.364070, acc.: 84.38%] [G loss: 6.967479]\n",
            "13960 [D loss: 0.276123, acc.: 88.67%] [G loss: 8.485699]\n",
            "13980 [D loss: 0.290549, acc.: 89.06%] [G loss: 7.530718]\n",
            "14000 [D loss: 0.377276, acc.: 82.03%] [G loss: 5.999337]\n",
            "14020 [D loss: 0.361896, acc.: 83.59%] [G loss: 6.617574]\n",
            "14040 [D loss: 0.376010, acc.: 80.47%] [G loss: 6.491834]\n",
            "14060 [D loss: 0.293769, acc.: 87.89%] [G loss: 7.419269]\n",
            "14080 [D loss: 0.455570, acc.: 79.69%] [G loss: 6.751736]\n",
            "14100 [D loss: 0.271754, acc.: 89.06%] [G loss: 7.220393]\n",
            "14120 [D loss: 0.217963, acc.: 93.36%] [G loss: 7.794847]\n",
            "14140 [D loss: 0.352043, acc.: 84.77%] [G loss: 7.529938]\n",
            "14160 [D loss: 0.222587, acc.: 92.58%] [G loss: 4.750657]\n",
            "14180 [D loss: 0.722212, acc.: 64.84%] [G loss: 6.601143]\n",
            "14200 [D loss: 0.510129, acc.: 77.34%] [G loss: 8.499405]\n",
            "14220 [D loss: 0.325030, acc.: 87.11%] [G loss: 7.579809]\n",
            "14240 [D loss: 0.444108, acc.: 79.69%] [G loss: 6.885326]\n",
            "14260 [D loss: 0.309239, acc.: 86.33%] [G loss: 7.392031]\n",
            "14280 [D loss: 0.560010, acc.: 71.88%] [G loss: 5.838940]\n",
            "14300 [D loss: 0.479080, acc.: 78.91%] [G loss: 7.742178]\n",
            "14320 [D loss: 0.377644, acc.: 83.98%] [G loss: 8.237241]\n",
            "14340 [D loss: 0.293239, acc.: 87.50%] [G loss: 8.307875]\n",
            "14360 [D loss: 0.267924, acc.: 89.84%] [G loss: 7.951932]\n",
            "14380 [D loss: 0.331728, acc.: 85.94%] [G loss: 9.364869]\n",
            "14400 [D loss: 0.273890, acc.: 88.67%] [G loss: 8.264042]\n",
            "14420 [D loss: 0.447195, acc.: 76.95%] [G loss: 6.377148]\n",
            "14440 [D loss: 0.347095, acc.: 83.59%] [G loss: 7.845825]\n",
            "14460 [D loss: 0.368592, acc.: 83.20%] [G loss: 6.963232]\n",
            "14480 [D loss: 0.252821, acc.: 91.02%] [G loss: 8.125404]\n",
            "14500 [D loss: 0.392899, acc.: 80.86%] [G loss: 8.145732]\n",
            "14520 [D loss: 0.273537, acc.: 90.62%] [G loss: 9.369278]\n",
            "14540 [D loss: 0.383888, acc.: 81.64%] [G loss: 7.363331]\n",
            "14560 [D loss: 0.287964, acc.: 89.06%] [G loss: 7.063092]\n",
            "14580 [D loss: 0.396134, acc.: 81.64%] [G loss: 7.405649]\n",
            "14600 [D loss: 0.278928, acc.: 88.28%] [G loss: 7.570201]\n",
            "14620 [D loss: 0.349573, acc.: 86.33%] [G loss: 9.392256]\n",
            "14640 [D loss: 0.365488, acc.: 86.72%] [G loss: 7.364864]\n",
            "14660 [D loss: 0.301676, acc.: 87.89%] [G loss: 9.329534]\n",
            "14680 [D loss: 0.285770, acc.: 87.89%] [G loss: 7.813848]\n",
            "14700 [D loss: 0.393715, acc.: 83.59%] [G loss: 8.030753]\n",
            "14720 [D loss: 0.323348, acc.: 86.72%] [G loss: 7.486773]\n",
            "14740 [D loss: 0.283868, acc.: 89.84%] [G loss: 8.221962]\n",
            "14760 [D loss: 0.368791, acc.: 82.42%] [G loss: 7.131596]\n",
            "14780 [D loss: 0.313060, acc.: 86.33%] [G loss: 8.452631]\n",
            "14800 [D loss: 0.292320, acc.: 87.11%] [G loss: 8.671141]\n",
            "14820 [D loss: 0.396827, acc.: 83.59%] [G loss: 7.439801]\n",
            "14840 [D loss: 0.336796, acc.: 85.94%] [G loss: 8.254946]\n",
            "14860 [D loss: 0.357122, acc.: 85.94%] [G loss: 7.264243]\n",
            "14880 [D loss: 0.339362, acc.: 87.11%] [G loss: 7.593790]\n",
            "14900 [D loss: 0.279481, acc.: 87.50%] [G loss: 10.893728]\n",
            "14920 [D loss: 0.276285, acc.: 89.45%] [G loss: 9.060791]\n",
            "14940 [D loss: 0.335369, acc.: 86.33%] [G loss: 8.484623]\n",
            "14960 [D loss: 0.346974, acc.: 85.94%] [G loss: 8.840242]\n",
            "14980 [D loss: 0.328458, acc.: 86.33%] [G loss: 7.815982]\n",
            "15000 [D loss: 0.320309, acc.: 85.55%] [G loss: 8.590067]\n",
            "15020 [D loss: 0.291286, acc.: 87.89%] [G loss: 7.671411]\n",
            "15040 [D loss: 0.295588, acc.: 88.28%] [G loss: 7.467619]\n",
            "15060 [D loss: 0.306290, acc.: 86.72%] [G loss: 7.490561]\n",
            "15080 [D loss: 0.268756, acc.: 89.84%] [G loss: 8.194138]\n",
            "15100 [D loss: 0.313783, acc.: 86.72%] [G loss: 7.602489]\n",
            "15120 [D loss: 0.331500, acc.: 85.55%] [G loss: 7.785259]\n",
            "15140 [D loss: 0.329361, acc.: 86.72%] [G loss: 9.389053]\n",
            "15160 [D loss: 0.351183, acc.: 82.81%] [G loss: 8.161432]\n",
            "15180 [D loss: 0.265731, acc.: 89.84%] [G loss: 10.319271]\n",
            "15200 [D loss: 0.320293, acc.: 85.94%] [G loss: 8.509283]\n",
            "15220 [D loss: 0.274079, acc.: 88.67%] [G loss: 8.663376]\n",
            "15240 [D loss: 0.309461, acc.: 87.89%] [G loss: 10.145952]\n",
            "15260 [D loss: 0.336405, acc.: 84.38%] [G loss: 8.098675]\n",
            "15280 [D loss: 0.371925, acc.: 85.55%] [G loss: 8.647551]\n",
            "15300 [D loss: 0.344379, acc.: 83.98%] [G loss: 9.179976]\n",
            "15320 [D loss: 0.287720, acc.: 87.89%] [G loss: 8.488598]\n",
            "15340 [D loss: 0.304948, acc.: 85.94%] [G loss: 8.975842]\n",
            "15360 [D loss: 0.287158, acc.: 89.06%] [G loss: 8.503466]\n",
            "15380 [D loss: 0.283750, acc.: 87.11%] [G loss: 8.693275]\n",
            "15400 [D loss: 0.249606, acc.: 89.06%] [G loss: 9.229154]\n",
            "15420 [D loss: 0.373093, acc.: 83.98%] [G loss: 8.939356]\n",
            "15440 [D loss: 0.263101, acc.: 88.28%] [G loss: 9.373507]\n",
            "15460 [D loss: 0.372619, acc.: 80.08%] [G loss: 7.456335]\n",
            "15480 [D loss: 0.302175, acc.: 87.50%] [G loss: 9.472545]\n",
            "15500 [D loss: 0.369620, acc.: 83.98%] [G loss: 8.113825]\n",
            "15520 [D loss: 0.273896, acc.: 91.02%] [G loss: 8.441060]\n",
            "15540 [D loss: 0.333516, acc.: 85.55%] [G loss: 7.911297]\n",
            "15560 [D loss: 0.363797, acc.: 83.59%] [G loss: 8.401590]\n",
            "15580 [D loss: 0.322042, acc.: 86.72%] [G loss: 8.192455]\n",
            "15600 [D loss: 0.355476, acc.: 86.72%] [G loss: 8.101763]\n",
            "15620 [D loss: 0.230380, acc.: 91.02%] [G loss: 10.185829]\n",
            "15640 [D loss: 0.282405, acc.: 89.06%] [G loss: 8.842545]\n",
            "15660 [D loss: 0.280840, acc.: 89.84%] [G loss: 9.457392]\n",
            "15680 [D loss: 0.333521, acc.: 87.50%] [G loss: 7.482080]\n",
            "15700 [D loss: 0.638052, acc.: 69.14%] [G loss: 4.649231]\n",
            "15720 [D loss: 0.610961, acc.: 75.78%] [G loss: 8.640831]\n",
            "15740 [D loss: 0.431812, acc.: 81.64%] [G loss: 8.514960]\n",
            "15760 [D loss: 0.328761, acc.: 84.38%] [G loss: 9.300325]\n",
            "15780 [D loss: 0.267559, acc.: 90.23%] [G loss: 8.525553]\n",
            "15800 [D loss: 0.262185, acc.: 89.06%] [G loss: 10.356505]\n",
            "15820 [D loss: 0.313806, acc.: 89.45%] [G loss: 9.641007]\n",
            "15840 [D loss: 0.269086, acc.: 88.67%] [G loss: 8.439914]\n",
            "15860 [D loss: 0.285633, acc.: 89.06%] [G loss: 9.772213]\n",
            "15880 [D loss: 0.312732, acc.: 86.33%] [G loss: 8.977978]\n",
            "15900 [D loss: 0.332813, acc.: 88.28%] [G loss: 8.824131]\n",
            "15920 [D loss: 0.339065, acc.: 87.89%] [G loss: 8.949429]\n",
            "15940 [D loss: 0.319951, acc.: 83.59%] [G loss: 7.775308]\n",
            "15960 [D loss: 0.289981, acc.: 87.89%] [G loss: 9.105509]\n",
            "15980 [D loss: 0.297711, acc.: 88.28%] [G loss: 7.487240]\n",
            "16000 [D loss: 0.268795, acc.: 88.67%] [G loss: 9.108410]\n",
            "16020 [D loss: 0.249352, acc.: 90.62%] [G loss: 10.610355]\n",
            "16040 [D loss: 0.334132, acc.: 85.55%] [G loss: 7.836429]\n",
            "16060 [D loss: 0.272330, acc.: 90.23%] [G loss: 8.531101]\n",
            "16080 [D loss: 0.255625, acc.: 90.62%] [G loss: 9.740484]\n",
            "16100 [D loss: 0.241187, acc.: 89.45%] [G loss: 8.579879]\n",
            "16120 [D loss: 0.340803, acc.: 84.38%] [G loss: 10.506703]\n",
            "16140 [D loss: 0.373491, acc.: 83.98%] [G loss: 8.399857]\n",
            "16160 [D loss: 0.342167, acc.: 82.42%] [G loss: 9.160913]\n",
            "16180 [D loss: 0.207041, acc.: 93.36%] [G loss: 9.267931]\n",
            "16200 [D loss: 0.262988, acc.: 90.62%] [G loss: 9.747753]\n",
            "16220 [D loss: 0.324472, acc.: 85.16%] [G loss: 8.993369]\n",
            "16240 [D loss: 0.259417, acc.: 90.23%] [G loss: 8.936046]\n",
            "16260 [D loss: 0.305394, acc.: 86.72%] [G loss: 8.933559]\n",
            "16280 [D loss: 0.229135, acc.: 89.84%] [G loss: 9.748272]\n",
            "16300 [D loss: 0.235236, acc.: 90.62%] [G loss: 8.566766]\n",
            "16320 [D loss: 0.321654, acc.: 87.11%] [G loss: 8.252929]\n",
            "16340 [D loss: 0.372313, acc.: 85.55%] [G loss: 8.930881]\n",
            "16360 [D loss: 0.254637, acc.: 91.02%] [G loss: 9.186800]\n",
            "16380 [D loss: 0.309107, acc.: 86.33%] [G loss: 9.494614]\n",
            "16400 [D loss: 0.295311, acc.: 87.11%] [G loss: 9.277696]\n",
            "16420 [D loss: 0.266813, acc.: 90.23%] [G loss: 9.703099]\n",
            "16440 [D loss: 0.273265, acc.: 89.06%] [G loss: 9.333949]\n",
            "16460 [D loss: 0.226169, acc.: 92.19%] [G loss: 9.042217]\n",
            "16480 [D loss: 0.195593, acc.: 91.80%] [G loss: 9.264971]\n",
            "16500 [D loss: 0.305652, acc.: 83.98%] [G loss: 8.626499]\n",
            "16520 [D loss: 0.228829, acc.: 90.23%] [G loss: 9.404742]\n",
            "16540 [D loss: 0.253294, acc.: 92.19%] [G loss: 7.795982]\n",
            "16560 [D loss: 0.285430, acc.: 86.72%] [G loss: 9.201789]\n",
            "16580 [D loss: 0.375024, acc.: 82.03%] [G loss: 8.443012]\n",
            "16600 [D loss: 0.266100, acc.: 87.89%] [G loss: 10.246552]\n",
            "16620 [D loss: 0.215758, acc.: 90.62%] [G loss: 10.006966]\n",
            "16640 [D loss: 0.265547, acc.: 89.84%] [G loss: 8.694842]\n",
            "16660 [D loss: 0.213861, acc.: 90.62%] [G loss: 10.521614]\n",
            "16680 [D loss: 0.274320, acc.: 90.62%] [G loss: 9.181342]\n",
            "16700 [D loss: 0.360979, acc.: 83.20%] [G loss: 9.608679]\n",
            "16720 [D loss: 0.267920, acc.: 91.02%] [G loss: 8.969095]\n",
            "16740 [D loss: 0.151538, acc.: 95.31%] [G loss: 7.235149]\n",
            "16760 [D loss: 0.420003, acc.: 82.81%] [G loss: 8.910173]\n",
            "16780 [D loss: 0.883185, acc.: 60.16%] [G loss: 6.660610]\n",
            "16800 [D loss: 0.437742, acc.: 81.64%] [G loss: 8.612006]\n",
            "16820 [D loss: 0.365387, acc.: 83.59%] [G loss: 8.260988]\n",
            "16840 [D loss: 0.278506, acc.: 89.06%] [G loss: 9.959484]\n",
            "16860 [D loss: 0.325938, acc.: 83.59%] [G loss: 10.503669]\n",
            "16880 [D loss: 0.265397, acc.: 88.67%] [G loss: 9.813401]\n",
            "16900 [D loss: 0.340010, acc.: 83.20%] [G loss: 9.029142]\n",
            "16920 [D loss: 0.365382, acc.: 84.38%] [G loss: 8.776790]\n",
            "16940 [D loss: 0.292359, acc.: 87.89%] [G loss: 9.852612]\n",
            "16960 [D loss: 0.285814, acc.: 89.06%] [G loss: 8.887915]\n",
            "16980 [D loss: 0.293634, acc.: 87.11%] [G loss: 10.189137]\n",
            "17000 [D loss: 0.267723, acc.: 90.62%] [G loss: 9.382627]\n",
            "17020 [D loss: 0.265848, acc.: 88.67%] [G loss: 7.953341]\n",
            "17040 [D loss: 0.232067, acc.: 90.62%] [G loss: 9.899487]\n",
            "17060 [D loss: 0.320213, acc.: 86.33%] [G loss: 9.433252]\n",
            "17080 [D loss: 0.427912, acc.: 82.81%] [G loss: 8.952535]\n",
            "17100 [D loss: 0.218522, acc.: 89.06%] [G loss: 9.826155]\n",
            "17120 [D loss: 0.291703, acc.: 88.67%] [G loss: 10.410776]\n",
            "17140 [D loss: 0.304893, acc.: 86.72%] [G loss: 8.877259]\n",
            "17160 [D loss: 0.243090, acc.: 91.80%] [G loss: 10.322001]\n",
            "17180 [D loss: 0.362312, acc.: 82.03%] [G loss: 9.181028]\n",
            "17200 [D loss: 0.325109, acc.: 85.16%] [G loss: 10.025671]\n",
            "17220 [D loss: 0.261276, acc.: 88.67%] [G loss: 9.674389]\n",
            "17240 [D loss: 0.292729, acc.: 88.28%] [G loss: 9.523443]\n",
            "17260 [D loss: 0.275716, acc.: 89.06%] [G loss: 10.855020]\n",
            "17280 [D loss: 0.307589, acc.: 87.11%] [G loss: 8.923751]\n",
            "17300 [D loss: 0.247952, acc.: 91.41%] [G loss: 9.839237]\n",
            "17320 [D loss: 0.308012, acc.: 86.72%] [G loss: 9.657414]\n",
            "17340 [D loss: 0.237919, acc.: 90.23%] [G loss: 10.788385]\n",
            "17360 [D loss: 0.250797, acc.: 89.45%] [G loss: 10.290545]\n",
            "17380 [D loss: 0.169196, acc.: 93.75%] [G loss: 11.614552]\n",
            "17400 [D loss: 0.327618, acc.: 85.16%] [G loss: 9.237288]\n",
            "17420 [D loss: 0.345022, acc.: 84.77%] [G loss: 9.321547]\n",
            "17440 [D loss: 0.331174, acc.: 86.33%] [G loss: 10.224644]\n",
            "17460 [D loss: 0.338627, acc.: 83.98%] [G loss: 9.879395]\n",
            "17480 [D loss: 0.287736, acc.: 87.11%] [G loss: 8.851128]\n",
            "17500 [D loss: 0.234280, acc.: 91.02%] [G loss: 10.700159]\n",
            "17520 [D loss: 0.306407, acc.: 86.33%] [G loss: 9.364180]\n",
            "17540 [D loss: 0.217985, acc.: 92.97%] [G loss: 9.951287]\n",
            "17560 [D loss: 0.266797, acc.: 87.89%] [G loss: 9.461124]\n",
            "17580 [D loss: 0.241862, acc.: 89.45%] [G loss: 10.341413]\n",
            "17600 [D loss: 0.280786, acc.: 89.06%] [G loss: 9.270263]\n",
            "17620 [D loss: 0.327941, acc.: 84.38%] [G loss: 8.816418]\n",
            "17640 [D loss: 0.301125, acc.: 87.11%] [G loss: 8.325890]\n",
            "17660 [D loss: 0.231504, acc.: 91.80%] [G loss: 9.360155]\n",
            "17680 [D loss: 0.210255, acc.: 92.19%] [G loss: 8.264905]\n",
            "17700 [D loss: 0.206899, acc.: 93.36%] [G loss: 8.199427]\n",
            "17720 [D loss: 0.706335, acc.: 71.88%] [G loss: 8.528545]\n",
            "17740 [D loss: 0.522730, acc.: 79.30%] [G loss: 10.076422]\n",
            "17760 [D loss: 0.285825, acc.: 86.72%] [G loss: 10.278851]\n",
            "17780 [D loss: 0.369709, acc.: 85.55%] [G loss: 8.868881]\n",
            "17800 [D loss: 0.234266, acc.: 90.62%] [G loss: 10.201171]\n",
            "17820 [D loss: 0.313565, acc.: 87.11%] [G loss: 9.457525]\n",
            "17840 [D loss: 0.238016, acc.: 92.19%] [G loss: 8.796425]\n",
            "17860 [D loss: 0.351244, acc.: 85.94%] [G loss: 10.377294]\n",
            "17880 [D loss: 0.263552, acc.: 90.23%] [G loss: 10.805002]\n",
            "17900 [D loss: 0.345302, acc.: 86.72%] [G loss: 8.297983]\n",
            "17920 [D loss: 0.257318, acc.: 88.28%] [G loss: 10.496535]\n",
            "17940 [D loss: 0.323193, acc.: 85.55%] [G loss: 10.414844]\n",
            "17960 [D loss: 0.236876, acc.: 90.23%] [G loss: 9.291885]\n",
            "17980 [D loss: 0.254699, acc.: 91.02%] [G loss: 8.481934]\n",
            "18000 [D loss: 0.265860, acc.: 90.23%] [G loss: 9.790733]\n",
            "18020 [D loss: 0.280874, acc.: 87.89%] [G loss: 9.900132]\n",
            "18040 [D loss: 0.302663, acc.: 88.28%] [G loss: 10.214960]\n",
            "18060 [D loss: 0.318822, acc.: 87.11%] [G loss: 10.709999]\n",
            "18080 [D loss: 0.194770, acc.: 92.97%] [G loss: 9.089429]\n",
            "18100 [D loss: 0.214101, acc.: 90.62%] [G loss: 10.012275]\n",
            "18120 [D loss: 0.194712, acc.: 92.19%] [G loss: 11.679053]\n",
            "18140 [D loss: 0.300756, acc.: 88.67%] [G loss: 9.365245]\n",
            "18160 [D loss: 0.226351, acc.: 91.02%] [G loss: 10.638149]\n",
            "18180 [D loss: 0.234321, acc.: 91.41%] [G loss: 12.664598]\n",
            "18200 [D loss: 0.272263, acc.: 88.67%] [G loss: 8.757252]\n",
            "18220 [D loss: 0.262579, acc.: 90.23%] [G loss: 8.888789]\n",
            "18240 [D loss: 0.278680, acc.: 87.89%] [G loss: 8.875593]\n",
            "18260 [D loss: 0.201289, acc.: 92.19%] [G loss: 11.205864]\n",
            "18280 [D loss: 0.210173, acc.: 91.80%] [G loss: 12.011633]\n",
            "18300 [D loss: 0.310619, acc.: 87.11%] [G loss: 9.378286]\n",
            "18320 [D loss: 0.257379, acc.: 86.72%] [G loss: 10.325244]\n",
            "18340 [D loss: 0.142332, acc.: 95.70%] [G loss: 10.544097]\n",
            "18360 [D loss: 0.245619, acc.: 91.41%] [G loss: 10.098005]\n",
            "18380 [D loss: 0.271733, acc.: 87.50%] [G loss: 10.071934]\n",
            "18400 [D loss: 0.190153, acc.: 92.97%] [G loss: 9.930861]\n",
            "18420 [D loss: 0.195916, acc.: 92.97%] [G loss: 11.166698]\n",
            "18440 [D loss: 0.372821, acc.: 84.38%] [G loss: 9.614422]\n",
            "18460 [D loss: 0.280357, acc.: 88.28%] [G loss: 9.168572]\n",
            "18480 [D loss: 0.241755, acc.: 88.67%] [G loss: 11.701811]\n",
            "18500 [D loss: 0.247065, acc.: 91.02%] [G loss: 9.677923]\n",
            "18520 [D loss: 0.320066, acc.: 85.16%] [G loss: 9.209315]\n",
            "18540 [D loss: 0.170394, acc.: 93.75%] [G loss: 11.195832]\n",
            "18560 [D loss: 0.333583, acc.: 86.33%] [G loss: 11.235685]\n",
            "18580 [D loss: 0.283549, acc.: 89.06%] [G loss: 10.269960]\n",
            "18600 [D loss: 0.221950, acc.: 92.19%] [G loss: 10.782342]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}