{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_Model_BiCoGAN_Scale.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dalia-Sher/Generating-Facial-Expressions-Bidirectional-Conditional-GAN/blob/Shir/Final_Model_BiCoGAN_Scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS516rLy1LYQ",
        "outputId": "02c970a0-abda-41e1-9db1-8711dd5a3a32"
      },
      "source": [
        "!unzip fer2013.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fer2013.zip\n",
            "  inflating: fer2013.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "jF8QfAYPg_71",
        "outputId": "b50b7db4-7489-4096-84c1-c190cefc0509"
      },
      "source": [
        "data = pd.read_csv('fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "aebebdc2-cedd-4984-c16e-09e8bc85bd4b"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5xEsrnIB0t"
      },
      "source": [
        "dic = {0:'Angry', 1:'disgust' , 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRPqlqU1IG94"
      },
      "source": [
        "The emotion disgust has too few samples, therefore we won't use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9swxs5P0w",
        "outputId": "31267a31-fa44-4d26-d077-ddcf98bb833c"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSAij3uINfw"
      },
      "source": [
        "We will redefine the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "641ef2fb-01bb-433c-c0b7-8dd4be0c39ea"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOXTev833cv",
        "outputId": "7b18d9f5-8115-442f-d695-77b45aac1e2d"
      },
      "source": [
        "epochs = X_train.shape[0]\n",
        "print(\"number of epochs:\", epochs)\n",
        "\n",
        "X_train_aug = X_train\n",
        "y_train_aug = y_train\n",
        "\n",
        "for k in range(epochs):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  scaled_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  scale_im=iaa.Affine(scale={\"x\": (1.5, 1.0), \"y\": (1.5, 1.0)})\n",
        "  scale_image =scale_im.augment_image(img)\n",
        "  scaled_images.append(scale_image)\n",
        "\n",
        "  scaled_images = np.array(scaled_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train_aug = np.concatenate((X_train_aug, scaled_images), axis=0)\n",
        "  emotions_list = [emotion]\n",
        "  y_train_aug = np.concatenate((y_train_aug, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration:\" , k,\", train shape:\", X_train_aug.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epochs: 35340\n",
            "iteration: 0 , train shape: (35341, 48, 48, 1)\n",
            "iteration: 100 , train shape: (35441, 48, 48, 1)\n",
            "iteration: 200 , train shape: (35541, 48, 48, 1)\n",
            "iteration: 300 , train shape: (35641, 48, 48, 1)\n",
            "iteration: 400 , train shape: (35741, 48, 48, 1)\n",
            "iteration: 500 , train shape: (35841, 48, 48, 1)\n",
            "iteration: 600 , train shape: (35941, 48, 48, 1)\n",
            "iteration: 700 , train shape: (36041, 48, 48, 1)\n",
            "iteration: 800 , train shape: (36141, 48, 48, 1)\n",
            "iteration: 900 , train shape: (36241, 48, 48, 1)\n",
            "iteration: 1000 , train shape: (36341, 48, 48, 1)\n",
            "iteration: 1100 , train shape: (36441, 48, 48, 1)\n",
            "iteration: 1200 , train shape: (36541, 48, 48, 1)\n",
            "iteration: 1300 , train shape: (36641, 48, 48, 1)\n",
            "iteration: 1400 , train shape: (36741, 48, 48, 1)\n",
            "iteration: 1500 , train shape: (36841, 48, 48, 1)\n",
            "iteration: 1600 , train shape: (36941, 48, 48, 1)\n",
            "iteration: 1700 , train shape: (37041, 48, 48, 1)\n",
            "iteration: 1800 , train shape: (37141, 48, 48, 1)\n",
            "iteration: 1900 , train shape: (37241, 48, 48, 1)\n",
            "iteration: 2000 , train shape: (37341, 48, 48, 1)\n",
            "iteration: 2100 , train shape: (37441, 48, 48, 1)\n",
            "iteration: 2200 , train shape: (37541, 48, 48, 1)\n",
            "iteration: 2300 , train shape: (37641, 48, 48, 1)\n",
            "iteration: 2400 , train shape: (37741, 48, 48, 1)\n",
            "iteration: 2500 , train shape: (37841, 48, 48, 1)\n",
            "iteration: 2600 , train shape: (37941, 48, 48, 1)\n",
            "iteration: 2700 , train shape: (38041, 48, 48, 1)\n",
            "iteration: 2800 , train shape: (38141, 48, 48, 1)\n",
            "iteration: 2900 , train shape: (38241, 48, 48, 1)\n",
            "iteration: 3000 , train shape: (38341, 48, 48, 1)\n",
            "iteration: 3100 , train shape: (38441, 48, 48, 1)\n",
            "iteration: 3200 , train shape: (38541, 48, 48, 1)\n",
            "iteration: 3300 , train shape: (38641, 48, 48, 1)\n",
            "iteration: 3400 , train shape: (38741, 48, 48, 1)\n",
            "iteration: 3500 , train shape: (38841, 48, 48, 1)\n",
            "iteration: 3600 , train shape: (38941, 48, 48, 1)\n",
            "iteration: 3700 , train shape: (39041, 48, 48, 1)\n",
            "iteration: 3800 , train shape: (39141, 48, 48, 1)\n",
            "iteration: 3900 , train shape: (39241, 48, 48, 1)\n",
            "iteration: 4000 , train shape: (39341, 48, 48, 1)\n",
            "iteration: 4100 , train shape: (39441, 48, 48, 1)\n",
            "iteration: 4200 , train shape: (39541, 48, 48, 1)\n",
            "iteration: 4300 , train shape: (39641, 48, 48, 1)\n",
            "iteration: 4400 , train shape: (39741, 48, 48, 1)\n",
            "iteration: 4500 , train shape: (39841, 48, 48, 1)\n",
            "iteration: 4600 , train shape: (39941, 48, 48, 1)\n",
            "iteration: 4700 , train shape: (40041, 48, 48, 1)\n",
            "iteration: 4800 , train shape: (40141, 48, 48, 1)\n",
            "iteration: 4900 , train shape: (40241, 48, 48, 1)\n",
            "iteration: 5000 , train shape: (40341, 48, 48, 1)\n",
            "iteration: 5100 , train shape: (40441, 48, 48, 1)\n",
            "iteration: 5200 , train shape: (40541, 48, 48, 1)\n",
            "iteration: 5300 , train shape: (40641, 48, 48, 1)\n",
            "iteration: 5400 , train shape: (40741, 48, 48, 1)\n",
            "iteration: 5500 , train shape: (40841, 48, 48, 1)\n",
            "iteration: 5600 , train shape: (40941, 48, 48, 1)\n",
            "iteration: 5700 , train shape: (41041, 48, 48, 1)\n",
            "iteration: 5800 , train shape: (41141, 48, 48, 1)\n",
            "iteration: 5900 , train shape: (41241, 48, 48, 1)\n",
            "iteration: 6000 , train shape: (41341, 48, 48, 1)\n",
            "iteration: 6100 , train shape: (41441, 48, 48, 1)\n",
            "iteration: 6200 , train shape: (41541, 48, 48, 1)\n",
            "iteration: 6300 , train shape: (41641, 48, 48, 1)\n",
            "iteration: 6400 , train shape: (41741, 48, 48, 1)\n",
            "iteration: 6500 , train shape: (41841, 48, 48, 1)\n",
            "iteration: 6600 , train shape: (41941, 48, 48, 1)\n",
            "iteration: 6700 , train shape: (42041, 48, 48, 1)\n",
            "iteration: 6800 , train shape: (42141, 48, 48, 1)\n",
            "iteration: 6900 , train shape: (42241, 48, 48, 1)\n",
            "iteration: 7000 , train shape: (42341, 48, 48, 1)\n",
            "iteration: 7100 , train shape: (42441, 48, 48, 1)\n",
            "iteration: 7200 , train shape: (42541, 48, 48, 1)\n",
            "iteration: 7300 , train shape: (42641, 48, 48, 1)\n",
            "iteration: 7400 , train shape: (42741, 48, 48, 1)\n",
            "iteration: 7500 , train shape: (42841, 48, 48, 1)\n",
            "iteration: 7600 , train shape: (42941, 48, 48, 1)\n",
            "iteration: 7700 , train shape: (43041, 48, 48, 1)\n",
            "iteration: 7800 , train shape: (43141, 48, 48, 1)\n",
            "iteration: 7900 , train shape: (43241, 48, 48, 1)\n",
            "iteration: 8000 , train shape: (43341, 48, 48, 1)\n",
            "iteration: 8100 , train shape: (43441, 48, 48, 1)\n",
            "iteration: 8200 , train shape: (43541, 48, 48, 1)\n",
            "iteration: 8300 , train shape: (43641, 48, 48, 1)\n",
            "iteration: 8400 , train shape: (43741, 48, 48, 1)\n",
            "iteration: 8500 , train shape: (43841, 48, 48, 1)\n",
            "iteration: 8600 , train shape: (43941, 48, 48, 1)\n",
            "iteration: 8700 , train shape: (44041, 48, 48, 1)\n",
            "iteration: 8800 , train shape: (44141, 48, 48, 1)\n",
            "iteration: 8900 , train shape: (44241, 48, 48, 1)\n",
            "iteration: 9000 , train shape: (44341, 48, 48, 1)\n",
            "iteration: 9100 , train shape: (44441, 48, 48, 1)\n",
            "iteration: 9200 , train shape: (44541, 48, 48, 1)\n",
            "iteration: 9300 , train shape: (44641, 48, 48, 1)\n",
            "iteration: 9400 , train shape: (44741, 48, 48, 1)\n",
            "iteration: 9500 , train shape: (44841, 48, 48, 1)\n",
            "iteration: 9700 , train shape: (45041, 48, 48, 1)\n",
            "iteration: 9800 , train shape: (45141, 48, 48, 1)\n",
            "iteration: 9900 , train shape: (45241, 48, 48, 1)\n",
            "iteration: 10000 , train shape: (45341, 48, 48, 1)\n",
            "iteration: 10100 , train shape: (45441, 48, 48, 1)\n",
            "iteration: 10200 , train shape: (45541, 48, 48, 1)\n",
            "iteration: 10300 , train shape: (45641, 48, 48, 1)\n",
            "iteration: 10400 , train shape: (45741, 48, 48, 1)\n",
            "iteration: 10500 , train shape: (45841, 48, 48, 1)\n",
            "iteration: 10600 , train shape: (45941, 48, 48, 1)\n",
            "iteration: 10700 , train shape: (46041, 48, 48, 1)\n",
            "iteration: 10800 , train shape: (46141, 48, 48, 1)\n",
            "iteration: 10900 , train shape: (46241, 48, 48, 1)\n",
            "iteration: 11000 , train shape: (46341, 48, 48, 1)\n",
            "iteration: 11100 , train shape: (46441, 48, 48, 1)\n",
            "iteration: 11200 , train shape: (46541, 48, 48, 1)\n",
            "iteration: 11300 , train shape: (46641, 48, 48, 1)\n",
            "iteration: 11400 , train shape: (46741, 48, 48, 1)\n",
            "iteration: 11500 , train shape: (46841, 48, 48, 1)\n",
            "iteration: 11600 , train shape: (46941, 48, 48, 1)\n",
            "iteration: 11700 , train shape: (47041, 48, 48, 1)\n",
            "iteration: 11800 , train shape: (47141, 48, 48, 1)\n",
            "iteration: 11900 , train shape: (47241, 48, 48, 1)\n",
            "iteration: 12000 , train shape: (47341, 48, 48, 1)\n",
            "iteration: 12100 , train shape: (47441, 48, 48, 1)\n",
            "iteration: 12200 , train shape: (47541, 48, 48, 1)\n",
            "iteration: 12300 , train shape: (47641, 48, 48, 1)\n",
            "iteration: 12400 , train shape: (47741, 48, 48, 1)\n",
            "iteration: 12500 , train shape: (47841, 48, 48, 1)\n",
            "iteration: 12600 , train shape: (47941, 48, 48, 1)\n",
            "iteration: 12700 , train shape: (48041, 48, 48, 1)\n",
            "iteration: 12800 , train shape: (48141, 48, 48, 1)\n",
            "iteration: 12900 , train shape: (48241, 48, 48, 1)\n",
            "iteration: 13000 , train shape: (48341, 48, 48, 1)\n",
            "iteration: 13100 , train shape: (48441, 48, 48, 1)\n",
            "iteration: 13200 , train shape: (48541, 48, 48, 1)\n",
            "iteration: 13300 , train shape: (48641, 48, 48, 1)\n",
            "iteration: 13400 , train shape: (48741, 48, 48, 1)\n",
            "iteration: 13500 , train shape: (48841, 48, 48, 1)\n",
            "iteration: 13600 , train shape: (48941, 48, 48, 1)\n",
            "iteration: 13700 , train shape: (49041, 48, 48, 1)\n",
            "iteration: 13800 , train shape: (49141, 48, 48, 1)\n",
            "iteration: 13900 , train shape: (49241, 48, 48, 1)\n",
            "iteration: 14000 , train shape: (49341, 48, 48, 1)\n",
            "iteration: 14100 , train shape: (49441, 48, 48, 1)\n",
            "iteration: 14200 , train shape: (49541, 48, 48, 1)\n",
            "iteration: 14300 , train shape: (49641, 48, 48, 1)\n",
            "iteration: 14400 , train shape: (49741, 48, 48, 1)\n",
            "iteration: 14500 , train shape: (49841, 48, 48, 1)\n",
            "iteration: 14600 , train shape: (49941, 48, 48, 1)\n",
            "iteration: 14700 , train shape: (50041, 48, 48, 1)\n",
            "iteration: 14800 , train shape: (50141, 48, 48, 1)\n",
            "iteration: 14900 , train shape: (50241, 48, 48, 1)\n",
            "iteration: 15000 , train shape: (50341, 48, 48, 1)\n",
            "iteration: 15100 , train shape: (50441, 48, 48, 1)\n",
            "iteration: 15200 , train shape: (50541, 48, 48, 1)\n",
            "iteration: 15300 , train shape: (50641, 48, 48, 1)\n",
            "iteration: 15400 , train shape: (50741, 48, 48, 1)\n",
            "iteration: 15500 , train shape: (50841, 48, 48, 1)\n",
            "iteration: 15600 , train shape: (50941, 48, 48, 1)\n",
            "iteration: 15700 , train shape: (51041, 48, 48, 1)\n",
            "iteration: 15800 , train shape: (51141, 48, 48, 1)\n",
            "iteration: 15900 , train shape: (51241, 48, 48, 1)\n",
            "iteration: 16000 , train shape: (51341, 48, 48, 1)\n",
            "iteration: 16100 , train shape: (51441, 48, 48, 1)\n",
            "iteration: 16200 , train shape: (51541, 48, 48, 1)\n",
            "iteration: 16300 , train shape: (51641, 48, 48, 1)\n",
            "iteration: 16400 , train shape: (51741, 48, 48, 1)\n",
            "iteration: 16500 , train shape: (51841, 48, 48, 1)\n",
            "iteration: 16600 , train shape: (51941, 48, 48, 1)\n",
            "iteration: 16700 , train shape: (52041, 48, 48, 1)\n",
            "iteration: 16800 , train shape: (52141, 48, 48, 1)\n",
            "iteration: 16900 , train shape: (52241, 48, 48, 1)\n",
            "iteration: 17000 , train shape: (52341, 48, 48, 1)\n",
            "iteration: 17100 , train shape: (52441, 48, 48, 1)\n",
            "iteration: 17200 , train shape: (52541, 48, 48, 1)\n",
            "iteration: 17300 , train shape: (52641, 48, 48, 1)\n",
            "iteration: 17400 , train shape: (52741, 48, 48, 1)\n",
            "iteration: 17500 , train shape: (52841, 48, 48, 1)\n",
            "iteration: 17600 , train shape: (52941, 48, 48, 1)\n",
            "iteration: 17700 , train shape: (53041, 48, 48, 1)\n",
            "iteration: 17800 , train shape: (53141, 48, 48, 1)\n",
            "iteration: 17900 , train shape: (53241, 48, 48, 1)\n",
            "iteration: 18000 , train shape: (53341, 48, 48, 1)\n",
            "iteration: 18200 , train shape: (53541, 48, 48, 1)\n",
            "iteration: 18300 , train shape: (53641, 48, 48, 1)\n",
            "iteration: 18400 , train shape: (53741, 48, 48, 1)\n",
            "iteration: 18500 , train shape: (53841, 48, 48, 1)\n",
            "iteration: 18600 , train shape: (53941, 48, 48, 1)\n",
            "iteration: 18700 , train shape: (54041, 48, 48, 1)\n",
            "iteration: 18800 , train shape: (54141, 48, 48, 1)\n",
            "iteration: 18900 , train shape: (54241, 48, 48, 1)\n",
            "iteration: 19000 , train shape: (54341, 48, 48, 1)\n",
            "iteration: 19100 , train shape: (54441, 48, 48, 1)\n",
            "iteration: 19200 , train shape: (54541, 48, 48, 1)\n",
            "iteration: 19300 , train shape: (54641, 48, 48, 1)\n",
            "iteration: 19400 , train shape: (54741, 48, 48, 1)\n",
            "iteration: 19500 , train shape: (54841, 48, 48, 1)\n",
            "iteration: 19600 , train shape: (54941, 48, 48, 1)\n",
            "iteration: 19700 , train shape: (55041, 48, 48, 1)\n",
            "iteration: 19800 , train shape: (55141, 48, 48, 1)\n",
            "iteration: 19900 , train shape: (55241, 48, 48, 1)\n",
            "iteration: 20000 , train shape: (55341, 48, 48, 1)\n",
            "iteration: 20100 , train shape: (55441, 48, 48, 1)\n",
            "iteration: 20200 , train shape: (55541, 48, 48, 1)\n",
            "iteration: 20300 , train shape: (55641, 48, 48, 1)\n",
            "iteration: 20400 , train shape: (55741, 48, 48, 1)\n",
            "iteration: 20500 , train shape: (55841, 48, 48, 1)\n",
            "iteration: 20600 , train shape: (55941, 48, 48, 1)\n",
            "iteration: 20700 , train shape: (56041, 48, 48, 1)\n",
            "iteration: 20800 , train shape: (56141, 48, 48, 1)\n",
            "iteration: 20900 , train shape: (56241, 48, 48, 1)\n",
            "iteration: 21000 , train shape: (56341, 48, 48, 1)\n",
            "iteration: 21100 , train shape: (56441, 48, 48, 1)\n",
            "iteration: 21200 , train shape: (56541, 48, 48, 1)\n",
            "iteration: 21300 , train shape: (56641, 48, 48, 1)\n",
            "iteration: 21400 , train shape: (56741, 48, 48, 1)\n",
            "iteration: 21500 , train shape: (56841, 48, 48, 1)\n",
            "iteration: 21600 , train shape: (56941, 48, 48, 1)\n",
            "iteration: 21700 , train shape: (57041, 48, 48, 1)\n",
            "iteration: 21800 , train shape: (57141, 48, 48, 1)\n",
            "iteration: 21900 , train shape: (57241, 48, 48, 1)\n",
            "iteration: 22000 , train shape: (57341, 48, 48, 1)\n",
            "iteration: 22100 , train shape: (57441, 48, 48, 1)\n",
            "iteration: 22200 , train shape: (57541, 48, 48, 1)\n",
            "iteration: 22300 , train shape: (57641, 48, 48, 1)\n",
            "iteration: 22400 , train shape: (57741, 48, 48, 1)\n",
            "iteration: 22500 , train shape: (57841, 48, 48, 1)\n",
            "iteration: 22600 , train shape: (57941, 48, 48, 1)\n",
            "iteration: 22700 , train shape: (58041, 48, 48, 1)\n",
            "iteration: 22800 , train shape: (58141, 48, 48, 1)\n",
            "iteration: 22900 , train shape: (58241, 48, 48, 1)\n",
            "iteration: 23000 , train shape: (58341, 48, 48, 1)\n",
            "iteration: 23100 , train shape: (58441, 48, 48, 1)\n",
            "iteration: 23200 , train shape: (58541, 48, 48, 1)\n",
            "iteration: 23300 , train shape: (58641, 48, 48, 1)\n",
            "iteration: 23400 , train shape: (58741, 48, 48, 1)\n",
            "iteration: 23500 , train shape: (58841, 48, 48, 1)\n",
            "iteration: 23600 , train shape: (58941, 48, 48, 1)\n",
            "iteration: 23700 , train shape: (59041, 48, 48, 1)\n",
            "iteration: 23800 , train shape: (59141, 48, 48, 1)\n",
            "iteration: 23900 , train shape: (59241, 48, 48, 1)\n",
            "iteration: 24000 , train shape: (59341, 48, 48, 1)\n",
            "iteration: 24100 , train shape: (59441, 48, 48, 1)\n",
            "iteration: 24200 , train shape: (59541, 48, 48, 1)\n",
            "iteration: 24300 , train shape: (59641, 48, 48, 1)\n",
            "iteration: 24400 , train shape: (59741, 48, 48, 1)\n",
            "iteration: 24500 , train shape: (59841, 48, 48, 1)\n",
            "iteration: 24600 , train shape: (59941, 48, 48, 1)\n",
            "iteration: 24700 , train shape: (60041, 48, 48, 1)\n",
            "iteration: 24800 , train shape: (60141, 48, 48, 1)\n",
            "iteration: 24900 , train shape: (60241, 48, 48, 1)\n",
            "iteration: 25000 , train shape: (60341, 48, 48, 1)\n",
            "iteration: 25100 , train shape: (60441, 48, 48, 1)\n",
            "iteration: 25300 , train shape: (60641, 48, 48, 1)\n",
            "iteration: 25400 , train shape: (60741, 48, 48, 1)\n",
            "iteration: 25500 , train shape: (60841, 48, 48, 1)\n",
            "iteration: 25600 , train shape: (60941, 48, 48, 1)\n",
            "iteration: 25700 , train shape: (61041, 48, 48, 1)\n",
            "iteration: 25800 , train shape: (61141, 48, 48, 1)\n",
            "iteration: 25900 , train shape: (61241, 48, 48, 1)\n",
            "iteration: 26000 , train shape: (61341, 48, 48, 1)\n",
            "iteration: 26100 , train shape: (61441, 48, 48, 1)\n",
            "iteration: 26200 , train shape: (61541, 48, 48, 1)\n",
            "iteration: 26300 , train shape: (61641, 48, 48, 1)\n",
            "iteration: 26400 , train shape: (61741, 48, 48, 1)\n",
            "iteration: 26500 , train shape: (61841, 48, 48, 1)\n",
            "iteration: 26600 , train shape: (61941, 48, 48, 1)\n",
            "iteration: 26700 , train shape: (62041, 48, 48, 1)\n",
            "iteration: 26800 , train shape: (62141, 48, 48, 1)\n",
            "iteration: 26900 , train shape: (62241, 48, 48, 1)\n",
            "iteration: 27000 , train shape: (62341, 48, 48, 1)\n",
            "iteration: 27100 , train shape: (62441, 48, 48, 1)\n",
            "iteration: 27200 , train shape: (62541, 48, 48, 1)\n",
            "iteration: 27300 , train shape: (62641, 48, 48, 1)\n",
            "iteration: 27400 , train shape: (62741, 48, 48, 1)\n",
            "iteration: 27500 , train shape: (62841, 48, 48, 1)\n",
            "iteration: 27600 , train shape: (62941, 48, 48, 1)\n",
            "iteration: 27700 , train shape: (63041, 48, 48, 1)\n",
            "iteration: 27800 , train shape: (63141, 48, 48, 1)\n",
            "iteration: 27900 , train shape: (63241, 48, 48, 1)\n",
            "iteration: 28000 , train shape: (63341, 48, 48, 1)\n",
            "iteration: 28100 , train shape: (63441, 48, 48, 1)\n",
            "iteration: 28200 , train shape: (63541, 48, 48, 1)\n",
            "iteration: 28300 , train shape: (63641, 48, 48, 1)\n",
            "iteration: 28400 , train shape: (63741, 48, 48, 1)\n",
            "iteration: 28500 , train shape: (63841, 48, 48, 1)\n",
            "iteration: 28600 , train shape: (63941, 48, 48, 1)\n",
            "iteration: 28700 , train shape: (64041, 48, 48, 1)\n",
            "iteration: 28800 , train shape: (64141, 48, 48, 1)\n",
            "iteration: 28900 , train shape: (64241, 48, 48, 1)\n",
            "iteration: 29000 , train shape: (64341, 48, 48, 1)\n",
            "iteration: 29100 , train shape: (64441, 48, 48, 1)\n",
            "iteration: 29200 , train shape: (64541, 48, 48, 1)\n",
            "iteration: 29300 , train shape: (64641, 48, 48, 1)\n",
            "iteration: 29400 , train shape: (64741, 48, 48, 1)\n",
            "iteration: 29500 , train shape: (64841, 48, 48, 1)\n",
            "iteration: 29600 , train shape: (64941, 48, 48, 1)\n",
            "iteration: 29700 , train shape: (65041, 48, 48, 1)\n",
            "iteration: 29800 , train shape: (65141, 48, 48, 1)\n",
            "iteration: 29900 , train shape: (65241, 48, 48, 1)\n",
            "iteration: 30000 , train shape: (65341, 48, 48, 1)\n",
            "iteration: 30100 , train shape: (65441, 48, 48, 1)\n",
            "iteration: 30200 , train shape: (65541, 48, 48, 1)\n",
            "iteration: 30300 , train shape: (65641, 48, 48, 1)\n",
            "iteration: 30400 , train shape: (65741, 48, 48, 1)\n",
            "iteration: 30500 , train shape: (65841, 48, 48, 1)\n",
            "iteration: 30600 , train shape: (65941, 48, 48, 1)\n",
            "iteration: 30700 , train shape: (66041, 48, 48, 1)\n",
            "iteration: 30800 , train shape: (66141, 48, 48, 1)\n",
            "iteration: 30900 , train shape: (66241, 48, 48, 1)\n",
            "iteration: 31000 , train shape: (66341, 48, 48, 1)\n",
            "iteration: 31100 , train shape: (66441, 48, 48, 1)\n",
            "iteration: 31200 , train shape: (66541, 48, 48, 1)\n",
            "iteration: 31300 , train shape: (66641, 48, 48, 1)\n",
            "iteration: 31400 , train shape: (66741, 48, 48, 1)\n",
            "iteration: 31500 , train shape: (66841, 48, 48, 1)\n",
            "iteration: 31600 , train shape: (66941, 48, 48, 1)\n",
            "iteration: 31700 , train shape: (67041, 48, 48, 1)\n",
            "iteration: 31800 , train shape: (67141, 48, 48, 1)\n",
            "iteration: 31900 , train shape: (67241, 48, 48, 1)\n",
            "iteration: 32000 , train shape: (67341, 48, 48, 1)\n",
            "iteration: 32200 , train shape: (67541, 48, 48, 1)\n",
            "iteration: 32300 , train shape: (67641, 48, 48, 1)\n",
            "iteration: 32400 , train shape: (67741, 48, 48, 1)\n",
            "iteration: 32500 , train shape: (67841, 48, 48, 1)\n",
            "iteration: 32600 , train shape: (67941, 48, 48, 1)\n",
            "iteration: 32700 , train shape: (68041, 48, 48, 1)\n",
            "iteration: 32800 , train shape: (68141, 48, 48, 1)\n",
            "iteration: 32900 , train shape: (68241, 48, 48, 1)\n",
            "iteration: 33000 , train shape: (68341, 48, 48, 1)\n",
            "iteration: 33100 , train shape: (68441, 48, 48, 1)\n",
            "iteration: 33200 , train shape: (68541, 48, 48, 1)\n",
            "iteration: 33300 , train shape: (68641, 48, 48, 1)\n",
            "iteration: 33400 , train shape: (68741, 48, 48, 1)\n",
            "iteration: 33500 , train shape: (68841, 48, 48, 1)\n",
            "iteration: 33600 , train shape: (68941, 48, 48, 1)\n",
            "iteration: 33700 , train shape: (69041, 48, 48, 1)\n",
            "iteration: 33800 , train shape: (69141, 48, 48, 1)\n",
            "iteration: 33900 , train shape: (69241, 48, 48, 1)\n",
            "iteration: 34000 , train shape: (69341, 48, 48, 1)\n",
            "iteration: 34100 , train shape: (69441, 48, 48, 1)\n",
            "iteration: 34200 , train shape: (69541, 48, 48, 1)\n",
            "iteration: 34300 , train shape: (69641, 48, 48, 1)\n",
            "iteration: 34400 , train shape: (69741, 48, 48, 1)\n",
            "iteration: 34500 , train shape: (69841, 48, 48, 1)\n",
            "iteration: 34600 , train shape: (69941, 48, 48, 1)\n",
            "iteration: 34700 , train shape: (70041, 48, 48, 1)\n",
            "iteration: 34800 , train shape: (70141, 48, 48, 1)\n",
            "iteration: 34900 , train shape: (70241, 48, 48, 1)\n",
            "iteration: 35000 , train shape: (70341, 48, 48, 1)\n",
            "iteration: 35100 , train shape: (70441, 48, 48, 1)\n",
            "iteration: 35200 , train shape: (70541, 48, 48, 1)\n",
            "iteration: 35300 , train shape: (70641, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWsEbPyuvHO8",
        "outputId": "d9f2f3fb-93fc-45f7-d545-6fe2ba2bb539"
      },
      "source": [
        "print(\"X_train_aug shape:\", X_train_aug.shape)\n",
        "print(\"y_train_aug shape:\", y_train_aug.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_aug shape: (70680, 48, 48, 1)\n",
            "y_train_aug shape: (70680, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding image of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "        zn = Flatten()(zi)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train_aug.shape[0], batch_size)\n",
        "            imgs, labels = X_train_aug[idx], y_train_aug[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(r\"C:\\Users\\shir2\\Desktop\\Shir\\MSc\\%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-_XDn64sD4v",
        "outputId": "cd81a721-b99b-424c-ee84-ec34939c1cc1"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18610, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.706323, acc.: 30.47%] [G loss: 1.466945]\n",
            "20 [D loss: 1.204662, acc.: 22.66%] [G loss: 0.713936]\n",
            "40 [D loss: 1.499086, acc.: 3.12%] [G loss: 0.478026]\n",
            "60 [D loss: 1.460349, acc.: 11.33%] [G loss: 1.725645]\n",
            "80 [D loss: 0.598324, acc.: 48.44%] [G loss: 1.619125]\n",
            "100 [D loss: 0.788052, acc.: 32.03%] [G loss: 2.002900]\n",
            "120 [D loss: 0.683453, acc.: 72.27%] [G loss: 2.158030]\n",
            "140 [D loss: 0.585449, acc.: 44.92%] [G loss: 2.383927]\n",
            "160 [D loss: 0.701355, acc.: 64.06%] [G loss: 1.467794]\n",
            "180 [D loss: 0.817192, acc.: 22.27%] [G loss: 1.478308]\n",
            "200 [D loss: 0.780699, acc.: 41.41%] [G loss: 1.846640]\n",
            "220 [D loss: 0.581875, acc.: 84.77%] [G loss: 2.054561]\n",
            "240 [D loss: 0.353949, acc.: 94.14%] [G loss: 3.414087]\n",
            "260 [D loss: 0.575014, acc.: 72.27%] [G loss: 1.909147]\n",
            "280 [D loss: 0.236919, acc.: 96.48%] [G loss: 6.547719]\n",
            "300 [D loss: 0.321117, acc.: 98.83%] [G loss: 3.922623]\n",
            "320 [D loss: 0.523754, acc.: 80.47%] [G loss: 5.783221]\n",
            "340 [D loss: 0.460849, acc.: 95.31%] [G loss: 4.278628]\n",
            "360 [D loss: 0.780492, acc.: 58.20%] [G loss: 1.504403]\n",
            "380 [D loss: 0.579701, acc.: 79.69%] [G loss: 1.871652]\n",
            "400 [D loss: 0.633437, acc.: 71.48%] [G loss: 1.892438]\n",
            "420 [D loss: 0.587965, acc.: 69.53%] [G loss: 3.059372]\n",
            "440 [D loss: 0.978946, acc.: 52.73%] [G loss: 1.229761]\n",
            "460 [D loss: 0.683760, acc.: 45.31%] [G loss: 1.827955]\n",
            "480 [D loss: 0.724373, acc.: 57.81%] [G loss: 3.068774]\n",
            "500 [D loss: 0.690269, acc.: 64.84%] [G loss: 2.017990]\n",
            "520 [D loss: 0.707409, acc.: 53.91%] [G loss: 3.341539]\n",
            "540 [D loss: 0.674175, acc.: 59.38%] [G loss: 1.887651]\n",
            "560 [D loss: 0.157903, acc.: 99.22%] [G loss: 10.113520]\n",
            "580 [D loss: 0.857235, acc.: 51.17%] [G loss: 0.964879]\n",
            "600 [D loss: 0.854538, acc.: 35.55%] [G loss: 2.309754]\n",
            "620 [D loss: 0.386334, acc.: 85.94%] [G loss: 6.948630]\n",
            "640 [D loss: 0.637362, acc.: 66.41%] [G loss: 2.344614]\n",
            "660 [D loss: 0.779975, acc.: 44.92%] [G loss: 1.964797]\n",
            "680 [D loss: 0.569711, acc.: 72.27%] [G loss: 2.700482]\n",
            "700 [D loss: 0.547043, acc.: 73.44%] [G loss: 3.068296]\n",
            "720 [D loss: 0.581754, acc.: 67.58%] [G loss: 2.740363]\n",
            "740 [D loss: 0.517373, acc.: 77.34%] [G loss: 2.702379]\n",
            "760 [D loss: 0.784615, acc.: 48.05%] [G loss: 2.342977]\n",
            "780 [D loss: 0.536568, acc.: 80.08%] [G loss: 2.402903]\n",
            "800 [D loss: 0.592642, acc.: 69.53%] [G loss: 2.858708]\n",
            "820 [D loss: 0.466800, acc.: 82.42%] [G loss: 3.481915]\n",
            "840 [D loss: 0.507268, acc.: 77.73%] [G loss: 2.031728]\n",
            "860 [D loss: 0.731721, acc.: 60.55%] [G loss: 1.672085]\n",
            "880 [D loss: 0.310361, acc.: 90.62%] [G loss: 6.361792]\n",
            "900 [D loss: 0.836389, acc.: 39.06%] [G loss: 2.803505]\n",
            "920 [D loss: 0.742135, acc.: 41.80%] [G loss: 1.757476]\n",
            "940 [D loss: 0.737797, acc.: 55.08%] [G loss: 2.167215]\n",
            "960 [D loss: 0.608402, acc.: 68.36%] [G loss: 1.727838]\n",
            "980 [D loss: 0.561851, acc.: 74.61%] [G loss: 2.213974]\n",
            "1000 [D loss: 0.518507, acc.: 82.03%] [G loss: 2.675312]\n",
            "1020 [D loss: 0.619802, acc.: 63.28%] [G loss: 2.739803]\n",
            "1040 [D loss: 0.635359, acc.: 67.19%] [G loss: 2.381801]\n",
            "1060 [D loss: 0.630225, acc.: 62.89%] [G loss: 2.621354]\n",
            "1080 [D loss: 0.666051, acc.: 59.77%] [G loss: 2.216423]\n",
            "1100 [D loss: 0.503088, acc.: 75.78%] [G loss: 3.337445]\n",
            "1120 [D loss: 0.471293, acc.: 78.91%] [G loss: 3.525707]\n",
            "1140 [D loss: 0.613823, acc.: 68.36%] [G loss: 2.502326]\n",
            "1160 [D loss: 0.651808, acc.: 63.28%] [G loss: 2.228386]\n",
            "1180 [D loss: 0.472291, acc.: 78.91%] [G loss: 2.981998]\n",
            "1200 [D loss: 0.538492, acc.: 73.05%] [G loss: 2.725915]\n",
            "1220 [D loss: 0.494937, acc.: 77.73%] [G loss: 2.801517]\n",
            "1240 [D loss: 0.546721, acc.: 69.53%] [G loss: 2.567812]\n",
            "1260 [D loss: 0.598299, acc.: 69.53%] [G loss: 2.462947]\n",
            "1280 [D loss: 0.547997, acc.: 74.22%] [G loss: 2.619303]\n",
            "1300 [D loss: 0.508377, acc.: 77.34%] [G loss: 2.995659]\n",
            "1320 [D loss: 0.623533, acc.: 64.06%] [G loss: 2.423146]\n",
            "1340 [D loss: 0.527029, acc.: 77.34%] [G loss: 2.661210]\n",
            "1360 [D loss: 0.607030, acc.: 66.80%] [G loss: 2.433450]\n",
            "1380 [D loss: 0.604083, acc.: 67.58%] [G loss: 2.541656]\n",
            "1400 [D loss: 0.577307, acc.: 73.44%] [G loss: 2.536599]\n",
            "1420 [D loss: 0.536302, acc.: 75.39%] [G loss: 2.811962]\n",
            "1440 [D loss: 0.548819, acc.: 72.66%] [G loss: 2.952540]\n",
            "1460 [D loss: 0.492554, acc.: 76.56%] [G loss: 2.833736]\n",
            "1480 [D loss: 0.630178, acc.: 68.75%] [G loss: 3.090085]\n",
            "1500 [D loss: 0.510263, acc.: 72.27%] [G loss: 3.105777]\n",
            "1520 [D loss: 0.544835, acc.: 71.09%] [G loss: 2.905693]\n",
            "1540 [D loss: 0.625756, acc.: 65.62%] [G loss: 2.569814]\n",
            "1560 [D loss: 0.514641, acc.: 75.39%] [G loss: 2.832296]\n",
            "1580 [D loss: 0.617789, acc.: 66.41%] [G loss: 2.587172]\n",
            "1600 [D loss: 0.581149, acc.: 69.53%] [G loss: 2.619152]\n",
            "1620 [D loss: 0.599050, acc.: 67.97%] [G loss: 2.459284]\n",
            "1640 [D loss: 0.651998, acc.: 62.50%] [G loss: 2.307377]\n",
            "1660 [D loss: 0.574920, acc.: 68.36%] [G loss: 2.533907]\n",
            "1680 [D loss: 0.613164, acc.: 65.23%] [G loss: 2.490387]\n",
            "1700 [D loss: 0.578765, acc.: 72.27%] [G loss: 2.419418]\n",
            "1720 [D loss: 0.595541, acc.: 66.02%] [G loss: 2.564251]\n",
            "1740 [D loss: 0.454817, acc.: 80.08%] [G loss: 2.958795]\n",
            "1760 [D loss: 0.635729, acc.: 61.33%] [G loss: 2.397896]\n",
            "1780 [D loss: 0.595818, acc.: 71.48%] [G loss: 2.456168]\n",
            "1800 [D loss: 0.538335, acc.: 71.48%] [G loss: 2.615913]\n",
            "1820 [D loss: 0.556089, acc.: 70.70%] [G loss: 2.917849]\n",
            "1840 [D loss: 0.675200, acc.: 60.94%] [G loss: 2.223918]\n",
            "1860 [D loss: 0.576812, acc.: 71.88%] [G loss: 2.321817]\n",
            "1880 [D loss: 0.619570, acc.: 64.45%] [G loss: 2.488098]\n",
            "1900 [D loss: 0.565344, acc.: 70.31%] [G loss: 2.408888]\n",
            "1920 [D loss: 0.602606, acc.: 65.62%] [G loss: 2.264151]\n",
            "1940 [D loss: 0.560607, acc.: 74.61%] [G loss: 2.541317]\n",
            "1960 [D loss: 0.594870, acc.: 67.97%] [G loss: 2.343794]\n",
            "1980 [D loss: 0.545615, acc.: 75.00%] [G loss: 2.550161]\n",
            "2000 [D loss: 0.649218, acc.: 64.45%] [G loss: 2.386383]\n",
            "2020 [D loss: 0.601816, acc.: 65.62%] [G loss: 2.407127]\n",
            "2040 [D loss: 0.578964, acc.: 68.36%] [G loss: 2.569063]\n",
            "2060 [D loss: 0.552044, acc.: 73.44%] [G loss: 2.613766]\n",
            "2080 [D loss: 0.583226, acc.: 67.58%] [G loss: 2.680823]\n",
            "2100 [D loss: 0.557892, acc.: 73.83%] [G loss: 2.419584]\n",
            "2120 [D loss: 0.623330, acc.: 66.80%] [G loss: 2.414896]\n",
            "2140 [D loss: 0.600863, acc.: 68.36%] [G loss: 2.483502]\n",
            "2160 [D loss: 0.573271, acc.: 69.92%] [G loss: 2.567658]\n",
            "2180 [D loss: 0.563834, acc.: 71.48%] [G loss: 2.751750]\n",
            "2200 [D loss: 0.501681, acc.: 76.95%] [G loss: 2.807117]\n",
            "2220 [D loss: 0.563038, acc.: 70.31%] [G loss: 2.464273]\n",
            "2240 [D loss: 0.607319, acc.: 67.97%] [G loss: 2.679750]\n",
            "2260 [D loss: 0.579980, acc.: 68.36%] [G loss: 2.561005]\n",
            "2280 [D loss: 0.593702, acc.: 68.36%] [G loss: 2.577490]\n",
            "2300 [D loss: 0.602596, acc.: 67.97%] [G loss: 2.480461]\n",
            "2320 [D loss: 0.555197, acc.: 71.88%] [G loss: 2.564956]\n",
            "2340 [D loss: 0.586059, acc.: 69.14%] [G loss: 2.402009]\n",
            "2360 [D loss: 0.590002, acc.: 66.80%] [G loss: 2.455092]\n",
            "2380 [D loss: 0.543935, acc.: 76.95%] [G loss: 2.529557]\n",
            "2400 [D loss: 0.526569, acc.: 75.00%] [G loss: 2.854325]\n",
            "2420 [D loss: 0.564867, acc.: 68.75%] [G loss: 2.744701]\n",
            "2440 [D loss: 0.547459, acc.: 75.39%] [G loss: 2.569819]\n",
            "2460 [D loss: 0.532763, acc.: 73.05%] [G loss: 2.747018]\n",
            "2480 [D loss: 0.582074, acc.: 70.70%] [G loss: 2.654181]\n",
            "2500 [D loss: 0.605676, acc.: 67.58%] [G loss: 2.375607]\n",
            "2520 [D loss: 0.588890, acc.: 71.09%] [G loss: 2.530663]\n",
            "2540 [D loss: 0.579713, acc.: 67.19%] [G loss: 2.709226]\n",
            "2560 [D loss: 0.534081, acc.: 75.39%] [G loss: 2.590774]\n",
            "2580 [D loss: 0.531673, acc.: 74.61%] [G loss: 2.693831]\n",
            "2600 [D loss: 0.568070, acc.: 69.14%] [G loss: 2.429550]\n",
            "2620 [D loss: 0.589758, acc.: 70.31%] [G loss: 2.479932]\n",
            "2640 [D loss: 0.575897, acc.: 72.27%] [G loss: 2.654496]\n",
            "2660 [D loss: 0.498506, acc.: 78.12%] [G loss: 2.999204]\n",
            "2680 [D loss: 0.518623, acc.: 75.39%] [G loss: 2.787318]\n",
            "2700 [D loss: 0.589881, acc.: 69.92%] [G loss: 2.588730]\n",
            "2720 [D loss: 0.523894, acc.: 76.17%] [G loss: 2.596900]\n",
            "2740 [D loss: 0.599535, acc.: 68.75%] [G loss: 2.554358]\n",
            "2760 [D loss: 0.507174, acc.: 76.17%] [G loss: 2.824107]\n",
            "2780 [D loss: 0.547408, acc.: 73.44%] [G loss: 2.700285]\n",
            "2800 [D loss: 0.586928, acc.: 68.36%] [G loss: 2.360365]\n",
            "2820 [D loss: 0.554043, acc.: 70.31%] [G loss: 2.651904]\n",
            "2840 [D loss: 0.532702, acc.: 76.17%] [G loss: 2.687328]\n",
            "2860 [D loss: 0.654155, acc.: 61.72%] [G loss: 2.229639]\n",
            "2880 [D loss: 0.557033, acc.: 67.97%] [G loss: 2.832279]\n",
            "2900 [D loss: 0.539899, acc.: 75.39%] [G loss: 2.750421]\n",
            "2920 [D loss: 0.600267, acc.: 70.31%] [G loss: 2.661646]\n",
            "2940 [D loss: 0.573324, acc.: 71.09%] [G loss: 2.598918]\n",
            "2960 [D loss: 0.483843, acc.: 79.69%] [G loss: 2.797290]\n",
            "2980 [D loss: 0.472635, acc.: 83.59%] [G loss: 2.759961]\n",
            "3000 [D loss: 0.844179, acc.: 32.03%] [G loss: 1.362612]\n",
            "3020 [D loss: 0.895332, acc.: 25.00%] [G loss: 1.427736]\n",
            "3040 [D loss: 0.662036, acc.: 62.11%] [G loss: 1.419513]\n",
            "3060 [D loss: 1.016733, acc.: 27.73%] [G loss: 1.672106]\n",
            "3080 [D loss: 0.734865, acc.: 52.34%] [G loss: 1.684832]\n",
            "3100 [D loss: 0.815483, acc.: 37.11%] [G loss: 1.346917]\n",
            "3120 [D loss: 0.750169, acc.: 48.44%] [G loss: 1.469705]\n",
            "3140 [D loss: 0.768283, acc.: 46.09%] [G loss: 1.564537]\n",
            "3160 [D loss: 0.669147, acc.: 59.38%] [G loss: 1.732254]\n",
            "3180 [D loss: 0.599543, acc.: 69.14%] [G loss: 2.068224]\n",
            "3200 [D loss: 0.644311, acc.: 64.45%] [G loss: 1.880387]\n",
            "3220 [D loss: 0.641760, acc.: 63.67%] [G loss: 1.885971]\n",
            "3240 [D loss: 0.649494, acc.: 63.67%] [G loss: 1.678271]\n",
            "3260 [D loss: 0.557430, acc.: 73.83%] [G loss: 2.159701]\n",
            "3280 [D loss: 0.706188, acc.: 53.91%] [G loss: 1.859170]\n",
            "3300 [D loss: 0.835434, acc.: 42.19%] [G loss: 1.546220]\n",
            "3320 [D loss: 0.595549, acc.: 68.75%] [G loss: 1.521233]\n",
            "3340 [D loss: 0.634937, acc.: 65.62%] [G loss: 2.287824]\n",
            "3360 [D loss: 0.607191, acc.: 71.09%] [G loss: 2.550756]\n",
            "3380 [D loss: 0.627877, acc.: 63.28%] [G loss: 2.079405]\n",
            "3400 [D loss: 0.581786, acc.: 69.92%] [G loss: 2.255072]\n",
            "3420 [D loss: 0.584033, acc.: 70.70%] [G loss: 2.191392]\n",
            "3440 [D loss: 0.632839, acc.: 64.84%] [G loss: 2.009352]\n",
            "3460 [D loss: 0.667421, acc.: 58.20%] [G loss: 2.115933]\n",
            "3480 [D loss: 0.582776, acc.: 72.66%] [G loss: 2.051164]\n",
            "3500 [D loss: 0.684735, acc.: 58.20%] [G loss: 2.044718]\n",
            "3520 [D loss: 0.637057, acc.: 60.94%] [G loss: 2.064557]\n",
            "3540 [D loss: 0.584801, acc.: 68.75%] [G loss: 2.137430]\n",
            "3560 [D loss: 0.598282, acc.: 68.75%] [G loss: 2.174508]\n",
            "3580 [D loss: 0.597415, acc.: 66.80%] [G loss: 2.200212]\n",
            "3600 [D loss: 0.591218, acc.: 68.36%] [G loss: 2.052017]\n",
            "3620 [D loss: 0.618089, acc.: 66.80%] [G loss: 2.169030]\n",
            "3640 [D loss: 0.615663, acc.: 66.02%] [G loss: 2.303552]\n",
            "3660 [D loss: 0.646146, acc.: 60.94%] [G loss: 2.104489]\n",
            "3680 [D loss: 0.611472, acc.: 68.75%] [G loss: 2.116403]\n",
            "3700 [D loss: 0.581839, acc.: 69.92%] [G loss: 2.331722]\n",
            "3720 [D loss: 0.528434, acc.: 76.17%] [G loss: 2.402355]\n",
            "3740 [D loss: 0.591642, acc.: 67.58%] [G loss: 2.208003]\n",
            "3760 [D loss: 0.591944, acc.: 69.53%] [G loss: 2.179625]\n",
            "3780 [D loss: 0.624427, acc.: 66.02%] [G loss: 2.225291]\n",
            "3800 [D loss: 0.602553, acc.: 66.41%] [G loss: 2.158654]\n",
            "3820 [D loss: 0.594457, acc.: 70.31%] [G loss: 2.073400]\n",
            "3840 [D loss: 0.621703, acc.: 65.62%] [G loss: 2.114734]\n",
            "3860 [D loss: 0.590384, acc.: 69.14%] [G loss: 2.217856]\n",
            "3880 [D loss: 0.626055, acc.: 66.02%] [G loss: 2.182703]\n",
            "3900 [D loss: 0.644977, acc.: 61.33%] [G loss: 2.153638]\n",
            "3920 [D loss: 0.571250, acc.: 71.48%] [G loss: 2.310712]\n",
            "3940 [D loss: 0.670638, acc.: 58.20%] [G loss: 2.135060]\n",
            "3960 [D loss: 0.647877, acc.: 64.45%] [G loss: 1.987211]\n",
            "3980 [D loss: 0.545104, acc.: 72.27%] [G loss: 2.060950]\n",
            "4000 [D loss: 0.385751, acc.: 89.06%] [G loss: 1.761025]\n",
            "4020 [D loss: 0.485367, acc.: 82.03%] [G loss: 1.407120]\n",
            "4040 [D loss: 0.691815, acc.: 58.59%] [G loss: 2.240568]\n",
            "4060 [D loss: 1.035881, acc.: 37.11%] [G loss: 2.082132]\n",
            "4080 [D loss: 0.920064, acc.: 40.23%] [G loss: 2.153923]\n",
            "4100 [D loss: 0.687043, acc.: 56.25%] [G loss: 1.704352]\n",
            "4120 [D loss: 0.580117, acc.: 71.09%] [G loss: 1.927446]\n",
            "4140 [D loss: 0.910431, acc.: 36.33%] [G loss: 2.233001]\n",
            "4160 [D loss: 0.645560, acc.: 60.94%] [G loss: 2.303790]\n",
            "4180 [D loss: 0.505659, acc.: 77.73%] [G loss: 2.678491]\n",
            "4200 [D loss: 0.858624, acc.: 41.80%] [G loss: 2.205399]\n",
            "4220 [D loss: 0.672721, acc.: 62.11%] [G loss: 2.406301]\n",
            "4240 [D loss: 0.638785, acc.: 63.28%] [G loss: 2.191468]\n",
            "4260 [D loss: 0.687410, acc.: 56.64%] [G loss: 2.095522]\n",
            "4280 [D loss: 0.529299, acc.: 74.61%] [G loss: 2.541818]\n",
            "4300 [D loss: 0.690276, acc.: 58.59%] [G loss: 2.136585]\n",
            "4320 [D loss: 0.611876, acc.: 63.67%] [G loss: 2.203951]\n",
            "4340 [D loss: 0.659494, acc.: 60.94%] [G loss: 2.194525]\n",
            "4360 [D loss: 0.628017, acc.: 64.84%] [G loss: 2.024955]\n",
            "4380 [D loss: 0.596232, acc.: 66.41%] [G loss: 2.167684]\n",
            "4400 [D loss: 0.405341, acc.: 87.50%] [G loss: 1.556829]\n",
            "4420 [D loss: 0.549244, acc.: 75.00%] [G loss: 1.530879]\n",
            "4440 [D loss: 0.453795, acc.: 86.33%] [G loss: 1.343086]\n",
            "4460 [D loss: 0.938698, acc.: 37.50%] [G loss: 2.331407]\n",
            "4480 [D loss: 0.557533, acc.: 71.88%] [G loss: 2.556187]\n",
            "4500 [D loss: 0.688217, acc.: 58.20%] [G loss: 2.185463]\n",
            "4520 [D loss: 0.513152, acc.: 73.44%] [G loss: 2.293645]\n",
            "4540 [D loss: 0.562863, acc.: 74.22%] [G loss: 2.453300]\n",
            "4560 [D loss: 0.666696, acc.: 61.72%] [G loss: 2.126927]\n",
            "4580 [D loss: 0.662472, acc.: 60.16%] [G loss: 2.231983]\n",
            "4600 [D loss: 0.670400, acc.: 58.98%] [G loss: 2.112002]\n",
            "4620 [D loss: 0.607044, acc.: 65.62%] [G loss: 2.157291]\n",
            "4640 [D loss: 0.613036, acc.: 64.06%] [G loss: 2.339340]\n",
            "4660 [D loss: 0.617926, acc.: 71.09%] [G loss: 2.304879]\n",
            "4680 [D loss: 0.590359, acc.: 69.14%] [G loss: 2.190791]\n",
            "4700 [D loss: 0.562153, acc.: 69.92%] [G loss: 2.209206]\n",
            "4720 [D loss: 0.548806, acc.: 74.61%] [G loss: 2.249993]\n",
            "4740 [D loss: 0.636045, acc.: 63.28%] [G loss: 2.223327]\n",
            "4760 [D loss: 0.582083, acc.: 69.53%] [G loss: 2.239736]\n",
            "4780 [D loss: 0.655515, acc.: 60.94%] [G loss: 2.142629]\n",
            "4800 [D loss: 0.585725, acc.: 69.92%] [G loss: 2.527981]\n",
            "4820 [D loss: 0.638438, acc.: 64.06%] [G loss: 2.115685]\n",
            "4840 [D loss: 0.708185, acc.: 55.47%] [G loss: 2.073520]\n",
            "4860 [D loss: 0.583254, acc.: 67.97%] [G loss: 2.480268]\n",
            "4880 [D loss: 0.595323, acc.: 66.80%] [G loss: 2.343763]\n",
            "4900 [D loss: 0.614750, acc.: 68.36%] [G loss: 2.132516]\n",
            "4920 [D loss: 0.569616, acc.: 71.88%] [G loss: 2.281167]\n",
            "4940 [D loss: 0.573719, acc.: 76.17%] [G loss: 2.251280]\n",
            "4960 [D loss: 0.590547, acc.: 66.80%] [G loss: 2.260079]\n",
            "4980 [D loss: 0.580258, acc.: 67.97%] [G loss: 2.344316]\n",
            "5000 [D loss: 0.623125, acc.: 63.67%] [G loss: 2.216895]\n",
            "5020 [D loss: 0.632179, acc.: 61.33%] [G loss: 2.274957]\n",
            "5040 [D loss: 0.609210, acc.: 64.06%] [G loss: 2.338047]\n",
            "5060 [D loss: 0.608680, acc.: 69.92%] [G loss: 2.067615]\n",
            "5080 [D loss: 0.486513, acc.: 81.64%] [G loss: 2.758275]\n",
            "5100 [D loss: 0.614328, acc.: 66.41%] [G loss: 1.948357]\n",
            "5120 [D loss: 1.258740, acc.: 30.47%] [G loss: 2.784147]\n",
            "5140 [D loss: 0.690797, acc.: 59.77%] [G loss: 2.909184]\n",
            "5160 [D loss: 0.645351, acc.: 60.55%] [G loss: 2.267882]\n",
            "5180 [D loss: 0.614727, acc.: 67.58%] [G loss: 2.341908]\n",
            "5200 [D loss: 0.576638, acc.: 67.58%] [G loss: 2.390060]\n",
            "5220 [D loss: 0.666231, acc.: 63.28%] [G loss: 2.187119]\n",
            "5240 [D loss: 0.609157, acc.: 69.14%] [G loss: 2.362589]\n",
            "5260 [D loss: 0.612204, acc.: 64.45%] [G loss: 2.368978]\n",
            "5280 [D loss: 0.585379, acc.: 71.09%] [G loss: 2.450591]\n",
            "5300 [D loss: 0.680909, acc.: 59.38%] [G loss: 2.202804]\n",
            "5320 [D loss: 0.580723, acc.: 67.97%] [G loss: 2.461041]\n",
            "5340 [D loss: 0.589095, acc.: 69.92%] [G loss: 2.247817]\n",
            "5360 [D loss: 0.661182, acc.: 57.42%] [G loss: 2.242820]\n",
            "5380 [D loss: 0.601903, acc.: 67.97%] [G loss: 2.311461]\n",
            "5400 [D loss: 0.636216, acc.: 60.16%] [G loss: 2.315403]\n",
            "5420 [D loss: 0.566296, acc.: 71.48%] [G loss: 2.532771]\n",
            "5440 [D loss: 0.641146, acc.: 64.06%] [G loss: 2.238838]\n",
            "5460 [D loss: 0.571483, acc.: 70.70%] [G loss: 2.429476]\n",
            "5480 [D loss: 0.610448, acc.: 66.02%] [G loss: 2.254882]\n",
            "5500 [D loss: 0.613195, acc.: 66.41%] [G loss: 2.334185]\n",
            "5520 [D loss: 0.587563, acc.: 67.19%] [G loss: 2.355899]\n",
            "5540 [D loss: 0.600605, acc.: 63.67%] [G loss: 2.284257]\n",
            "5560 [D loss: 0.670631, acc.: 58.20%] [G loss: 2.248077]\n",
            "5580 [D loss: 0.608765, acc.: 66.02%] [G loss: 2.448338]\n",
            "5600 [D loss: 0.588206, acc.: 68.36%] [G loss: 2.415442]\n",
            "5620 [D loss: 0.554634, acc.: 73.05%] [G loss: 2.380805]\n",
            "5640 [D loss: 0.584373, acc.: 67.58%] [G loss: 2.461569]\n",
            "5660 [D loss: 0.665178, acc.: 61.33%] [G loss: 2.225609]\n",
            "5680 [D loss: 0.610974, acc.: 63.67%] [G loss: 2.397381]\n",
            "5700 [D loss: 0.563268, acc.: 71.88%] [G loss: 2.549626]\n",
            "5720 [D loss: 0.616811, acc.: 63.67%] [G loss: 2.303772]\n",
            "5740 [D loss: 0.646856, acc.: 60.16%] [G loss: 2.139450]\n",
            "5760 [D loss: 0.562712, acc.: 72.66%] [G loss: 2.475266]\n",
            "5780 [D loss: 0.648400, acc.: 64.84%] [G loss: 2.239103]\n",
            "5800 [D loss: 0.610598, acc.: 66.41%] [G loss: 2.426995]\n",
            "5820 [D loss: 0.606543, acc.: 67.19%] [G loss: 2.421032]\n",
            "5840 [D loss: 0.586842, acc.: 70.31%] [G loss: 2.468158]\n",
            "5860 [D loss: 0.609765, acc.: 67.19%] [G loss: 2.289063]\n",
            "5880 [D loss: 0.590576, acc.: 68.75%] [G loss: 2.478488]\n",
            "5900 [D loss: 0.619220, acc.: 66.02%] [G loss: 2.302969]\n",
            "5920 [D loss: 0.602658, acc.: 67.97%] [G loss: 2.314465]\n",
            "5940 [D loss: 0.649925, acc.: 63.67%] [G loss: 2.200501]\n",
            "5960 [D loss: 0.549291, acc.: 69.14%] [G loss: 2.555748]\n",
            "5980 [D loss: 0.543462, acc.: 73.83%] [G loss: 2.626265]\n",
            "6000 [D loss: 0.577151, acc.: 71.48%] [G loss: 2.511072]\n",
            "6020 [D loss: 0.600652, acc.: 67.97%] [G loss: 2.558094]\n",
            "6040 [D loss: 0.604544, acc.: 68.36%] [G loss: 2.450074]\n",
            "6060 [D loss: 0.590967, acc.: 67.58%] [G loss: 2.547176]\n",
            "6080 [D loss: 0.611737, acc.: 65.23%] [G loss: 2.338952]\n",
            "6100 [D loss: 0.583991, acc.: 66.80%] [G loss: 2.633641]\n",
            "6120 [D loss: 0.617659, acc.: 63.67%] [G loss: 2.340277]\n",
            "6140 [D loss: 0.561788, acc.: 69.92%] [G loss: 2.122804]\n",
            "6160 [D loss: 0.632869, acc.: 65.23%] [G loss: 2.469278]\n",
            "6180 [D loss: 0.611901, acc.: 66.02%] [G loss: 2.557575]\n",
            "6200 [D loss: 0.765316, acc.: 53.91%] [G loss: 2.169783]\n",
            "6220 [D loss: 0.598500, acc.: 64.84%] [G loss: 2.382632]\n",
            "6240 [D loss: 0.658656, acc.: 64.84%] [G loss: 2.331552]\n",
            "6260 [D loss: 0.630495, acc.: 60.94%] [G loss: 2.454536]\n",
            "6280 [D loss: 0.583799, acc.: 69.53%] [G loss: 2.528316]\n",
            "6300 [D loss: 0.642257, acc.: 63.67%] [G loss: 2.462571]\n",
            "6320 [D loss: 0.522588, acc.: 73.05%] [G loss: 2.767188]\n",
            "6340 [D loss: 0.584220, acc.: 67.58%] [G loss: 2.421348]\n",
            "6360 [D loss: 0.558973, acc.: 71.48%] [G loss: 2.432091]\n",
            "6380 [D loss: 0.610856, acc.: 66.80%] [G loss: 2.374302]\n",
            "6400 [D loss: 0.601688, acc.: 67.58%] [G loss: 2.586357]\n",
            "6420 [D loss: 0.554762, acc.: 72.66%] [G loss: 2.550215]\n",
            "6440 [D loss: 0.622367, acc.: 62.89%] [G loss: 2.345274]\n",
            "6460 [D loss: 0.512874, acc.: 78.52%] [G loss: 2.698363]\n",
            "6480 [D loss: 0.591943, acc.: 70.31%] [G loss: 2.656558]\n",
            "6500 [D loss: 0.573187, acc.: 68.75%] [G loss: 2.592165]\n",
            "6520 [D loss: 0.616345, acc.: 65.62%] [G loss: 2.501428]\n",
            "6540 [D loss: 0.593207, acc.: 69.53%] [G loss: 2.404250]\n",
            "6560 [D loss: 0.572635, acc.: 71.88%] [G loss: 2.586968]\n",
            "6580 [D loss: 0.561718, acc.: 71.09%] [G loss: 2.423415]\n",
            "6600 [D loss: 0.592263, acc.: 70.70%] [G loss: 2.492227]\n",
            "6620 [D loss: 0.531728, acc.: 74.22%] [G loss: 2.736609]\n",
            "6640 [D loss: 0.564427, acc.: 69.92%] [G loss: 2.559254]\n",
            "6660 [D loss: 0.728768, acc.: 54.30%] [G loss: 2.010824]\n",
            "6680 [D loss: 0.488609, acc.: 75.39%] [G loss: 2.720704]\n",
            "6700 [D loss: 0.575703, acc.: 71.09%] [G loss: 2.600320]\n",
            "6720 [D loss: 0.608988, acc.: 67.58%] [G loss: 2.853380]\n",
            "6740 [D loss: 0.577837, acc.: 68.75%] [G loss: 2.764628]\n",
            "6760 [D loss: 0.610780, acc.: 65.23%] [G loss: 2.493033]\n",
            "6780 [D loss: 0.538530, acc.: 72.27%] [G loss: 2.699120]\n",
            "6800 [D loss: 0.557713, acc.: 70.70%] [G loss: 2.521058]\n",
            "6820 [D loss: 0.583527, acc.: 68.36%] [G loss: 2.534425]\n",
            "6840 [D loss: 0.525453, acc.: 74.61%] [G loss: 2.585474]\n",
            "6860 [D loss: 0.572684, acc.: 69.14%] [G loss: 2.597283]\n",
            "6880 [D loss: 0.613733, acc.: 67.58%] [G loss: 2.461730]\n",
            "6900 [D loss: 0.546967, acc.: 71.09%] [G loss: 2.646467]\n",
            "6920 [D loss: 0.611874, acc.: 66.41%] [G loss: 2.462930]\n",
            "6940 [D loss: 0.590833, acc.: 69.14%] [G loss: 2.668380]\n",
            "6960 [D loss: 0.590513, acc.: 69.14%] [G loss: 2.553640]\n",
            "6980 [D loss: 0.583205, acc.: 73.83%] [G loss: 2.531401]\n",
            "7000 [D loss: 0.566315, acc.: 71.48%] [G loss: 2.455915]\n",
            "7020 [D loss: 0.585426, acc.: 71.48%] [G loss: 2.500695]\n",
            "7040 [D loss: 0.625566, acc.: 68.36%] [G loss: 2.442598]\n",
            "7060 [D loss: 0.614415, acc.: 66.41%] [G loss: 2.495522]\n",
            "7080 [D loss: 0.527876, acc.: 70.70%] [G loss: 2.696505]\n",
            "7100 [D loss: 0.603677, acc.: 66.02%] [G loss: 2.430251]\n",
            "7120 [D loss: 0.575559, acc.: 70.70%] [G loss: 2.701457]\n",
            "7140 [D loss: 0.616533, acc.: 67.58%] [G loss: 2.777336]\n",
            "7160 [D loss: 0.530347, acc.: 72.66%] [G loss: 2.688187]\n",
            "7180 [D loss: 0.125725, acc.: 99.61%] [G loss: 2.305769]\n",
            "7200 [D loss: 0.149554, acc.: 98.83%] [G loss: 2.213598]\n",
            "7220 [D loss: 0.330447, acc.: 89.06%] [G loss: 1.754806]\n",
            "7240 [D loss: 1.232676, acc.: 24.61%] [G loss: 1.226170]\n",
            "7260 [D loss: 0.602565, acc.: 70.70%] [G loss: 1.503689]\n",
            "7280 [D loss: 1.506519, acc.: 21.88%] [G loss: 2.153012]\n",
            "7300 [D loss: 0.609816, acc.: 69.92%] [G loss: 1.933721]\n",
            "7320 [D loss: 0.943017, acc.: 47.27%] [G loss: 2.482183]\n",
            "7340 [D loss: 0.901816, acc.: 44.53%] [G loss: 2.200723]\n",
            "7360 [D loss: 0.796268, acc.: 50.00%] [G loss: 2.427839]\n",
            "7380 [D loss: 0.703589, acc.: 56.25%] [G loss: 2.317981]\n",
            "7400 [D loss: 0.773202, acc.: 51.95%] [G loss: 2.326449]\n",
            "7420 [D loss: 0.678556, acc.: 57.81%] [G loss: 2.192839]\n",
            "7440 [D loss: 0.712142, acc.: 56.25%] [G loss: 2.121468]\n",
            "7460 [D loss: 0.605576, acc.: 64.84%] [G loss: 2.132767]\n",
            "7480 [D loss: 0.732111, acc.: 51.56%] [G loss: 2.031737]\n",
            "7500 [D loss: 0.578592, acc.: 71.88%] [G loss: 2.194351]\n",
            "7520 [D loss: 0.653542, acc.: 59.38%] [G loss: 2.127630]\n",
            "7540 [D loss: 0.520927, acc.: 77.34%] [G loss: 2.337496]\n",
            "7560 [D loss: 0.578706, acc.: 68.36%] [G loss: 2.120021]\n",
            "7580 [D loss: 0.569963, acc.: 71.48%] [G loss: 2.186131]\n",
            "7600 [D loss: 0.593395, acc.: 67.58%] [G loss: 2.177555]\n",
            "7620 [D loss: 0.587510, acc.: 68.36%] [G loss: 2.254149]\n",
            "7640 [D loss: 0.630804, acc.: 66.41%] [G loss: 2.140789]\n",
            "7660 [D loss: 0.569827, acc.: 72.27%] [G loss: 2.255735]\n",
            "7680 [D loss: 0.626652, acc.: 64.84%] [G loss: 2.048166]\n",
            "7700 [D loss: 0.603022, acc.: 66.80%] [G loss: 2.280983]\n",
            "7720 [D loss: 0.636822, acc.: 64.84%] [G loss: 2.113893]\n",
            "7740 [D loss: 0.610505, acc.: 65.23%] [G loss: 2.172444]\n",
            "7760 [D loss: 0.612459, acc.: 67.58%] [G loss: 2.172188]\n",
            "7780 [D loss: 0.648031, acc.: 60.16%] [G loss: 2.022537]\n",
            "7800 [D loss: 0.597644, acc.: 67.19%] [G loss: 2.234333]\n",
            "7820 [D loss: 0.672555, acc.: 58.20%] [G loss: 2.098722]\n",
            "7840 [D loss: 0.565943, acc.: 71.09%] [G loss: 2.554378]\n",
            "7860 [D loss: 0.657873, acc.: 61.33%] [G loss: 2.192026]\n",
            "7880 [D loss: 0.549372, acc.: 73.44%] [G loss: 2.443873]\n",
            "7900 [D loss: 0.664810, acc.: 60.55%] [G loss: 2.155454]\n",
            "7920 [D loss: 0.546899, acc.: 76.17%] [G loss: 2.311885]\n",
            "7940 [D loss: 0.586302, acc.: 71.09%] [G loss: 2.351793]\n",
            "7960 [D loss: 0.633549, acc.: 66.80%] [G loss: 2.239283]\n",
            "7980 [D loss: 0.596028, acc.: 66.41%] [G loss: 2.381354]\n",
            "8000 [D loss: 0.631549, acc.: 64.06%] [G loss: 2.264802]\n",
            "8020 [D loss: 0.552243, acc.: 74.61%] [G loss: 2.339262]\n",
            "8040 [D loss: 0.634039, acc.: 64.84%] [G loss: 2.179633]\n",
            "8060 [D loss: 0.558139, acc.: 73.44%] [G loss: 2.556776]\n",
            "8080 [D loss: 0.579339, acc.: 69.14%] [G loss: 2.392216]\n",
            "8100 [D loss: 0.607492, acc.: 69.14%] [G loss: 2.331966]\n",
            "8120 [D loss: 0.591889, acc.: 70.70%] [G loss: 2.283714]\n",
            "8140 [D loss: 0.567418, acc.: 73.05%] [G loss: 2.374310]\n",
            "8160 [D loss: 0.531264, acc.: 76.95%] [G loss: 2.520020]\n",
            "8180 [D loss: 0.538172, acc.: 76.56%] [G loss: 2.481438]\n",
            "8200 [D loss: 0.617713, acc.: 66.80%] [G loss: 2.480845]\n",
            "8220 [D loss: 0.604068, acc.: 68.36%] [G loss: 2.425726]\n",
            "8240 [D loss: 0.576020, acc.: 71.88%] [G loss: 2.418057]\n",
            "8260 [D loss: 0.622489, acc.: 64.06%] [G loss: 2.563910]\n",
            "8280 [D loss: 0.597824, acc.: 68.36%] [G loss: 2.437211]\n",
            "8300 [D loss: 0.556550, acc.: 75.78%] [G loss: 2.633302]\n",
            "8320 [D loss: 0.628459, acc.: 66.02%] [G loss: 2.334085]\n",
            "8340 [D loss: 0.581538, acc.: 71.48%] [G loss: 2.524805]\n",
            "8360 [D loss: 0.625087, acc.: 65.62%] [G loss: 2.340956]\n",
            "8380 [D loss: 0.549393, acc.: 73.44%] [G loss: 2.429310]\n",
            "8400 [D loss: 0.611268, acc.: 63.28%] [G loss: 2.386619]\n",
            "8420 [D loss: 0.593855, acc.: 67.58%] [G loss: 2.405827]\n",
            "8440 [D loss: 0.473601, acc.: 80.86%] [G loss: 2.116920]\n",
            "8460 [D loss: 0.644537, acc.: 62.11%] [G loss: 1.508561]\n",
            "8480 [D loss: 0.568649, acc.: 72.27%] [G loss: 1.453066]\n",
            "8500 [D loss: 0.778437, acc.: 50.00%] [G loss: 2.915257]\n",
            "8520 [D loss: 0.840643, acc.: 42.19%] [G loss: 2.596273]\n",
            "8540 [D loss: 0.773467, acc.: 52.73%] [G loss: 2.459512]\n",
            "8560 [D loss: 0.626904, acc.: 64.84%] [G loss: 2.303083]\n",
            "8580 [D loss: 0.546559, acc.: 71.88%] [G loss: 2.248038]\n",
            "8600 [D loss: 0.880673, acc.: 43.75%] [G loss: 2.364298]\n",
            "8620 [D loss: 0.777854, acc.: 52.73%] [G loss: 2.355046]\n",
            "8640 [D loss: 0.553283, acc.: 73.83%] [G loss: 2.763361]\n",
            "8660 [D loss: 0.587528, acc.: 69.53%] [G loss: 2.571974]\n",
            "8680 [D loss: 0.583288, acc.: 70.70%] [G loss: 2.755317]\n",
            "8700 [D loss: 0.631544, acc.: 62.50%] [G loss: 2.462557]\n",
            "8720 [D loss: 0.596197, acc.: 65.23%] [G loss: 2.614049]\n",
            "8740 [D loss: 0.541876, acc.: 74.61%] [G loss: 2.842533]\n",
            "8760 [D loss: 0.634236, acc.: 62.50%] [G loss: 2.487120]\n",
            "8780 [D loss: 0.604166, acc.: 68.75%] [G loss: 2.452475]\n",
            "8800 [D loss: 0.584513, acc.: 71.88%] [G loss: 2.413385]\n",
            "8820 [D loss: 0.623078, acc.: 66.02%] [G loss: 2.378094]\n",
            "8840 [D loss: 0.612501, acc.: 67.19%] [G loss: 2.513036]\n",
            "8860 [D loss: 0.531442, acc.: 76.56%] [G loss: 2.624340]\n",
            "8880 [D loss: 0.501936, acc.: 77.73%] [G loss: 2.963798]\n",
            "8900 [D loss: 0.547483, acc.: 71.09%] [G loss: 2.661099]\n",
            "8920 [D loss: 0.481193, acc.: 79.30%] [G loss: 2.840958]\n",
            "8940 [D loss: 0.599458, acc.: 67.97%] [G loss: 2.453791]\n",
            "8960 [D loss: 0.571321, acc.: 69.92%] [G loss: 2.669162]\n",
            "8980 [D loss: 0.537047, acc.: 73.83%] [G loss: 2.755094]\n",
            "9000 [D loss: 0.523930, acc.: 76.56%] [G loss: 2.737601]\n",
            "9020 [D loss: 0.559601, acc.: 71.88%] [G loss: 2.842171]\n",
            "9040 [D loss: 0.593704, acc.: 66.02%] [G loss: 2.811663]\n",
            "9060 [D loss: 0.560645, acc.: 70.70%] [G loss: 2.790774]\n",
            "9080 [D loss: 0.588492, acc.: 68.75%] [G loss: 2.569914]\n",
            "9100 [D loss: 0.534450, acc.: 75.39%] [G loss: 2.798273]\n",
            "9120 [D loss: 0.583081, acc.: 69.53%] [G loss: 2.907620]\n",
            "9140 [D loss: 0.535976, acc.: 74.22%] [G loss: 2.721373]\n",
            "9160 [D loss: 0.561907, acc.: 74.22%] [G loss: 2.584458]\n",
            "9180 [D loss: 0.494952, acc.: 78.91%] [G loss: 2.016740]\n",
            "9200 [D loss: 0.665908, acc.: 60.55%] [G loss: 1.678733]\n",
            "9220 [D loss: 0.316971, acc.: 92.58%] [G loss: 1.899709]\n",
            "9240 [D loss: 1.060054, acc.: 33.98%] [G loss: 1.718851]\n",
            "9260 [D loss: 0.579727, acc.: 69.53%] [G loss: 2.027503]\n",
            "9280 [D loss: 0.594768, acc.: 66.80%] [G loss: 1.672282]\n",
            "9300 [D loss: 0.617245, acc.: 67.58%] [G loss: 2.646905]\n",
            "9320 [D loss: 0.590204, acc.: 67.58%] [G loss: 2.794707]\n",
            "9340 [D loss: 0.570914, acc.: 68.75%] [G loss: 2.778679]\n",
            "9360 [D loss: 0.566306, acc.: 66.41%] [G loss: 3.038418]\n",
            "9380 [D loss: 0.667049, acc.: 60.16%] [G loss: 2.840028]\n",
            "9400 [D loss: 0.599457, acc.: 68.75%] [G loss: 2.628131]\n",
            "9420 [D loss: 0.500576, acc.: 75.39%] [G loss: 2.725830]\n",
            "9440 [D loss: 0.640121, acc.: 64.06%] [G loss: 2.637993]\n",
            "9460 [D loss: 0.554466, acc.: 68.75%] [G loss: 2.965539]\n",
            "9480 [D loss: 0.513783, acc.: 75.00%] [G loss: 2.777637]\n",
            "9500 [D loss: 0.541167, acc.: 72.27%] [G loss: 2.964225]\n",
            "9520 [D loss: 0.502788, acc.: 75.78%] [G loss: 2.732509]\n",
            "9540 [D loss: 0.582022, acc.: 72.66%] [G loss: 2.607302]\n",
            "9560 [D loss: 0.534634, acc.: 73.05%] [G loss: 2.847729]\n",
            "9580 [D loss: 0.551710, acc.: 70.70%] [G loss: 2.693497]\n",
            "9600 [D loss: 0.527683, acc.: 71.09%] [G loss: 2.914474]\n",
            "9620 [D loss: 0.520689, acc.: 73.44%] [G loss: 2.916338]\n",
            "9640 [D loss: 0.602874, acc.: 68.36%] [G loss: 2.807617]\n",
            "9660 [D loss: 0.524881, acc.: 75.78%] [G loss: 3.108367]\n",
            "9680 [D loss: 0.542090, acc.: 70.31%] [G loss: 2.805994]\n",
            "9700 [D loss: 0.542044, acc.: 75.00%] [G loss: 2.887167]\n",
            "9720 [D loss: 0.574745, acc.: 69.14%] [G loss: 2.817844]\n",
            "9740 [D loss: 0.574983, acc.: 70.31%] [G loss: 2.746022]\n",
            "9760 [D loss: 0.534818, acc.: 75.39%] [G loss: 3.108397]\n",
            "9780 [D loss: 0.522710, acc.: 72.66%] [G loss: 2.997902]\n",
            "9800 [D loss: 0.649373, acc.: 62.11%] [G loss: 2.708778]\n",
            "9820 [D loss: 0.506235, acc.: 76.17%] [G loss: 3.283512]\n",
            "9840 [D loss: 0.541675, acc.: 71.09%] [G loss: 2.889417]\n",
            "9860 [D loss: 0.543622, acc.: 72.66%] [G loss: 3.086806]\n",
            "9880 [D loss: 0.601185, acc.: 67.19%] [G loss: 2.896232]\n",
            "9900 [D loss: 0.528452, acc.: 75.39%] [G loss: 2.951954]\n",
            "9920 [D loss: 0.449687, acc.: 78.12%] [G loss: 3.311049]\n",
            "9940 [D loss: 0.573350, acc.: 69.92%] [G loss: 2.944811]\n",
            "9960 [D loss: 0.511535, acc.: 73.83%] [G loss: 3.066134]\n",
            "9980 [D loss: 0.637063, acc.: 61.72%] [G loss: 2.764073]\n",
            "10000 [D loss: 0.470885, acc.: 79.69%] [G loss: 3.072285]\n",
            "10020 [D loss: 0.511261, acc.: 74.61%] [G loss: 3.218085]\n",
            "10040 [D loss: 0.543821, acc.: 72.66%] [G loss: 2.884133]\n",
            "10060 [D loss: 0.573028, acc.: 69.14%] [G loss: 2.949024]\n",
            "10080 [D loss: 0.541000, acc.: 71.48%] [G loss: 2.931597]\n",
            "10100 [D loss: 0.590680, acc.: 65.23%] [G loss: 2.667783]\n",
            "10120 [D loss: 0.519402, acc.: 76.56%] [G loss: 2.982893]\n",
            "10140 [D loss: 0.496021, acc.: 76.56%] [G loss: 3.066095]\n",
            "10160 [D loss: 0.542222, acc.: 72.27%] [G loss: 3.192456]\n",
            "10180 [D loss: 0.532714, acc.: 72.27%] [G loss: 3.003022]\n",
            "10200 [D loss: 0.574147, acc.: 67.58%] [G loss: 2.903954]\n",
            "10220 [D loss: 0.516816, acc.: 72.66%] [G loss: 3.017673]\n",
            "10240 [D loss: 0.637434, acc.: 62.11%] [G loss: 2.847299]\n",
            "10260 [D loss: 0.487274, acc.: 75.78%] [G loss: 2.963725]\n",
            "10280 [D loss: 0.531185, acc.: 75.78%] [G loss: 3.007076]\n",
            "10300 [D loss: 0.541145, acc.: 72.27%] [G loss: 3.041999]\n",
            "10320 [D loss: 0.538357, acc.: 71.48%] [G loss: 3.067235]\n",
            "10340 [D loss: 0.597442, acc.: 65.23%] [G loss: 3.018599]\n",
            "10360 [D loss: 0.546687, acc.: 72.66%] [G loss: 3.116367]\n",
            "10380 [D loss: 0.544790, acc.: 73.83%] [G loss: 3.073342]\n",
            "10400 [D loss: 0.636544, acc.: 63.67%] [G loss: 2.833164]\n",
            "10420 [D loss: 0.479133, acc.: 79.69%] [G loss: 2.967975]\n",
            "10440 [D loss: 0.414247, acc.: 85.16%] [G loss: 1.772945]\n",
            "10460 [D loss: 0.582529, acc.: 70.70%] [G loss: 1.918154]\n",
            "10480 [D loss: 0.624521, acc.: 63.67%] [G loss: 2.100931]\n",
            "10500 [D loss: 1.330706, acc.: 26.56%] [G loss: 1.976039]\n",
            "10520 [D loss: 0.820222, acc.: 49.22%] [G loss: 3.461841]\n",
            "10540 [D loss: 0.659510, acc.: 62.89%] [G loss: 2.738092]\n",
            "10560 [D loss: 0.642513, acc.: 65.23%] [G loss: 3.264841]\n",
            "10580 [D loss: 0.805860, acc.: 48.05%] [G loss: 2.513907]\n",
            "10600 [D loss: 0.673189, acc.: 60.94%] [G loss: 2.656070]\n",
            "10620 [D loss: 0.451275, acc.: 81.64%] [G loss: 3.497855]\n",
            "10640 [D loss: 0.603780, acc.: 66.02%] [G loss: 3.099943]\n",
            "10660 [D loss: 0.432180, acc.: 81.25%] [G loss: 3.396376]\n",
            "10680 [D loss: 0.630772, acc.: 64.45%] [G loss: 3.189546]\n",
            "10700 [D loss: 0.511980, acc.: 76.56%] [G loss: 3.040618]\n",
            "10720 [D loss: 0.574498, acc.: 69.92%] [G loss: 3.141856]\n",
            "10740 [D loss: 0.543129, acc.: 71.88%] [G loss: 2.968035]\n",
            "10760 [D loss: 0.577738, acc.: 68.75%] [G loss: 3.255183]\n",
            "10780 [D loss: 0.508049, acc.: 75.00%] [G loss: 3.226800]\n",
            "10800 [D loss: 0.502627, acc.: 75.39%] [G loss: 3.240125]\n",
            "10820 [D loss: 0.583853, acc.: 64.84%] [G loss: 2.798474]\n",
            "10840 [D loss: 0.505757, acc.: 73.83%] [G loss: 2.897751]\n",
            "10860 [D loss: 0.610487, acc.: 63.28%] [G loss: 3.027667]\n",
            "10880 [D loss: 0.516162, acc.: 75.39%] [G loss: 3.199420]\n",
            "10900 [D loss: 0.474336, acc.: 77.34%] [G loss: 3.481690]\n",
            "10920 [D loss: 0.488870, acc.: 75.78%] [G loss: 3.305444]\n",
            "10940 [D loss: 0.474649, acc.: 79.69%] [G loss: 3.340988]\n",
            "10960 [D loss: 0.605079, acc.: 69.92%] [G loss: 3.105086]\n",
            "10980 [D loss: 0.530690, acc.: 75.78%] [G loss: 3.012115]\n",
            "11000 [D loss: 0.589034, acc.: 71.48%] [G loss: 2.986004]\n",
            "11020 [D loss: 0.478614, acc.: 78.52%] [G loss: 3.330048]\n",
            "11040 [D loss: 0.513709, acc.: 76.17%] [G loss: 3.543699]\n",
            "11060 [D loss: 0.539447, acc.: 71.88%] [G loss: 3.102354]\n",
            "11080 [D loss: 0.507051, acc.: 74.22%] [G loss: 3.588733]\n",
            "11100 [D loss: 0.571164, acc.: 70.31%] [G loss: 3.280488]\n",
            "11120 [D loss: 0.522609, acc.: 73.83%] [G loss: 3.066967]\n",
            "11140 [D loss: 0.585154, acc.: 69.53%] [G loss: 2.952328]\n",
            "11160 [D loss: 0.583010, acc.: 71.48%] [G loss: 3.303550]\n",
            "11180 [D loss: 0.502822, acc.: 76.95%] [G loss: 3.249533]\n",
            "11200 [D loss: 0.504798, acc.: 76.17%] [G loss: 3.164426]\n",
            "11220 [D loss: 0.518395, acc.: 72.27%] [G loss: 3.454750]\n",
            "11240 [D loss: 0.560283, acc.: 72.27%] [G loss: 3.169283]\n",
            "11260 [D loss: 0.570113, acc.: 66.80%] [G loss: 3.156564]\n",
            "11280 [D loss: 0.494347, acc.: 76.95%] [G loss: 3.245133]\n",
            "11300 [D loss: 0.524554, acc.: 73.44%] [G loss: 3.179125]\n",
            "11320 [D loss: 0.563348, acc.: 69.53%] [G loss: 3.328102]\n",
            "11340 [D loss: 0.521858, acc.: 73.05%] [G loss: 3.186270]\n",
            "11360 [D loss: 0.496525, acc.: 76.17%] [G loss: 3.241667]\n",
            "11380 [D loss: 0.546714, acc.: 73.05%] [G loss: 3.148981]\n",
            "11400 [D loss: 0.507362, acc.: 75.00%] [G loss: 3.502708]\n",
            "11420 [D loss: 0.487700, acc.: 77.73%] [G loss: 3.373672]\n",
            "11440 [D loss: 0.531541, acc.: 75.00%] [G loss: 3.142478]\n",
            "11460 [D loss: 0.495758, acc.: 77.34%] [G loss: 3.449029]\n",
            "11480 [D loss: 0.540542, acc.: 73.83%] [G loss: 3.232079]\n",
            "11500 [D loss: 0.499236, acc.: 76.17%] [G loss: 3.260865]\n",
            "11520 [D loss: 0.468685, acc.: 78.52%] [G loss: 3.297558]\n",
            "11540 [D loss: 0.565560, acc.: 73.44%] [G loss: 2.992424]\n",
            "11560 [D loss: 0.390345, acc.: 83.98%] [G loss: 2.986042]\n",
            "11580 [D loss: 0.478469, acc.: 78.91%] [G loss: 2.198299]\n",
            "11600 [D loss: 0.828604, acc.: 55.47%] [G loss: 2.136942]\n",
            "11620 [D loss: 0.742923, acc.: 60.94%] [G loss: 3.525122]\n",
            "11640 [D loss: 0.675425, acc.: 59.77%] [G loss: 2.894258]\n",
            "11660 [D loss: 0.465677, acc.: 77.73%] [G loss: 3.670548]\n",
            "11680 [D loss: 0.627510, acc.: 69.53%] [G loss: 3.478709]\n",
            "11700 [D loss: 0.463339, acc.: 79.30%] [G loss: 3.692599]\n",
            "11720 [D loss: 0.569157, acc.: 71.88%] [G loss: 3.284455]\n",
            "11740 [D loss: 0.551754, acc.: 72.66%] [G loss: 3.773970]\n",
            "11760 [D loss: 0.517791, acc.: 74.22%] [G loss: 3.585101]\n",
            "11780 [D loss: 0.554553, acc.: 69.14%] [G loss: 3.097334]\n",
            "11800 [D loss: 0.539992, acc.: 73.83%] [G loss: 3.494704]\n",
            "11820 [D loss: 0.466702, acc.: 75.00%] [G loss: 3.801535]\n",
            "11840 [D loss: 0.489685, acc.: 76.56%] [G loss: 3.385441]\n",
            "11860 [D loss: 0.465085, acc.: 81.64%] [G loss: 3.998358]\n",
            "11880 [D loss: 0.472015, acc.: 79.69%] [G loss: 3.474390]\n",
            "11900 [D loss: 0.517603, acc.: 73.44%] [G loss: 3.430966]\n",
            "11920 [D loss: 0.470901, acc.: 78.91%] [G loss: 3.450666]\n",
            "11940 [D loss: 0.508683, acc.: 73.44%] [G loss: 3.299294]\n",
            "11960 [D loss: 0.462278, acc.: 78.52%] [G loss: 3.608100]\n",
            "11980 [D loss: 0.480426, acc.: 78.12%] [G loss: 3.499630]\n",
            "12000 [D loss: 0.515757, acc.: 74.22%] [G loss: 3.308200]\n",
            "12020 [D loss: 0.457594, acc.: 79.69%] [G loss: 3.807975]\n",
            "12040 [D loss: 0.574736, acc.: 71.48%] [G loss: 3.308953]\n",
            "12060 [D loss: 0.584094, acc.: 70.31%] [G loss: 3.365221]\n",
            "12080 [D loss: 0.478168, acc.: 75.78%] [G loss: 3.530066]\n",
            "12100 [D loss: 0.499062, acc.: 77.34%] [G loss: 3.343831]\n",
            "12120 [D loss: 0.450983, acc.: 80.47%] [G loss: 3.514217]\n",
            "12140 [D loss: 0.612918, acc.: 68.75%] [G loss: 3.345132]\n",
            "12160 [D loss: 0.485205, acc.: 75.00%] [G loss: 3.617118]\n",
            "12180 [D loss: 0.582637, acc.: 69.14%] [G loss: 3.373041]\n",
            "12200 [D loss: 0.494974, acc.: 72.66%] [G loss: 3.515482]\n",
            "12220 [D loss: 0.562185, acc.: 69.14%] [G loss: 3.588341]\n",
            "12240 [D loss: 0.491010, acc.: 76.56%] [G loss: 3.619176]\n",
            "12260 [D loss: 0.492256, acc.: 75.78%] [G loss: 3.516841]\n",
            "12280 [D loss: 0.528099, acc.: 73.05%] [G loss: 3.667639]\n",
            "12300 [D loss: 0.480096, acc.: 78.12%] [G loss: 3.599898]\n",
            "12320 [D loss: 0.457694, acc.: 76.56%] [G loss: 3.893212]\n",
            "12340 [D loss: 0.558204, acc.: 70.31%] [G loss: 3.298181]\n",
            "12360 [D loss: 0.490995, acc.: 76.17%] [G loss: 3.722162]\n",
            "12380 [D loss: 0.491545, acc.: 79.30%] [G loss: 3.645373]\n",
            "12400 [D loss: 0.511157, acc.: 76.17%] [G loss: 3.427606]\n",
            "12420 [D loss: 0.671686, acc.: 63.28%] [G loss: 3.295997]\n",
            "12440 [D loss: 0.493202, acc.: 75.78%] [G loss: 3.577524]\n",
            "12460 [D loss: 0.605431, acc.: 67.97%] [G loss: 3.106953]\n",
            "12480 [D loss: 0.449933, acc.: 76.56%] [G loss: 3.826738]\n",
            "12500 [D loss: 0.485662, acc.: 76.95%] [G loss: 3.610541]\n",
            "12520 [D loss: 0.478583, acc.: 78.91%] [G loss: 3.510416]\n",
            "12540 [D loss: 0.537650, acc.: 71.48%] [G loss: 3.687121]\n",
            "12560 [D loss: 0.529475, acc.: 72.66%] [G loss: 3.573367]\n",
            "12580 [D loss: 0.511704, acc.: 71.48%] [G loss: 3.775380]\n",
            "12600 [D loss: 0.447495, acc.: 79.30%] [G loss: 3.951094]\n",
            "12620 [D loss: 0.487321, acc.: 74.61%] [G loss: 3.780288]\n",
            "12640 [D loss: 0.486517, acc.: 77.73%] [G loss: 4.199965]\n",
            "12660 [D loss: 0.520655, acc.: 75.78%] [G loss: 3.692106]\n",
            "12680 [D loss: 0.540353, acc.: 71.09%] [G loss: 3.699658]\n",
            "12700 [D loss: 0.480207, acc.: 81.25%] [G loss: 3.632570]\n",
            "12720 [D loss: 0.489774, acc.: 75.00%] [G loss: 3.605725]\n",
            "12740 [D loss: 0.551492, acc.: 73.44%] [G loss: 3.180797]\n",
            "12760 [D loss: 0.488237, acc.: 76.17%] [G loss: 3.417622]\n",
            "12780 [D loss: 0.488630, acc.: 75.39%] [G loss: 3.761364]\n",
            "12800 [D loss: 0.478628, acc.: 78.91%] [G loss: 3.638129]\n",
            "12820 [D loss: 0.487154, acc.: 76.95%] [G loss: 3.723963]\n",
            "12840 [D loss: 0.493558, acc.: 77.34%] [G loss: 3.621305]\n",
            "12860 [D loss: 0.455024, acc.: 75.39%] [G loss: 3.756918]\n",
            "12880 [D loss: 0.452837, acc.: 79.69%] [G loss: 3.683211]\n",
            "12900 [D loss: 0.468661, acc.: 77.73%] [G loss: 3.528625]\n",
            "12920 [D loss: 0.515002, acc.: 74.22%] [G loss: 3.670376]\n",
            "12940 [D loss: 0.545078, acc.: 73.83%] [G loss: 3.256910]\n",
            "12960 [D loss: 0.424769, acc.: 80.08%] [G loss: 3.823939]\n",
            "12980 [D loss: 0.492298, acc.: 75.00%] [G loss: 3.603442]\n",
            "13000 [D loss: 0.432103, acc.: 82.03%] [G loss: 3.783756]\n",
            "13020 [D loss: 0.517645, acc.: 74.61%] [G loss: 3.799623]\n",
            "13040 [D loss: 0.444191, acc.: 80.08%] [G loss: 3.850834]\n",
            "13060 [D loss: 0.538159, acc.: 72.66%] [G loss: 3.674020]\n",
            "13080 [D loss: 0.475715, acc.: 76.95%] [G loss: 3.560460]\n",
            "13100 [D loss: 0.523423, acc.: 73.83%] [G loss: 3.350858]\n",
            "13120 [D loss: 0.510387, acc.: 76.17%] [G loss: 3.921591]\n",
            "13140 [D loss: 0.465140, acc.: 78.12%] [G loss: 3.554409]\n",
            "13160 [D loss: 0.490017, acc.: 76.95%] [G loss: 3.965898]\n",
            "13180 [D loss: 0.514950, acc.: 73.83%] [G loss: 3.511276]\n",
            "13200 [D loss: 0.442506, acc.: 80.47%] [G loss: 3.940258]\n",
            "13220 [D loss: 0.482270, acc.: 77.73%] [G loss: 3.637553]\n",
            "13240 [D loss: 0.540838, acc.: 71.88%] [G loss: 3.601780]\n",
            "13260 [D loss: 0.455422, acc.: 76.95%] [G loss: 3.839467]\n",
            "13280 [D loss: 0.447907, acc.: 76.17%] [G loss: 2.759164]\n",
            "13300 [D loss: 0.777103, acc.: 54.30%] [G loss: 3.267496]\n",
            "13320 [D loss: 0.676009, acc.: 62.89%] [G loss: 3.038053]\n",
            "13340 [D loss: 0.747270, acc.: 61.33%] [G loss: 3.941363]\n",
            "13360 [D loss: 0.571809, acc.: 75.39%] [G loss: 3.634997]\n",
            "13380 [D loss: 0.597001, acc.: 67.58%] [G loss: 3.508675]\n",
            "13400 [D loss: 0.503596, acc.: 76.56%] [G loss: 3.322770]\n",
            "13420 [D loss: 0.559299, acc.: 71.88%] [G loss: 2.393901]\n",
            "13440 [D loss: 0.434877, acc.: 80.47%] [G loss: 2.894203]\n",
            "13460 [D loss: 0.432981, acc.: 82.42%] [G loss: 4.933245]\n",
            "13480 [D loss: 0.517763, acc.: 72.27%] [G loss: 4.185384]\n",
            "13500 [D loss: 0.605159, acc.: 68.36%] [G loss: 3.859009]\n",
            "13520 [D loss: 0.509817, acc.: 75.00%] [G loss: 3.887347]\n",
            "13540 [D loss: 0.561651, acc.: 67.19%] [G loss: 3.692417]\n",
            "13560 [D loss: 0.448631, acc.: 79.30%] [G loss: 4.062733]\n",
            "13580 [D loss: 0.470985, acc.: 78.91%] [G loss: 3.722137]\n",
            "13600 [D loss: 0.500870, acc.: 72.27%] [G loss: 3.600498]\n",
            "13620 [D loss: 0.421290, acc.: 80.86%] [G loss: 3.842117]\n",
            "13640 [D loss: 0.481311, acc.: 78.52%] [G loss: 3.717232]\n",
            "13660 [D loss: 0.521642, acc.: 71.09%] [G loss: 3.657314]\n",
            "13680 [D loss: 0.436893, acc.: 78.91%] [G loss: 4.014368]\n",
            "13700 [D loss: 0.507040, acc.: 76.95%] [G loss: 3.849769]\n",
            "13720 [D loss: 0.401433, acc.: 80.08%] [G loss: 4.025479]\n",
            "13740 [D loss: 0.456588, acc.: 80.86%] [G loss: 3.915627]\n",
            "13760 [D loss: 0.504279, acc.: 75.00%] [G loss: 3.858476]\n",
            "13780 [D loss: 0.410490, acc.: 80.47%] [G loss: 4.325686]\n",
            "13800 [D loss: 0.379995, acc.: 82.81%] [G loss: 4.045510]\n",
            "13820 [D loss: 0.466364, acc.: 79.69%] [G loss: 3.722054]\n",
            "13840 [D loss: 0.365402, acc.: 85.94%] [G loss: 4.428402]\n",
            "13860 [D loss: 0.425227, acc.: 84.38%] [G loss: 4.195365]\n",
            "13880 [D loss: 0.499267, acc.: 75.78%] [G loss: 4.299092]\n",
            "13900 [D loss: 0.485132, acc.: 77.73%] [G loss: 4.280088]\n",
            "13920 [D loss: 0.408853, acc.: 83.20%] [G loss: 4.632682]\n",
            "13940 [D loss: 0.455220, acc.: 78.91%] [G loss: 3.811874]\n",
            "13960 [D loss: 0.433681, acc.: 80.08%] [G loss: 4.476915]\n",
            "13980 [D loss: 0.496863, acc.: 76.56%] [G loss: 3.933153]\n",
            "14000 [D loss: 0.466540, acc.: 76.95%] [G loss: 3.943943]\n",
            "14020 [D loss: 0.473993, acc.: 78.91%] [G loss: 3.766036]\n",
            "14040 [D loss: 0.427086, acc.: 80.86%] [G loss: 4.297112]\n",
            "14060 [D loss: 0.530292, acc.: 70.70%] [G loss: 3.738087]\n",
            "14080 [D loss: 0.388797, acc.: 86.33%] [G loss: 3.180384]\n",
            "14100 [D loss: 0.306939, acc.: 87.50%] [G loss: 2.639461]\n",
            "14120 [D loss: 0.363408, acc.: 84.77%] [G loss: 2.270330]\n",
            "14140 [D loss: 1.470499, acc.: 25.00%] [G loss: 3.223104]\n",
            "14160 [D loss: 0.980526, acc.: 48.83%] [G loss: 3.998147]\n",
            "14180 [D loss: 0.769518, acc.: 61.33%] [G loss: 3.879442]\n",
            "14200 [D loss: 0.262262, acc.: 92.97%] [G loss: 2.385429]\n",
            "14220 [D loss: 1.121941, acc.: 46.48%] [G loss: 3.222373]\n",
            "14240 [D loss: 0.711596, acc.: 63.67%] [G loss: 3.054563]\n",
            "14260 [D loss: 0.686027, acc.: 67.97%] [G loss: 4.671456]\n",
            "14280 [D loss: 0.734790, acc.: 62.11%] [G loss: 4.287718]\n",
            "14300 [D loss: 0.545092, acc.: 75.00%] [G loss: 4.191175]\n",
            "14320 [D loss: 0.480244, acc.: 75.39%] [G loss: 3.856424]\n",
            "14340 [D loss: 0.601191, acc.: 70.70%] [G loss: 3.566712]\n",
            "14360 [D loss: 0.490504, acc.: 74.61%] [G loss: 3.734139]\n",
            "14380 [D loss: 0.537226, acc.: 73.83%] [G loss: 3.921724]\n",
            "14400 [D loss: 0.495363, acc.: 76.95%] [G loss: 3.779716]\n",
            "14420 [D loss: 0.440343, acc.: 77.34%] [G loss: 3.821154]\n",
            "14440 [D loss: 0.487969, acc.: 78.52%] [G loss: 3.915595]\n",
            "14460 [D loss: 0.472174, acc.: 76.95%] [G loss: 3.962627]\n",
            "14480 [D loss: 0.457887, acc.: 78.52%] [G loss: 4.014090]\n",
            "14500 [D loss: 0.404306, acc.: 82.03%] [G loss: 4.255551]\n",
            "14520 [D loss: 0.447416, acc.: 78.52%] [G loss: 3.751406]\n",
            "14540 [D loss: 0.484454, acc.: 75.39%] [G loss: 3.845271]\n",
            "14560 [D loss: 0.468034, acc.: 78.52%] [G loss: 4.171497]\n",
            "14580 [D loss: 0.432399, acc.: 79.30%] [G loss: 4.242598]\n",
            "14600 [D loss: 0.490709, acc.: 73.44%] [G loss: 4.262420]\n",
            "14620 [D loss: 0.487895, acc.: 76.56%] [G loss: 3.823763]\n",
            "14640 [D loss: 0.414516, acc.: 82.03%] [G loss: 4.269754]\n",
            "14660 [D loss: 0.469552, acc.: 78.52%] [G loss: 4.181529]\n",
            "14680 [D loss: 0.433448, acc.: 81.64%] [G loss: 4.135051]\n",
            "14700 [D loss: 0.426560, acc.: 82.42%] [G loss: 4.331366]\n",
            "14720 [D loss: 0.496702, acc.: 76.17%] [G loss: 4.041632]\n",
            "14740 [D loss: 0.443615, acc.: 78.91%] [G loss: 3.977447]\n",
            "14760 [D loss: 0.393970, acc.: 83.98%] [G loss: 4.345496]\n",
            "14780 [D loss: 0.392193, acc.: 85.94%] [G loss: 4.207296]\n",
            "14800 [D loss: 0.431805, acc.: 81.64%] [G loss: 4.328810]\n",
            "14820 [D loss: 0.417183, acc.: 83.59%] [G loss: 4.371823]\n",
            "14840 [D loss: 0.405542, acc.: 82.81%] [G loss: 4.388150]\n",
            "14860 [D loss: 0.439162, acc.: 80.47%] [G loss: 4.078679]\n",
            "14880 [D loss: 0.384783, acc.: 82.81%] [G loss: 4.208892]\n",
            "14900 [D loss: 0.489311, acc.: 76.95%] [G loss: 4.174603]\n",
            "14920 [D loss: 0.410883, acc.: 81.25%] [G loss: 4.670946]\n",
            "14940 [D loss: 0.547522, acc.: 71.48%] [G loss: 3.797606]\n",
            "14960 [D loss: 0.469830, acc.: 77.73%] [G loss: 4.124572]\n",
            "14980 [D loss: 0.451956, acc.: 79.30%] [G loss: 3.960086]\n",
            "15000 [D loss: 0.446261, acc.: 79.69%] [G loss: 4.136053]\n",
            "15020 [D loss: 0.396676, acc.: 82.03%] [G loss: 4.681150]\n",
            "15040 [D loss: 0.448992, acc.: 81.25%] [G loss: 4.291993]\n",
            "15060 [D loss: 0.453368, acc.: 80.08%] [G loss: 3.907290]\n",
            "15080 [D loss: 0.410221, acc.: 82.81%] [G loss: 4.168509]\n",
            "15100 [D loss: 0.392610, acc.: 81.25%] [G loss: 4.462986]\n",
            "15120 [D loss: 0.425826, acc.: 80.08%] [G loss: 4.088558]\n",
            "15140 [D loss: 0.414964, acc.: 82.03%] [G loss: 4.397114]\n",
            "15160 [D loss: 0.419449, acc.: 80.47%] [G loss: 4.092388]\n",
            "15180 [D loss: 0.436890, acc.: 79.30%] [G loss: 4.450234]\n",
            "15200 [D loss: 0.449115, acc.: 78.91%] [G loss: 4.348115]\n",
            "15220 [D loss: 0.451734, acc.: 81.25%] [G loss: 4.022134]\n",
            "15240 [D loss: 0.505037, acc.: 76.95%] [G loss: 4.054724]\n",
            "15260 [D loss: 0.425096, acc.: 81.64%] [G loss: 4.117021]\n",
            "15280 [D loss: 0.472546, acc.: 77.34%] [G loss: 4.422746]\n",
            "15300 [D loss: 0.414074, acc.: 81.64%] [G loss: 4.419353]\n",
            "15320 [D loss: 0.491301, acc.: 75.00%] [G loss: 4.180203]\n",
            "15340 [D loss: 0.438992, acc.: 78.12%] [G loss: 4.229813]\n",
            "15360 [D loss: 0.405956, acc.: 82.03%] [G loss: 4.493354]\n",
            "15380 [D loss: 0.361220, acc.: 84.38%] [G loss: 5.007316]\n",
            "15400 [D loss: 0.518825, acc.: 73.83%] [G loss: 4.338915]\n",
            "15420 [D loss: 0.401742, acc.: 79.30%] [G loss: 4.450905]\n",
            "15440 [D loss: 0.403439, acc.: 84.38%] [G loss: 4.363975]\n",
            "15460 [D loss: 0.458354, acc.: 78.91%] [G loss: 4.444795]\n",
            "15480 [D loss: 0.498310, acc.: 73.05%] [G loss: 4.328468]\n",
            "15500 [D loss: 0.513045, acc.: 75.39%] [G loss: 3.918593]\n",
            "15520 [D loss: 0.528437, acc.: 73.44%] [G loss: 4.409147]\n",
            "15540 [D loss: 0.426259, acc.: 80.47%] [G loss: 4.658106]\n",
            "15560 [D loss: 0.567030, acc.: 71.88%] [G loss: 3.927747]\n",
            "15580 [D loss: 0.406092, acc.: 82.03%] [G loss: 4.552408]\n",
            "15600 [D loss: 0.479119, acc.: 77.34%] [G loss: 4.443943]\n",
            "15620 [D loss: 0.514651, acc.: 71.48%] [G loss: 4.106854]\n",
            "15640 [D loss: 0.513284, acc.: 74.61%] [G loss: 4.422845]\n",
            "15660 [D loss: 0.414548, acc.: 79.69%] [G loss: 4.713196]\n",
            "15680 [D loss: 0.485012, acc.: 77.73%] [G loss: 4.340976]\n",
            "15700 [D loss: 0.349628, acc.: 88.28%] [G loss: 3.319634]\n",
            "15720 [D loss: 0.461318, acc.: 79.69%] [G loss: 3.954995]\n",
            "15740 [D loss: 0.452210, acc.: 78.91%] [G loss: 4.512598]\n",
            "15760 [D loss: 0.460763, acc.: 79.69%] [G loss: 4.605072]\n",
            "15780 [D loss: 0.484609, acc.: 76.56%] [G loss: 4.183094]\n",
            "15800 [D loss: 0.545431, acc.: 71.88%] [G loss: 4.453772]\n",
            "15820 [D loss: 0.476415, acc.: 77.73%] [G loss: 4.776829]\n",
            "15840 [D loss: 0.373772, acc.: 83.98%] [G loss: 4.383667]\n",
            "15860 [D loss: 0.403286, acc.: 83.59%] [G loss: 5.058902]\n",
            "15880 [D loss: 0.396528, acc.: 81.64%] [G loss: 4.496496]\n",
            "15900 [D loss: 0.371370, acc.: 84.77%] [G loss: 5.028993]\n",
            "15920 [D loss: 0.475078, acc.: 79.30%] [G loss: 4.140124]\n",
            "15940 [D loss: 0.511645, acc.: 76.17%] [G loss: 4.215744]\n",
            "15960 [D loss: 0.408632, acc.: 82.81%] [G loss: 4.565737]\n",
            "15980 [D loss: 0.463144, acc.: 77.73%] [G loss: 4.539938]\n",
            "16000 [D loss: 0.435611, acc.: 80.47%] [G loss: 4.907251]\n",
            "16020 [D loss: 0.497434, acc.: 79.69%] [G loss: 4.479740]\n",
            "16040 [D loss: 0.471865, acc.: 78.12%] [G loss: 4.244232]\n",
            "16060 [D loss: 0.479878, acc.: 75.39%] [G loss: 4.329868]\n",
            "16080 [D loss: 0.356500, acc.: 83.59%] [G loss: 4.681089]\n",
            "16100 [D loss: 0.403395, acc.: 80.86%] [G loss: 4.371491]\n",
            "16120 [D loss: 0.500003, acc.: 75.78%] [G loss: 4.126794]\n",
            "16140 [D loss: 0.462856, acc.: 78.91%] [G loss: 4.483397]\n",
            "16160 [D loss: 0.405383, acc.: 84.77%] [G loss: 4.565841]\n",
            "16180 [D loss: 0.394061, acc.: 82.81%] [G loss: 4.636230]\n",
            "16200 [D loss: 0.450796, acc.: 76.95%] [G loss: 4.394433]\n",
            "16220 [D loss: 0.464583, acc.: 76.56%] [G loss: 4.559381]\n",
            "16240 [D loss: 0.473920, acc.: 78.12%] [G loss: 4.251530]\n",
            "16260 [D loss: 0.498796, acc.: 75.39%] [G loss: 4.628928]\n",
            "16280 [D loss: 0.403480, acc.: 82.03%] [G loss: 5.175748]\n",
            "16300 [D loss: 0.469018, acc.: 79.69%] [G loss: 4.277114]\n",
            "16320 [D loss: 0.462037, acc.: 76.95%] [G loss: 4.496184]\n",
            "16340 [D loss: 0.444925, acc.: 79.30%] [G loss: 4.300325]\n",
            "16360 [D loss: 0.419094, acc.: 82.03%] [G loss: 4.811031]\n",
            "16380 [D loss: 0.416195, acc.: 80.08%] [G loss: 4.483048]\n",
            "16400 [D loss: 0.534406, acc.: 73.44%] [G loss: 4.411606]\n",
            "16420 [D loss: 0.442264, acc.: 79.69%] [G loss: 5.192403]\n",
            "16440 [D loss: 0.590045, acc.: 70.70%] [G loss: 4.432266]\n",
            "16460 [D loss: 0.381895, acc.: 84.38%] [G loss: 5.262319]\n",
            "16480 [D loss: 0.416645, acc.: 80.86%] [G loss: 4.608866]\n",
            "16500 [D loss: 0.525606, acc.: 71.48%] [G loss: 4.495549]\n",
            "16520 [D loss: 0.313138, acc.: 87.50%] [G loss: 5.144814]\n",
            "16540 [D loss: 0.433329, acc.: 80.08%] [G loss: 5.314988]\n",
            "16560 [D loss: 0.416587, acc.: 82.42%] [G loss: 4.404133]\n",
            "16580 [D loss: 0.390361, acc.: 83.20%] [G loss: 5.202195]\n",
            "16600 [D loss: 0.312475, acc.: 88.28%] [G loss: 4.609299]\n",
            "16620 [D loss: 0.425790, acc.: 77.73%] [G loss: 3.668221]\n",
            "16640 [D loss: 0.316900, acc.: 88.28%] [G loss: 2.916548]\n",
            "16660 [D loss: 0.537879, acc.: 76.56%] [G loss: 2.342048]\n",
            "16680 [D loss: 0.834209, acc.: 60.94%] [G loss: 2.559158]\n",
            "16700 [D loss: 0.904806, acc.: 54.69%] [G loss: 5.697089]\n",
            "16720 [D loss: 1.060272, acc.: 51.95%] [G loss: 4.339818]\n",
            "16740 [D loss: 0.912106, acc.: 60.55%] [G loss: 5.021738]\n",
            "16760 [D loss: 0.494760, acc.: 76.95%] [G loss: 4.726037]\n",
            "16780 [D loss: 0.501706, acc.: 73.44%] [G loss: 4.948298]\n",
            "16800 [D loss: 0.547346, acc.: 74.22%] [G loss: 4.522659]\n",
            "16820 [D loss: 0.440962, acc.: 77.34%] [G loss: 5.060385]\n",
            "16840 [D loss: 0.438382, acc.: 80.08%] [G loss: 4.534210]\n",
            "16860 [D loss: 0.403955, acc.: 84.77%] [G loss: 4.915852]\n",
            "16880 [D loss: 0.444978, acc.: 80.47%] [G loss: 4.567666]\n",
            "16900 [D loss: 0.356881, acc.: 84.77%] [G loss: 4.749103]\n",
            "16920 [D loss: 0.410073, acc.: 83.98%] [G loss: 4.643299]\n",
            "16940 [D loss: 0.399823, acc.: 80.08%] [G loss: 4.651511]\n",
            "16960 [D loss: 0.453717, acc.: 79.30%] [G loss: 4.441349]\n",
            "16980 [D loss: 0.470073, acc.: 76.95%] [G loss: 4.352519]\n",
            "17000 [D loss: 0.366470, acc.: 84.38%] [G loss: 4.746583]\n",
            "17020 [D loss: 0.473018, acc.: 76.95%] [G loss: 4.573498]\n",
            "17040 [D loss: 0.458982, acc.: 77.34%] [G loss: 4.616008]\n",
            "17060 [D loss: 0.418124, acc.: 79.30%] [G loss: 4.517262]\n",
            "17080 [D loss: 0.400187, acc.: 83.20%] [G loss: 4.591843]\n",
            "17100 [D loss: 0.489121, acc.: 75.78%] [G loss: 4.643111]\n",
            "17120 [D loss: 0.402659, acc.: 81.25%] [G loss: 4.615548]\n",
            "17140 [D loss: 0.379704, acc.: 83.20%] [G loss: 5.056406]\n",
            "17160 [D loss: 0.445643, acc.: 78.91%] [G loss: 4.979579]\n",
            "17180 [D loss: 0.341674, acc.: 85.16%] [G loss: 5.464826]\n",
            "17200 [D loss: 0.373649, acc.: 84.77%] [G loss: 4.691474]\n",
            "17220 [D loss: 0.422374, acc.: 80.47%] [G loss: 4.423506]\n",
            "17240 [D loss: 0.404834, acc.: 82.03%] [G loss: 4.889428]\n",
            "17260 [D loss: 0.403537, acc.: 80.47%] [G loss: 4.751269]\n",
            "17280 [D loss: 0.464390, acc.: 77.73%] [G loss: 4.340008]\n",
            "17300 [D loss: 0.404335, acc.: 82.81%] [G loss: 5.009501]\n",
            "17320 [D loss: 0.368249, acc.: 85.55%] [G loss: 4.661641]\n",
            "17340 [D loss: 0.434587, acc.: 78.91%] [G loss: 4.791728]\n",
            "17360 [D loss: 0.392016, acc.: 83.59%] [G loss: 4.901679]\n",
            "17380 [D loss: 0.441671, acc.: 78.91%] [G loss: 4.873913]\n",
            "17400 [D loss: 0.400734, acc.: 83.20%] [G loss: 4.759240]\n",
            "17420 [D loss: 0.435034, acc.: 80.86%] [G loss: 4.772414]\n",
            "17440 [D loss: 0.356838, acc.: 85.94%] [G loss: 5.501213]\n",
            "17460 [D loss: 0.429215, acc.: 76.95%] [G loss: 4.946195]\n",
            "17480 [D loss: 0.465035, acc.: 77.73%] [G loss: 4.445612]\n",
            "17500 [D loss: 0.400007, acc.: 82.42%] [G loss: 4.947104]\n",
            "17520 [D loss: 0.469070, acc.: 79.69%] [G loss: 4.664060]\n",
            "17540 [D loss: 0.393398, acc.: 82.81%] [G loss: 4.878925]\n",
            "17560 [D loss: 0.366000, acc.: 86.33%] [G loss: 4.858205]\n",
            "17580 [D loss: 0.373190, acc.: 84.38%] [G loss: 5.658450]\n",
            "17600 [D loss: 0.329919, acc.: 84.77%] [G loss: 4.923306]\n",
            "17620 [D loss: 0.404425, acc.: 83.20%] [G loss: 5.103981]\n",
            "17640 [D loss: 0.352829, acc.: 86.33%] [G loss: 5.101260]\n",
            "17660 [D loss: 0.417488, acc.: 80.08%] [G loss: 4.699083]\n",
            "17680 [D loss: 0.363012, acc.: 83.59%] [G loss: 4.898824]\n",
            "17700 [D loss: 0.379316, acc.: 82.03%] [G loss: 4.963213]\n",
            "17720 [D loss: 0.512355, acc.: 73.44%] [G loss: 4.716199]\n",
            "17740 [D loss: 0.374674, acc.: 83.20%] [G loss: 5.252618]\n",
            "17760 [D loss: 0.400222, acc.: 82.81%] [G loss: 5.353424]\n",
            "17780 [D loss: 0.437101, acc.: 78.91%] [G loss: 4.514562]\n",
            "17800 [D loss: 0.428741, acc.: 78.12%] [G loss: 5.048187]\n",
            "17820 [D loss: 0.528162, acc.: 75.39%] [G loss: 4.413342]\n",
            "17840 [D loss: 0.282876, acc.: 90.23%] [G loss: 5.380236]\n",
            "17860 [D loss: 0.411687, acc.: 82.03%] [G loss: 5.042712]\n",
            "17880 [D loss: 0.456657, acc.: 78.91%] [G loss: 4.830601]\n",
            "17900 [D loss: 0.403674, acc.: 83.20%] [G loss: 5.344800]\n",
            "17920 [D loss: 0.480694, acc.: 76.17%] [G loss: 4.276624]\n",
            "17940 [D loss: 0.348729, acc.: 85.94%] [G loss: 4.797805]\n",
            "17960 [D loss: 0.361956, acc.: 85.16%] [G loss: 5.170873]\n",
            "17980 [D loss: 0.430281, acc.: 78.91%] [G loss: 5.515908]\n",
            "18000 [D loss: 0.302992, acc.: 88.28%] [G loss: 3.986206]\n",
            "18020 [D loss: 0.667625, acc.: 68.36%] [G loss: 3.668444]\n",
            "18040 [D loss: 0.428581, acc.: 81.64%] [G loss: 6.434139]\n",
            "18060 [D loss: 0.823457, acc.: 60.94%] [G loss: 5.434307]\n",
            "18080 [D loss: 0.597740, acc.: 74.22%] [G loss: 5.941543]\n",
            "18100 [D loss: 0.387540, acc.: 83.59%] [G loss: 5.125514]\n",
            "18120 [D loss: 0.413032, acc.: 82.42%] [G loss: 4.947441]\n",
            "18140 [D loss: 0.479898, acc.: 76.17%] [G loss: 5.229119]\n",
            "18160 [D loss: 0.429651, acc.: 81.64%] [G loss: 5.057684]\n",
            "18180 [D loss: 0.433253, acc.: 80.47%] [G loss: 4.970109]\n",
            "18200 [D loss: 0.383411, acc.: 82.42%] [G loss: 5.656885]\n",
            "18220 [D loss: 0.470848, acc.: 76.17%] [G loss: 4.984237]\n",
            "18240 [D loss: 0.459424, acc.: 78.91%] [G loss: 4.997733]\n",
            "18260 [D loss: 0.387771, acc.: 80.86%] [G loss: 5.150550]\n",
            "18280 [D loss: 0.524349, acc.: 77.73%] [G loss: 4.899230]\n",
            "18300 [D loss: 0.371196, acc.: 81.64%] [G loss: 5.583899]\n",
            "18320 [D loss: 0.357295, acc.: 86.33%] [G loss: 5.691835]\n",
            "18340 [D loss: 0.377237, acc.: 82.03%] [G loss: 5.809303]\n",
            "18360 [D loss: 0.426009, acc.: 79.69%] [G loss: 5.211788]\n",
            "18380 [D loss: 0.377666, acc.: 82.42%] [G loss: 5.400752]\n",
            "18400 [D loss: 0.440397, acc.: 77.34%] [G loss: 4.920369]\n",
            "18420 [D loss: 0.328267, acc.: 85.55%] [G loss: 5.101871]\n",
            "18440 [D loss: 0.346054, acc.: 85.16%] [G loss: 4.850483]\n",
            "18460 [D loss: 0.444543, acc.: 77.73%] [G loss: 5.274377]\n",
            "18480 [D loss: 0.415121, acc.: 82.03%] [G loss: 5.305892]\n",
            "18500 [D loss: 0.351878, acc.: 83.98%] [G loss: 5.525507]\n",
            "18520 [D loss: 0.389182, acc.: 83.59%] [G loss: 5.016284]\n",
            "18540 [D loss: 0.385619, acc.: 82.03%] [G loss: 4.740427]\n",
            "18560 [D loss: 0.321250, acc.: 87.11%] [G loss: 5.493331]\n",
            "18580 [D loss: 0.450876, acc.: 80.08%] [G loss: 5.103118]\n",
            "18600 [D loss: 0.388598, acc.: 82.42%] [G loss: 5.273328]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1Lof-jm3i0Q",
        "outputId": "bc1768b8-9165-4e50-a02e-b3a784112757"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18600, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.696359, acc.: 62.89%] [G loss: 1.446211]\n",
            "20 [D loss: 1.163523, acc.: 22.27%] [G loss: 0.713011]\n",
            "40 [D loss: 1.220478, acc.: 17.58%] [G loss: 1.237201]\n",
            "60 [D loss: 0.655929, acc.: 73.05%] [G loss: 3.198225]\n",
            "80 [D loss: 0.475856, acc.: 79.69%] [G loss: 4.155136]\n",
            "100 [D loss: 1.030986, acc.: 50.39%] [G loss: 1.380551]\n",
            "120 [D loss: 0.413875, acc.: 85.55%] [G loss: 12.094809]\n",
            "140 [D loss: 0.699246, acc.: 65.62%] [G loss: 1.606556]\n",
            "160 [D loss: 0.506435, acc.: 87.89%] [G loss: 3.299470]\n",
            "180 [D loss: 0.251355, acc.: 90.62%] [G loss: 7.642454]\n",
            "200 [D loss: 1.369268, acc.: 51.56%] [G loss: 1.898375]\n",
            "220 [D loss: 0.571116, acc.: 69.53%] [G loss: 2.657409]\n",
            "240 [D loss: 0.554591, acc.: 73.83%] [G loss: 1.613303]\n",
            "260 [D loss: 0.281924, acc.: 91.41%] [G loss: 3.086654]\n",
            "280 [D loss: 0.360274, acc.: 88.28%] [G loss: 2.700082]\n",
            "300 [D loss: 0.528984, acc.: 80.86%] [G loss: 2.713992]\n",
            "320 [D loss: 0.384624, acc.: 90.23%] [G loss: 1.856140]\n",
            "340 [D loss: 0.645530, acc.: 66.41%] [G loss: 1.617378]\n",
            "360 [D loss: 0.340297, acc.: 90.62%] [G loss: 4.522230]\n",
            "380 [D loss: 0.291186, acc.: 87.89%] [G loss: 5.616865]\n",
            "400 [D loss: 0.681895, acc.: 57.42%] [G loss: 2.202144]\n",
            "420 [D loss: 0.515111, acc.: 78.12%] [G loss: 2.813591]\n",
            "440 [D loss: 0.522725, acc.: 73.83%] [G loss: 3.063996]\n",
            "460 [D loss: 0.501683, acc.: 71.09%] [G loss: 4.516839]\n",
            "480 [D loss: 0.471221, acc.: 81.25%] [G loss: 2.272563]\n",
            "500 [D loss: 0.551517, acc.: 71.09%] [G loss: 3.269455]\n",
            "520 [D loss: 0.550512, acc.: 73.44%] [G loss: 2.471466]\n",
            "540 [D loss: 0.317920, acc.: 88.67%] [G loss: 5.961580]\n",
            "560 [D loss: 0.389808, acc.: 83.20%] [G loss: 4.262944]\n",
            "580 [D loss: 0.460144, acc.: 79.30%] [G loss: 4.905150]\n",
            "600 [D loss: 0.320170, acc.: 87.89%] [G loss: 6.336742]\n",
            "620 [D loss: 0.567837, acc.: 72.27%] [G loss: 3.545739]\n",
            "640 [D loss: 0.653890, acc.: 65.62%] [G loss: 3.399276]\n",
            "660 [D loss: 0.492451, acc.: 76.56%] [G loss: 4.467133]\n",
            "680 [D loss: 0.516664, acc.: 76.95%] [G loss: 4.192909]\n",
            "700 [D loss: 0.449728, acc.: 81.64%] [G loss: 4.639505]\n",
            "720 [D loss: 0.576106, acc.: 73.05%] [G loss: 3.609271]\n",
            "740 [D loss: 0.641553, acc.: 68.36%] [G loss: 4.200828]\n",
            "760 [D loss: 0.583512, acc.: 68.75%] [G loss: 2.751359]\n",
            "780 [D loss: 0.730590, acc.: 60.16%] [G loss: 3.544207]\n",
            "800 [D loss: 0.393353, acc.: 84.77%] [G loss: 5.141732]\n",
            "820 [D loss: 0.442701, acc.: 78.52%] [G loss: 4.386707]\n",
            "840 [D loss: 0.357923, acc.: 87.50%] [G loss: 4.665219]\n",
            "860 [D loss: 0.477971, acc.: 79.30%] [G loss: 4.352232]\n",
            "880 [D loss: 0.562903, acc.: 70.31%] [G loss: 4.327156]\n",
            "900 [D loss: 0.493504, acc.: 75.78%] [G loss: 4.012523]\n",
            "920 [D loss: 0.540231, acc.: 76.95%] [G loss: 4.362349]\n",
            "940 [D loss: 0.455799, acc.: 77.73%] [G loss: 3.636975]\n",
            "960 [D loss: 0.477792, acc.: 80.47%] [G loss: 3.706674]\n",
            "980 [D loss: 0.357467, acc.: 87.11%] [G loss: 4.805582]\n",
            "1000 [D loss: 0.363910, acc.: 85.16%] [G loss: 4.385890]\n",
            "1020 [D loss: 0.395662, acc.: 83.20%] [G loss: 4.251680]\n",
            "1040 [D loss: 0.544617, acc.: 73.83%] [G loss: 3.971273]\n",
            "1060 [D loss: 0.406575, acc.: 83.98%] [G loss: 4.369378]\n",
            "1080 [D loss: 0.433259, acc.: 80.08%] [G loss: 3.938429]\n",
            "1100 [D loss: 0.455807, acc.: 79.69%] [G loss: 3.812581]\n",
            "1120 [D loss: 0.478660, acc.: 77.73%] [G loss: 3.781468]\n",
            "1140 [D loss: 0.463711, acc.: 78.12%] [G loss: 3.752246]\n",
            "1160 [D loss: 0.539984, acc.: 72.27%] [G loss: 3.729447]\n",
            "1180 [D loss: 0.443357, acc.: 78.91%] [G loss: 3.888325]\n",
            "1200 [D loss: 0.485968, acc.: 73.05%] [G loss: 4.196789]\n",
            "1220 [D loss: 0.433135, acc.: 80.86%] [G loss: 3.941662]\n",
            "1240 [D loss: 0.505657, acc.: 75.78%] [G loss: 3.800249]\n",
            "1260 [D loss: 0.448927, acc.: 78.91%] [G loss: 3.956593]\n",
            "1280 [D loss: 0.411726, acc.: 81.25%] [G loss: 4.155158]\n",
            "1300 [D loss: 0.434513, acc.: 79.69%] [G loss: 4.001882]\n",
            "1320 [D loss: 0.455434, acc.: 79.69%] [G loss: 3.718468]\n",
            "1340 [D loss: 0.393134, acc.: 84.77%] [G loss: 4.017004]\n",
            "1360 [D loss: 0.445426, acc.: 78.52%] [G loss: 3.952610]\n",
            "1380 [D loss: 0.397145, acc.: 82.81%] [G loss: 4.576160]\n",
            "1400 [D loss: 0.530536, acc.: 74.22%] [G loss: 4.221869]\n",
            "1420 [D loss: 0.429820, acc.: 83.98%] [G loss: 3.989506]\n",
            "1440 [D loss: 0.497594, acc.: 75.00%] [G loss: 4.096679]\n",
            "1460 [D loss: 0.522586, acc.: 77.34%] [G loss: 4.010116]\n",
            "1480 [D loss: 0.458121, acc.: 82.81%] [G loss: 4.037869]\n",
            "1500 [D loss: 0.534014, acc.: 75.39%] [G loss: 3.837799]\n",
            "1520 [D loss: 0.398399, acc.: 81.64%] [G loss: 4.133334]\n",
            "1540 [D loss: 0.431214, acc.: 81.25%] [G loss: 4.334744]\n",
            "1560 [D loss: 0.783335, acc.: 50.39%] [G loss: 2.173300]\n",
            "1580 [D loss: 0.717588, acc.: 56.64%] [G loss: 2.319828]\n",
            "1600 [D loss: 0.709927, acc.: 57.42%] [G loss: 1.991293]\n",
            "1620 [D loss: 0.638347, acc.: 60.16%] [G loss: 2.269987]\n",
            "1640 [D loss: 0.665323, acc.: 57.03%] [G loss: 2.005648]\n",
            "1660 [D loss: 0.518655, acc.: 75.78%] [G loss: 2.671816]\n",
            "1680 [D loss: 0.523227, acc.: 78.52%] [G loss: 2.755344]\n",
            "1700 [D loss: 0.579249, acc.: 68.75%] [G loss: 2.956488]\n",
            "1720 [D loss: 0.468646, acc.: 78.91%] [G loss: 3.110859]\n",
            "1740 [D loss: 0.447784, acc.: 80.47%] [G loss: 3.289743]\n",
            "1760 [D loss: 0.460743, acc.: 78.52%] [G loss: 3.678903]\n",
            "1780 [D loss: 0.448870, acc.: 83.59%] [G loss: 3.301871]\n",
            "1800 [D loss: 0.490325, acc.: 78.91%] [G loss: 3.015180]\n",
            "1820 [D loss: 0.445600, acc.: 82.03%] [G loss: 3.613938]\n",
            "1840 [D loss: 0.436003, acc.: 83.98%] [G loss: 3.533766]\n",
            "1860 [D loss: 0.498762, acc.: 78.91%] [G loss: 3.809198]\n",
            "1880 [D loss: 0.526313, acc.: 73.44%] [G loss: 3.296524]\n",
            "1900 [D loss: 0.471828, acc.: 78.91%] [G loss: 3.538340]\n",
            "1920 [D loss: 0.561648, acc.: 70.70%] [G loss: 3.407851]\n",
            "1940 [D loss: 0.493469, acc.: 75.78%] [G loss: 3.683469]\n",
            "1960 [D loss: 0.472527, acc.: 78.52%] [G loss: 3.501588]\n",
            "1980 [D loss: 0.497438, acc.: 76.95%] [G loss: 3.714194]\n",
            "2000 [D loss: 0.462745, acc.: 78.91%] [G loss: 3.386624]\n",
            "2020 [D loss: 0.420221, acc.: 81.64%] [G loss: 3.648956]\n",
            "2040 [D loss: 0.493041, acc.: 77.73%] [G loss: 3.207301]\n",
            "2060 [D loss: 0.522109, acc.: 75.39%] [G loss: 3.402582]\n",
            "2080 [D loss: 0.509177, acc.: 76.95%] [G loss: 3.566931]\n",
            "2100 [D loss: 0.463567, acc.: 79.69%] [G loss: 3.492286]\n",
            "2120 [D loss: 0.433554, acc.: 81.25%] [G loss: 3.631058]\n",
            "2140 [D loss: 0.527185, acc.: 72.66%] [G loss: 3.487663]\n",
            "2160 [D loss: 0.471744, acc.: 75.39%] [G loss: 3.367339]\n",
            "2180 [D loss: 0.507224, acc.: 76.95%] [G loss: 3.251434]\n",
            "2200 [D loss: 0.469351, acc.: 77.34%] [G loss: 3.594208]\n",
            "2220 [D loss: 0.457895, acc.: 79.30%] [G loss: 3.720166]\n",
            "2240 [D loss: 0.480834, acc.: 79.30%] [G loss: 3.223764]\n",
            "2260 [D loss: 0.501214, acc.: 74.61%] [G loss: 3.476759]\n",
            "2280 [D loss: 0.558307, acc.: 66.41%] [G loss: 3.187800]\n",
            "2300 [D loss: 0.526974, acc.: 73.44%] [G loss: 3.721486]\n",
            "2320 [D loss: 0.501747, acc.: 75.78%] [G loss: 3.540772]\n",
            "2340 [D loss: 0.422631, acc.: 82.42%] [G loss: 3.702205]\n",
            "2360 [D loss: 0.458724, acc.: 80.47%] [G loss: 3.259301]\n",
            "2380 [D loss: 0.587042, acc.: 65.62%] [G loss: 3.306011]\n",
            "2400 [D loss: 0.434851, acc.: 82.42%] [G loss: 3.875597]\n",
            "2420 [D loss: 0.577661, acc.: 67.58%] [G loss: 3.114182]\n",
            "2440 [D loss: 0.495277, acc.: 75.00%] [G loss: 3.272084]\n",
            "2460 [D loss: 0.508051, acc.: 73.83%] [G loss: 3.589397]\n",
            "2480 [D loss: 0.457798, acc.: 78.12%] [G loss: 3.579377]\n",
            "2500 [D loss: 0.493405, acc.: 76.95%] [G loss: 3.440243]\n",
            "2520 [D loss: 0.561211, acc.: 69.53%] [G loss: 3.333122]\n",
            "2540 [D loss: 0.465456, acc.: 75.78%] [G loss: 2.658821]\n",
            "2560 [D loss: 0.610560, acc.: 66.80%] [G loss: 3.313526]\n",
            "2580 [D loss: 0.819851, acc.: 57.42%] [G loss: 3.394972]\n",
            "2600 [D loss: 0.506641, acc.: 75.00%] [G loss: 3.527428]\n",
            "2620 [D loss: 0.366536, acc.: 85.16%] [G loss: 4.327864]\n",
            "2640 [D loss: 0.430177, acc.: 80.08%] [G loss: 2.379280]\n",
            "2660 [D loss: 0.476316, acc.: 77.73%] [G loss: 1.879889]\n",
            "2680 [D loss: 0.308654, acc.: 90.23%] [G loss: 2.107726]\n",
            "2700 [D loss: 1.276173, acc.: 44.14%] [G loss: 4.422312]\n",
            "2720 [D loss: 0.566286, acc.: 72.66%] [G loss: 2.964157]\n",
            "2740 [D loss: 0.519149, acc.: 70.70%] [G loss: 2.615463]\n",
            "2760 [D loss: 0.394649, acc.: 83.20%] [G loss: 3.842801]\n",
            "2780 [D loss: 0.411506, acc.: 83.20%] [G loss: 3.776368]\n",
            "2800 [D loss: 0.434963, acc.: 80.47%] [G loss: 3.866957]\n",
            "2820 [D loss: 0.446653, acc.: 82.81%] [G loss: 3.701382]\n",
            "2840 [D loss: 0.438237, acc.: 81.64%] [G loss: 3.880510]\n",
            "2860 [D loss: 0.465442, acc.: 80.47%] [G loss: 3.495365]\n",
            "2880 [D loss: 0.498248, acc.: 74.61%] [G loss: 3.903087]\n",
            "2900 [D loss: 0.527697, acc.: 71.88%] [G loss: 3.249183]\n",
            "2920 [D loss: 0.548208, acc.: 70.70%] [G loss: 3.584967]\n",
            "2940 [D loss: 0.543540, acc.: 74.22%] [G loss: 3.689894]\n",
            "2960 [D loss: 0.522666, acc.: 75.00%] [G loss: 3.657451]\n",
            "2980 [D loss: 0.451316, acc.: 80.47%] [G loss: 3.589772]\n",
            "3000 [D loss: 0.456579, acc.: 79.30%] [G loss: 3.440430]\n",
            "3020 [D loss: 0.385899, acc.: 85.55%] [G loss: 3.883299]\n",
            "3040 [D loss: 0.470351, acc.: 79.69%] [G loss: 3.780955]\n",
            "3060 [D loss: 0.503027, acc.: 76.56%] [G loss: 3.554591]\n",
            "3080 [D loss: 0.496454, acc.: 75.00%] [G loss: 3.324713]\n",
            "3100 [D loss: 0.398832, acc.: 81.64%] [G loss: 4.244615]\n",
            "3120 [D loss: 0.568626, acc.: 71.88%] [G loss: 3.400843]\n",
            "3140 [D loss: 0.426111, acc.: 78.52%] [G loss: 3.812358]\n",
            "3160 [D loss: 0.574710, acc.: 67.19%] [G loss: 3.353335]\n",
            "3180 [D loss: 0.498938, acc.: 77.73%] [G loss: 3.650208]\n",
            "3200 [D loss: 0.365601, acc.: 85.94%] [G loss: 4.111368]\n",
            "3220 [D loss: 0.516018, acc.: 75.39%] [G loss: 3.455418]\n",
            "3240 [D loss: 0.494219, acc.: 72.27%] [G loss: 3.612473]\n",
            "3260 [D loss: 0.440146, acc.: 79.69%] [G loss: 3.942535]\n",
            "3280 [D loss: 0.531865, acc.: 73.83%] [G loss: 3.394681]\n",
            "3300 [D loss: 0.489271, acc.: 77.73%] [G loss: 3.377133]\n",
            "3320 [D loss: 0.551878, acc.: 73.44%] [G loss: 3.622541]\n",
            "3340 [D loss: 0.465189, acc.: 81.64%] [G loss: 3.586190]\n",
            "3360 [D loss: 0.521616, acc.: 75.78%] [G loss: 3.424371]\n",
            "3380 [D loss: 0.406729, acc.: 82.03%] [G loss: 3.889325]\n",
            "3400 [D loss: 0.542310, acc.: 74.61%] [G loss: 4.021432]\n",
            "3420 [D loss: 0.621952, acc.: 66.41%] [G loss: 3.588639]\n",
            "3440 [D loss: 0.561079, acc.: 71.48%] [G loss: 3.718172]\n",
            "3460 [D loss: 0.478455, acc.: 75.78%] [G loss: 3.916556]\n",
            "3480 [D loss: 0.453478, acc.: 77.34%] [G loss: 4.023107]\n",
            "3500 [D loss: 0.453086, acc.: 78.12%] [G loss: 3.654559]\n",
            "3520 [D loss: 0.478267, acc.: 78.52%] [G loss: 3.676326]\n",
            "3540 [D loss: 0.514600, acc.: 73.05%] [G loss: 3.569495]\n",
            "3560 [D loss: 0.584826, acc.: 68.36%] [G loss: 3.276380]\n",
            "3580 [D loss: 0.449984, acc.: 80.08%] [G loss: 3.572367]\n",
            "3600 [D loss: 0.457138, acc.: 80.08%] [G loss: 3.839814]\n",
            "3620 [D loss: 0.534657, acc.: 71.88%] [G loss: 3.494092]\n",
            "3640 [D loss: 0.434491, acc.: 82.03%] [G loss: 4.018171]\n",
            "3660 [D loss: 0.595732, acc.: 67.97%] [G loss: 3.356070]\n",
            "3680 [D loss: 0.438684, acc.: 81.25%] [G loss: 3.594168]\n",
            "3700 [D loss: 0.463088, acc.: 77.34%] [G loss: 4.194972]\n",
            "3720 [D loss: 0.484385, acc.: 76.95%] [G loss: 3.512795]\n",
            "3740 [D loss: 0.482548, acc.: 75.00%] [G loss: 3.865248]\n",
            "3760 [D loss: 0.487501, acc.: 78.12%] [G loss: 3.512489]\n",
            "3780 [D loss: 0.475157, acc.: 78.91%] [G loss: 3.973012]\n",
            "3800 [D loss: 0.479466, acc.: 77.73%] [G loss: 4.028281]\n",
            "3820 [D loss: 0.477754, acc.: 75.78%] [G loss: 4.027597]\n",
            "3840 [D loss: 0.501783, acc.: 75.78%] [G loss: 3.664977]\n",
            "3860 [D loss: 0.468632, acc.: 78.91%] [G loss: 3.862001]\n",
            "3880 [D loss: 0.446175, acc.: 79.69%] [G loss: 4.066327]\n",
            "3900 [D loss: 0.508537, acc.: 77.34%] [G loss: 3.726521]\n",
            "3920 [D loss: 0.416865, acc.: 79.69%] [G loss: 4.074302]\n",
            "3940 [D loss: 0.548299, acc.: 72.27%] [G loss: 3.360701]\n",
            "3960 [D loss: 0.462850, acc.: 80.86%] [G loss: 3.864856]\n",
            "3980 [D loss: 0.456271, acc.: 80.08%] [G loss: 3.770685]\n",
            "4000 [D loss: 0.523453, acc.: 74.22%] [G loss: 3.741747]\n",
            "4020 [D loss: 0.413136, acc.: 80.47%] [G loss: 3.889089]\n",
            "4040 [D loss: 0.380443, acc.: 85.55%] [G loss: 3.371760]\n",
            "4060 [D loss: 0.518200, acc.: 75.39%] [G loss: 3.529043]\n",
            "4080 [D loss: 0.524756, acc.: 75.78%] [G loss: 4.188472]\n",
            "4100 [D loss: 0.431759, acc.: 80.86%] [G loss: 3.948927]\n",
            "4120 [D loss: 0.462378, acc.: 76.95%] [G loss: 4.372771]\n",
            "4140 [D loss: 0.410177, acc.: 82.03%] [G loss: 4.271484]\n",
            "4160 [D loss: 0.438372, acc.: 78.91%] [G loss: 4.332627]\n",
            "4180 [D loss: 0.406245, acc.: 83.98%] [G loss: 4.045570]\n",
            "4200 [D loss: 0.425405, acc.: 79.69%] [G loss: 3.941288]\n",
            "4220 [D loss: 0.458968, acc.: 78.52%] [G loss: 4.050042]\n",
            "4240 [D loss: 0.456712, acc.: 75.78%] [G loss: 4.253774]\n",
            "4260 [D loss: 0.447605, acc.: 80.08%] [G loss: 4.088403]\n",
            "4280 [D loss: 0.493892, acc.: 74.22%] [G loss: 3.997761]\n",
            "4300 [D loss: 0.506475, acc.: 73.44%] [G loss: 3.891933]\n",
            "4320 [D loss: 0.523100, acc.: 73.44%] [G loss: 3.609283]\n",
            "4340 [D loss: 0.476631, acc.: 74.22%] [G loss: 3.693956]\n",
            "4360 [D loss: 0.450906, acc.: 79.30%] [G loss: 4.125407]\n",
            "4380 [D loss: 0.401704, acc.: 83.59%] [G loss: 4.032945]\n",
            "4400 [D loss: 0.366981, acc.: 85.16%] [G loss: 4.965491]\n",
            "4420 [D loss: 0.483119, acc.: 78.12%] [G loss: 3.959276]\n",
            "4440 [D loss: 0.478138, acc.: 78.12%] [G loss: 4.065999]\n",
            "4460 [D loss: 0.513172, acc.: 75.00%] [G loss: 3.918742]\n",
            "4480 [D loss: 0.440384, acc.: 77.34%] [G loss: 3.804497]\n",
            "4500 [D loss: 0.424071, acc.: 80.08%] [G loss: 4.051526]\n",
            "4520 [D loss: 0.391699, acc.: 83.20%] [G loss: 4.338980]\n",
            "4540 [D loss: 0.429648, acc.: 77.34%] [G loss: 4.165473]\n",
            "4560 [D loss: 0.521977, acc.: 72.27%] [G loss: 3.860038]\n",
            "4580 [D loss: 0.494420, acc.: 75.78%] [G loss: 3.667827]\n",
            "4600 [D loss: 0.526068, acc.: 71.09%] [G loss: 3.937126]\n",
            "4620 [D loss: 0.466164, acc.: 77.73%] [G loss: 4.031190]\n",
            "4640 [D loss: 0.500973, acc.: 76.17%] [G loss: 4.000288]\n",
            "4660 [D loss: 0.462947, acc.: 79.30%] [G loss: 3.940690]\n",
            "4680 [D loss: 0.445987, acc.: 80.08%] [G loss: 3.975265]\n",
            "4700 [D loss: 0.338693, acc.: 88.67%] [G loss: 3.881504]\n",
            "4720 [D loss: 1.364135, acc.: 22.27%] [G loss: 3.498910]\n",
            "4740 [D loss: 1.220002, acc.: 24.61%] [G loss: 1.440901]\n",
            "4760 [D loss: 0.815321, acc.: 50.78%] [G loss: 2.027054]\n",
            "4780 [D loss: 0.527013, acc.: 73.44%] [G loss: 3.342511]\n",
            "4800 [D loss: 0.490507, acc.: 75.78%] [G loss: 3.929365]\n",
            "4820 [D loss: 0.469592, acc.: 76.56%] [G loss: 4.080887]\n",
            "4840 [D loss: 0.501878, acc.: 76.95%] [G loss: 3.811781]\n",
            "4860 [D loss: 0.512156, acc.: 77.34%] [G loss: 3.159840]\n",
            "4880 [D loss: 0.456744, acc.: 76.56%] [G loss: 3.744370]\n",
            "4900 [D loss: 0.418101, acc.: 83.20%] [G loss: 3.878281]\n",
            "4920 [D loss: 0.450059, acc.: 80.08%] [G loss: 3.872850]\n",
            "4940 [D loss: 0.494588, acc.: 76.95%] [G loss: 3.634113]\n",
            "4960 [D loss: 0.496361, acc.: 78.52%] [G loss: 3.841659]\n",
            "4980 [D loss: 0.537523, acc.: 75.78%] [G loss: 3.448776]\n",
            "5000 [D loss: 0.387303, acc.: 84.38%] [G loss: 4.449680]\n",
            "5020 [D loss: 0.446595, acc.: 81.64%] [G loss: 4.179013]\n",
            "5040 [D loss: 0.491357, acc.: 78.91%] [G loss: 4.075773]\n",
            "5060 [D loss: 0.494061, acc.: 77.34%] [G loss: 3.908288]\n",
            "5080 [D loss: 0.449637, acc.: 80.08%] [G loss: 4.150727]\n",
            "5100 [D loss: 0.534417, acc.: 70.70%] [G loss: 3.892555]\n",
            "5120 [D loss: 0.506463, acc.: 74.61%] [G loss: 3.522201]\n",
            "5140 [D loss: 0.540387, acc.: 73.44%] [G loss: 3.527501]\n",
            "5160 [D loss: 0.462005, acc.: 77.34%] [G loss: 4.123527]\n",
            "5180 [D loss: 0.484270, acc.: 77.34%] [G loss: 3.705617]\n",
            "5200 [D loss: 0.485644, acc.: 73.83%] [G loss: 4.230354]\n",
            "5220 [D loss: 0.521139, acc.: 74.22%] [G loss: 3.942693]\n",
            "5240 [D loss: 0.535371, acc.: 73.05%] [G loss: 3.471163]\n",
            "5260 [D loss: 0.445176, acc.: 79.69%] [G loss: 4.029202]\n",
            "5280 [D loss: 0.477804, acc.: 79.69%] [G loss: 3.866130]\n",
            "5300 [D loss: 0.550279, acc.: 73.44%] [G loss: 3.744251]\n",
            "5320 [D loss: 0.526031, acc.: 73.44%] [G loss: 3.403838]\n",
            "5340 [D loss: 0.461962, acc.: 79.69%] [G loss: 4.350829]\n",
            "5360 [D loss: 0.510082, acc.: 73.83%] [G loss: 3.866315]\n",
            "5380 [D loss: 0.499755, acc.: 74.61%] [G loss: 3.524013]\n",
            "5400 [D loss: 0.433077, acc.: 81.64%] [G loss: 4.295393]\n",
            "5420 [D loss: 0.462055, acc.: 78.91%] [G loss: 3.972794]\n",
            "5440 [D loss: 0.476090, acc.: 76.17%] [G loss: 3.947177]\n",
            "5460 [D loss: 0.386328, acc.: 86.33%] [G loss: 4.168908]\n",
            "5480 [D loss: 0.473253, acc.: 80.86%] [G loss: 4.079563]\n",
            "5500 [D loss: 0.472191, acc.: 79.30%] [G loss: 3.799051]\n",
            "5520 [D loss: 0.411991, acc.: 81.25%] [G loss: 4.349048]\n",
            "5540 [D loss: 0.474183, acc.: 79.69%] [G loss: 4.105576]\n",
            "5560 [D loss: 0.484537, acc.: 75.78%] [G loss: 4.006234]\n",
            "5580 [D loss: 0.518917, acc.: 76.56%] [G loss: 3.760057]\n",
            "5600 [D loss: 0.451715, acc.: 81.25%] [G loss: 4.220232]\n",
            "5620 [D loss: 0.433375, acc.: 80.08%] [G loss: 3.920801]\n",
            "5640 [D loss: 0.430463, acc.: 79.69%] [G loss: 4.275650]\n",
            "5660 [D loss: 0.524736, acc.: 73.44%] [G loss: 4.272923]\n",
            "5680 [D loss: 0.545726, acc.: 70.70%] [G loss: 3.449869]\n",
            "5700 [D loss: 0.431793, acc.: 76.95%] [G loss: 4.387297]\n",
            "5720 [D loss: 0.416544, acc.: 81.64%] [G loss: 4.229410]\n",
            "5740 [D loss: 0.480941, acc.: 76.95%] [G loss: 3.962344]\n",
            "5760 [D loss: 0.467794, acc.: 77.34%] [G loss: 3.821721]\n",
            "5780 [D loss: 0.448771, acc.: 77.34%] [G loss: 4.016366]\n",
            "5800 [D loss: 0.432905, acc.: 82.42%] [G loss: 4.255185]\n",
            "5820 [D loss: 0.465397, acc.: 78.91%] [G loss: 4.086635]\n",
            "5840 [D loss: 0.489000, acc.: 74.22%] [G loss: 3.801020]\n",
            "5860 [D loss: 0.433365, acc.: 82.42%] [G loss: 4.001574]\n",
            "5880 [D loss: 0.570723, acc.: 67.58%] [G loss: 3.340290]\n",
            "5900 [D loss: 1.184468, acc.: 40.62%] [G loss: 2.755370]\n",
            "5920 [D loss: 0.661499, acc.: 58.98%] [G loss: 1.248495]\n",
            "5940 [D loss: 1.434140, acc.: 19.14%] [G loss: 1.353012]\n",
            "5960 [D loss: 0.558359, acc.: 73.83%] [G loss: 3.883487]\n",
            "5980 [D loss: 0.449113, acc.: 78.52%] [G loss: 3.760363]\n",
            "6000 [D loss: 0.452505, acc.: 80.47%] [G loss: 3.661014]\n",
            "6020 [D loss: 0.531718, acc.: 71.48%] [G loss: 3.289155]\n",
            "6040 [D loss: 0.391921, acc.: 82.03%] [G loss: 4.357686]\n",
            "6060 [D loss: 0.558357, acc.: 72.66%] [G loss: 3.410869]\n",
            "6080 [D loss: 0.529585, acc.: 76.17%] [G loss: 3.552475]\n",
            "6100 [D loss: 0.621368, acc.: 67.19%] [G loss: 3.159965]\n",
            "6120 [D loss: 0.435864, acc.: 80.86%] [G loss: 3.791369]\n",
            "6140 [D loss: 0.514782, acc.: 73.44%] [G loss: 3.359994]\n",
            "6160 [D loss: 0.437265, acc.: 80.86%] [G loss: 3.959708]\n",
            "6180 [D loss: 0.517861, acc.: 75.39%] [G loss: 3.651121]\n",
            "6200 [D loss: 0.518135, acc.: 73.44%] [G loss: 3.404040]\n",
            "6220 [D loss: 0.458233, acc.: 76.95%] [G loss: 3.610273]\n",
            "6240 [D loss: 0.453167, acc.: 80.08%] [G loss: 3.671097]\n",
            "6260 [D loss: 0.403220, acc.: 84.38%] [G loss: 3.931660]\n",
            "6280 [D loss: 0.447648, acc.: 78.91%] [G loss: 4.014430]\n",
            "6300 [D loss: 0.404328, acc.: 81.64%] [G loss: 4.651325]\n",
            "6320 [D loss: 0.416422, acc.: 82.03%] [G loss: 3.755052]\n",
            "6340 [D loss: 0.489167, acc.: 76.95%] [G loss: 3.427245]\n",
            "6360 [D loss: 0.523445, acc.: 74.61%] [G loss: 3.770679]\n",
            "6380 [D loss: 0.446362, acc.: 80.08%] [G loss: 3.598990]\n",
            "6400 [D loss: 0.492297, acc.: 76.17%] [G loss: 3.748757]\n",
            "6420 [D loss: 0.558397, acc.: 68.36%] [G loss: 3.754236]\n",
            "6440 [D loss: 0.453882, acc.: 77.73%] [G loss: 3.590938]\n",
            "6460 [D loss: 0.549618, acc.: 72.66%] [G loss: 3.753695]\n",
            "6480 [D loss: 0.430891, acc.: 80.08%] [G loss: 3.991786]\n",
            "6500 [D loss: 0.519057, acc.: 75.00%] [G loss: 3.353537]\n",
            "6520 [D loss: 0.437584, acc.: 80.08%] [G loss: 4.426554]\n",
            "6540 [D loss: 0.499923, acc.: 75.78%] [G loss: 3.499173]\n",
            "6560 [D loss: 0.466319, acc.: 80.08%] [G loss: 3.744201]\n",
            "6580 [D loss: 0.470756, acc.: 76.56%] [G loss: 3.738044]\n",
            "6600 [D loss: 0.395009, acc.: 85.94%] [G loss: 4.261152]\n",
            "6620 [D loss: 0.443802, acc.: 78.91%] [G loss: 4.273849]\n",
            "6640 [D loss: 0.475945, acc.: 79.30%] [G loss: 3.712089]\n",
            "6660 [D loss: 0.489327, acc.: 78.52%] [G loss: 3.837448]\n",
            "6680 [D loss: 0.517320, acc.: 75.78%] [G loss: 3.994417]\n",
            "6700 [D loss: 0.470912, acc.: 75.78%] [G loss: 4.399851]\n",
            "6720 [D loss: 0.431276, acc.: 83.59%] [G loss: 3.888953]\n",
            "6740 [D loss: 0.464994, acc.: 78.52%] [G loss: 3.740701]\n",
            "6760 [D loss: 0.534988, acc.: 72.27%] [G loss: 3.620614]\n",
            "6780 [D loss: 0.400307, acc.: 84.38%] [G loss: 4.062611]\n",
            "6800 [D loss: 0.480052, acc.: 76.17%] [G loss: 3.882315]\n",
            "6820 [D loss: 0.395697, acc.: 84.38%] [G loss: 4.012372]\n",
            "6840 [D loss: 0.414366, acc.: 80.08%] [G loss: 4.492537]\n",
            "6860 [D loss: 0.515453, acc.: 75.78%] [G loss: 3.840596]\n",
            "6880 [D loss: 0.392044, acc.: 82.81%] [G loss: 4.691299]\n",
            "6900 [D loss: 0.430791, acc.: 79.30%] [G loss: 4.203332]\n",
            "6920 [D loss: 0.481881, acc.: 76.95%] [G loss: 3.824632]\n",
            "6940 [D loss: 0.490377, acc.: 76.17%] [G loss: 3.669713]\n",
            "6960 [D loss: 0.401364, acc.: 83.59%] [G loss: 4.638701]\n",
            "6980 [D loss: 0.428792, acc.: 82.42%] [G loss: 3.883505]\n",
            "7000 [D loss: 0.440642, acc.: 79.30%] [G loss: 3.723340]\n",
            "7020 [D loss: 0.602773, acc.: 69.14%] [G loss: 4.041219]\n",
            "7040 [D loss: 0.472020, acc.: 76.17%] [G loss: 3.515181]\n",
            "7060 [D loss: 0.545482, acc.: 73.05%] [G loss: 4.035499]\n",
            "7080 [D loss: 0.414555, acc.: 83.20%] [G loss: 4.098302]\n",
            "7100 [D loss: 0.529623, acc.: 72.27%] [G loss: 4.301728]\n",
            "7120 [D loss: 0.555657, acc.: 70.70%] [G loss: 3.897317]\n",
            "7140 [D loss: 0.435829, acc.: 80.86%] [G loss: 4.367137]\n",
            "7160 [D loss: 0.549072, acc.: 71.48%] [G loss: 3.487244]\n",
            "7180 [D loss: 0.571417, acc.: 67.97%] [G loss: 3.973169]\n",
            "7200 [D loss: 0.449170, acc.: 78.91%] [G loss: 4.194701]\n",
            "7220 [D loss: 0.421336, acc.: 81.64%] [G loss: 3.949815]\n",
            "7240 [D loss: 0.443558, acc.: 78.91%] [G loss: 4.082131]\n",
            "7260 [D loss: 0.442217, acc.: 79.69%] [G loss: 4.423971]\n",
            "7280 [D loss: 0.496390, acc.: 78.52%] [G loss: 4.187441]\n",
            "7300 [D loss: 0.430028, acc.: 78.52%] [G loss: 4.230793]\n",
            "7320 [D loss: 0.600982, acc.: 66.41%] [G loss: 3.600776]\n",
            "7340 [D loss: 0.417268, acc.: 81.25%] [G loss: 4.087482]\n",
            "7360 [D loss: 0.539738, acc.: 74.61%] [G loss: 3.834679]\n",
            "7380 [D loss: 0.479893, acc.: 78.12%] [G loss: 4.084527]\n",
            "7400 [D loss: 0.485716, acc.: 77.34%] [G loss: 4.698217]\n",
            "7420 [D loss: 0.405280, acc.: 80.47%] [G loss: 4.788971]\n",
            "7440 [D loss: 0.439409, acc.: 80.47%] [G loss: 4.280563]\n",
            "7460 [D loss: 0.432841, acc.: 79.69%] [G loss: 4.161861]\n",
            "7480 [D loss: 0.416406, acc.: 81.25%] [G loss: 4.827913]\n",
            "7500 [D loss: 0.447153, acc.: 82.03%] [G loss: 3.923577]\n",
            "7520 [D loss: 0.417991, acc.: 80.86%] [G loss: 3.985697]\n",
            "7540 [D loss: 0.401134, acc.: 84.38%] [G loss: 4.309304]\n",
            "7560 [D loss: 0.465119, acc.: 76.56%] [G loss: 4.041121]\n",
            "7580 [D loss: 0.483935, acc.: 76.17%] [G loss: 4.425219]\n",
            "7600 [D loss: 0.467907, acc.: 78.91%] [G loss: 4.081348]\n",
            "7620 [D loss: 0.434769, acc.: 80.08%] [G loss: 4.078183]\n",
            "7640 [D loss: 0.425896, acc.: 78.52%] [G loss: 4.359330]\n",
            "7660 [D loss: 0.396057, acc.: 82.81%] [G loss: 4.607359]\n",
            "7680 [D loss: 0.461577, acc.: 77.73%] [G loss: 4.207151]\n",
            "7700 [D loss: 0.447097, acc.: 76.17%] [G loss: 3.983339]\n",
            "7720 [D loss: 0.395722, acc.: 82.81%] [G loss: 4.307073]\n",
            "7740 [D loss: 0.440221, acc.: 78.52%] [G loss: 4.509144]\n",
            "7760 [D loss: 0.441964, acc.: 80.08%] [G loss: 4.438655]\n",
            "7780 [D loss: 0.383546, acc.: 83.98%] [G loss: 4.573860]\n",
            "7800 [D loss: 0.436608, acc.: 80.47%] [G loss: 4.189570]\n",
            "7820 [D loss: 0.485452, acc.: 78.52%] [G loss: 4.388052]\n",
            "7840 [D loss: 0.468393, acc.: 77.73%] [G loss: 4.009923]\n",
            "7860 [D loss: 0.523841, acc.: 74.22%] [G loss: 4.318661]\n",
            "7880 [D loss: 0.581120, acc.: 73.44%] [G loss: 3.588532]\n",
            "7900 [D loss: 0.627213, acc.: 67.19%] [G loss: 3.830516]\n",
            "7920 [D loss: 0.662497, acc.: 67.58%] [G loss: 3.500309]\n",
            "7940 [D loss: 0.521467, acc.: 75.78%] [G loss: 4.443075]\n",
            "7960 [D loss: 0.448294, acc.: 80.47%] [G loss: 4.385406]\n",
            "7980 [D loss: 0.525751, acc.: 72.27%] [G loss: 3.909022]\n",
            "8000 [D loss: 0.405081, acc.: 82.81%] [G loss: 3.010660]\n",
            "8020 [D loss: 0.814304, acc.: 57.03%] [G loss: 4.047432]\n",
            "8040 [D loss: 0.761669, acc.: 61.72%] [G loss: 4.456038]\n",
            "8060 [D loss: 0.829897, acc.: 60.16%] [G loss: 4.032511]\n",
            "8080 [D loss: 0.471897, acc.: 77.34%] [G loss: 4.889156]\n",
            "8100 [D loss: 0.364450, acc.: 85.16%] [G loss: 4.754349]\n",
            "8120 [D loss: 0.493843, acc.: 76.95%] [G loss: 4.203110]\n",
            "8140 [D loss: 0.560520, acc.: 73.83%] [G loss: 4.330490]\n",
            "8160 [D loss: 0.503012, acc.: 75.39%] [G loss: 3.978580]\n",
            "8180 [D loss: 0.403843, acc.: 83.59%] [G loss: 4.379108]\n",
            "8200 [D loss: 0.395414, acc.: 81.64%] [G loss: 4.853371]\n",
            "8220 [D loss: 0.434186, acc.: 80.08%] [G loss: 4.434687]\n",
            "8240 [D loss: 0.466415, acc.: 77.34%] [G loss: 4.071208]\n",
            "8260 [D loss: 0.385557, acc.: 82.03%] [G loss: 4.877956]\n",
            "8280 [D loss: 0.343655, acc.: 85.55%] [G loss: 5.068187]\n",
            "8300 [D loss: 0.391632, acc.: 83.20%] [G loss: 4.461371]\n",
            "8320 [D loss: 0.450425, acc.: 79.30%] [G loss: 4.721270]\n",
            "8340 [D loss: 0.435197, acc.: 76.95%] [G loss: 4.621767]\n",
            "8360 [D loss: 0.464482, acc.: 76.56%] [G loss: 4.313802]\n",
            "8380 [D loss: 0.464122, acc.: 81.25%] [G loss: 4.049200]\n",
            "8400 [D loss: 0.413482, acc.: 82.81%] [G loss: 4.400159]\n",
            "8420 [D loss: 0.450674, acc.: 77.34%] [G loss: 4.579131]\n",
            "8440 [D loss: 0.419680, acc.: 82.03%] [G loss: 5.217090]\n",
            "8460 [D loss: 0.483819, acc.: 76.17%] [G loss: 4.116592]\n",
            "8480 [D loss: 0.521834, acc.: 75.39%] [G loss: 4.632746]\n",
            "8500 [D loss: 0.444417, acc.: 79.30%] [G loss: 4.122975]\n",
            "8520 [D loss: 0.467254, acc.: 76.17%] [G loss: 4.956676]\n",
            "8540 [D loss: 0.454999, acc.: 81.25%] [G loss: 4.244261]\n",
            "8560 [D loss: 0.445828, acc.: 80.86%] [G loss: 4.550985]\n",
            "8580 [D loss: 0.515280, acc.: 76.56%] [G loss: 4.022831]\n",
            "8600 [D loss: 0.414122, acc.: 81.25%] [G loss: 4.489841]\n",
            "8620 [D loss: 0.445019, acc.: 80.08%] [G loss: 4.639054]\n",
            "8640 [D loss: 0.433097, acc.: 80.08%] [G loss: 4.542360]\n",
            "8660 [D loss: 0.508373, acc.: 75.78%] [G loss: 4.007074]\n",
            "8680 [D loss: 0.494769, acc.: 78.52%] [G loss: 4.564508]\n",
            "8700 [D loss: 0.461846, acc.: 78.91%] [G loss: 4.268891]\n",
            "8720 [D loss: 0.558022, acc.: 70.31%] [G loss: 4.238678]\n",
            "8740 [D loss: 0.403040, acc.: 79.69%] [G loss: 4.628141]\n",
            "8760 [D loss: 0.450536, acc.: 79.30%] [G loss: 4.304743]\n",
            "8780 [D loss: 0.474493, acc.: 78.91%] [G loss: 4.660646]\n",
            "8800 [D loss: 0.416110, acc.: 80.86%] [G loss: 4.808954]\n",
            "8820 [D loss: 0.419097, acc.: 81.64%] [G loss: 4.747538]\n",
            "8840 [D loss: 0.523655, acc.: 74.22%] [G loss: 4.743878]\n",
            "8860 [D loss: 0.418037, acc.: 82.42%] [G loss: 4.118615]\n",
            "8880 [D loss: 0.424758, acc.: 79.69%] [G loss: 4.643620]\n",
            "8900 [D loss: 0.415731, acc.: 83.59%] [G loss: 4.486989]\n",
            "8920 [D loss: 0.442579, acc.: 82.81%] [G loss: 4.787256]\n",
            "8940 [D loss: 0.378622, acc.: 84.77%] [G loss: 4.496308]\n",
            "8960 [D loss: 0.372041, acc.: 83.59%] [G loss: 4.886470]\n",
            "8980 [D loss: 0.446799, acc.: 82.03%] [G loss: 4.024635]\n",
            "9000 [D loss: 0.406316, acc.: 83.98%] [G loss: 4.558497]\n",
            "9020 [D loss: 0.429719, acc.: 83.98%] [G loss: 4.531271]\n",
            "9040 [D loss: 0.460592, acc.: 76.95%] [G loss: 4.689012]\n",
            "9060 [D loss: 0.357934, acc.: 85.94%] [G loss: 4.852410]\n",
            "9080 [D loss: 0.388284, acc.: 83.59%] [G loss: 4.941038]\n",
            "9100 [D loss: 0.435654, acc.: 80.47%] [G loss: 4.733607]\n",
            "9120 [D loss: 0.362489, acc.: 84.38%] [G loss: 4.873433]\n",
            "9140 [D loss: 0.408110, acc.: 83.20%] [G loss: 4.610844]\n",
            "9160 [D loss: 0.429552, acc.: 79.69%] [G loss: 4.707217]\n",
            "9180 [D loss: 0.391391, acc.: 83.98%] [G loss: 4.713454]\n",
            "9200 [D loss: 0.421143, acc.: 79.69%] [G loss: 4.861547]\n",
            "9220 [D loss: 0.454530, acc.: 77.73%] [G loss: 4.625570]\n",
            "9240 [D loss: 0.399664, acc.: 82.42%] [G loss: 5.000984]\n",
            "9260 [D loss: 0.430951, acc.: 80.08%] [G loss: 4.620400]\n",
            "9280 [D loss: 0.430116, acc.: 79.30%] [G loss: 4.291290]\n",
            "9300 [D loss: 0.428269, acc.: 80.08%] [G loss: 4.625419]\n",
            "9320 [D loss: 0.442564, acc.: 81.25%] [G loss: 4.962641]\n",
            "9340 [D loss: 0.422421, acc.: 81.64%] [G loss: 5.114931]\n",
            "9360 [D loss: 0.430364, acc.: 78.91%] [G loss: 4.474069]\n",
            "9380 [D loss: 0.399250, acc.: 83.20%] [G loss: 5.020226]\n",
            "9400 [D loss: 0.503349, acc.: 74.61%] [G loss: 4.808851]\n",
            "9420 [D loss: 0.329813, acc.: 86.33%] [G loss: 5.074733]\n",
            "9440 [D loss: 0.436401, acc.: 78.12%] [G loss: 4.595596]\n",
            "9460 [D loss: 0.496773, acc.: 78.91%] [G loss: 4.817913]\n",
            "9480 [D loss: 0.497857, acc.: 76.95%] [G loss: 5.215463]\n",
            "9500 [D loss: 0.463829, acc.: 76.56%] [G loss: 4.363512]\n",
            "9520 [D loss: 0.434687, acc.: 78.91%] [G loss: 4.770395]\n",
            "9540 [D loss: 0.496067, acc.: 78.12%] [G loss: 3.881543]\n",
            "9560 [D loss: 0.213889, acc.: 91.41%] [G loss: 3.149117]\n",
            "9580 [D loss: 0.734178, acc.: 64.45%] [G loss: 2.862748]\n",
            "9600 [D loss: 0.818585, acc.: 60.16%] [G loss: 6.183168]\n",
            "9620 [D loss: 0.551982, acc.: 74.61%] [G loss: 4.424582]\n",
            "9640 [D loss: 0.323390, acc.: 88.28%] [G loss: 3.019742]\n",
            "9660 [D loss: 0.525289, acc.: 71.48%] [G loss: 3.331948]\n",
            "9680 [D loss: 0.628522, acc.: 67.58%] [G loss: 4.980312]\n",
            "9700 [D loss: 0.536022, acc.: 74.61%] [G loss: 4.950047]\n",
            "9720 [D loss: 0.396292, acc.: 80.86%] [G loss: 4.004656]\n",
            "9740 [D loss: 0.527533, acc.: 75.78%] [G loss: 4.494216]\n",
            "9760 [D loss: 0.402482, acc.: 82.03%] [G loss: 5.241381]\n",
            "9780 [D loss: 0.353052, acc.: 86.72%] [G loss: 4.668776]\n",
            "9800 [D loss: 0.456102, acc.: 78.52%] [G loss: 4.356057]\n",
            "9820 [D loss: 0.378511, acc.: 83.59%] [G loss: 5.101445]\n",
            "9840 [D loss: 0.415848, acc.: 81.25%] [G loss: 4.853883]\n",
            "9860 [D loss: 0.412939, acc.: 80.47%] [G loss: 4.987749]\n",
            "9880 [D loss: 0.361143, acc.: 85.55%] [G loss: 5.019882]\n",
            "9900 [D loss: 0.509136, acc.: 73.05%] [G loss: 4.587637]\n",
            "9920 [D loss: 0.474729, acc.: 74.22%] [G loss: 4.705740]\n",
            "9940 [D loss: 0.403932, acc.: 81.64%] [G loss: 4.743962]\n",
            "9960 [D loss: 0.443180, acc.: 78.12%] [G loss: 4.487846]\n",
            "9980 [D loss: 0.418630, acc.: 79.69%] [G loss: 4.729626]\n",
            "10000 [D loss: 0.446188, acc.: 78.91%] [G loss: 4.882220]\n",
            "10020 [D loss: 0.410152, acc.: 82.81%] [G loss: 5.015415]\n",
            "10040 [D loss: 0.575711, acc.: 68.75%] [G loss: 4.078319]\n",
            "10060 [D loss: 0.401933, acc.: 80.86%] [G loss: 4.899767]\n",
            "10080 [D loss: 0.486373, acc.: 76.95%] [G loss: 4.203418]\n",
            "10100 [D loss: 0.448979, acc.: 79.30%] [G loss: 4.505579]\n",
            "10120 [D loss: 0.464840, acc.: 76.56%] [G loss: 4.587682]\n",
            "10140 [D loss: 0.419798, acc.: 79.30%] [G loss: 4.897750]\n",
            "10160 [D loss: 0.375199, acc.: 83.20%] [G loss: 4.840167]\n",
            "10180 [D loss: 0.358824, acc.: 85.55%] [G loss: 4.893845]\n",
            "10200 [D loss: 0.384754, acc.: 83.98%] [G loss: 5.320091]\n",
            "10220 [D loss: 0.370766, acc.: 83.59%] [G loss: 4.855222]\n",
            "10240 [D loss: 0.485331, acc.: 75.00%] [G loss: 4.786329]\n",
            "10260 [D loss: 0.372941, acc.: 82.03%] [G loss: 4.966037]\n",
            "10280 [D loss: 0.422915, acc.: 80.86%] [G loss: 4.900920]\n",
            "10300 [D loss: 0.495204, acc.: 77.34%] [G loss: 5.107511]\n",
            "10320 [D loss: 0.411564, acc.: 81.64%] [G loss: 5.302939]\n",
            "10340 [D loss: 0.463300, acc.: 77.34%] [G loss: 5.056416]\n",
            "10360 [D loss: 0.366472, acc.: 84.77%] [G loss: 4.528131]\n",
            "10380 [D loss: 0.433938, acc.: 79.69%] [G loss: 4.815724]\n",
            "10400 [D loss: 0.425014, acc.: 80.47%] [G loss: 4.701516]\n",
            "10420 [D loss: 0.451267, acc.: 80.86%] [G loss: 4.619641]\n",
            "10440 [D loss: 0.332426, acc.: 87.11%] [G loss: 4.934545]\n",
            "10460 [D loss: 0.369458, acc.: 83.59%] [G loss: 4.915126]\n",
            "10480 [D loss: 0.396546, acc.: 83.20%] [G loss: 5.143405]\n",
            "10500 [D loss: 0.342190, acc.: 85.55%] [G loss: 4.603179]\n",
            "10520 [D loss: 0.467440, acc.: 78.91%] [G loss: 4.477571]\n",
            "10540 [D loss: 0.436679, acc.: 73.83%] [G loss: 5.084624]\n",
            "10560 [D loss: 0.385656, acc.: 82.81%] [G loss: 5.269370]\n",
            "10580 [D loss: 0.494068, acc.: 77.34%] [G loss: 5.014127]\n",
            "10600 [D loss: 0.394993, acc.: 81.25%] [G loss: 5.241515]\n",
            "10620 [D loss: 0.396303, acc.: 80.86%] [G loss: 5.528327]\n",
            "10640 [D loss: 0.268321, acc.: 89.84%] [G loss: 3.594240]\n",
            "10660 [D loss: 0.333500, acc.: 84.38%] [G loss: 3.331997]\n",
            "10680 [D loss: 0.836885, acc.: 58.20%] [G loss: 4.790165]\n",
            "10700 [D loss: 0.451292, acc.: 78.91%] [G loss: 5.634530]\n",
            "10720 [D loss: 0.523128, acc.: 75.00%] [G loss: 5.629357]\n",
            "10740 [D loss: 0.393909, acc.: 81.64%] [G loss: 5.403531]\n",
            "10760 [D loss: 0.495837, acc.: 76.56%] [G loss: 5.855182]\n",
            "10780 [D loss: 0.390965, acc.: 84.38%] [G loss: 5.348322]\n",
            "10800 [D loss: 0.451935, acc.: 79.30%] [G loss: 5.160943]\n",
            "10820 [D loss: 0.355188, acc.: 87.11%] [G loss: 4.911385]\n",
            "10840 [D loss: 0.496621, acc.: 76.56%] [G loss: 4.953232]\n",
            "10860 [D loss: 0.451013, acc.: 79.69%] [G loss: 4.963518]\n",
            "10880 [D loss: 0.440517, acc.: 75.39%] [G loss: 5.434465]\n",
            "10900 [D loss: 0.489761, acc.: 77.73%] [G loss: 5.425671]\n",
            "10920 [D loss: 0.419537, acc.: 81.64%] [G loss: 5.636090]\n",
            "10940 [D loss: 0.404790, acc.: 82.81%] [G loss: 5.259549]\n",
            "10960 [D loss: 0.350341, acc.: 85.16%] [G loss: 5.319713]\n",
            "10980 [D loss: 0.359147, acc.: 83.59%] [G loss: 4.913852]\n",
            "11000 [D loss: 0.399719, acc.: 82.81%] [G loss: 5.288548]\n",
            "11020 [D loss: 0.399698, acc.: 82.03%] [G loss: 5.430717]\n",
            "11040 [D loss: 0.419493, acc.: 80.08%] [G loss: 5.009459]\n",
            "11060 [D loss: 0.464421, acc.: 78.91%] [G loss: 4.775235]\n",
            "11080 [D loss: 0.344406, acc.: 85.94%] [G loss: 5.122595]\n",
            "11100 [D loss: 0.347601, acc.: 84.77%] [G loss: 4.798695]\n",
            "11120 [D loss: 0.492728, acc.: 75.78%] [G loss: 4.605649]\n",
            "11140 [D loss: 0.417832, acc.: 80.86%] [G loss: 5.458483]\n",
            "11160 [D loss: 0.422043, acc.: 82.81%] [G loss: 5.109190]\n",
            "11180 [D loss: 0.340653, acc.: 86.33%] [G loss: 5.689024]\n",
            "11200 [D loss: 0.389694, acc.: 83.98%] [G loss: 5.411591]\n",
            "11220 [D loss: 0.406294, acc.: 76.95%] [G loss: 4.929880]\n",
            "11240 [D loss: 0.382116, acc.: 84.77%] [G loss: 5.208610]\n",
            "11260 [D loss: 0.386659, acc.: 83.98%] [G loss: 5.481782]\n",
            "11280 [D loss: 0.366539, acc.: 85.16%] [G loss: 5.337728]\n",
            "11300 [D loss: 0.457340, acc.: 78.52%] [G loss: 4.559757]\n",
            "11320 [D loss: 0.454996, acc.: 78.12%] [G loss: 5.450555]\n",
            "11340 [D loss: 0.399445, acc.: 82.42%] [G loss: 4.878756]\n",
            "11360 [D loss: 0.438293, acc.: 80.47%] [G loss: 5.147255]\n",
            "11380 [D loss: 0.472171, acc.: 77.34%] [G loss: 4.640181]\n",
            "11400 [D loss: 0.421914, acc.: 80.08%] [G loss: 4.849022]\n",
            "11420 [D loss: 0.444233, acc.: 79.69%] [G loss: 4.905552]\n",
            "11440 [D loss: 0.413176, acc.: 81.25%] [G loss: 5.142303]\n",
            "11460 [D loss: 0.431507, acc.: 81.25%] [G loss: 4.816772]\n",
            "11480 [D loss: 0.383659, acc.: 83.20%] [G loss: 5.196069]\n",
            "11500 [D loss: 0.385614, acc.: 82.81%] [G loss: 5.332002]\n",
            "11520 [D loss: 0.393623, acc.: 80.47%] [G loss: 5.251717]\n",
            "11540 [D loss: 0.440175, acc.: 79.30%] [G loss: 4.752868]\n",
            "11560 [D loss: 0.448988, acc.: 80.08%] [G loss: 5.215461]\n",
            "11580 [D loss: 0.425406, acc.: 81.64%] [G loss: 5.187115]\n",
            "11600 [D loss: 0.393310, acc.: 81.25%] [G loss: 5.125435]\n",
            "11620 [D loss: 0.371746, acc.: 83.98%] [G loss: 5.713981]\n",
            "11640 [D loss: 0.370914, acc.: 83.59%] [G loss: 5.438190]\n",
            "11660 [D loss: 0.404276, acc.: 80.08%] [G loss: 5.812816]\n",
            "11680 [D loss: 0.311055, acc.: 87.89%] [G loss: 6.133957]\n",
            "11700 [D loss: 0.377688, acc.: 84.77%] [G loss: 5.967576]\n",
            "11720 [D loss: 0.404799, acc.: 79.69%] [G loss: 5.185723]\n",
            "11740 [D loss: 0.358628, acc.: 83.20%] [G loss: 4.971561]\n",
            "11760 [D loss: 0.355555, acc.: 83.98%] [G loss: 5.480636]\n",
            "11780 [D loss: 0.350601, acc.: 85.55%] [G loss: 4.888783]\n",
            "11800 [D loss: 0.497967, acc.: 75.78%] [G loss: 5.226287]\n",
            "11820 [D loss: 0.428572, acc.: 78.91%] [G loss: 5.246504]\n",
            "11840 [D loss: 0.427988, acc.: 78.52%] [G loss: 5.202655]\n",
            "11860 [D loss: 0.361948, acc.: 85.16%] [G loss: 5.613132]\n",
            "11880 [D loss: 0.421719, acc.: 81.64%] [G loss: 4.929526]\n",
            "11900 [D loss: 0.442316, acc.: 82.03%] [G loss: 5.717480]\n",
            "11920 [D loss: 0.377654, acc.: 80.47%] [G loss: 5.767615]\n",
            "11940 [D loss: 0.360441, acc.: 85.16%] [G loss: 4.312012]\n",
            "11960 [D loss: 0.300574, acc.: 88.28%] [G loss: 3.245686]\n",
            "11980 [D loss: 0.933560, acc.: 55.47%] [G loss: 4.099139]\n",
            "12000 [D loss: 0.567492, acc.: 74.22%] [G loss: 5.601203]\n",
            "12020 [D loss: 0.394550, acc.: 83.59%] [G loss: 5.870492]\n",
            "12040 [D loss: 0.384195, acc.: 83.98%] [G loss: 5.649734]\n",
            "12060 [D loss: 0.433226, acc.: 78.91%] [G loss: 5.412020]\n",
            "12080 [D loss: 0.384923, acc.: 83.20%] [G loss: 5.922874]\n",
            "12100 [D loss: 0.380173, acc.: 82.03%] [G loss: 5.267431]\n",
            "12120 [D loss: 0.354983, acc.: 86.72%] [G loss: 5.448306]\n",
            "12140 [D loss: 0.466459, acc.: 77.73%] [G loss: 5.710581]\n",
            "12160 [D loss: 0.262427, acc.: 91.41%] [G loss: 6.133436]\n",
            "12180 [D loss: 0.414443, acc.: 78.52%] [G loss: 5.536724]\n",
            "12200 [D loss: 0.372253, acc.: 84.38%] [G loss: 6.058190]\n",
            "12220 [D loss: 0.347257, acc.: 85.55%] [G loss: 5.480070]\n",
            "12240 [D loss: 0.367087, acc.: 84.38%] [G loss: 6.272826]\n",
            "12260 [D loss: 0.391568, acc.: 79.30%] [G loss: 5.323415]\n",
            "12280 [D loss: 0.417134, acc.: 78.91%] [G loss: 5.522684]\n",
            "12300 [D loss: 0.383903, acc.: 81.25%] [G loss: 5.519178]\n",
            "12320 [D loss: 0.378010, acc.: 83.59%] [G loss: 5.978211]\n",
            "12340 [D loss: 0.326033, acc.: 86.33%] [G loss: 5.958590]\n",
            "12360 [D loss: 0.329661, acc.: 87.11%] [G loss: 5.882007]\n",
            "12380 [D loss: 0.371883, acc.: 85.16%] [G loss: 5.497624]\n",
            "12400 [D loss: 0.403750, acc.: 80.86%] [G loss: 5.336827]\n",
            "12420 [D loss: 0.358533, acc.: 85.16%] [G loss: 5.611672]\n",
            "12440 [D loss: 0.404064, acc.: 81.25%] [G loss: 5.417833]\n",
            "12460 [D loss: 0.457135, acc.: 77.73%] [G loss: 5.015707]\n",
            "12480 [D loss: 0.370300, acc.: 83.98%] [G loss: 5.379239]\n",
            "12500 [D loss: 0.376555, acc.: 85.16%] [G loss: 5.889815]\n",
            "12520 [D loss: 0.426866, acc.: 79.30%] [G loss: 5.560868]\n",
            "12540 [D loss: 0.398566, acc.: 81.64%] [G loss: 5.532619]\n",
            "12560 [D loss: 0.331112, acc.: 85.16%] [G loss: 5.518989]\n",
            "12580 [D loss: 0.414125, acc.: 82.42%] [G loss: 6.373175]\n",
            "12600 [D loss: 0.398754, acc.: 81.64%] [G loss: 5.113152]\n",
            "12620 [D loss: 0.441586, acc.: 77.34%] [G loss: 5.798169]\n",
            "12640 [D loss: 0.422790, acc.: 80.47%] [G loss: 5.325742]\n",
            "12660 [D loss: 0.356213, acc.: 85.94%] [G loss: 6.112675]\n",
            "12680 [D loss: 0.409566, acc.: 78.91%] [G loss: 5.686810]\n",
            "12700 [D loss: 0.375890, acc.: 82.81%] [G loss: 5.608773]\n",
            "12720 [D loss: 0.309722, acc.: 87.11%] [G loss: 6.142492]\n",
            "12740 [D loss: 0.316013, acc.: 89.45%] [G loss: 5.183302]\n",
            "12760 [D loss: 0.332229, acc.: 85.94%] [G loss: 5.554205]\n",
            "12780 [D loss: 0.473701, acc.: 78.12%] [G loss: 5.508623]\n",
            "12800 [D loss: 0.326893, acc.: 85.94%] [G loss: 5.961319]\n",
            "12820 [D loss: 0.379821, acc.: 82.03%] [G loss: 5.709311]\n",
            "12840 [D loss: 0.348136, acc.: 86.72%] [G loss: 5.733764]\n",
            "12860 [D loss: 0.365921, acc.: 85.94%] [G loss: 5.267403]\n",
            "12880 [D loss: 0.473818, acc.: 78.12%] [G loss: 5.015100]\n",
            "12900 [D loss: 0.388453, acc.: 83.59%] [G loss: 5.632430]\n",
            "12920 [D loss: 0.412578, acc.: 80.86%] [G loss: 5.292470]\n",
            "12940 [D loss: 0.420454, acc.: 79.69%] [G loss: 6.090324]\n",
            "12960 [D loss: 0.330676, acc.: 84.38%] [G loss: 6.274776]\n",
            "12980 [D loss: 0.325952, acc.: 87.89%] [G loss: 6.080885]\n",
            "13000 [D loss: 0.392209, acc.: 82.03%] [G loss: 6.153457]\n",
            "13020 [D loss: 0.291795, acc.: 89.45%] [G loss: 5.745456]\n",
            "13040 [D loss: 0.434288, acc.: 80.47%] [G loss: 5.643303]\n",
            "13060 [D loss: 0.468479, acc.: 76.95%] [G loss: 5.241950]\n",
            "13080 [D loss: 0.336457, acc.: 85.94%] [G loss: 6.132161]\n",
            "13100 [D loss: 0.366932, acc.: 84.77%] [G loss: 6.261245]\n",
            "13120 [D loss: 0.327160, acc.: 86.72%] [G loss: 6.042880]\n",
            "13140 [D loss: 0.347647, acc.: 84.77%] [G loss: 6.094627]\n",
            "13160 [D loss: 0.414867, acc.: 79.69%] [G loss: 5.992234]\n",
            "13180 [D loss: 0.314972, acc.: 87.50%] [G loss: 5.822639]\n",
            "13200 [D loss: 0.347329, acc.: 84.77%] [G loss: 5.449367]\n",
            "13220 [D loss: 0.371441, acc.: 83.59%] [G loss: 5.533103]\n",
            "13240 [D loss: 0.395764, acc.: 80.08%] [G loss: 5.804195]\n",
            "13260 [D loss: 0.339982, acc.: 86.33%] [G loss: 6.107881]\n",
            "13280 [D loss: 0.396547, acc.: 82.03%] [G loss: 5.205791]\n",
            "13300 [D loss: 0.264270, acc.: 91.02%] [G loss: 5.423467]\n",
            "13320 [D loss: 0.719425, acc.: 67.97%] [G loss: 4.895162]\n",
            "13340 [D loss: 0.291886, acc.: 90.23%] [G loss: 5.449629]\n",
            "13360 [D loss: 0.339581, acc.: 84.38%] [G loss: 6.392636]\n",
            "13380 [D loss: 0.471879, acc.: 76.56%] [G loss: 5.679469]\n",
            "13400 [D loss: 0.461517, acc.: 79.30%] [G loss: 5.820807]\n",
            "13420 [D loss: 0.346495, acc.: 82.42%] [G loss: 5.653154]\n",
            "13440 [D loss: 0.389049, acc.: 83.59%] [G loss: 5.567899]\n",
            "13460 [D loss: 0.269496, acc.: 88.28%] [G loss: 6.195388]\n",
            "13480 [D loss: 0.359417, acc.: 83.98%] [G loss: 5.761148]\n",
            "13500 [D loss: 0.426787, acc.: 79.69%] [G loss: 6.347794]\n",
            "13520 [D loss: 0.347586, acc.: 86.33%] [G loss: 5.982882]\n",
            "13540 [D loss: 0.339921, acc.: 85.94%] [G loss: 6.056694]\n",
            "13560 [D loss: 0.439902, acc.: 83.20%] [G loss: 5.242191]\n",
            "13580 [D loss: 0.372562, acc.: 85.94%] [G loss: 7.309321]\n",
            "13600 [D loss: 0.351827, acc.: 86.72%] [G loss: 5.647311]\n",
            "13620 [D loss: 0.402840, acc.: 82.03%] [G loss: 5.528349]\n",
            "13640 [D loss: 0.426404, acc.: 82.03%] [G loss: 6.629462]\n",
            "13660 [D loss: 0.355599, acc.: 82.42%] [G loss: 6.098036]\n",
            "13680 [D loss: 0.294297, acc.: 89.06%] [G loss: 6.015219]\n",
            "13700 [D loss: 0.353335, acc.: 85.16%] [G loss: 5.596107]\n",
            "13720 [D loss: 0.375260, acc.: 82.81%] [G loss: 6.989185]\n",
            "13740 [D loss: 0.457202, acc.: 77.34%] [G loss: 6.157535]\n",
            "13760 [D loss: 0.300078, acc.: 87.89%] [G loss: 6.093851]\n",
            "13780 [D loss: 0.300677, acc.: 87.50%] [G loss: 6.556529]\n",
            "13800 [D loss: 0.379395, acc.: 83.59%] [G loss: 6.249478]\n",
            "13820 [D loss: 0.412470, acc.: 84.38%] [G loss: 5.569220]\n",
            "13840 [D loss: 0.405519, acc.: 83.98%] [G loss: 5.970872]\n",
            "13860 [D loss: 0.345835, acc.: 83.98%] [G loss: 5.997483]\n",
            "13880 [D loss: 0.357748, acc.: 86.33%] [G loss: 5.690675]\n",
            "13900 [D loss: 0.309004, acc.: 86.72%] [G loss: 6.941115]\n",
            "13920 [D loss: 0.293935, acc.: 88.67%] [G loss: 6.642017]\n",
            "13940 [D loss: 0.403417, acc.: 81.25%] [G loss: 5.416062]\n",
            "13960 [D loss: 0.312857, acc.: 85.94%] [G loss: 6.378864]\n",
            "13980 [D loss: 0.417658, acc.: 83.59%] [G loss: 5.542074]\n",
            "14000 [D loss: 0.284846, acc.: 88.28%] [G loss: 6.869186]\n",
            "14020 [D loss: 0.323633, acc.: 89.45%] [G loss: 5.428090]\n",
            "14040 [D loss: 0.418161, acc.: 81.25%] [G loss: 6.109464]\n",
            "14060 [D loss: 0.261743, acc.: 90.23%] [G loss: 4.353913]\n",
            "14080 [D loss: 0.413301, acc.: 82.03%] [G loss: 5.114247]\n",
            "14100 [D loss: 0.461376, acc.: 78.12%] [G loss: 7.534500]\n",
            "14120 [D loss: 0.287895, acc.: 90.62%] [G loss: 7.368559]\n",
            "14140 [D loss: 0.398067, acc.: 80.47%] [G loss: 7.183949]\n",
            "14160 [D loss: 0.332274, acc.: 87.11%] [G loss: 6.280947]\n",
            "14180 [D loss: 0.438608, acc.: 82.42%] [G loss: 6.828174]\n",
            "14200 [D loss: 0.255044, acc.: 92.97%] [G loss: 7.202744]\n",
            "14220 [D loss: 0.342124, acc.: 84.77%] [G loss: 6.193193]\n",
            "14240 [D loss: 0.335056, acc.: 84.77%] [G loss: 6.546109]\n",
            "14260 [D loss: 0.361926, acc.: 84.38%] [G loss: 6.236453]\n",
            "14280 [D loss: 0.253380, acc.: 90.62%] [G loss: 6.150144]\n",
            "14300 [D loss: 0.305694, acc.: 88.28%] [G loss: 7.141849]\n",
            "14320 [D loss: 0.306029, acc.: 90.23%] [G loss: 6.022050]\n",
            "14340 [D loss: 0.453507, acc.: 77.73%] [G loss: 5.446638]\n",
            "14360 [D loss: 0.301949, acc.: 85.16%] [G loss: 7.303663]\n",
            "14380 [D loss: 0.432122, acc.: 78.91%] [G loss: 5.774291]\n",
            "14400 [D loss: 0.314446, acc.: 83.98%] [G loss: 6.725939]\n",
            "14420 [D loss: 0.340659, acc.: 83.59%] [G loss: 6.708808]\n",
            "14440 [D loss: 0.320345, acc.: 87.89%] [G loss: 6.367020]\n",
            "14460 [D loss: 0.317644, acc.: 87.11%] [G loss: 5.732747]\n",
            "14480 [D loss: 0.308173, acc.: 85.16%] [G loss: 7.330625]\n",
            "14500 [D loss: 0.338685, acc.: 83.98%] [G loss: 6.672957]\n",
            "14520 [D loss: 0.416461, acc.: 79.69%] [G loss: 6.934550]\n",
            "14540 [D loss: 0.378538, acc.: 83.20%] [G loss: 6.125237]\n",
            "14560 [D loss: 0.312952, acc.: 87.11%] [G loss: 6.900836]\n",
            "14580 [D loss: 0.259133, acc.: 90.23%] [G loss: 6.650978]\n",
            "14600 [D loss: 0.484129, acc.: 76.95%] [G loss: 6.304809]\n",
            "14620 [D loss: 0.245665, acc.: 90.23%] [G loss: 7.485578]\n",
            "14640 [D loss: 0.416954, acc.: 82.81%] [G loss: 6.322415]\n",
            "14660 [D loss: 0.389211, acc.: 82.42%] [G loss: 6.685702]\n",
            "14680 [D loss: 0.399046, acc.: 82.42%] [G loss: 5.862173]\n",
            "14700 [D loss: 0.358573, acc.: 84.77%] [G loss: 6.465609]\n",
            "14720 [D loss: 0.312454, acc.: 86.33%] [G loss: 6.456112]\n",
            "14740 [D loss: 0.373351, acc.: 85.55%] [G loss: 6.195218]\n",
            "14760 [D loss: 0.359301, acc.: 84.38%] [G loss: 5.836758]\n",
            "14780 [D loss: 0.333058, acc.: 84.77%] [G loss: 6.907163]\n",
            "14800 [D loss: 0.313566, acc.: 87.11%] [G loss: 6.633384]\n",
            "14820 [D loss: 0.332800, acc.: 86.33%] [G loss: 6.308480]\n",
            "14840 [D loss: 0.328745, acc.: 87.89%] [G loss: 6.065392]\n",
            "14860 [D loss: 0.435673, acc.: 80.47%] [G loss: 5.545542]\n",
            "14880 [D loss: 0.294503, acc.: 87.50%] [G loss: 6.635432]\n",
            "14900 [D loss: 0.372896, acc.: 81.25%] [G loss: 6.899356]\n",
            "14920 [D loss: 0.350821, acc.: 86.33%] [G loss: 6.513855]\n",
            "14940 [D loss: 0.364285, acc.: 85.55%] [G loss: 6.406915]\n",
            "14960 [D loss: 0.301735, acc.: 87.11%] [G loss: 6.646298]\n",
            "14980 [D loss: 0.326279, acc.: 85.55%] [G loss: 7.544748]\n",
            "15000 [D loss: 0.348254, acc.: 86.72%] [G loss: 7.088714]\n",
            "15020 [D loss: 0.387221, acc.: 85.94%] [G loss: 6.802312]\n",
            "15040 [D loss: 0.335573, acc.: 89.45%] [G loss: 6.871318]\n",
            "15060 [D loss: 0.275146, acc.: 89.45%] [G loss: 6.345190]\n",
            "15080 [D loss: 0.349685, acc.: 85.55%] [G loss: 6.747897]\n",
            "15100 [D loss: 0.349725, acc.: 84.38%] [G loss: 6.227100]\n",
            "15120 [D loss: 0.319280, acc.: 82.03%] [G loss: 6.758388]\n",
            "15140 [D loss: 0.374680, acc.: 83.59%] [G loss: 6.620772]\n",
            "15160 [D loss: 0.256075, acc.: 90.62%] [G loss: 7.424584]\n",
            "15180 [D loss: 0.422042, acc.: 81.64%] [G loss: 6.296021]\n",
            "15200 [D loss: 0.287753, acc.: 87.50%] [G loss: 6.628824]\n",
            "15220 [D loss: 0.269249, acc.: 89.06%] [G loss: 6.370145]\n",
            "15240 [D loss: 0.342222, acc.: 81.25%] [G loss: 6.275812]\n",
            "15260 [D loss: 0.275980, acc.: 88.28%] [G loss: 6.026658]\n",
            "15280 [D loss: 0.309238, acc.: 86.72%] [G loss: 6.660358]\n",
            "15300 [D loss: 0.416287, acc.: 82.03%] [G loss: 6.574394]\n",
            "15320 [D loss: 0.347983, acc.: 85.16%] [G loss: 6.745453]\n",
            "15340 [D loss: 0.277011, acc.: 88.67%] [G loss: 6.273462]\n",
            "15360 [D loss: 0.300505, acc.: 86.33%] [G loss: 6.810410]\n",
            "15380 [D loss: 0.236990, acc.: 91.41%] [G loss: 6.813428]\n",
            "15400 [D loss: 0.294236, acc.: 87.11%] [G loss: 6.232323]\n",
            "15420 [D loss: 0.320071, acc.: 85.55%] [G loss: 7.068229]\n",
            "15440 [D loss: 0.326696, acc.: 85.55%] [G loss: 6.362178]\n",
            "15460 [D loss: 0.286238, acc.: 89.84%] [G loss: 7.057376]\n",
            "15480 [D loss: 0.385547, acc.: 82.81%] [G loss: 6.808632]\n",
            "15500 [D loss: 0.293000, acc.: 89.45%] [G loss: 7.433755]\n",
            "15520 [D loss: 0.265109, acc.: 88.28%] [G loss: 6.874872]\n",
            "15540 [D loss: 0.446864, acc.: 80.08%] [G loss: 6.599197]\n",
            "15560 [D loss: 0.357087, acc.: 84.38%] [G loss: 6.868115]\n",
            "15580 [D loss: 0.298627, acc.: 88.28%] [G loss: 6.548272]\n",
            "15600 [D loss: 0.396041, acc.: 82.81%] [G loss: 6.100401]\n",
            "15620 [D loss: 0.354617, acc.: 83.59%] [G loss: 6.442180]\n",
            "15640 [D loss: 0.339523, acc.: 87.89%] [G loss: 6.502493]\n",
            "15660 [D loss: 0.317409, acc.: 85.55%] [G loss: 6.452634]\n",
            "15680 [D loss: 0.304161, acc.: 89.06%] [G loss: 7.551525]\n",
            "15700 [D loss: 0.360077, acc.: 83.98%] [G loss: 7.104665]\n",
            "15720 [D loss: 0.347308, acc.: 85.55%] [G loss: 6.943678]\n",
            "15740 [D loss: 0.341177, acc.: 88.28%] [G loss: 7.737453]\n",
            "15760 [D loss: 0.346869, acc.: 85.16%] [G loss: 6.774923]\n",
            "15780 [D loss: 0.370005, acc.: 83.59%] [G loss: 6.184312]\n",
            "15800 [D loss: 0.306859, acc.: 85.94%] [G loss: 6.943725]\n",
            "15820 [D loss: 0.371402, acc.: 82.42%] [G loss: 6.984959]\n",
            "15840 [D loss: 0.407651, acc.: 82.42%] [G loss: 6.595233]\n",
            "15860 [D loss: 0.333899, acc.: 86.72%] [G loss: 6.623397]\n",
            "15880 [D loss: 0.403159, acc.: 81.64%] [G loss: 6.446514]\n",
            "15900 [D loss: 0.266128, acc.: 89.84%] [G loss: 7.233557]\n",
            "15920 [D loss: 0.263096, acc.: 89.45%] [G loss: 4.663024]\n",
            "15940 [D loss: 1.489011, acc.: 39.06%] [G loss: 4.228615]\n",
            "15960 [D loss: 1.446993, acc.: 35.16%] [G loss: 3.576192]\n",
            "15980 [D loss: 0.543816, acc.: 75.39%] [G loss: 5.351245]\n",
            "16000 [D loss: 0.423456, acc.: 84.77%] [G loss: 6.389330]\n",
            "16020 [D loss: 0.372977, acc.: 84.77%] [G loss: 6.285897]\n",
            "16040 [D loss: 0.321273, acc.: 85.55%] [G loss: 6.668163]\n",
            "16060 [D loss: 0.209253, acc.: 92.19%] [G loss: 7.846203]\n",
            "16080 [D loss: 0.418959, acc.: 81.64%] [G loss: 6.478230]\n",
            "16100 [D loss: 0.290355, acc.: 88.67%] [G loss: 7.444997]\n",
            "16120 [D loss: 0.451434, acc.: 80.86%] [G loss: 6.301401]\n",
            "16140 [D loss: 0.328394, acc.: 87.11%] [G loss: 6.021435]\n",
            "16160 [D loss: 0.292134, acc.: 90.23%] [G loss: 7.098208]\n",
            "16180 [D loss: 0.317309, acc.: 83.98%] [G loss: 6.525447]\n",
            "16200 [D loss: 0.270237, acc.: 90.23%] [G loss: 7.434779]\n",
            "16220 [D loss: 0.336311, acc.: 87.50%] [G loss: 6.991700]\n",
            "16240 [D loss: 0.241565, acc.: 89.45%] [G loss: 7.805356]\n",
            "16260 [D loss: 0.346332, acc.: 87.11%] [G loss: 6.385931]\n",
            "16280 [D loss: 0.253002, acc.: 89.84%] [G loss: 6.423689]\n",
            "16300 [D loss: 0.338440, acc.: 86.33%] [G loss: 6.435522]\n",
            "16320 [D loss: 0.328373, acc.: 85.94%] [G loss: 6.446262]\n",
            "16340 [D loss: 0.297951, acc.: 87.89%] [G loss: 6.664164]\n",
            "16360 [D loss: 0.294918, acc.: 89.06%] [G loss: 7.363577]\n",
            "16380 [D loss: 0.332329, acc.: 87.11%] [G loss: 6.964708]\n",
            "16400 [D loss: 0.354879, acc.: 87.11%] [G loss: 6.648660]\n",
            "16420 [D loss: 0.372095, acc.: 83.20%] [G loss: 6.716496]\n",
            "16440 [D loss: 0.276707, acc.: 89.84%] [G loss: 7.070796]\n",
            "16460 [D loss: 0.391506, acc.: 82.81%] [G loss: 7.090064]\n",
            "16480 [D loss: 0.267094, acc.: 89.45%] [G loss: 7.872658]\n",
            "16500 [D loss: 0.249378, acc.: 91.80%] [G loss: 6.298358]\n",
            "16520 [D loss: 0.318036, acc.: 84.77%] [G loss: 6.626436]\n",
            "16540 [D loss: 0.329129, acc.: 86.33%] [G loss: 6.417892]\n",
            "16560 [D loss: 0.331748, acc.: 87.11%] [G loss: 7.110525]\n",
            "16580 [D loss: 0.328703, acc.: 89.06%] [G loss: 7.814747]\n",
            "16600 [D loss: 0.306871, acc.: 86.33%] [G loss: 7.267191]\n",
            "16620 [D loss: 0.303580, acc.: 88.28%] [G loss: 6.823442]\n",
            "16640 [D loss: 0.335926, acc.: 85.55%] [G loss: 7.444491]\n",
            "16660 [D loss: 0.261764, acc.: 89.06%] [G loss: 7.264169]\n",
            "16680 [D loss: 0.357545, acc.: 86.33%] [G loss: 6.316022]\n",
            "16700 [D loss: 0.326072, acc.: 85.55%] [G loss: 7.283370]\n",
            "16720 [D loss: 0.365972, acc.: 84.38%] [G loss: 6.610703]\n",
            "16740 [D loss: 0.364077, acc.: 84.38%] [G loss: 6.763168]\n",
            "16760 [D loss: 0.375916, acc.: 83.20%] [G loss: 6.244235]\n",
            "16780 [D loss: 0.245386, acc.: 91.80%] [G loss: 7.290134]\n",
            "16800 [D loss: 0.264105, acc.: 87.89%] [G loss: 7.299521]\n",
            "16820 [D loss: 0.389101, acc.: 82.42%] [G loss: 7.155339]\n",
            "16840 [D loss: 0.299101, acc.: 88.67%] [G loss: 7.336498]\n",
            "16860 [D loss: 0.326876, acc.: 87.50%] [G loss: 7.627811]\n",
            "16880 [D loss: 0.306326, acc.: 87.50%] [G loss: 7.200567]\n",
            "16900 [D loss: 0.277840, acc.: 89.45%] [G loss: 7.295238]\n",
            "16920 [D loss: 0.271619, acc.: 88.28%] [G loss: 7.527098]\n",
            "16940 [D loss: 0.447956, acc.: 80.47%] [G loss: 6.888499]\n",
            "16960 [D loss: 0.260203, acc.: 87.11%] [G loss: 7.612825]\n",
            "16980 [D loss: 0.407541, acc.: 80.08%] [G loss: 6.146461]\n",
            "17000 [D loss: 0.313712, acc.: 86.33%] [G loss: 7.252116]\n",
            "17020 [D loss: 0.322383, acc.: 87.11%] [G loss: 7.116953]\n",
            "17040 [D loss: 0.350891, acc.: 85.94%] [G loss: 5.778086]\n",
            "17060 [D loss: 0.279776, acc.: 88.67%] [G loss: 7.233074]\n",
            "17080 [D loss: 0.374751, acc.: 84.77%] [G loss: 7.191188]\n",
            "17100 [D loss: 0.382637, acc.: 83.59%] [G loss: 6.913104]\n",
            "17120 [D loss: 0.345611, acc.: 83.59%] [G loss: 7.210632]\n",
            "17140 [D loss: 0.303632, acc.: 87.11%] [G loss: 7.321196]\n",
            "17160 [D loss: 0.277408, acc.: 89.84%] [G loss: 6.634642]\n",
            "17180 [D loss: 0.217449, acc.: 91.41%] [G loss: 7.326530]\n",
            "17200 [D loss: 0.312978, acc.: 87.11%] [G loss: 7.096844]\n",
            "17220 [D loss: 0.369925, acc.: 82.03%] [G loss: 6.189439]\n",
            "17240 [D loss: 0.230820, acc.: 91.02%] [G loss: 7.687977]\n",
            "17260 [D loss: 0.307437, acc.: 89.06%] [G loss: 6.319919]\n",
            "17280 [D loss: 0.423093, acc.: 81.64%] [G loss: 6.406345]\n",
            "17300 [D loss: 0.370508, acc.: 85.16%] [G loss: 6.583936]\n",
            "17320 [D loss: 0.355609, acc.: 86.72%] [G loss: 7.090985]\n",
            "17340 [D loss: 0.260083, acc.: 90.23%] [G loss: 6.982968]\n",
            "17360 [D loss: 0.300784, acc.: 85.55%] [G loss: 7.141093]\n",
            "17380 [D loss: 0.281773, acc.: 89.84%] [G loss: 7.430314]\n",
            "17400 [D loss: 0.288613, acc.: 86.33%] [G loss: 7.634693]\n",
            "17420 [D loss: 0.227713, acc.: 91.41%] [G loss: 7.522998]\n",
            "17440 [D loss: 0.289925, acc.: 87.50%] [G loss: 8.030455]\n",
            "17460 [D loss: 0.272390, acc.: 87.89%] [G loss: 7.644981]\n",
            "17480 [D loss: 0.373550, acc.: 82.42%] [G loss: 8.563738]\n",
            "17500 [D loss: 0.213133, acc.: 91.80%] [G loss: 8.143873]\n",
            "17520 [D loss: 0.286524, acc.: 87.50%] [G loss: 6.820018]\n",
            "17540 [D loss: 0.307152, acc.: 87.50%] [G loss: 7.559629]\n",
            "17560 [D loss: 0.275419, acc.: 89.45%] [G loss: 6.507249]\n",
            "17580 [D loss: 0.417028, acc.: 82.03%] [G loss: 6.285392]\n",
            "17600 [D loss: 0.270422, acc.: 91.02%] [G loss: 7.784558]\n",
            "17620 [D loss: 0.383629, acc.: 81.25%] [G loss: 7.122539]\n",
            "17640 [D loss: 0.221368, acc.: 92.19%] [G loss: 7.324263]\n",
            "17660 [D loss: 0.310597, acc.: 87.11%] [G loss: 7.151425]\n",
            "17680 [D loss: 0.276211, acc.: 89.45%] [G loss: 8.139668]\n",
            "17700 [D loss: 0.257451, acc.: 90.23%] [G loss: 7.262533]\n",
            "17720 [D loss: 0.346159, acc.: 85.55%] [G loss: 7.142326]\n",
            "17740 [D loss: 0.249752, acc.: 91.41%] [G loss: 7.826004]\n",
            "17760 [D loss: 0.368095, acc.: 86.33%] [G loss: 7.388266]\n",
            "17780 [D loss: 0.299206, acc.: 87.50%] [G loss: 7.970586]\n",
            "17800 [D loss: 0.253099, acc.: 89.84%] [G loss: 7.824533]\n",
            "17820 [D loss: 0.304311, acc.: 86.33%] [G loss: 7.776467]\n",
            "17840 [D loss: 0.355614, acc.: 82.03%] [G loss: 7.248128]\n",
            "17860 [D loss: 0.318689, acc.: 87.11%] [G loss: 7.832550]\n",
            "17880 [D loss: 0.318094, acc.: 85.16%] [G loss: 7.641703]\n",
            "17900 [D loss: 0.183013, acc.: 92.19%] [G loss: 7.510130]\n",
            "17920 [D loss: 0.324595, acc.: 88.28%] [G loss: 7.682969]\n",
            "17940 [D loss: 0.315460, acc.: 85.55%] [G loss: 7.079403]\n",
            "17960 [D loss: 0.300372, acc.: 87.11%] [G loss: 6.539782]\n",
            "17980 [D loss: 0.247787, acc.: 88.67%] [G loss: 9.782124]\n",
            "18000 [D loss: 0.290877, acc.: 87.89%] [G loss: 7.606002]\n",
            "18020 [D loss: 0.395400, acc.: 82.81%] [G loss: 6.972595]\n",
            "18040 [D loss: 0.292838, acc.: 86.72%] [G loss: 8.986767]\n",
            "18060 [D loss: 0.351471, acc.: 86.33%] [G loss: 7.717530]\n",
            "18080 [D loss: 0.335142, acc.: 85.94%] [G loss: 7.192846]\n",
            "18100 [D loss: 0.298280, acc.: 87.89%] [G loss: 7.545443]\n",
            "18120 [D loss: 0.311399, acc.: 87.11%] [G loss: 8.306311]\n",
            "18140 [D loss: 0.280586, acc.: 89.45%] [G loss: 8.706306]\n",
            "18160 [D loss: 0.349888, acc.: 87.89%] [G loss: 7.504266]\n",
            "18180 [D loss: 0.205998, acc.: 92.19%] [G loss: 8.018051]\n",
            "18200 [D loss: 0.257358, acc.: 90.23%] [G loss: 7.690401]\n",
            "18220 [D loss: 0.202032, acc.: 93.36%] [G loss: 9.167788]\n",
            "18240 [D loss: 0.188980, acc.: 93.36%] [G loss: 7.757931]\n",
            "18260 [D loss: 0.374448, acc.: 85.55%] [G loss: 6.454613]\n",
            "18280 [D loss: 0.320915, acc.: 83.98%] [G loss: 7.460946]\n",
            "18300 [D loss: 0.328598, acc.: 87.11%] [G loss: 7.405629]\n",
            "18320 [D loss: 0.283725, acc.: 88.28%] [G loss: 7.592457]\n",
            "18340 [D loss: 0.331086, acc.: 85.16%] [G loss: 7.657226]\n",
            "18360 [D loss: 0.205221, acc.: 92.97%] [G loss: 8.101967]\n",
            "18380 [D loss: 0.309239, acc.: 88.28%] [G loss: 7.487924]\n",
            "18400 [D loss: 0.260994, acc.: 89.84%] [G loss: 7.585829]\n",
            "18420 [D loss: 0.298824, acc.: 85.94%] [G loss: 7.283183]\n",
            "18440 [D loss: 0.306284, acc.: 87.50%] [G loss: 7.763386]\n",
            "18460 [D loss: 0.257985, acc.: 90.23%] [G loss: 7.063719]\n",
            "18480 [D loss: 0.298385, acc.: 87.50%] [G loss: 7.911516]\n",
            "18500 [D loss: 0.383202, acc.: 83.20%] [G loss: 8.218309]\n",
            "18520 [D loss: 0.391838, acc.: 82.03%] [G loss: 7.459663]\n",
            "18540 [D loss: 0.347757, acc.: 85.55%] [G loss: 7.919562]\n",
            "18560 [D loss: 0.312514, acc.: 87.89%] [G loss: 6.460115]\n",
            "18580 [D loss: 0.180717, acc.: 93.36%] [G loss: 4.792183]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}