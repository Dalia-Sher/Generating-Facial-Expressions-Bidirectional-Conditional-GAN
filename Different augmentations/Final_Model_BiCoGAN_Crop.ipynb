{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_Model_BiCoGAN_Crop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dalia-Sher/Generating-Facial-Expressions-Bidirectional-Conditional-GAN/blob/Shir/Final_Model_BiCoGAN_Crop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS516rLy1LYQ",
        "outputId": "fa569726-776e-44a7-d9c3-768875683e77"
      },
      "source": [
        "!unzip fer2013.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fer2013.zip\n",
            "  inflating: fer2013.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "jF8QfAYPg_71",
        "outputId": "6ab3b8b4-84bc-41ad-960c-a886e78e9e37"
      },
      "source": [
        "data = pd.read_csv('fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "820fc73c-efd6-45cb-efa5-b31788f71a11"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5xEsrnIB0t"
      },
      "source": [
        "dic = {0:'Angry', 1:'disgust' , 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRPqlqU1IG94"
      },
      "source": [
        "The emotion disgust has too few samples, therefore we won't use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9swxs5P0w",
        "outputId": "f5a80377-5597-4664-ca10-7316eff2c1be"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSAij3uINfw"
      },
      "source": [
        "We will redefine the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "8e140982-15ba-45a6-a4ca-9ac9a6e2021a"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xOXTev833cv",
        "outputId": "720974df-c180-496a-dece-f83ebd3cbad9"
      },
      "source": [
        "epochs = X_train.shape[0]\n",
        "print(\"number of epochs:\", epochs)\n",
        "\n",
        "X_train_aug = X_train.copy()\n",
        "y_train_aug = y_train.copy()\n",
        "\n",
        "for k in range(epochs):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  cropped_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  crop = iaa.Crop(percent=(0, 0.2))\n",
        "  corp_image = crop.augment_image(img)\n",
        "  cropped_images.append(corp_image)\n",
        "\n",
        "  cropped_images = np.array(cropped_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train_aug = np.concatenate((X_train_aug, cropped_images), axis=0)\n",
        "  emotions_list = [emotion]\n",
        "  y_train_aug = np.concatenate((y_train_aug, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration:\" , k,\", train shape:\", X_train_aug.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epochs: 35340\n",
            "iteration: 0 , train shape: (35341, 48, 48, 1)\n",
            "iteration: 100 , train shape: (35441, 48, 48, 1)\n",
            "iteration: 200 , train shape: (35541, 48, 48, 1)\n",
            "iteration: 300 , train shape: (35641, 48, 48, 1)\n",
            "iteration: 400 , train shape: (35741, 48, 48, 1)\n",
            "iteration: 500 , train shape: (35841, 48, 48, 1)\n",
            "iteration: 600 , train shape: (35941, 48, 48, 1)\n",
            "iteration: 700 , train shape: (36041, 48, 48, 1)\n",
            "iteration: 800 , train shape: (36141, 48, 48, 1)\n",
            "iteration: 900 , train shape: (36241, 48, 48, 1)\n",
            "iteration: 1000 , train shape: (36341, 48, 48, 1)\n",
            "iteration: 1100 , train shape: (36441, 48, 48, 1)\n",
            "iteration: 1200 , train shape: (36541, 48, 48, 1)\n",
            "iteration: 1300 , train shape: (36641, 48, 48, 1)\n",
            "iteration: 1400 , train shape: (36741, 48, 48, 1)\n",
            "iteration: 1500 , train shape: (36841, 48, 48, 1)\n",
            "iteration: 1600 , train shape: (36941, 48, 48, 1)\n",
            "iteration: 1700 , train shape: (37041, 48, 48, 1)\n",
            "iteration: 1800 , train shape: (37141, 48, 48, 1)\n",
            "iteration: 1900 , train shape: (37241, 48, 48, 1)\n",
            "iteration: 2000 , train shape: (37341, 48, 48, 1)\n",
            "iteration: 2100 , train shape: (37441, 48, 48, 1)\n",
            "iteration: 2200 , train shape: (37541, 48, 48, 1)\n",
            "iteration: 2300 , train shape: (37641, 48, 48, 1)\n",
            "iteration: 2400 , train shape: (37741, 48, 48, 1)\n",
            "iteration: 2500 , train shape: (37841, 48, 48, 1)\n",
            "iteration: 2600 , train shape: (37941, 48, 48, 1)\n",
            "iteration: 2700 , train shape: (38041, 48, 48, 1)\n",
            "iteration: 2800 , train shape: (38141, 48, 48, 1)\n",
            "iteration: 2900 , train shape: (38241, 48, 48, 1)\n",
            "iteration: 3000 , train shape: (38341, 48, 48, 1)\n",
            "iteration: 3100 , train shape: (38441, 48, 48, 1)\n",
            "iteration: 3200 , train shape: (38541, 48, 48, 1)\n",
            "iteration: 3300 , train shape: (38641, 48, 48, 1)\n",
            "iteration: 3400 , train shape: (38741, 48, 48, 1)\n",
            "iteration: 3500 , train shape: (38841, 48, 48, 1)\n",
            "iteration: 3600 , train shape: (38941, 48, 48, 1)\n",
            "iteration: 3700 , train shape: (39041, 48, 48, 1)\n",
            "iteration: 3800 , train shape: (39141, 48, 48, 1)\n",
            "iteration: 3900 , train shape: (39241, 48, 48, 1)\n",
            "iteration: 4000 , train shape: (39341, 48, 48, 1)\n",
            "iteration: 4100 , train shape: (39441, 48, 48, 1)\n",
            "iteration: 4200 , train shape: (39541, 48, 48, 1)\n",
            "iteration: 4300 , train shape: (39641, 48, 48, 1)\n",
            "iteration: 4400 , train shape: (39741, 48, 48, 1)\n",
            "iteration: 4500 , train shape: (39841, 48, 48, 1)\n",
            "iteration: 4600 , train shape: (39941, 48, 48, 1)\n",
            "iteration: 4700 , train shape: (40041, 48, 48, 1)\n",
            "iteration: 4800 , train shape: (40141, 48, 48, 1)\n",
            "iteration: 4900 , train shape: (40241, 48, 48, 1)\n",
            "iteration: 5000 , train shape: (40341, 48, 48, 1)\n",
            "iteration: 5100 , train shape: (40441, 48, 48, 1)\n",
            "iteration: 5200 , train shape: (40541, 48, 48, 1)\n",
            "iteration: 5300 , train shape: (40641, 48, 48, 1)\n",
            "iteration: 5400 , train shape: (40741, 48, 48, 1)\n",
            "iteration: 5500 , train shape: (40841, 48, 48, 1)\n",
            "iteration: 5600 , train shape: (40941, 48, 48, 1)\n",
            "iteration: 5700 , train shape: (41041, 48, 48, 1)\n",
            "iteration: 5800 , train shape: (41141, 48, 48, 1)\n",
            "iteration: 5900 , train shape: (41241, 48, 48, 1)\n",
            "iteration: 6000 , train shape: (41341, 48, 48, 1)\n",
            "iteration: 6100 , train shape: (41441, 48, 48, 1)\n",
            "iteration: 6200 , train shape: (41541, 48, 48, 1)\n",
            "iteration: 6300 , train shape: (41641, 48, 48, 1)\n",
            "iteration: 6400 , train shape: (41741, 48, 48, 1)\n",
            "iteration: 6500 , train shape: (41841, 48, 48, 1)\n",
            "iteration: 6600 , train shape: (41941, 48, 48, 1)\n",
            "iteration: 6700 , train shape: (42041, 48, 48, 1)\n",
            "iteration: 6800 , train shape: (42141, 48, 48, 1)\n",
            "iteration: 6900 , train shape: (42241, 48, 48, 1)\n",
            "iteration: 7000 , train shape: (42341, 48, 48, 1)\n",
            "iteration: 7100 , train shape: (42441, 48, 48, 1)\n",
            "iteration: 7200 , train shape: (42541, 48, 48, 1)\n",
            "iteration: 7300 , train shape: (42641, 48, 48, 1)\n",
            "iteration: 7400 , train shape: (42741, 48, 48, 1)\n",
            "iteration: 7500 , train shape: (42841, 48, 48, 1)\n",
            "iteration: 7600 , train shape: (42941, 48, 48, 1)\n",
            "iteration: 7700 , train shape: (43041, 48, 48, 1)\n",
            "iteration: 7800 , train shape: (43141, 48, 48, 1)\n",
            "iteration: 7900 , train shape: (43241, 48, 48, 1)\n",
            "iteration: 8000 , train shape: (43341, 48, 48, 1)\n",
            "iteration: 8100 , train shape: (43441, 48, 48, 1)\n",
            "iteration: 8200 , train shape: (43541, 48, 48, 1)\n",
            "iteration: 8300 , train shape: (43641, 48, 48, 1)\n",
            "iteration: 8400 , train shape: (43741, 48, 48, 1)\n",
            "iteration: 8500 , train shape: (43841, 48, 48, 1)\n",
            "iteration: 8600 , train shape: (43941, 48, 48, 1)\n",
            "iteration: 8700 , train shape: (44041, 48, 48, 1)\n",
            "iteration: 8800 , train shape: (44141, 48, 48, 1)\n",
            "iteration: 8900 , train shape: (44241, 48, 48, 1)\n",
            "iteration: 9000 , train shape: (44341, 48, 48, 1)\n",
            "iteration: 9100 , train shape: (44441, 48, 48, 1)\n",
            "iteration: 9200 , train shape: (44541, 48, 48, 1)\n",
            "iteration: 9300 , train shape: (44641, 48, 48, 1)\n",
            "iteration: 9400 , train shape: (44741, 48, 48, 1)\n",
            "iteration: 9500 , train shape: (44841, 48, 48, 1)\n",
            "iteration: 9600 , train shape: (44941, 48, 48, 1)\n",
            "iteration: 9700 , train shape: (45041, 48, 48, 1)\n",
            "iteration: 9800 , train shape: (45141, 48, 48, 1)\n",
            "iteration: 9900 , train shape: (45241, 48, 48, 1)\n",
            "iteration: 10000 , train shape: (45341, 48, 48, 1)\n",
            "iteration: 10100 , train shape: (45441, 48, 48, 1)\n",
            "iteration: 10200 , train shape: (45541, 48, 48, 1)\n",
            "iteration: 10300 , train shape: (45641, 48, 48, 1)\n",
            "iteration: 10400 , train shape: (45741, 48, 48, 1)\n",
            "iteration: 10500 , train shape: (45841, 48, 48, 1)\n",
            "iteration: 10600 , train shape: (45941, 48, 48, 1)\n",
            "iteration: 10700 , train shape: (46041, 48, 48, 1)\n",
            "iteration: 10800 , train shape: (46141, 48, 48, 1)\n",
            "iteration: 10900 , train shape: (46241, 48, 48, 1)\n",
            "iteration: 11000 , train shape: (46341, 48, 48, 1)\n",
            "iteration: 11100 , train shape: (46441, 48, 48, 1)\n",
            "iteration: 11200 , train shape: (46541, 48, 48, 1)\n",
            "iteration: 11300 , train shape: (46641, 48, 48, 1)\n",
            "iteration: 11400 , train shape: (46741, 48, 48, 1)\n",
            "iteration: 11500 , train shape: (46841, 48, 48, 1)\n",
            "iteration: 11600 , train shape: (46941, 48, 48, 1)\n",
            "iteration: 11700 , train shape: (47041, 48, 48, 1)\n",
            "iteration: 11800 , train shape: (47141, 48, 48, 1)\n",
            "iteration: 11900 , train shape: (47241, 48, 48, 1)\n",
            "iteration: 12000 , train shape: (47341, 48, 48, 1)\n",
            "iteration: 12100 , train shape: (47441, 48, 48, 1)\n",
            "iteration: 12200 , train shape: (47541, 48, 48, 1)\n",
            "iteration: 12300 , train shape: (47641, 48, 48, 1)\n",
            "iteration: 12400 , train shape: (47741, 48, 48, 1)\n",
            "iteration: 12500 , train shape: (47841, 48, 48, 1)\n",
            "iteration: 12600 , train shape: (47941, 48, 48, 1)\n",
            "iteration: 12700 , train shape: (48041, 48, 48, 1)\n",
            "iteration: 12800 , train shape: (48141, 48, 48, 1)\n",
            "iteration: 12900 , train shape: (48241, 48, 48, 1)\n",
            "iteration: 13000 , train shape: (48341, 48, 48, 1)\n",
            "iteration: 13100 , train shape: (48441, 48, 48, 1)\n",
            "iteration: 13200 , train shape: (48541, 48, 48, 1)\n",
            "iteration: 13300 , train shape: (48641, 48, 48, 1)\n",
            "iteration: 13400 , train shape: (48741, 48, 48, 1)\n",
            "iteration: 13500 , train shape: (48841, 48, 48, 1)\n",
            "iteration: 13600 , train shape: (48941, 48, 48, 1)\n",
            "iteration: 13700 , train shape: (49041, 48, 48, 1)\n",
            "iteration: 13800 , train shape: (49141, 48, 48, 1)\n",
            "iteration: 13900 , train shape: (49241, 48, 48, 1)\n",
            "iteration: 14000 , train shape: (49341, 48, 48, 1)\n",
            "iteration: 14100 , train shape: (49441, 48, 48, 1)\n",
            "iteration: 14200 , train shape: (49541, 48, 48, 1)\n",
            "iteration: 14300 , train shape: (49641, 48, 48, 1)\n",
            "iteration: 14400 , train shape: (49741, 48, 48, 1)\n",
            "iteration: 14500 , train shape: (49841, 48, 48, 1)\n",
            "iteration: 14600 , train shape: (49941, 48, 48, 1)\n",
            "iteration: 14700 , train shape: (50041, 48, 48, 1)\n",
            "iteration: 14800 , train shape: (50141, 48, 48, 1)\n",
            "iteration: 14900 , train shape: (50241, 48, 48, 1)\n",
            "iteration: 15000 , train shape: (50341, 48, 48, 1)\n",
            "iteration: 15100 , train shape: (50441, 48, 48, 1)\n",
            "iteration: 15200 , train shape: (50541, 48, 48, 1)\n",
            "iteration: 15300 , train shape: (50641, 48, 48, 1)\n",
            "iteration: 15400 , train shape: (50741, 48, 48, 1)\n",
            "iteration: 15500 , train shape: (50841, 48, 48, 1)\n",
            "iteration: 15600 , train shape: (50941, 48, 48, 1)\n",
            "iteration: 15700 , train shape: (51041, 48, 48, 1)\n",
            "iteration: 15800 , train shape: (51141, 48, 48, 1)\n",
            "iteration: 15900 , train shape: (51241, 48, 48, 1)\n",
            "iteration: 16000 , train shape: (51341, 48, 48, 1)\n",
            "iteration: 16100 , train shape: (51441, 48, 48, 1)\n",
            "iteration: 16200 , train shape: (51541, 48, 48, 1)\n",
            "iteration: 16300 , train shape: (51641, 48, 48, 1)\n",
            "iteration: 16400 , train shape: (51741, 48, 48, 1)\n",
            "iteration: 16500 , train shape: (51841, 48, 48, 1)\n",
            "iteration: 16600 , train shape: (51941, 48, 48, 1)\n",
            "iteration: 16700 , train shape: (52041, 48, 48, 1)\n",
            "iteration: 16800 , train shape: (52141, 48, 48, 1)\n",
            "iteration: 16900 , train shape: (52241, 48, 48, 1)\n",
            "iteration: 17000 , train shape: (52341, 48, 48, 1)\n",
            "iteration: 17100 , train shape: (52441, 48, 48, 1)\n",
            "iteration: 17200 , train shape: (52541, 48, 48, 1)\n",
            "iteration: 17300 , train shape: (52641, 48, 48, 1)\n",
            "iteration: 17400 , train shape: (52741, 48, 48, 1)\n",
            "iteration: 17500 , train shape: (52841, 48, 48, 1)\n",
            "iteration: 17600 , train shape: (52941, 48, 48, 1)\n",
            "iteration: 17700 , train shape: (53041, 48, 48, 1)\n",
            "iteration: 17800 , train shape: (53141, 48, 48, 1)\n",
            "iteration: 17900 , train shape: (53241, 48, 48, 1)\n",
            "iteration: 18000 , train shape: (53341, 48, 48, 1)\n",
            "iteration: 18100 , train shape: (53441, 48, 48, 1)\n",
            "iteration: 18200 , train shape: (53541, 48, 48, 1)\n",
            "iteration: 18300 , train shape: (53641, 48, 48, 1)\n",
            "iteration: 18400 , train shape: (53741, 48, 48, 1)\n",
            "iteration: 18500 , train shape: (53841, 48, 48, 1)\n",
            "iteration: 18600 , train shape: (53941, 48, 48, 1)\n",
            "iteration: 18700 , train shape: (54041, 48, 48, 1)\n",
            "iteration: 18800 , train shape: (54141, 48, 48, 1)\n",
            "iteration: 18900 , train shape: (54241, 48, 48, 1)\n",
            "iteration: 19000 , train shape: (54341, 48, 48, 1)\n",
            "iteration: 19100 , train shape: (54441, 48, 48, 1)\n",
            "iteration: 19200 , train shape: (54541, 48, 48, 1)\n",
            "iteration: 19300 , train shape: (54641, 48, 48, 1)\n",
            "iteration: 19400 , train shape: (54741, 48, 48, 1)\n",
            "iteration: 19500 , train shape: (54841, 48, 48, 1)\n",
            "iteration: 19600 , train shape: (54941, 48, 48, 1)\n",
            "iteration: 19700 , train shape: (55041, 48, 48, 1)\n",
            "iteration: 19800 , train shape: (55141, 48, 48, 1)\n",
            "iteration: 19900 , train shape: (55241, 48, 48, 1)\n",
            "iteration: 20000 , train shape: (55341, 48, 48, 1)\n",
            "iteration: 20100 , train shape: (55441, 48, 48, 1)\n",
            "iteration: 20200 , train shape: (55541, 48, 48, 1)\n",
            "iteration: 20300 , train shape: (55641, 48, 48, 1)\n",
            "iteration: 20400 , train shape: (55741, 48, 48, 1)\n",
            "iteration: 20500 , train shape: (55841, 48, 48, 1)\n",
            "iteration: 20600 , train shape: (55941, 48, 48, 1)\n",
            "iteration: 20700 , train shape: (56041, 48, 48, 1)\n",
            "iteration: 20800 , train shape: (56141, 48, 48, 1)\n",
            "iteration: 20900 , train shape: (56241, 48, 48, 1)\n",
            "iteration: 21000 , train shape: (56341, 48, 48, 1)\n",
            "iteration: 21100 , train shape: (56441, 48, 48, 1)\n",
            "iteration: 21200 , train shape: (56541, 48, 48, 1)\n",
            "iteration: 21300 , train shape: (56641, 48, 48, 1)\n",
            "iteration: 21400 , train shape: (56741, 48, 48, 1)\n",
            "iteration: 21500 , train shape: (56841, 48, 48, 1)\n",
            "iteration: 21600 , train shape: (56941, 48, 48, 1)\n",
            "iteration: 21700 , train shape: (57041, 48, 48, 1)\n",
            "iteration: 21800 , train shape: (57141, 48, 48, 1)\n",
            "iteration: 21900 , train shape: (57241, 48, 48, 1)\n",
            "iteration: 22000 , train shape: (57341, 48, 48, 1)\n",
            "iteration: 22100 , train shape: (57441, 48, 48, 1)\n",
            "iteration: 22200 , train shape: (57541, 48, 48, 1)\n",
            "iteration: 22300 , train shape: (57641, 48, 48, 1)\n",
            "iteration: 22400 , train shape: (57741, 48, 48, 1)\n",
            "iteration: 22500 , train shape: (57841, 48, 48, 1)\n",
            "iteration: 22600 , train shape: (57941, 48, 48, 1)\n",
            "iteration: 22700 , train shape: (58041, 48, 48, 1)\n",
            "iteration: 22800 , train shape: (58141, 48, 48, 1)\n",
            "iteration: 22900 , train shape: (58241, 48, 48, 1)\n",
            "iteration: 23000 , train shape: (58341, 48, 48, 1)\n",
            "iteration: 23100 , train shape: (58441, 48, 48, 1)\n",
            "iteration: 23200 , train shape: (58541, 48, 48, 1)\n",
            "iteration: 23300 , train shape: (58641, 48, 48, 1)\n",
            "iteration: 23400 , train shape: (58741, 48, 48, 1)\n",
            "iteration: 23500 , train shape: (58841, 48, 48, 1)\n",
            "iteration: 23600 , train shape: (58941, 48, 48, 1)\n",
            "iteration: 23700 , train shape: (59041, 48, 48, 1)\n",
            "iteration: 23800 , train shape: (59141, 48, 48, 1)\n",
            "iteration: 23900 , train shape: (59241, 48, 48, 1)\n",
            "iteration: 24000 , train shape: (59341, 48, 48, 1)\n",
            "iteration: 24100 , train shape: (59441, 48, 48, 1)\n",
            "iteration: 24200 , train shape: (59541, 48, 48, 1)\n",
            "iteration: 24300 , train shape: (59641, 48, 48, 1)\n",
            "iteration: 24400 , train shape: (59741, 48, 48, 1)\n",
            "iteration: 24500 , train shape: (59841, 48, 48, 1)\n",
            "iteration: 24600 , train shape: (59941, 48, 48, 1)\n",
            "iteration: 24700 , train shape: (60041, 48, 48, 1)\n",
            "iteration: 24800 , train shape: (60141, 48, 48, 1)\n",
            "iteration: 24900 , train shape: (60241, 48, 48, 1)\n",
            "iteration: 25000 , train shape: (60341, 48, 48, 1)\n",
            "iteration: 25100 , train shape: (60441, 48, 48, 1)\n",
            "iteration: 25200 , train shape: (60541, 48, 48, 1)\n",
            "iteration: 25300 , train shape: (60641, 48, 48, 1)\n",
            "iteration: 25400 , train shape: (60741, 48, 48, 1)\n",
            "iteration: 25500 , train shape: (60841, 48, 48, 1)\n",
            "iteration: 25600 , train shape: (60941, 48, 48, 1)\n",
            "iteration: 25700 , train shape: (61041, 48, 48, 1)\n",
            "iteration: 25800 , train shape: (61141, 48, 48, 1)\n",
            "iteration: 25900 , train shape: (61241, 48, 48, 1)\n",
            "iteration: 26000 , train shape: (61341, 48, 48, 1)\n",
            "iteration: 26100 , train shape: (61441, 48, 48, 1)\n",
            "iteration: 26200 , train shape: (61541, 48, 48, 1)\n",
            "iteration: 26300 , train shape: (61641, 48, 48, 1)\n",
            "iteration: 26400 , train shape: (61741, 48, 48, 1)\n",
            "iteration: 26500 , train shape: (61841, 48, 48, 1)\n",
            "iteration: 26600 , train shape: (61941, 48, 48, 1)\n",
            "iteration: 26700 , train shape: (62041, 48, 48, 1)\n",
            "iteration: 26800 , train shape: (62141, 48, 48, 1)\n",
            "iteration: 26900 , train shape: (62241, 48, 48, 1)\n",
            "iteration: 27000 , train shape: (62341, 48, 48, 1)\n",
            "iteration: 27100 , train shape: (62441, 48, 48, 1)\n",
            "iteration: 27200 , train shape: (62541, 48, 48, 1)\n",
            "iteration: 27300 , train shape: (62641, 48, 48, 1)\n",
            "iteration: 27400 , train shape: (62741, 48, 48, 1)\n",
            "iteration: 27500 , train shape: (62841, 48, 48, 1)\n",
            "iteration: 27600 , train shape: (62941, 48, 48, 1)\n",
            "iteration: 27700 , train shape: (63041, 48, 48, 1)\n",
            "iteration: 27800 , train shape: (63141, 48, 48, 1)\n",
            "iteration: 27900 , train shape: (63241, 48, 48, 1)\n",
            "iteration: 28000 , train shape: (63341, 48, 48, 1)\n",
            "iteration: 28100 , train shape: (63441, 48, 48, 1)\n",
            "iteration: 28200 , train shape: (63541, 48, 48, 1)\n",
            "iteration: 28300 , train shape: (63641, 48, 48, 1)\n",
            "iteration: 28400 , train shape: (63741, 48, 48, 1)\n",
            "iteration: 28500 , train shape: (63841, 48, 48, 1)\n",
            "iteration: 28600 , train shape: (63941, 48, 48, 1)\n",
            "iteration: 28700 , train shape: (64041, 48, 48, 1)\n",
            "iteration: 28800 , train shape: (64141, 48, 48, 1)\n",
            "iteration: 28900 , train shape: (64241, 48, 48, 1)\n",
            "iteration: 29000 , train shape: (64341, 48, 48, 1)\n",
            "iteration: 29100 , train shape: (64441, 48, 48, 1)\n",
            "iteration: 29200 , train shape: (64541, 48, 48, 1)\n",
            "iteration: 29300 , train shape: (64641, 48, 48, 1)\n",
            "iteration: 29400 , train shape: (64741, 48, 48, 1)\n",
            "iteration: 29500 , train shape: (64841, 48, 48, 1)\n",
            "iteration: 29600 , train shape: (64941, 48, 48, 1)\n",
            "iteration: 29700 , train shape: (65041, 48, 48, 1)\n",
            "iteration: 29800 , train shape: (65141, 48, 48, 1)\n",
            "iteration: 29900 , train shape: (65241, 48, 48, 1)\n",
            "iteration: 30000 , train shape: (65341, 48, 48, 1)\n",
            "iteration: 30100 , train shape: (65441, 48, 48, 1)\n",
            "iteration: 30200 , train shape: (65541, 48, 48, 1)\n",
            "iteration: 30300 , train shape: (65641, 48, 48, 1)\n",
            "iteration: 30400 , train shape: (65741, 48, 48, 1)\n",
            "iteration: 30500 , train shape: (65841, 48, 48, 1)\n",
            "iteration: 30600 , train shape: (65941, 48, 48, 1)\n",
            "iteration: 30700 , train shape: (66041, 48, 48, 1)\n",
            "iteration: 30800 , train shape: (66141, 48, 48, 1)\n",
            "iteration: 30900 , train shape: (66241, 48, 48, 1)\n",
            "iteration: 31000 , train shape: (66341, 48, 48, 1)\n",
            "iteration: 31100 , train shape: (66441, 48, 48, 1)\n",
            "iteration: 31200 , train shape: (66541, 48, 48, 1)\n",
            "iteration: 31300 , train shape: (66641, 48, 48, 1)\n",
            "iteration: 31400 , train shape: (66741, 48, 48, 1)\n",
            "iteration: 31500 , train shape: (66841, 48, 48, 1)\n",
            "iteration: 31600 , train shape: (66941, 48, 48, 1)\n",
            "iteration: 31700 , train shape: (67041, 48, 48, 1)\n",
            "iteration: 31800 , train shape: (67141, 48, 48, 1)\n",
            "iteration: 31900 , train shape: (67241, 48, 48, 1)\n",
            "iteration: 32000 , train shape: (67341, 48, 48, 1)\n",
            "iteration: 32100 , train shape: (67441, 48, 48, 1)\n",
            "iteration: 32200 , train shape: (67541, 48, 48, 1)\n",
            "iteration: 32300 , train shape: (67641, 48, 48, 1)\n",
            "iteration: 32400 , train shape: (67741, 48, 48, 1)\n",
            "iteration: 32500 , train shape: (67841, 48, 48, 1)\n",
            "iteration: 32600 , train shape: (67941, 48, 48, 1)\n",
            "iteration: 32700 , train shape: (68041, 48, 48, 1)\n",
            "iteration: 32800 , train shape: (68141, 48, 48, 1)\n",
            "iteration: 32900 , train shape: (68241, 48, 48, 1)\n",
            "iteration: 33000 , train shape: (68341, 48, 48, 1)\n",
            "iteration: 33100 , train shape: (68441, 48, 48, 1)\n",
            "iteration: 33200 , train shape: (68541, 48, 48, 1)\n",
            "iteration: 33300 , train shape: (68641, 48, 48, 1)\n",
            "iteration: 33400 , train shape: (68741, 48, 48, 1)\n",
            "iteration: 33500 , train shape: (68841, 48, 48, 1)\n",
            "iteration: 33600 , train shape: (68941, 48, 48, 1)\n",
            "iteration: 33700 , train shape: (69041, 48, 48, 1)\n",
            "iteration: 33800 , train shape: (69141, 48, 48, 1)\n",
            "iteration: 33900 , train shape: (69241, 48, 48, 1)\n",
            "iteration: 34000 , train shape: (69341, 48, 48, 1)\n",
            "iteration: 34100 , train shape: (69441, 48, 48, 1)\n",
            "iteration: 34200 , train shape: (69541, 48, 48, 1)\n",
            "iteration: 34300 , train shape: (69641, 48, 48, 1)\n",
            "iteration: 34400 , train shape: (69741, 48, 48, 1)\n",
            "iteration: 34500 , train shape: (69841, 48, 48, 1)\n",
            "iteration: 34600 , train shape: (69941, 48, 48, 1)\n",
            "iteration: 34700 , train shape: (70041, 48, 48, 1)\n",
            "iteration: 34800 , train shape: (70141, 48, 48, 1)\n",
            "iteration: 34900 , train shape: (70241, 48, 48, 1)\n",
            "iteration: 35000 , train shape: (70341, 48, 48, 1)\n",
            "iteration: 35100 , train shape: (70441, 48, 48, 1)\n",
            "iteration: 35200 , train shape: (70541, 48, 48, 1)\n",
            "iteration: 35300 , train shape: (70641, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWsEbPyuvHO8",
        "outputId": "3ac8100f-1134-4be0-990c-851774035da5"
      },
      "source": [
        "print(\"X_train_aug shape:\", X_train_aug.shape)\n",
        "print(\"y_train_aug shape:\", y_train_aug.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_aug shape: (70680, 48, 48, 1)\n",
            "y_train_aug shape: (70680, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding image of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "        zn = Flatten()(zi)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train_aug.shape[0], batch_size)\n",
        "            imgs, labels = X_train_aug[idx], y_train_aug[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(r\"C:\\Users\\shir2\\Desktop\\%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5GK1G4XQFj0",
        "outputId": "ee53bb9d-afb1-4d15-c385-2b162b15aa89"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18610, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.713815, acc.: 43.75%] [G loss: 1.464917]\n",
            "20 [D loss: 1.216423, acc.: 16.41%] [G loss: 0.701755]\n",
            "40 [D loss: 1.341811, acc.: 12.50%] [G loss: 0.815962]\n",
            "60 [D loss: 0.768335, acc.: 60.94%] [G loss: 5.146943]\n",
            "80 [D loss: 0.736260, acc.: 36.33%] [G loss: 2.803811]\n",
            "100 [D loss: 0.552230, acc.: 66.02%] [G loss: 2.127367]\n",
            "120 [D loss: 0.713486, acc.: 65.23%] [G loss: 3.728397]\n",
            "140 [D loss: 0.472640, acc.: 76.17%] [G loss: 1.727263]\n",
            "160 [D loss: 0.290819, acc.: 94.14%] [G loss: 8.310419]\n",
            "180 [D loss: 0.761461, acc.: 65.62%] [G loss: 2.794189]\n",
            "200 [D loss: 0.457494, acc.: 99.22%] [G loss: 3.194974]\n",
            "220 [D loss: 0.651058, acc.: 72.66%] [G loss: 2.333631]\n",
            "240 [D loss: 0.462749, acc.: 78.91%] [G loss: 5.039837]\n",
            "260 [D loss: 0.445606, acc.: 83.59%] [G loss: 2.895598]\n",
            "280 [D loss: 0.794912, acc.: 62.11%] [G loss: 2.330894]\n",
            "300 [D loss: 0.775773, acc.: 64.06%] [G loss: 2.462265]\n",
            "320 [D loss: 0.453494, acc.: 77.73%] [G loss: 3.828300]\n",
            "340 [D loss: 0.566135, acc.: 69.14%] [G loss: 3.942676]\n",
            "360 [D loss: 0.702895, acc.: 67.97%] [G loss: 3.168615]\n",
            "380 [D loss: 0.807296, acc.: 51.95%] [G loss: 2.011274]\n",
            "400 [D loss: 0.300534, acc.: 88.67%] [G loss: 6.210848]\n",
            "420 [D loss: 0.582350, acc.: 64.84%] [G loss: 3.194121]\n",
            "440 [D loss: 0.459012, acc.: 80.47%] [G loss: 4.754416]\n",
            "460 [D loss: 0.529183, acc.: 73.05%] [G loss: 3.888099]\n",
            "480 [D loss: 0.514183, acc.: 72.66%] [G loss: 4.574651]\n",
            "500 [D loss: 0.529423, acc.: 72.27%] [G loss: 3.092367]\n",
            "520 [D loss: 0.485583, acc.: 77.73%] [G loss: 3.409781]\n",
            "540 [D loss: 0.671973, acc.: 59.77%] [G loss: 2.768636]\n",
            "560 [D loss: 0.535236, acc.: 69.53%] [G loss: 2.878621]\n",
            "580 [D loss: 0.467609, acc.: 81.64%] [G loss: 3.235182]\n",
            "600 [D loss: 0.400509, acc.: 83.59%] [G loss: 4.643323]\n",
            "620 [D loss: 0.663045, acc.: 59.77%] [G loss: 2.580896]\n",
            "640 [D loss: 0.708245, acc.: 62.50%] [G loss: 1.887530]\n",
            "660 [D loss: 0.609629, acc.: 64.84%] [G loss: 2.582450]\n",
            "680 [D loss: 0.540885, acc.: 73.44%] [G loss: 3.252695]\n",
            "700 [D loss: 0.518393, acc.: 75.39%] [G loss: 4.871997]\n",
            "720 [D loss: 0.498347, acc.: 79.69%] [G loss: 2.681623]\n",
            "740 [D loss: 0.333202, acc.: 87.11%] [G loss: 6.032172]\n",
            "760 [D loss: 0.506339, acc.: 73.83%] [G loss: 3.250460]\n",
            "780 [D loss: 0.530706, acc.: 73.83%] [G loss: 3.309617]\n",
            "800 [D loss: 0.561007, acc.: 70.31%] [G loss: 2.482278]\n",
            "820 [D loss: 0.521382, acc.: 74.22%] [G loss: 2.874556]\n",
            "840 [D loss: 0.502905, acc.: 78.91%] [G loss: 2.933999]\n",
            "860 [D loss: 0.544007, acc.: 71.48%] [G loss: 2.799255]\n",
            "880 [D loss: 0.482905, acc.: 80.47%] [G loss: 3.250840]\n",
            "900 [D loss: 0.483615, acc.: 76.95%] [G loss: 3.053282]\n",
            "920 [D loss: 0.418737, acc.: 85.16%] [G loss: 3.731601]\n",
            "940 [D loss: 0.474910, acc.: 75.00%] [G loss: 3.420846]\n",
            "960 [D loss: 0.542538, acc.: 78.12%] [G loss: 3.462269]\n",
            "980 [D loss: 0.547132, acc.: 71.88%] [G loss: 3.634694]\n",
            "1000 [D loss: 0.494476, acc.: 76.56%] [G loss: 3.578008]\n",
            "1020 [D loss: 0.547034, acc.: 70.31%] [G loss: 2.758028]\n",
            "1040 [D loss: 0.624189, acc.: 66.02%] [G loss: 2.500589]\n",
            "1060 [D loss: 0.548156, acc.: 71.09%] [G loss: 2.922811]\n",
            "1080 [D loss: 0.540107, acc.: 73.44%] [G loss: 2.931180]\n",
            "1100 [D loss: 0.494578, acc.: 78.12%] [G loss: 3.096200]\n",
            "1120 [D loss: 0.518911, acc.: 75.00%] [G loss: 2.797499]\n",
            "1140 [D loss: 0.558146, acc.: 70.70%] [G loss: 2.848915]\n",
            "1160 [D loss: 0.419026, acc.: 84.38%] [G loss: 3.634684]\n",
            "1180 [D loss: 0.608828, acc.: 65.23%] [G loss: 3.455203]\n",
            "1200 [D loss: 0.471272, acc.: 79.30%] [G loss: 3.112056]\n",
            "1220 [D loss: 0.556373, acc.: 70.70%] [G loss: 3.093167]\n",
            "1240 [D loss: 0.459097, acc.: 80.86%] [G loss: 3.529114]\n",
            "1260 [D loss: 0.512147, acc.: 74.61%] [G loss: 3.306969]\n",
            "1280 [D loss: 0.509105, acc.: 72.66%] [G loss: 3.270307]\n",
            "1300 [D loss: 0.557847, acc.: 71.09%] [G loss: 3.221519]\n",
            "1320 [D loss: 0.529063, acc.: 75.00%] [G loss: 2.795586]\n",
            "1340 [D loss: 0.542026, acc.: 71.09%] [G loss: 3.171018]\n",
            "1360 [D loss: 0.387305, acc.: 84.77%] [G loss: 3.016227]\n",
            "1380 [D loss: 0.905244, acc.: 50.39%] [G loss: 2.993938]\n",
            "1400 [D loss: 0.768133, acc.: 58.98%] [G loss: 2.574736]\n",
            "1420 [D loss: 0.530642, acc.: 74.22%] [G loss: 3.042671]\n",
            "1440 [D loss: 0.360328, acc.: 87.89%] [G loss: 4.003502]\n",
            "1460 [D loss: 0.568517, acc.: 69.53%] [G loss: 2.388592]\n",
            "1480 [D loss: 0.647559, acc.: 62.11%] [G loss: 2.506496]\n",
            "1500 [D loss: 0.537122, acc.: 75.00%] [G loss: 2.801897]\n",
            "1520 [D loss: 3.245996, acc.: 19.14%] [G loss: 2.869269]\n",
            "1540 [D loss: 0.800253, acc.: 55.47%] [G loss: 2.470595]\n",
            "1560 [D loss: 0.487424, acc.: 83.59%] [G loss: 2.574246]\n",
            "1580 [D loss: 0.693914, acc.: 58.20%] [G loss: 2.517337]\n",
            "1600 [D loss: 0.556180, acc.: 71.09%] [G loss: 2.614802]\n",
            "1620 [D loss: 0.633688, acc.: 60.55%] [G loss: 2.428762]\n",
            "1640 [D loss: 0.470864, acc.: 80.86%] [G loss: 3.040797]\n",
            "1660 [D loss: 0.528429, acc.: 75.00%] [G loss: 2.858768]\n",
            "1680 [D loss: 0.514200, acc.: 75.39%] [G loss: 3.164742]\n",
            "1700 [D loss: 0.546220, acc.: 75.78%] [G loss: 2.772143]\n",
            "1720 [D loss: 0.529279, acc.: 74.61%] [G loss: 3.051643]\n",
            "1740 [D loss: 0.468124, acc.: 80.08%] [G loss: 3.064984]\n",
            "1760 [D loss: 0.528600, acc.: 75.00%] [G loss: 2.985373]\n",
            "1780 [D loss: 0.459007, acc.: 81.64%] [G loss: 3.078534]\n",
            "1800 [D loss: 0.563753, acc.: 71.09%] [G loss: 2.715739]\n",
            "1820 [D loss: 0.555879, acc.: 72.27%] [G loss: 2.747668]\n",
            "1840 [D loss: 0.543449, acc.: 74.61%] [G loss: 2.689451]\n",
            "1860 [D loss: 0.560649, acc.: 69.92%] [G loss: 2.614350]\n",
            "1880 [D loss: 0.501904, acc.: 78.12%] [G loss: 2.641221]\n",
            "1900 [D loss: 0.753872, acc.: 55.86%] [G loss: 2.373092]\n",
            "1920 [D loss: 0.560056, acc.: 70.70%] [G loss: 3.001401]\n",
            "1940 [D loss: 0.535433, acc.: 75.39%] [G loss: 3.012889]\n",
            "1960 [D loss: 0.592579, acc.: 67.58%] [G loss: 2.853446]\n",
            "1980 [D loss: 0.614155, acc.: 67.19%] [G loss: 2.697653]\n",
            "2000 [D loss: 0.645458, acc.: 60.94%] [G loss: 2.433101]\n",
            "2020 [D loss: 0.564239, acc.: 69.92%] [G loss: 2.688430]\n",
            "2040 [D loss: 0.550352, acc.: 72.66%] [G loss: 2.676042]\n",
            "2060 [D loss: 0.585013, acc.: 68.36%] [G loss: 2.798478]\n",
            "2080 [D loss: 0.604988, acc.: 67.19%] [G loss: 2.528838]\n",
            "2100 [D loss: 0.470525, acc.: 77.34%] [G loss: 2.910759]\n",
            "2120 [D loss: 0.535543, acc.: 73.05%] [G loss: 2.805406]\n",
            "2140 [D loss: 0.575946, acc.: 65.23%] [G loss: 2.637581]\n",
            "2160 [D loss: 0.608887, acc.: 64.06%] [G loss: 2.631211]\n",
            "2180 [D loss: 0.587163, acc.: 70.31%] [G loss: 2.559998]\n",
            "2200 [D loss: 0.558024, acc.: 71.48%] [G loss: 2.739963]\n",
            "2220 [D loss: 0.502841, acc.: 76.95%] [G loss: 2.658070]\n",
            "2240 [D loss: 0.592447, acc.: 67.58%] [G loss: 2.532337]\n",
            "2260 [D loss: 0.540875, acc.: 74.61%] [G loss: 2.609999]\n",
            "2280 [D loss: 0.515147, acc.: 76.17%] [G loss: 2.961514]\n",
            "2300 [D loss: 0.587587, acc.: 71.09%] [G loss: 2.638791]\n",
            "2320 [D loss: 0.568556, acc.: 71.09%] [G loss: 2.551488]\n",
            "2340 [D loss: 0.548125, acc.: 74.22%] [G loss: 2.629685]\n",
            "2360 [D loss: 0.595771, acc.: 66.41%] [G loss: 2.511956]\n",
            "2380 [D loss: 0.566769, acc.: 70.70%] [G loss: 2.664062]\n",
            "2400 [D loss: 0.519074, acc.: 76.56%] [G loss: 2.881299]\n",
            "2420 [D loss: 0.627473, acc.: 68.36%] [G loss: 2.394492]\n",
            "2440 [D loss: 0.546318, acc.: 74.22%] [G loss: 2.599963]\n",
            "2460 [D loss: 0.543001, acc.: 70.31%] [G loss: 2.735461]\n",
            "2480 [D loss: 0.596964, acc.: 66.80%] [G loss: 2.585777]\n",
            "2500 [D loss: 0.546323, acc.: 74.22%] [G loss: 2.598194]\n",
            "2520 [D loss: 0.538997, acc.: 73.05%] [G loss: 2.481353]\n",
            "2540 [D loss: 0.577903, acc.: 68.36%] [G loss: 2.554359]\n",
            "2560 [D loss: 0.524882, acc.: 72.27%] [G loss: 2.659684]\n",
            "2580 [D loss: 0.581787, acc.: 69.53%] [G loss: 2.727448]\n",
            "2600 [D loss: 0.594509, acc.: 67.19%] [G loss: 2.707459]\n",
            "2620 [D loss: 0.466890, acc.: 79.69%] [G loss: 2.579076]\n",
            "2640 [D loss: 0.641831, acc.: 63.67%] [G loss: 2.068990]\n",
            "2660 [D loss: 0.726866, acc.: 56.25%] [G loss: 2.673640]\n",
            "2680 [D loss: 0.641020, acc.: 63.28%] [G loss: 2.687622]\n",
            "2700 [D loss: 0.613907, acc.: 68.36%] [G loss: 2.770665]\n",
            "2720 [D loss: 0.588355, acc.: 71.48%] [G loss: 2.653878]\n",
            "2740 [D loss: 0.505858, acc.: 78.12%] [G loss: 2.888841]\n",
            "2760 [D loss: 0.674414, acc.: 59.77%] [G loss: 2.535233]\n",
            "2780 [D loss: 0.579031, acc.: 66.80%] [G loss: 2.783712]\n",
            "2800 [D loss: 0.599665, acc.: 67.97%] [G loss: 2.710576]\n",
            "2820 [D loss: 0.490029, acc.: 76.17%] [G loss: 2.888308]\n",
            "2840 [D loss: 0.559797, acc.: 73.44%] [G loss: 2.924796]\n",
            "2860 [D loss: 0.748480, acc.: 54.69%] [G loss: 2.524983]\n",
            "2880 [D loss: 0.523243, acc.: 75.00%] [G loss: 2.959496]\n",
            "2900 [D loss: 0.522734, acc.: 76.56%] [G loss: 2.781785]\n",
            "2920 [D loss: 0.591422, acc.: 66.02%] [G loss: 2.614297]\n",
            "2940 [D loss: 0.534701, acc.: 73.83%] [G loss: 2.851176]\n",
            "2960 [D loss: 0.590601, acc.: 67.97%] [G loss: 2.711897]\n",
            "2980 [D loss: 0.548250, acc.: 73.05%] [G loss: 2.729564]\n",
            "3000 [D loss: 0.486674, acc.: 76.95%] [G loss: 2.841930]\n",
            "3020 [D loss: 0.646393, acc.: 61.33%] [G loss: 2.528862]\n",
            "3040 [D loss: 0.513447, acc.: 75.78%] [G loss: 2.639036]\n",
            "3060 [D loss: 0.664412, acc.: 59.77%] [G loss: 1.702322]\n",
            "3080 [D loss: 0.745665, acc.: 40.23%] [G loss: 1.532451]\n",
            "3100 [D loss: 0.664036, acc.: 58.59%] [G loss: 1.660240]\n",
            "3120 [D loss: 0.711092, acc.: 46.88%] [G loss: 1.611123]\n",
            "3140 [D loss: 0.680528, acc.: 57.81%] [G loss: 1.738720]\n",
            "3160 [D loss: 0.665175, acc.: 61.33%] [G loss: 1.495984]\n",
            "3180 [D loss: 0.589618, acc.: 73.05%] [G loss: 1.951481]\n",
            "3200 [D loss: 0.565689, acc.: 76.56%] [G loss: 1.683809]\n",
            "3220 [D loss: 0.883381, acc.: 40.62%] [G loss: 1.994888]\n",
            "3240 [D loss: 0.742688, acc.: 53.12%] [G loss: 2.215013]\n",
            "3260 [D loss: 0.677897, acc.: 55.47%] [G loss: 2.282136]\n",
            "3280 [D loss: 0.594142, acc.: 71.88%] [G loss: 2.336688]\n",
            "3300 [D loss: 0.458320, acc.: 84.38%] [G loss: 2.820597]\n",
            "3320 [D loss: 0.653619, acc.: 62.50%] [G loss: 2.101415]\n",
            "3340 [D loss: 0.566714, acc.: 73.05%] [G loss: 2.314678]\n",
            "3360 [D loss: 0.587197, acc.: 69.92%] [G loss: 2.408746]\n",
            "3380 [D loss: 0.589653, acc.: 67.97%] [G loss: 2.354812]\n",
            "3400 [D loss: 0.601715, acc.: 66.02%] [G loss: 2.419466]\n",
            "3420 [D loss: 0.608071, acc.: 67.97%] [G loss: 2.518591]\n",
            "3440 [D loss: 0.525692, acc.: 76.95%] [G loss: 2.556502]\n",
            "3460 [D loss: 0.592899, acc.: 67.19%] [G loss: 2.328084]\n",
            "3480 [D loss: 0.576758, acc.: 72.66%] [G loss: 2.422248]\n",
            "3500 [D loss: 0.625095, acc.: 65.62%] [G loss: 2.320192]\n",
            "3520 [D loss: 0.537958, acc.: 73.44%] [G loss: 2.598453]\n",
            "3540 [D loss: 0.521218, acc.: 77.34%] [G loss: 2.622884]\n",
            "3560 [D loss: 0.596791, acc.: 69.14%] [G loss: 2.463584]\n",
            "3580 [D loss: 0.606410, acc.: 68.36%] [G loss: 2.418896]\n",
            "3600 [D loss: 0.618348, acc.: 65.23%] [G loss: 2.273677]\n",
            "3620 [D loss: 0.613274, acc.: 65.62%] [G loss: 2.212939]\n",
            "3640 [D loss: 0.567678, acc.: 73.83%] [G loss: 2.270427]\n",
            "3660 [D loss: 0.418005, acc.: 87.11%] [G loss: 1.787056]\n",
            "3680 [D loss: 0.593022, acc.: 69.14%] [G loss: 1.335514]\n",
            "3700 [D loss: 1.520029, acc.: 15.23%] [G loss: 1.149816]\n",
            "3720 [D loss: 0.657300, acc.: 59.38%] [G loss: 3.141562]\n",
            "3740 [D loss: 0.794358, acc.: 52.73%] [G loss: 2.606052]\n",
            "3760 [D loss: 0.677265, acc.: 59.38%] [G loss: 2.891389]\n",
            "3780 [D loss: 0.727249, acc.: 56.64%] [G loss: 2.722034]\n",
            "3800 [D loss: 0.706742, acc.: 57.81%] [G loss: 2.258779]\n",
            "3820 [D loss: 0.722721, acc.: 55.08%] [G loss: 2.226044]\n",
            "3840 [D loss: 0.563596, acc.: 71.88%] [G loss: 2.634127]\n",
            "3860 [D loss: 0.621924, acc.: 64.84%] [G loss: 2.508152]\n",
            "3880 [D loss: 0.663157, acc.: 63.28%] [G loss: 2.353721]\n",
            "3900 [D loss: 0.600278, acc.: 66.41%] [G loss: 2.573144]\n",
            "3920 [D loss: 0.586968, acc.: 70.70%] [G loss: 2.445951]\n",
            "3940 [D loss: 0.569632, acc.: 71.88%] [G loss: 2.411447]\n",
            "3960 [D loss: 0.597218, acc.: 67.58%] [G loss: 2.339085]\n",
            "3980 [D loss: 0.557583, acc.: 71.48%] [G loss: 2.474424]\n",
            "4000 [D loss: 0.618755, acc.: 64.45%] [G loss: 2.396148]\n",
            "4020 [D loss: 0.573911, acc.: 71.09%] [G loss: 2.298775]\n",
            "4040 [D loss: 0.545443, acc.: 75.00%] [G loss: 2.446781]\n",
            "4060 [D loss: 0.575755, acc.: 70.70%] [G loss: 2.538808]\n",
            "4080 [D loss: 0.630465, acc.: 62.50%] [G loss: 2.332539]\n",
            "4100 [D loss: 0.566125, acc.: 71.88%] [G loss: 2.611660]\n",
            "4120 [D loss: 0.620494, acc.: 62.89%] [G loss: 2.487648]\n",
            "4140 [D loss: 0.508081, acc.: 71.88%] [G loss: 2.829769]\n",
            "4160 [D loss: 0.656823, acc.: 62.89%] [G loss: 2.455589]\n",
            "4180 [D loss: 0.586179, acc.: 67.97%] [G loss: 2.384863]\n",
            "4200 [D loss: 0.602608, acc.: 67.58%] [G loss: 2.451812]\n",
            "4220 [D loss: 0.566454, acc.: 67.97%] [G loss: 2.566762]\n",
            "4240 [D loss: 0.587584, acc.: 69.14%] [G loss: 2.339155]\n",
            "4260 [D loss: 0.536689, acc.: 73.44%] [G loss: 2.660351]\n",
            "4280 [D loss: 0.541927, acc.: 73.44%] [G loss: 1.992924]\n",
            "4300 [D loss: 0.553393, acc.: 75.00%] [G loss: 2.066249]\n",
            "4320 [D loss: 0.649455, acc.: 64.84%] [G loss: 1.403845]\n",
            "4340 [D loss: 1.081834, acc.: 37.11%] [G loss: 2.126339]\n",
            "4360 [D loss: 0.832150, acc.: 46.09%] [G loss: 2.083871]\n",
            "4380 [D loss: 0.667229, acc.: 62.50%] [G loss: 2.018073]\n",
            "4400 [D loss: 0.592097, acc.: 72.27%] [G loss: 2.822808]\n",
            "4420 [D loss: 0.673630, acc.: 59.77%] [G loss: 2.393186]\n",
            "4440 [D loss: 0.618390, acc.: 65.23%] [G loss: 2.641086]\n",
            "4460 [D loss: 0.594065, acc.: 69.53%] [G loss: 2.891587]\n",
            "4480 [D loss: 0.460767, acc.: 79.69%] [G loss: 2.858342]\n",
            "4500 [D loss: 0.625175, acc.: 62.89%] [G loss: 2.477585]\n",
            "4520 [D loss: 0.547556, acc.: 72.66%] [G loss: 2.675874]\n",
            "4540 [D loss: 0.684453, acc.: 62.11%] [G loss: 2.322582]\n",
            "4560 [D loss: 0.539791, acc.: 73.05%] [G loss: 2.706357]\n",
            "4580 [D loss: 0.609496, acc.: 68.75%] [G loss: 2.367404]\n",
            "4600 [D loss: 0.592671, acc.: 68.75%] [G loss: 2.509464]\n",
            "4620 [D loss: 0.468094, acc.: 81.64%] [G loss: 1.604818]\n",
            "4640 [D loss: 0.344059, acc.: 91.41%] [G loss: 1.842033]\n",
            "4660 [D loss: 0.816109, acc.: 50.78%] [G loss: 1.198936]\n",
            "4680 [D loss: 0.816703, acc.: 46.48%] [G loss: 1.599550]\n",
            "4700 [D loss: 0.541261, acc.: 73.44%] [G loss: 2.625093]\n",
            "4720 [D loss: 0.606768, acc.: 69.14%] [G loss: 3.028953]\n",
            "4740 [D loss: 0.552797, acc.: 71.88%] [G loss: 2.647648]\n",
            "4760 [D loss: 0.634995, acc.: 65.23%] [G loss: 2.117567]\n",
            "4780 [D loss: 0.678339, acc.: 59.38%] [G loss: 2.503648]\n",
            "4800 [D loss: 0.600258, acc.: 64.84%] [G loss: 2.978870]\n",
            "4820 [D loss: 0.631048, acc.: 66.80%] [G loss: 2.378020]\n",
            "4840 [D loss: 0.558004, acc.: 71.09%] [G loss: 2.720431]\n",
            "4860 [D loss: 0.592846, acc.: 69.53%] [G loss: 2.631028]\n",
            "4880 [D loss: 0.564007, acc.: 72.66%] [G loss: 2.773992]\n",
            "4900 [D loss: 0.552360, acc.: 73.83%] [G loss: 2.862599]\n",
            "4920 [D loss: 0.595836, acc.: 69.14%] [G loss: 2.478806]\n",
            "4940 [D loss: 0.536017, acc.: 74.61%] [G loss: 2.811643]\n",
            "4960 [D loss: 0.665954, acc.: 58.20%] [G loss: 2.482611]\n",
            "4980 [D loss: 0.609021, acc.: 64.84%] [G loss: 2.624558]\n",
            "5000 [D loss: 0.620249, acc.: 69.14%] [G loss: 2.539466]\n",
            "5020 [D loss: 0.624342, acc.: 63.67%] [G loss: 2.625324]\n",
            "5040 [D loss: 0.512287, acc.: 75.00%] [G loss: 2.703628]\n",
            "5060 [D loss: 0.575318, acc.: 69.53%] [G loss: 2.725449]\n",
            "5080 [D loss: 0.601431, acc.: 66.80%] [G loss: 2.454364]\n",
            "5100 [D loss: 0.528521, acc.: 74.61%] [G loss: 2.698012]\n",
            "5120 [D loss: 0.606991, acc.: 67.19%] [G loss: 2.559309]\n",
            "5140 [D loss: 0.586771, acc.: 67.19%] [G loss: 2.483922]\n",
            "5160 [D loss: 0.528802, acc.: 72.27%] [G loss: 2.950044]\n",
            "5180 [D loss: 0.470341, acc.: 79.30%] [G loss: 2.929168]\n",
            "5200 [D loss: 0.568891, acc.: 72.27%] [G loss: 2.561809]\n",
            "5220 [D loss: 0.564227, acc.: 69.92%] [G loss: 2.749274]\n",
            "5240 [D loss: 0.635811, acc.: 64.06%] [G loss: 2.593138]\n",
            "5260 [D loss: 0.550896, acc.: 73.05%] [G loss: 2.692921]\n",
            "5280 [D loss: 0.550250, acc.: 71.48%] [G loss: 2.612362]\n",
            "5300 [D loss: 0.554760, acc.: 71.48%] [G loss: 2.685159]\n",
            "5320 [D loss: 0.491048, acc.: 79.69%] [G loss: 3.093201]\n",
            "5340 [D loss: 0.597446, acc.: 65.62%] [G loss: 2.599389]\n",
            "5360 [D loss: 0.518195, acc.: 73.44%] [G loss: 2.898492]\n",
            "5380 [D loss: 0.503718, acc.: 75.78%] [G loss: 2.697905]\n",
            "5400 [D loss: 0.572110, acc.: 72.27%] [G loss: 2.793272]\n",
            "5420 [D loss: 0.542375, acc.: 72.27%] [G loss: 2.686579]\n",
            "5440 [D loss: 0.580814, acc.: 69.14%] [G loss: 2.697688]\n",
            "5460 [D loss: 0.498686, acc.: 77.73%] [G loss: 2.803936]\n",
            "5480 [D loss: 0.604413, acc.: 69.92%] [G loss: 2.496984]\n",
            "5500 [D loss: 0.542305, acc.: 75.00%] [G loss: 2.699320]\n",
            "5520 [D loss: 0.509070, acc.: 78.52%] [G loss: 2.665548]\n",
            "5540 [D loss: 0.632313, acc.: 66.41%] [G loss: 2.545857]\n",
            "5560 [D loss: 0.509249, acc.: 76.95%] [G loss: 2.793588]\n",
            "5580 [D loss: 0.460632, acc.: 81.25%] [G loss: 2.210417]\n",
            "5600 [D loss: 0.514177, acc.: 76.17%] [G loss: 2.068514]\n",
            "5620 [D loss: 0.839104, acc.: 53.52%] [G loss: 1.850707]\n",
            "5640 [D loss: 0.868091, acc.: 41.02%] [G loss: 2.195457]\n",
            "5660 [D loss: 0.811132, acc.: 43.75%] [G loss: 2.729961]\n",
            "5680 [D loss: 0.526872, acc.: 72.27%] [G loss: 2.820825]\n",
            "5700 [D loss: 0.651966, acc.: 64.06%] [G loss: 2.328168]\n",
            "5720 [D loss: 0.603671, acc.: 63.28%] [G loss: 2.726821]\n",
            "5740 [D loss: 0.583119, acc.: 68.36%] [G loss: 2.870000]\n",
            "5760 [D loss: 0.594116, acc.: 70.70%] [G loss: 2.633886]\n",
            "5780 [D loss: 0.553672, acc.: 69.14%] [G loss: 2.913268]\n",
            "5800 [D loss: 0.535690, acc.: 71.88%] [G loss: 2.704342]\n",
            "5820 [D loss: 0.589012, acc.: 66.02%] [G loss: 2.749311]\n",
            "5840 [D loss: 0.516206, acc.: 74.61%] [G loss: 3.002418]\n",
            "5860 [D loss: 0.592501, acc.: 65.62%] [G loss: 2.610202]\n",
            "5880 [D loss: 0.558779, acc.: 72.66%] [G loss: 2.781803]\n",
            "5900 [D loss: 0.555548, acc.: 71.88%] [G loss: 2.644004]\n",
            "5920 [D loss: 0.560305, acc.: 69.92%] [G loss: 2.905796]\n",
            "5940 [D loss: 0.489822, acc.: 78.91%] [G loss: 2.876171]\n",
            "5960 [D loss: 0.601196, acc.: 65.62%] [G loss: 2.807082]\n",
            "5980 [D loss: 0.564827, acc.: 69.53%] [G loss: 2.814660]\n",
            "6000 [D loss: 0.554048, acc.: 71.48%] [G loss: 2.772602]\n",
            "6020 [D loss: 0.527808, acc.: 74.22%] [G loss: 2.658766]\n",
            "6040 [D loss: 0.499615, acc.: 75.78%] [G loss: 3.222530]\n",
            "6060 [D loss: 0.612586, acc.: 67.19%] [G loss: 2.860051]\n",
            "6080 [D loss: 0.518757, acc.: 75.39%] [G loss: 3.082365]\n",
            "6100 [D loss: 0.519859, acc.: 75.39%] [G loss: 2.729416]\n",
            "6120 [D loss: 0.527820, acc.: 73.05%] [G loss: 3.112731]\n",
            "6140 [D loss: 0.557808, acc.: 70.31%] [G loss: 2.944877]\n",
            "6160 [D loss: 0.567675, acc.: 70.70%] [G loss: 2.831415]\n",
            "6180 [D loss: 0.622180, acc.: 66.02%] [G loss: 2.850366]\n",
            "6200 [D loss: 0.489063, acc.: 78.12%] [G loss: 3.124459]\n",
            "6220 [D loss: 0.562044, acc.: 71.48%] [G loss: 2.889248]\n",
            "6240 [D loss: 0.644221, acc.: 64.06%] [G loss: 2.914605]\n",
            "6260 [D loss: 0.554411, acc.: 71.88%] [G loss: 2.749750]\n",
            "6280 [D loss: 0.545348, acc.: 71.88%] [G loss: 2.682316]\n",
            "6300 [D loss: 0.513393, acc.: 75.00%] [G loss: 2.917480]\n",
            "6320 [D loss: 0.520768, acc.: 76.95%] [G loss: 2.940192]\n",
            "6340 [D loss: 0.581315, acc.: 69.14%] [G loss: 2.788397]\n",
            "6360 [D loss: 0.557233, acc.: 70.70%] [G loss: 2.940812]\n",
            "6380 [D loss: 0.698691, acc.: 60.16%] [G loss: 2.546960]\n",
            "6400 [D loss: 0.499893, acc.: 76.95%] [G loss: 3.279195]\n",
            "6420 [D loss: 0.570904, acc.: 72.27%] [G loss: 2.872866]\n",
            "6440 [D loss: 0.528772, acc.: 74.22%] [G loss: 3.024359]\n",
            "6460 [D loss: 0.577029, acc.: 71.88%] [G loss: 3.327772]\n",
            "6480 [D loss: 0.579572, acc.: 68.75%] [G loss: 2.883757]\n",
            "6500 [D loss: 0.527095, acc.: 74.22%] [G loss: 2.824043]\n",
            "6520 [D loss: 0.502769, acc.: 75.00%] [G loss: 3.014560]\n",
            "6540 [D loss: 0.492861, acc.: 78.12%] [G loss: 3.112438]\n",
            "6560 [D loss: 0.554247, acc.: 70.70%] [G loss: 3.012650]\n",
            "6580 [D loss: 0.518630, acc.: 74.61%] [G loss: 3.058880]\n",
            "6600 [D loss: 0.535446, acc.: 73.05%] [G loss: 3.001637]\n",
            "6620 [D loss: 0.461375, acc.: 78.12%] [G loss: 3.354010]\n",
            "6640 [D loss: 0.588231, acc.: 68.75%] [G loss: 2.825142]\n",
            "6660 [D loss: 0.545538, acc.: 70.70%] [G loss: 2.984393]\n",
            "6680 [D loss: 0.521986, acc.: 73.83%] [G loss: 2.862982]\n",
            "6700 [D loss: 0.549507, acc.: 72.27%] [G loss: 2.929526]\n",
            "6720 [D loss: 0.507256, acc.: 77.73%] [G loss: 3.272951]\n",
            "6740 [D loss: 0.593834, acc.: 67.97%] [G loss: 2.833016]\n",
            "6760 [D loss: 0.531183, acc.: 73.83%] [G loss: 3.332238]\n",
            "6780 [D loss: 0.566884, acc.: 71.48%] [G loss: 2.900844]\n",
            "6800 [D loss: 0.542988, acc.: 70.70%] [G loss: 2.855332]\n",
            "6820 [D loss: 0.522445, acc.: 74.22%] [G loss: 2.940929]\n",
            "6840 [D loss: 0.546401, acc.: 74.22%] [G loss: 2.750804]\n",
            "6860 [D loss: 0.543900, acc.: 74.22%] [G loss: 3.092743]\n",
            "6880 [D loss: 0.559271, acc.: 69.92%] [G loss: 2.648446]\n",
            "6900 [D loss: 0.500862, acc.: 75.39%] [G loss: 3.070590]\n",
            "6920 [D loss: 0.508697, acc.: 78.12%] [G loss: 3.049725]\n",
            "6940 [D loss: 0.519505, acc.: 77.73%] [G loss: 2.907967]\n",
            "6960 [D loss: 0.473664, acc.: 79.30%] [G loss: 3.169094]\n",
            "6980 [D loss: 0.517666, acc.: 74.61%] [G loss: 3.068241]\n",
            "7000 [D loss: 0.498100, acc.: 77.73%] [G loss: 3.100056]\n",
            "7020 [D loss: 0.525679, acc.: 73.83%] [G loss: 2.685768]\n",
            "7040 [D loss: 0.546700, acc.: 73.05%] [G loss: 2.953342]\n",
            "7060 [D loss: 0.564269, acc.: 72.66%] [G loss: 2.981326]\n",
            "7080 [D loss: 0.499871, acc.: 77.34%] [G loss: 3.011868]\n",
            "7100 [D loss: 0.569822, acc.: 70.31%] [G loss: 3.037768]\n",
            "7120 [D loss: 0.506987, acc.: 73.83%] [G loss: 2.890715]\n",
            "7140 [D loss: 0.537694, acc.: 73.44%] [G loss: 2.994213]\n",
            "7160 [D loss: 0.467894, acc.: 78.12%] [G loss: 3.285072]\n",
            "7180 [D loss: 0.504290, acc.: 74.61%] [G loss: 2.977401]\n",
            "7200 [D loss: 0.533915, acc.: 73.05%] [G loss: 3.003671]\n",
            "7220 [D loss: 0.582942, acc.: 65.62%] [G loss: 3.014498]\n",
            "7240 [D loss: 0.524285, acc.: 71.88%] [G loss: 3.121064]\n",
            "7260 [D loss: 0.549918, acc.: 76.17%] [G loss: 2.837120]\n",
            "7280 [D loss: 0.521937, acc.: 75.00%] [G loss: 3.032494]\n",
            "7300 [D loss: 0.546678, acc.: 71.09%] [G loss: 2.946620]\n",
            "7320 [D loss: 0.592615, acc.: 66.80%] [G loss: 3.184445]\n",
            "7340 [D loss: 0.574874, acc.: 66.80%] [G loss: 3.033424]\n",
            "7360 [D loss: 0.501692, acc.: 78.12%] [G loss: 3.537405]\n",
            "7380 [D loss: 0.560385, acc.: 69.53%] [G loss: 2.926249]\n",
            "7400 [D loss: 0.608177, acc.: 66.80%] [G loss: 3.039710]\n",
            "7420 [D loss: 0.528559, acc.: 72.66%] [G loss: 3.073142]\n",
            "7440 [D loss: 0.562316, acc.: 70.70%] [G loss: 2.785039]\n",
            "7460 [D loss: 0.555332, acc.: 70.31%] [G loss: 3.161453]\n",
            "7480 [D loss: 0.483513, acc.: 79.30%] [G loss: 3.297520]\n",
            "7500 [D loss: 0.491876, acc.: 79.30%] [G loss: 2.956971]\n",
            "7520 [D loss: 0.504678, acc.: 74.61%] [G loss: 3.119707]\n",
            "7540 [D loss: 0.466807, acc.: 79.69%] [G loss: 3.373012]\n",
            "7560 [D loss: 0.549532, acc.: 74.61%] [G loss: 3.237418]\n",
            "7580 [D loss: 0.475453, acc.: 79.30%] [G loss: 3.288899]\n",
            "7600 [D loss: 0.467863, acc.: 79.69%] [G loss: 3.449228]\n",
            "7620 [D loss: 0.585435, acc.: 69.92%] [G loss: 2.774668]\n",
            "7640 [D loss: 0.401132, acc.: 84.77%] [G loss: 3.447384]\n",
            "7660 [D loss: 0.605181, acc.: 67.97%] [G loss: 2.926118]\n",
            "7680 [D loss: 0.531994, acc.: 74.61%] [G loss: 3.141743]\n",
            "7700 [D loss: 0.523203, acc.: 75.00%] [G loss: 3.177360]\n",
            "7720 [D loss: 0.535679, acc.: 74.22%] [G loss: 3.208471]\n",
            "7740 [D loss: 0.549631, acc.: 73.05%] [G loss: 3.329988]\n",
            "7760 [D loss: 0.623421, acc.: 68.36%] [G loss: 3.231248]\n",
            "7780 [D loss: 0.532476, acc.: 74.22%] [G loss: 3.114951]\n",
            "7800 [D loss: 0.496359, acc.: 76.95%] [G loss: 3.219207]\n",
            "7820 [D loss: 0.493137, acc.: 76.56%] [G loss: 3.026422]\n",
            "7840 [D loss: 0.556287, acc.: 70.70%] [G loss: 2.996613]\n",
            "7860 [D loss: 0.512223, acc.: 75.00%] [G loss: 3.045487]\n",
            "7880 [D loss: 0.495619, acc.: 76.95%] [G loss: 3.040879]\n",
            "7900 [D loss: 0.495467, acc.: 77.73%] [G loss: 3.368224]\n",
            "7920 [D loss: 0.535277, acc.: 75.00%] [G loss: 2.965650]\n",
            "7940 [D loss: 0.553063, acc.: 72.27%] [G loss: 3.173407]\n",
            "7960 [D loss: 0.507109, acc.: 76.95%] [G loss: 3.151795]\n",
            "7980 [D loss: 0.525921, acc.: 72.66%] [G loss: 3.354044]\n",
            "8000 [D loss: 0.494470, acc.: 76.95%] [G loss: 3.072932]\n",
            "8020 [D loss: 0.577198, acc.: 68.36%] [G loss: 3.062149]\n",
            "8040 [D loss: 0.506139, acc.: 76.95%] [G loss: 3.137970]\n",
            "8060 [D loss: 0.519768, acc.: 76.17%] [G loss: 3.292947]\n",
            "8080 [D loss: 0.498131, acc.: 73.05%] [G loss: 3.146217]\n",
            "8100 [D loss: 0.517415, acc.: 75.00%] [G loss: 3.375129]\n",
            "8120 [D loss: 0.500722, acc.: 75.78%] [G loss: 3.302024]\n",
            "8140 [D loss: 0.511213, acc.: 75.39%] [G loss: 3.150124]\n",
            "8160 [D loss: 0.535868, acc.: 74.22%] [G loss: 3.056844]\n",
            "8180 [D loss: 0.528706, acc.: 74.61%] [G loss: 2.904606]\n",
            "8200 [D loss: 0.551998, acc.: 73.44%] [G loss: 2.987675]\n",
            "8220 [D loss: 0.519205, acc.: 73.05%] [G loss: 3.027511]\n",
            "8240 [D loss: 0.563639, acc.: 69.14%] [G loss: 3.152012]\n",
            "8260 [D loss: 0.554476, acc.: 71.48%] [G loss: 3.080224]\n",
            "8280 [D loss: 0.523531, acc.: 73.05%] [G loss: 3.226330]\n",
            "8300 [D loss: 0.518638, acc.: 78.52%] [G loss: 3.001082]\n",
            "8320 [D loss: 0.535019, acc.: 73.05%] [G loss: 3.101243]\n",
            "8340 [D loss: 0.499466, acc.: 76.56%] [G loss: 3.087920]\n",
            "8360 [D loss: 0.485392, acc.: 78.12%] [G loss: 3.185053]\n",
            "8380 [D loss: 0.472741, acc.: 78.52%] [G loss: 3.210963]\n",
            "8400 [D loss: 0.527664, acc.: 71.09%] [G loss: 3.090158]\n",
            "8420 [D loss: 0.539881, acc.: 69.53%] [G loss: 3.008501]\n",
            "8440 [D loss: 0.502350, acc.: 78.52%] [G loss: 3.179609]\n",
            "8460 [D loss: 0.578414, acc.: 68.36%] [G loss: 2.813481]\n",
            "8480 [D loss: 0.525562, acc.: 74.61%] [G loss: 3.489588]\n",
            "8500 [D loss: 0.563857, acc.: 69.14%] [G loss: 2.975542]\n",
            "8520 [D loss: 0.498753, acc.: 76.56%] [G loss: 3.262979]\n",
            "8540 [D loss: 0.556694, acc.: 73.05%] [G loss: 3.186725]\n",
            "8560 [D loss: 0.588224, acc.: 71.48%] [G loss: 2.865908]\n",
            "8580 [D loss: 0.523702, acc.: 74.61%] [G loss: 3.194248]\n",
            "8600 [D loss: 0.559949, acc.: 71.88%] [G loss: 3.239913]\n",
            "8620 [D loss: 0.483030, acc.: 75.00%] [G loss: 3.230685]\n",
            "8640 [D loss: 0.459696, acc.: 82.03%] [G loss: 3.279979]\n",
            "8660 [D loss: 0.477624, acc.: 75.78%] [G loss: 3.515340]\n",
            "8680 [D loss: 0.553488, acc.: 68.75%] [G loss: 3.047132]\n",
            "8700 [D loss: 0.550139, acc.: 72.66%] [G loss: 3.131695]\n",
            "8720 [D loss: 0.485076, acc.: 76.56%] [G loss: 3.107314]\n",
            "8740 [D loss: 0.526049, acc.: 72.27%] [G loss: 3.313993]\n",
            "8760 [D loss: 0.461674, acc.: 78.91%] [G loss: 3.665876]\n",
            "8780 [D loss: 0.572702, acc.: 71.48%] [G loss: 3.120072]\n",
            "8800 [D loss: 0.497058, acc.: 78.12%] [G loss: 3.418794]\n",
            "8820 [D loss: 0.588499, acc.: 68.36%] [G loss: 2.969362]\n",
            "8840 [D loss: 0.529436, acc.: 72.66%] [G loss: 3.136176]\n",
            "8860 [D loss: 0.533589, acc.: 74.22%] [G loss: 3.252176]\n",
            "8880 [D loss: 0.511085, acc.: 76.17%] [G loss: 3.195484]\n",
            "8900 [D loss: 0.451338, acc.: 80.47%] [G loss: 3.533535]\n",
            "8920 [D loss: 0.438252, acc.: 80.47%] [G loss: 3.596352]\n",
            "8940 [D loss: 0.547382, acc.: 70.70%] [G loss: 3.255219]\n",
            "8960 [D loss: 0.481782, acc.: 78.12%] [G loss: 3.214671]\n",
            "8980 [D loss: 0.546547, acc.: 73.83%] [G loss: 3.517159]\n",
            "9000 [D loss: 0.486805, acc.: 76.95%] [G loss: 3.595166]\n",
            "9020 [D loss: 0.488002, acc.: 79.30%] [G loss: 2.978767]\n",
            "9040 [D loss: 0.404604, acc.: 84.77%] [G loss: 2.671043]\n",
            "9060 [D loss: 0.237181, acc.: 94.53%] [G loss: 2.188428]\n",
            "9080 [D loss: 2.134174, acc.: 12.89%] [G loss: 1.243058]\n",
            "9100 [D loss: 0.331713, acc.: 91.80%] [G loss: 13.086748]\n",
            "9120 [D loss: 0.273346, acc.: 88.67%] [G loss: 6.006526]\n",
            "9140 [D loss: 0.602132, acc.: 69.53%] [G loss: 4.129548]\n",
            "9160 [D loss: 0.425545, acc.: 80.86%] [G loss: 4.352221]\n",
            "9180 [D loss: 0.475512, acc.: 76.95%] [G loss: 4.168723]\n",
            "9200 [D loss: 0.514435, acc.: 76.17%] [G loss: 3.479000]\n",
            "9220 [D loss: 0.558547, acc.: 71.48%] [G loss: 2.993107]\n",
            "9240 [D loss: 0.548915, acc.: 73.83%] [G loss: 3.112568]\n",
            "9260 [D loss: 0.527982, acc.: 74.61%] [G loss: 3.026455]\n",
            "9280 [D loss: 0.617824, acc.: 71.48%] [G loss: 2.624548]\n",
            "9300 [D loss: 0.504033, acc.: 74.61%] [G loss: 2.967821]\n",
            "9320 [D loss: 0.545672, acc.: 75.00%] [G loss: 2.889486]\n",
            "9340 [D loss: 0.557487, acc.: 76.17%] [G loss: 2.681308]\n",
            "9360 [D loss: 0.491872, acc.: 75.39%] [G loss: 2.799293]\n",
            "9380 [D loss: 0.551298, acc.: 73.05%] [G loss: 2.816807]\n",
            "9400 [D loss: 0.541499, acc.: 70.31%] [G loss: 2.730692]\n",
            "9420 [D loss: 0.550061, acc.: 75.00%] [G loss: 2.606426]\n",
            "9440 [D loss: 0.542858, acc.: 73.83%] [G loss: 2.773609]\n",
            "9460 [D loss: 0.530412, acc.: 72.66%] [G loss: 2.742271]\n",
            "9480 [D loss: 0.530054, acc.: 74.61%] [G loss: 2.684612]\n",
            "9500 [D loss: 0.565520, acc.: 70.70%] [G loss: 2.681462]\n",
            "9520 [D loss: 0.495362, acc.: 78.91%] [G loss: 2.854294]\n",
            "9540 [D loss: 0.525637, acc.: 75.00%] [G loss: 2.698491]\n",
            "9560 [D loss: 0.493388, acc.: 77.34%] [G loss: 2.753043]\n",
            "9580 [D loss: 0.517369, acc.: 75.39%] [G loss: 2.861989]\n",
            "9600 [D loss: 0.592088, acc.: 70.70%] [G loss: 2.452038]\n",
            "9620 [D loss: 0.534362, acc.: 71.48%] [G loss: 2.800180]\n",
            "9640 [D loss: 0.533161, acc.: 74.61%] [G loss: 2.818917]\n",
            "9660 [D loss: 0.549191, acc.: 73.05%] [G loss: 2.753461]\n",
            "9680 [D loss: 0.537724, acc.: 74.61%] [G loss: 2.631869]\n",
            "9700 [D loss: 0.595371, acc.: 69.14%] [G loss: 2.773736]\n",
            "9720 [D loss: 0.544708, acc.: 72.66%] [G loss: 2.624079]\n",
            "9740 [D loss: 0.565742, acc.: 69.92%] [G loss: 2.732330]\n",
            "9760 [D loss: 0.469889, acc.: 80.47%] [G loss: 2.790588]\n",
            "9780 [D loss: 0.546368, acc.: 70.70%] [G loss: 2.709071]\n",
            "9800 [D loss: 0.593713, acc.: 67.19%] [G loss: 2.684555]\n",
            "9820 [D loss: 0.478861, acc.: 78.52%] [G loss: 2.972380]\n",
            "9840 [D loss: 0.591820, acc.: 68.75%] [G loss: 2.595369]\n",
            "9860 [D loss: 0.600427, acc.: 66.02%] [G loss: 2.750883]\n",
            "9880 [D loss: 0.512995, acc.: 74.22%] [G loss: 2.864339]\n",
            "9900 [D loss: 0.504616, acc.: 76.95%] [G loss: 2.855614]\n",
            "9920 [D loss: 0.587399, acc.: 68.75%] [G loss: 2.505417]\n",
            "9940 [D loss: 0.525567, acc.: 75.00%] [G loss: 2.772263]\n",
            "9960 [D loss: 0.510662, acc.: 75.78%] [G loss: 2.736653]\n",
            "9980 [D loss: 0.506331, acc.: 74.22%] [G loss: 2.946406]\n",
            "10000 [D loss: 0.577027, acc.: 67.97%] [G loss: 2.719907]\n",
            "10020 [D loss: 0.588212, acc.: 68.36%] [G loss: 2.679274]\n",
            "10040 [D loss: 0.503762, acc.: 77.73%] [G loss: 2.773920]\n",
            "10060 [D loss: 0.497643, acc.: 75.78%] [G loss: 2.826823]\n",
            "10080 [D loss: 0.525628, acc.: 73.44%] [G loss: 2.873326]\n",
            "10100 [D loss: 0.523969, acc.: 76.56%] [G loss: 2.846399]\n",
            "10120 [D loss: 0.515757, acc.: 76.17%] [G loss: 2.895417]\n",
            "10140 [D loss: 0.528739, acc.: 75.39%] [G loss: 2.764200]\n",
            "10160 [D loss: 0.510021, acc.: 78.52%] [G loss: 3.002246]\n",
            "10180 [D loss: 0.562343, acc.: 73.05%] [G loss: 2.846515]\n",
            "10200 [D loss: 0.520916, acc.: 75.39%] [G loss: 2.721115]\n",
            "10220 [D loss: 0.572914, acc.: 69.92%] [G loss: 2.746754]\n",
            "10240 [D loss: 0.524421, acc.: 73.44%] [G loss: 3.066767]\n",
            "10260 [D loss: 0.559060, acc.: 73.05%] [G loss: 2.776776]\n",
            "10280 [D loss: 0.572073, acc.: 68.36%] [G loss: 2.846307]\n",
            "10300 [D loss: 0.553639, acc.: 68.36%] [G loss: 2.647925]\n",
            "10320 [D loss: 0.533260, acc.: 75.78%] [G loss: 2.952570]\n",
            "10340 [D loss: 0.472517, acc.: 78.52%] [G loss: 3.024915]\n",
            "10360 [D loss: 0.555838, acc.: 73.83%] [G loss: 2.909587]\n",
            "10380 [D loss: 0.553344, acc.: 72.66%] [G loss: 2.841916]\n",
            "10400 [D loss: 0.545255, acc.: 70.70%] [G loss: 2.869805]\n",
            "10420 [D loss: 0.506352, acc.: 76.17%] [G loss: 2.833272]\n",
            "10440 [D loss: 0.544489, acc.: 71.09%] [G loss: 2.780111]\n",
            "10460 [D loss: 0.519211, acc.: 74.61%] [G loss: 3.058137]\n",
            "10480 [D loss: 0.528704, acc.: 73.05%] [G loss: 3.105923]\n",
            "10500 [D loss: 0.473735, acc.: 80.47%] [G loss: 2.976610]\n",
            "10520 [D loss: 0.495821, acc.: 79.69%] [G loss: 2.870633]\n",
            "10540 [D loss: 0.502704, acc.: 78.91%] [G loss: 2.959127]\n",
            "10560 [D loss: 0.542355, acc.: 72.27%] [G loss: 3.070041]\n",
            "10580 [D loss: 0.609905, acc.: 66.02%] [G loss: 2.725794]\n",
            "10600 [D loss: 0.496524, acc.: 75.39%] [G loss: 2.979050]\n",
            "10620 [D loss: 0.493843, acc.: 75.00%] [G loss: 2.960571]\n",
            "10640 [D loss: 0.506697, acc.: 76.17%] [G loss: 3.040734]\n",
            "10660 [D loss: 0.521275, acc.: 75.39%] [G loss: 2.795105]\n",
            "10680 [D loss: 0.491721, acc.: 79.30%] [G loss: 2.968365]\n",
            "10700 [D loss: 0.466663, acc.: 80.47%] [G loss: 3.289815]\n",
            "10720 [D loss: 0.492893, acc.: 78.12%] [G loss: 3.296497]\n",
            "10740 [D loss: 0.642079, acc.: 66.02%] [G loss: 2.713791]\n",
            "10760 [D loss: 0.557108, acc.: 72.27%] [G loss: 3.024951]\n",
            "10780 [D loss: 0.510387, acc.: 74.22%] [G loss: 3.188245]\n",
            "10800 [D loss: 0.520297, acc.: 73.05%] [G loss: 3.137566]\n",
            "10820 [D loss: 0.513768, acc.: 74.22%] [G loss: 2.927194]\n",
            "10840 [D loss: 0.493117, acc.: 76.56%] [G loss: 3.096934]\n",
            "10860 [D loss: 0.569740, acc.: 71.09%] [G loss: 2.798455]\n",
            "10880 [D loss: 0.514725, acc.: 75.00%] [G loss: 3.140786]\n",
            "10900 [D loss: 0.575836, acc.: 72.27%] [G loss: 2.902496]\n",
            "10920 [D loss: 0.506067, acc.: 73.05%] [G loss: 3.000560]\n",
            "10940 [D loss: 0.473282, acc.: 79.69%] [G loss: 3.299687]\n",
            "10960 [D loss: 0.698850, acc.: 60.55%] [G loss: 2.508939]\n",
            "10980 [D loss: 0.526412, acc.: 76.56%] [G loss: 3.174415]\n",
            "11000 [D loss: 0.544413, acc.: 72.27%] [G loss: 3.314524]\n",
            "11020 [D loss: 0.563960, acc.: 73.83%] [G loss: 3.099643]\n",
            "11040 [D loss: 0.505559, acc.: 74.61%] [G loss: 3.082212]\n",
            "11060 [D loss: 0.573774, acc.: 71.09%] [G loss: 3.007143]\n",
            "11080 [D loss: 0.548417, acc.: 72.27%] [G loss: 2.963845]\n",
            "11100 [D loss: 0.506090, acc.: 71.48%] [G loss: 2.983720]\n",
            "11120 [D loss: 0.519859, acc.: 73.83%] [G loss: 3.277889]\n",
            "11140 [D loss: 0.564428, acc.: 73.44%] [G loss: 2.589667]\n",
            "11160 [D loss: 0.626790, acc.: 62.89%] [G loss: 2.373127]\n",
            "11180 [D loss: 0.770030, acc.: 60.16%] [G loss: 1.898842]\n",
            "11200 [D loss: 0.760894, acc.: 56.64%] [G loss: 3.354090]\n",
            "11220 [D loss: 0.646900, acc.: 65.23%] [G loss: 3.397824]\n",
            "11240 [D loss: 0.633505, acc.: 68.36%] [G loss: 3.438190]\n",
            "11260 [D loss: 0.541863, acc.: 73.83%] [G loss: 3.493299]\n",
            "11280 [D loss: 0.514351, acc.: 75.39%] [G loss: 3.620341]\n",
            "11300 [D loss: 0.635700, acc.: 65.23%] [G loss: 2.943118]\n",
            "11320 [D loss: 0.507426, acc.: 75.00%] [G loss: 3.317508]\n",
            "11340 [D loss: 0.457778, acc.: 78.52%] [G loss: 3.667034]\n",
            "11360 [D loss: 0.554494, acc.: 71.88%] [G loss: 3.284499]\n",
            "11380 [D loss: 0.532415, acc.: 72.27%] [G loss: 3.144455]\n",
            "11400 [D loss: 0.446865, acc.: 79.69%] [G loss: 3.467811]\n",
            "11420 [D loss: 0.654698, acc.: 66.02%] [G loss: 2.835895]\n",
            "11440 [D loss: 0.487104, acc.: 75.78%] [G loss: 3.328783]\n",
            "11460 [D loss: 0.584455, acc.: 66.02%] [G loss: 2.806594]\n",
            "11480 [D loss: 0.465176, acc.: 76.56%] [G loss: 3.238753]\n",
            "11500 [D loss: 0.501099, acc.: 73.44%] [G loss: 3.194947]\n",
            "11520 [D loss: 0.507316, acc.: 79.30%] [G loss: 3.322936]\n",
            "11540 [D loss: 0.507261, acc.: 75.78%] [G loss: 3.311060]\n",
            "11560 [D loss: 0.591307, acc.: 67.97%] [G loss: 3.168664]\n",
            "11580 [D loss: 0.474567, acc.: 76.17%] [G loss: 3.456683]\n",
            "11600 [D loss: 0.414393, acc.: 81.25%] [G loss: 3.534833]\n",
            "11620 [D loss: 0.529013, acc.: 72.27%] [G loss: 3.117354]\n",
            "11640 [D loss: 0.439680, acc.: 81.25%] [G loss: 3.574519]\n",
            "11660 [D loss: 0.506436, acc.: 75.00%] [G loss: 3.231771]\n",
            "11680 [D loss: 0.539890, acc.: 72.66%] [G loss: 3.171978]\n",
            "11700 [D loss: 0.419763, acc.: 82.42%] [G loss: 3.567382]\n",
            "11720 [D loss: 0.543536, acc.: 72.27%] [G loss: 3.312037]\n",
            "11740 [D loss: 0.561320, acc.: 68.36%] [G loss: 3.204923]\n",
            "11760 [D loss: 0.530408, acc.: 71.09%] [G loss: 3.245843]\n",
            "11780 [D loss: 0.426002, acc.: 82.81%] [G loss: 3.795239]\n",
            "11800 [D loss: 0.522429, acc.: 74.61%] [G loss: 3.231136]\n",
            "11820 [D loss: 0.507387, acc.: 73.05%] [G loss: 3.356532]\n",
            "11840 [D loss: 0.438524, acc.: 82.81%] [G loss: 3.383389]\n",
            "11860 [D loss: 0.487489, acc.: 77.34%] [G loss: 3.334106]\n",
            "11880 [D loss: 0.450706, acc.: 78.91%] [G loss: 3.375892]\n",
            "11900 [D loss: 0.459316, acc.: 75.78%] [G loss: 3.374160]\n",
            "11920 [D loss: 0.565749, acc.: 68.75%] [G loss: 3.206144]\n",
            "11940 [D loss: 0.521150, acc.: 72.27%] [G loss: 3.102263]\n",
            "11960 [D loss: 0.558795, acc.: 73.44%] [G loss: 3.255319]\n",
            "11980 [D loss: 0.077862, acc.: 99.61%] [G loss: 2.811309]\n",
            "12000 [D loss: 0.367928, acc.: 85.16%] [G loss: 5.026889]\n",
            "12020 [D loss: 0.196693, acc.: 96.48%] [G loss: 2.503439]\n",
            "12040 [D loss: 0.553872, acc.: 71.09%] [G loss: 3.205183]\n",
            "12060 [D loss: 0.922575, acc.: 50.00%] [G loss: 2.241417]\n",
            "12080 [D loss: 0.944816, acc.: 47.66%] [G loss: 2.871845]\n",
            "12100 [D loss: 0.603118, acc.: 65.62%] [G loss: 3.285772]\n",
            "12120 [D loss: 0.467990, acc.: 75.00%] [G loss: 3.719380]\n",
            "12140 [D loss: 0.450430, acc.: 80.47%] [G loss: 3.539186]\n",
            "12160 [D loss: 0.512830, acc.: 71.48%] [G loss: 3.334655]\n",
            "12180 [D loss: 0.485783, acc.: 76.17%] [G loss: 3.447111]\n",
            "12200 [D loss: 0.488918, acc.: 77.34%] [G loss: 3.560208]\n",
            "12220 [D loss: 0.474676, acc.: 77.73%] [G loss: 3.617566]\n",
            "12240 [D loss: 0.460301, acc.: 80.08%] [G loss: 3.554918]\n",
            "12260 [D loss: 0.551181, acc.: 73.44%] [G loss: 3.153755]\n",
            "12280 [D loss: 0.542744, acc.: 69.53%] [G loss: 3.105505]\n",
            "12300 [D loss: 0.440811, acc.: 83.98%] [G loss: 3.646533]\n",
            "12320 [D loss: 0.545546, acc.: 69.92%] [G loss: 3.331943]\n",
            "12340 [D loss: 0.468944, acc.: 76.17%] [G loss: 3.676947]\n",
            "12360 [D loss: 0.513383, acc.: 73.44%] [G loss: 3.429147]\n",
            "12380 [D loss: 0.546881, acc.: 73.44%] [G loss: 3.365611]\n",
            "12400 [D loss: 0.460125, acc.: 80.08%] [G loss: 3.635138]\n",
            "12420 [D loss: 0.507368, acc.: 74.61%] [G loss: 3.415329]\n",
            "12440 [D loss: 0.438774, acc.: 78.52%] [G loss: 3.704539]\n",
            "12460 [D loss: 0.459318, acc.: 82.42%] [G loss: 3.593858]\n",
            "12480 [D loss: 0.564260, acc.: 71.09%] [G loss: 3.320265]\n",
            "12500 [D loss: 0.470406, acc.: 77.34%] [G loss: 3.533223]\n",
            "12520 [D loss: 0.428077, acc.: 81.25%] [G loss: 3.778720]\n",
            "12540 [D loss: 0.504573, acc.: 75.39%] [G loss: 3.579618]\n",
            "12560 [D loss: 0.454217, acc.: 80.08%] [G loss: 3.773346]\n",
            "12580 [D loss: 0.459393, acc.: 79.30%] [G loss: 3.405481]\n",
            "12600 [D loss: 0.439387, acc.: 80.08%] [G loss: 3.453179]\n",
            "12620 [D loss: 0.375305, acc.: 86.33%] [G loss: 4.255021]\n",
            "12640 [D loss: 0.423040, acc.: 80.47%] [G loss: 3.907249]\n",
            "12660 [D loss: 0.507522, acc.: 75.00%] [G loss: 3.701533]\n",
            "12680 [D loss: 0.545727, acc.: 72.66%] [G loss: 3.466779]\n",
            "12700 [D loss: 0.495527, acc.: 76.56%] [G loss: 3.745367]\n",
            "12720 [D loss: 0.428510, acc.: 82.81%] [G loss: 3.861328]\n",
            "12740 [D loss: 0.479145, acc.: 76.17%] [G loss: 3.724152]\n",
            "12760 [D loss: 0.441749, acc.: 82.03%] [G loss: 3.771219]\n",
            "12780 [D loss: 0.486703, acc.: 76.95%] [G loss: 3.453702]\n",
            "12800 [D loss: 0.485214, acc.: 75.00%] [G loss: 3.887805]\n",
            "12820 [D loss: 0.448306, acc.: 78.12%] [G loss: 3.976105]\n",
            "12840 [D loss: 0.846084, acc.: 44.53%] [G loss: 2.087813]\n",
            "12860 [D loss: 0.791357, acc.: 44.53%] [G loss: 1.782589]\n",
            "12880 [D loss: 0.675466, acc.: 63.28%] [G loss: 2.244158]\n",
            "12900 [D loss: 0.614492, acc.: 67.19%] [G loss: 2.521664]\n",
            "12920 [D loss: 0.552304, acc.: 73.44%] [G loss: 2.622021]\n",
            "12940 [D loss: 0.557633, acc.: 70.70%] [G loss: 2.691894]\n",
            "12960 [D loss: 0.591509, acc.: 69.92%] [G loss: 2.670024]\n",
            "12980 [D loss: 0.625847, acc.: 62.11%] [G loss: 2.600732]\n",
            "13000 [D loss: 0.574625, acc.: 69.14%] [G loss: 2.953601]\n",
            "13020 [D loss: 0.573004, acc.: 69.53%] [G loss: 2.622460]\n",
            "13040 [D loss: 0.545413, acc.: 70.31%] [G loss: 3.012315]\n",
            "13060 [D loss: 0.570231, acc.: 69.14%] [G loss: 2.809123]\n",
            "13080 [D loss: 0.552452, acc.: 74.61%] [G loss: 2.983422]\n",
            "13100 [D loss: 0.503624, acc.: 76.56%] [G loss: 3.029706]\n",
            "13120 [D loss: 0.564088, acc.: 73.44%] [G loss: 2.919616]\n",
            "13140 [D loss: 0.458921, acc.: 78.52%] [G loss: 3.165051]\n",
            "13160 [D loss: 0.554460, acc.: 70.70%] [G loss: 2.621463]\n",
            "13180 [D loss: 0.568855, acc.: 72.27%] [G loss: 3.003594]\n",
            "13200 [D loss: 0.619877, acc.: 64.84%] [G loss: 2.630341]\n",
            "13220 [D loss: 0.471826, acc.: 80.08%] [G loss: 3.139840]\n",
            "13240 [D loss: 0.526703, acc.: 74.61%] [G loss: 3.028931]\n",
            "13260 [D loss: 0.512318, acc.: 76.17%] [G loss: 3.243076]\n",
            "13280 [D loss: 0.500607, acc.: 74.61%] [G loss: 3.169282]\n",
            "13300 [D loss: 0.518392, acc.: 73.05%] [G loss: 2.952681]\n",
            "13320 [D loss: 0.523765, acc.: 75.00%] [G loss: 3.111936]\n",
            "13340 [D loss: 0.513673, acc.: 76.17%] [G loss: 3.198158]\n",
            "13360 [D loss: 0.515278, acc.: 73.44%] [G loss: 2.807825]\n",
            "13380 [D loss: 0.455820, acc.: 81.25%] [G loss: 3.303866]\n",
            "13400 [D loss: 0.489352, acc.: 82.03%] [G loss: 3.111689]\n",
            "13420 [D loss: 0.499299, acc.: 76.17%] [G loss: 3.074043]\n",
            "13440 [D loss: 0.477369, acc.: 76.95%] [G loss: 3.161453]\n",
            "13460 [D loss: 0.498300, acc.: 76.95%] [G loss: 3.453455]\n",
            "13480 [D loss: 0.550479, acc.: 70.70%] [G loss: 3.298821]\n",
            "13500 [D loss: 0.498216, acc.: 75.00%] [G loss: 3.090842]\n",
            "13520 [D loss: 0.496910, acc.: 78.91%] [G loss: 3.257681]\n",
            "13540 [D loss: 0.491932, acc.: 76.95%] [G loss: 3.290048]\n",
            "13560 [D loss: 0.553454, acc.: 72.66%] [G loss: 3.145594]\n",
            "13580 [D loss: 0.531570, acc.: 74.61%] [G loss: 3.013499]\n",
            "13600 [D loss: 0.512022, acc.: 74.22%] [G loss: 3.214929]\n",
            "13620 [D loss: 0.554482, acc.: 69.14%] [G loss: 3.162534]\n",
            "13640 [D loss: 0.476435, acc.: 77.34%] [G loss: 3.307050]\n",
            "13660 [D loss: 0.486192, acc.: 75.00%] [G loss: 3.432660]\n",
            "13680 [D loss: 0.483079, acc.: 75.78%] [G loss: 3.283582]\n",
            "13700 [D loss: 0.442766, acc.: 83.59%] [G loss: 3.237695]\n",
            "13720 [D loss: 0.503620, acc.: 75.00%] [G loss: 3.419814]\n",
            "13740 [D loss: 0.480728, acc.: 78.91%] [G loss: 3.194624]\n",
            "13760 [D loss: 0.588444, acc.: 72.27%] [G loss: 2.979780]\n",
            "13780 [D loss: 0.469675, acc.: 80.86%] [G loss: 3.401901]\n",
            "13800 [D loss: 0.785948, acc.: 61.33%] [G loss: 4.896160]\n",
            "13820 [D loss: 1.029681, acc.: 45.70%] [G loss: 2.727921]\n",
            "13840 [D loss: 0.511268, acc.: 76.95%] [G loss: 3.322994]\n",
            "13860 [D loss: 0.744419, acc.: 55.47%] [G loss: 3.172679]\n",
            "13880 [D loss: 0.490994, acc.: 78.12%] [G loss: 3.590151]\n",
            "13900 [D loss: 0.574654, acc.: 70.70%] [G loss: 3.389326]\n",
            "13920 [D loss: 0.473782, acc.: 74.61%] [G loss: 3.639288]\n",
            "13940 [D loss: 0.539500, acc.: 70.70%] [G loss: 3.287990]\n",
            "13960 [D loss: 0.508890, acc.: 73.05%] [G loss: 3.346546]\n",
            "13980 [D loss: 0.504799, acc.: 75.39%] [G loss: 3.343493]\n",
            "14000 [D loss: 0.527942, acc.: 74.61%] [G loss: 3.281374]\n",
            "14020 [D loss: 0.485440, acc.: 75.78%] [G loss: 3.317081]\n",
            "14040 [D loss: 0.527724, acc.: 72.66%] [G loss: 3.452330]\n",
            "14060 [D loss: 0.448264, acc.: 79.30%] [G loss: 3.623664]\n",
            "14080 [D loss: 0.557461, acc.: 69.92%] [G loss: 3.238868]\n",
            "14100 [D loss: 0.457422, acc.: 77.34%] [G loss: 3.630646]\n",
            "14120 [D loss: 0.477658, acc.: 78.12%] [G loss: 3.234628]\n",
            "14140 [D loss: 0.529786, acc.: 73.05%] [G loss: 3.159780]\n",
            "14160 [D loss: 0.450282, acc.: 81.64%] [G loss: 3.613758]\n",
            "14180 [D loss: 0.542316, acc.: 73.05%] [G loss: 3.388750]\n",
            "14200 [D loss: 0.533643, acc.: 71.88%] [G loss: 3.350494]\n",
            "14220 [D loss: 0.412946, acc.: 83.20%] [G loss: 3.416901]\n",
            "14240 [D loss: 0.484469, acc.: 76.95%] [G loss: 3.493882]\n",
            "14260 [D loss: 0.485748, acc.: 77.34%] [G loss: 3.633022]\n",
            "14280 [D loss: 0.522966, acc.: 73.44%] [G loss: 3.197870]\n",
            "14300 [D loss: 0.474336, acc.: 76.17%] [G loss: 3.479342]\n",
            "14320 [D loss: 0.469246, acc.: 74.22%] [G loss: 4.035184]\n",
            "14340 [D loss: 0.512816, acc.: 74.22%] [G loss: 3.747917]\n",
            "14360 [D loss: 0.471017, acc.: 78.52%] [G loss: 3.589724]\n",
            "14380 [D loss: 0.529874, acc.: 72.66%] [G loss: 3.697619]\n",
            "14400 [D loss: 0.467554, acc.: 78.91%] [G loss: 3.894388]\n",
            "14420 [D loss: 0.531015, acc.: 72.27%] [G loss: 3.756697]\n",
            "14440 [D loss: 0.481078, acc.: 78.52%] [G loss: 3.507844]\n",
            "14460 [D loss: 0.513526, acc.: 72.27%] [G loss: 3.656963]\n",
            "14480 [D loss: 0.520966, acc.: 71.88%] [G loss: 3.607710]\n",
            "14500 [D loss: 0.459515, acc.: 78.12%] [G loss: 3.988366]\n",
            "14520 [D loss: 0.557627, acc.: 69.92%] [G loss: 3.595991]\n",
            "14540 [D loss: 0.484624, acc.: 76.56%] [G loss: 3.496303]\n",
            "14560 [D loss: 0.445922, acc.: 79.69%] [G loss: 3.785725]\n",
            "14580 [D loss: 0.587942, acc.: 69.14%] [G loss: 3.145349]\n",
            "14600 [D loss: 0.418707, acc.: 81.64%] [G loss: 3.721117]\n",
            "14620 [D loss: 0.477413, acc.: 78.52%] [G loss: 3.975642]\n",
            "14640 [D loss: 0.491040, acc.: 76.95%] [G loss: 3.613445]\n",
            "14660 [D loss: 0.435832, acc.: 80.86%] [G loss: 4.397635]\n",
            "14680 [D loss: 0.472619, acc.: 80.47%] [G loss: 3.511051]\n",
            "14700 [D loss: 0.445885, acc.: 80.08%] [G loss: 3.724352]\n",
            "14720 [D loss: 0.462502, acc.: 78.12%] [G loss: 3.760056]\n",
            "14740 [D loss: 0.456557, acc.: 76.56%] [G loss: 4.046819]\n",
            "14760 [D loss: 0.482349, acc.: 76.56%] [G loss: 3.752608]\n",
            "14780 [D loss: 0.466681, acc.: 76.56%] [G loss: 3.806159]\n",
            "14800 [D loss: 0.458334, acc.: 79.69%] [G loss: 3.643999]\n",
            "14820 [D loss: 0.478118, acc.: 78.12%] [G loss: 3.687073]\n",
            "14840 [D loss: 0.403816, acc.: 83.59%] [G loss: 4.253506]\n",
            "14860 [D loss: 0.455975, acc.: 78.91%] [G loss: 3.788819]\n",
            "14880 [D loss: 0.465140, acc.: 77.73%] [G loss: 3.650719]\n",
            "14900 [D loss: 0.466391, acc.: 76.95%] [G loss: 4.072765]\n",
            "14920 [D loss: 0.505416, acc.: 75.39%] [G loss: 3.765016]\n",
            "14940 [D loss: 0.506678, acc.: 75.39%] [G loss: 3.774758]\n",
            "14960 [D loss: 0.481505, acc.: 76.17%] [G loss: 3.798863]\n",
            "14980 [D loss: 0.430896, acc.: 83.20%] [G loss: 3.751429]\n",
            "15000 [D loss: 0.458714, acc.: 81.64%] [G loss: 4.168437]\n",
            "15020 [D loss: 0.446797, acc.: 77.73%] [G loss: 4.105661]\n",
            "15040 [D loss: 0.433405, acc.: 81.64%] [G loss: 4.174823]\n",
            "15060 [D loss: 0.464035, acc.: 75.78%] [G loss: 4.271073]\n",
            "15080 [D loss: 0.525180, acc.: 71.09%] [G loss: 3.859435]\n",
            "15100 [D loss: 0.465351, acc.: 80.47%] [G loss: 3.945693]\n",
            "15120 [D loss: 0.492035, acc.: 75.39%] [G loss: 3.680850]\n",
            "15140 [D loss: 0.478277, acc.: 78.91%] [G loss: 3.803140]\n",
            "15160 [D loss: 0.442666, acc.: 80.08%] [G loss: 4.085079]\n",
            "15180 [D loss: 0.385569, acc.: 84.77%] [G loss: 4.550632]\n",
            "15200 [D loss: 0.540141, acc.: 73.05%] [G loss: 3.795214]\n",
            "15220 [D loss: 0.407144, acc.: 82.03%] [G loss: 4.507972]\n",
            "15240 [D loss: 0.393928, acc.: 84.38%] [G loss: 4.267452]\n",
            "15260 [D loss: 0.505855, acc.: 75.00%] [G loss: 3.833901]\n",
            "15280 [D loss: 0.398075, acc.: 84.77%] [G loss: 4.297688]\n",
            "15300 [D loss: 0.515360, acc.: 71.88%] [G loss: 3.695936]\n",
            "15320 [D loss: 0.462438, acc.: 77.34%] [G loss: 3.763459]\n",
            "15340 [D loss: 0.521326, acc.: 75.78%] [G loss: 4.147512]\n",
            "15360 [D loss: 0.450197, acc.: 78.12%] [G loss: 4.054942]\n",
            "15380 [D loss: 0.459087, acc.: 75.00%] [G loss: 3.514007]\n",
            "15400 [D loss: 0.438021, acc.: 78.12%] [G loss: 4.299571]\n",
            "15420 [D loss: 0.480087, acc.: 77.73%] [G loss: 4.010191]\n",
            "15440 [D loss: 0.431406, acc.: 78.12%] [G loss: 4.076120]\n",
            "15460 [D loss: 0.510785, acc.: 74.22%] [G loss: 4.042256]\n",
            "15480 [D loss: 0.423074, acc.: 80.47%] [G loss: 4.032153]\n",
            "15500 [D loss: 0.504101, acc.: 75.00%] [G loss: 3.714095]\n",
            "15520 [D loss: 0.481650, acc.: 76.56%] [G loss: 4.177690]\n",
            "15540 [D loss: 0.469534, acc.: 80.08%] [G loss: 4.328707]\n",
            "15560 [D loss: 0.395607, acc.: 83.20%] [G loss: 4.057971]\n",
            "15580 [D loss: 0.371671, acc.: 84.77%] [G loss: 4.417598]\n",
            "15600 [D loss: 0.469473, acc.: 78.52%] [G loss: 4.033223]\n",
            "15620 [D loss: 0.367331, acc.: 86.33%] [G loss: 4.556493]\n",
            "15640 [D loss: 0.475302, acc.: 78.91%] [G loss: 3.846319]\n",
            "15660 [D loss: 0.405015, acc.: 80.08%] [G loss: 4.208066]\n",
            "15680 [D loss: 0.458627, acc.: 77.73%] [G loss: 4.004001]\n",
            "15700 [D loss: 0.490499, acc.: 78.12%] [G loss: 4.169179]\n",
            "15720 [D loss: 0.480768, acc.: 74.61%] [G loss: 4.165738]\n",
            "15740 [D loss: 0.481844, acc.: 80.08%] [G loss: 3.895675]\n",
            "15760 [D loss: 0.458577, acc.: 77.73%] [G loss: 3.972927]\n",
            "15780 [D loss: 0.477693, acc.: 74.22%] [G loss: 3.912381]\n",
            "15800 [D loss: 0.466719, acc.: 75.39%] [G loss: 4.087830]\n",
            "15820 [D loss: 0.463724, acc.: 78.52%] [G loss: 4.007878]\n",
            "15840 [D loss: 0.424819, acc.: 82.42%] [G loss: 4.334946]\n",
            "15860 [D loss: 0.452703, acc.: 80.86%] [G loss: 3.955364]\n",
            "15880 [D loss: 0.396308, acc.: 82.42%] [G loss: 4.546968]\n",
            "15900 [D loss: 0.443451, acc.: 78.91%] [G loss: 3.883495]\n",
            "15920 [D loss: 0.478269, acc.: 75.39%] [G loss: 4.066658]\n",
            "15940 [D loss: 0.491773, acc.: 75.39%] [G loss: 4.246646]\n",
            "15960 [D loss: 0.393779, acc.: 83.20%] [G loss: 4.490225]\n",
            "15980 [D loss: 0.420033, acc.: 81.25%] [G loss: 3.998176]\n",
            "16000 [D loss: 0.418858, acc.: 82.42%] [G loss: 4.259408]\n",
            "16020 [D loss: 0.450035, acc.: 80.47%] [G loss: 3.823319]\n",
            "16040 [D loss: 0.469387, acc.: 76.95%] [G loss: 3.961945]\n",
            "16060 [D loss: 0.489889, acc.: 77.34%] [G loss: 4.312740]\n",
            "16080 [D loss: 0.422514, acc.: 83.20%] [G loss: 4.049900]\n",
            "16100 [D loss: 0.433965, acc.: 80.08%] [G loss: 4.421045]\n",
            "16120 [D loss: 0.507547, acc.: 78.91%] [G loss: 4.705372]\n",
            "16140 [D loss: 0.648838, acc.: 62.89%] [G loss: 3.839990]\n",
            "16160 [D loss: 0.434100, acc.: 78.91%] [G loss: 4.124402]\n",
            "16180 [D loss: 0.520445, acc.: 75.00%] [G loss: 4.083956]\n",
            "16200 [D loss: 0.471476, acc.: 78.12%] [G loss: 4.234083]\n",
            "16220 [D loss: 0.466422, acc.: 79.30%] [G loss: 4.214331]\n",
            "16240 [D loss: 0.493592, acc.: 76.17%] [G loss: 4.130515]\n",
            "16260 [D loss: 0.480369, acc.: 77.34%] [G loss: 4.395501]\n",
            "16280 [D loss: 0.452813, acc.: 76.95%] [G loss: 4.333783]\n",
            "16300 [D loss: 0.455554, acc.: 79.69%] [G loss: 4.391864]\n",
            "16320 [D loss: 0.472277, acc.: 76.95%] [G loss: 4.057978]\n",
            "16340 [D loss: 0.413077, acc.: 81.25%] [G loss: 4.491687]\n",
            "16360 [D loss: 0.534852, acc.: 70.70%] [G loss: 4.252034]\n",
            "16380 [D loss: 0.412141, acc.: 82.03%] [G loss: 4.385931]\n",
            "16400 [D loss: 0.368688, acc.: 87.11%] [G loss: 4.377646]\n",
            "16420 [D loss: 0.419978, acc.: 79.69%] [G loss: 4.607144]\n",
            "16440 [D loss: 0.497773, acc.: 73.05%] [G loss: 4.559546]\n",
            "16460 [D loss: 0.361650, acc.: 83.98%] [G loss: 4.899717]\n",
            "16480 [D loss: 0.457254, acc.: 80.47%] [G loss: 4.594975]\n",
            "16500 [D loss: 0.451928, acc.: 77.73%] [G loss: 4.469520]\n",
            "16520 [D loss: 0.379662, acc.: 81.25%] [G loss: 4.416817]\n",
            "16540 [D loss: 0.402177, acc.: 81.64%] [G loss: 4.583776]\n",
            "16560 [D loss: 0.430562, acc.: 79.69%] [G loss: 4.329306]\n",
            "16580 [D loss: 0.530281, acc.: 74.61%] [G loss: 3.823483]\n",
            "16600 [D loss: 0.493152, acc.: 76.56%] [G loss: 4.272224]\n",
            "16620 [D loss: 0.416292, acc.: 77.73%] [G loss: 4.646237]\n",
            "16640 [D loss: 0.430153, acc.: 79.30%] [G loss: 4.563437]\n",
            "16660 [D loss: 0.564630, acc.: 73.05%] [G loss: 4.081323]\n",
            "16680 [D loss: 0.402949, acc.: 84.38%] [G loss: 4.557511]\n",
            "16700 [D loss: 0.375391, acc.: 84.38%] [G loss: 4.511670]\n",
            "16720 [D loss: 0.393062, acc.: 82.81%] [G loss: 4.834181]\n",
            "16740 [D loss: 0.409038, acc.: 82.03%] [G loss: 4.368547]\n",
            "16760 [D loss: 0.421388, acc.: 80.86%] [G loss: 4.329879]\n",
            "16780 [D loss: 0.413690, acc.: 81.25%] [G loss: 4.638389]\n",
            "16800 [D loss: 0.328492, acc.: 85.16%] [G loss: 5.046725]\n",
            "16820 [D loss: 0.369179, acc.: 84.77%] [G loss: 5.198291]\n",
            "16840 [D loss: 0.444361, acc.: 80.47%] [G loss: 4.492225]\n",
            "16860 [D loss: 0.371204, acc.: 83.59%] [G loss: 4.316965]\n",
            "16880 [D loss: 0.365889, acc.: 83.59%] [G loss: 4.935753]\n",
            "16900 [D loss: 0.443694, acc.: 79.69%] [G loss: 4.345809]\n",
            "16920 [D loss: 0.484559, acc.: 77.34%] [G loss: 4.426804]\n",
            "16940 [D loss: 0.512340, acc.: 75.39%] [G loss: 4.315082]\n",
            "16960 [D loss: 0.566958, acc.: 71.48%] [G loss: 4.099850]\n",
            "16980 [D loss: 0.379354, acc.: 83.59%] [G loss: 4.616225]\n",
            "17000 [D loss: 0.425985, acc.: 84.77%] [G loss: 4.472430]\n",
            "17020 [D loss: 0.374907, acc.: 83.20%] [G loss: 4.771636]\n",
            "17040 [D loss: 0.417105, acc.: 83.59%] [G loss: 4.365343]\n",
            "17060 [D loss: 0.449147, acc.: 81.64%] [G loss: 4.392863]\n",
            "17080 [D loss: 0.416713, acc.: 82.42%] [G loss: 4.880661]\n",
            "17100 [D loss: 0.417647, acc.: 78.91%] [G loss: 4.911220]\n",
            "17120 [D loss: 0.437319, acc.: 80.08%] [G loss: 4.781505]\n",
            "17140 [D loss: 0.450162, acc.: 79.30%] [G loss: 4.477587]\n",
            "17160 [D loss: 0.391983, acc.: 82.03%] [G loss: 4.596561]\n",
            "17180 [D loss: 0.404656, acc.: 82.81%] [G loss: 4.793170]\n",
            "17200 [D loss: 0.437241, acc.: 78.52%] [G loss: 4.821020]\n",
            "17220 [D loss: 0.424701, acc.: 79.30%] [G loss: 4.860720]\n",
            "17240 [D loss: 0.445426, acc.: 78.12%] [G loss: 4.755125]\n",
            "17260 [D loss: 0.425671, acc.: 82.42%] [G loss: 4.871841]\n",
            "17280 [D loss: 0.451168, acc.: 77.73%] [G loss: 4.357300]\n",
            "17300 [D loss: 0.490410, acc.: 77.34%] [G loss: 4.799212]\n",
            "17320 [D loss: 0.413524, acc.: 80.47%] [G loss: 4.807293]\n",
            "17340 [D loss: 0.331122, acc.: 85.16%] [G loss: 5.379495]\n",
            "17360 [D loss: 0.548702, acc.: 71.88%] [G loss: 4.616296]\n",
            "17380 [D loss: 0.498037, acc.: 78.91%] [G loss: 4.571552]\n",
            "17400 [D loss: 0.390104, acc.: 82.03%] [G loss: 5.006787]\n",
            "17420 [D loss: 0.353667, acc.: 84.77%] [G loss: 5.251502]\n",
            "17440 [D loss: 0.431794, acc.: 80.08%] [G loss: 4.486847]\n",
            "17460 [D loss: 0.349348, acc.: 83.59%] [G loss: 5.069486]\n",
            "17480 [D loss: 0.389279, acc.: 83.98%] [G loss: 4.987775]\n",
            "17500 [D loss: 0.443629, acc.: 78.12%] [G loss: 4.730805]\n",
            "17520 [D loss: 0.377515, acc.: 84.77%] [G loss: 4.620017]\n",
            "17540 [D loss: 0.449416, acc.: 78.52%] [G loss: 4.481583]\n",
            "17560 [D loss: 0.360955, acc.: 85.55%] [G loss: 5.201402]\n",
            "17580 [D loss: 0.485077, acc.: 75.00%] [G loss: 4.681120]\n",
            "17600 [D loss: 0.444834, acc.: 79.69%] [G loss: 5.258919]\n",
            "17620 [D loss: 0.358080, acc.: 85.55%] [G loss: 5.152805]\n",
            "17640 [D loss: 0.479477, acc.: 77.34%] [G loss: 4.868327]\n",
            "17660 [D loss: 0.315443, acc.: 84.77%] [G loss: 3.454443]\n",
            "17680 [D loss: 0.078012, acc.: 99.22%] [G loss: 3.820745]\n",
            "17700 [D loss: 5.939842, acc.: 2.34%] [G loss: 3.889915]\n",
            "17720 [D loss: 1.386491, acc.: 67.97%] [G loss: 10.739682]\n",
            "17740 [D loss: 1.095899, acc.: 56.64%] [G loss: 4.912753]\n",
            "17760 [D loss: 1.010192, acc.: 53.12%] [G loss: 3.526024]\n",
            "17780 [D loss: 1.095473, acc.: 48.05%] [G loss: 3.792003]\n",
            "17800 [D loss: 0.886103, acc.: 51.56%] [G loss: 2.634119]\n",
            "17820 [D loss: 0.883214, acc.: 50.00%] [G loss: 2.475605]\n",
            "17840 [D loss: 0.845369, acc.: 55.08%] [G loss: 2.497735]\n",
            "17860 [D loss: 0.777448, acc.: 57.81%] [G loss: 2.528073]\n",
            "17880 [D loss: 0.654774, acc.: 64.06%] [G loss: 2.530356]\n",
            "17900 [D loss: 0.617595, acc.: 67.58%] [G loss: 2.635678]\n",
            "17920 [D loss: 0.799115, acc.: 51.17%] [G loss: 2.384752]\n",
            "17940 [D loss: 0.802283, acc.: 56.64%] [G loss: 2.344873]\n",
            "17960 [D loss: 0.794662, acc.: 55.47%] [G loss: 2.472500]\n",
            "17980 [D loss: 0.623381, acc.: 67.19%] [G loss: 2.471292]\n",
            "18000 [D loss: 0.696121, acc.: 62.11%] [G loss: 2.546082]\n",
            "18020 [D loss: 0.622559, acc.: 67.58%] [G loss: 2.633694]\n",
            "18040 [D loss: 0.683978, acc.: 62.50%] [G loss: 2.486564]\n",
            "18060 [D loss: 0.693594, acc.: 61.33%] [G loss: 2.373977]\n",
            "18080 [D loss: 0.712803, acc.: 60.55%] [G loss: 2.417871]\n",
            "18100 [D loss: 0.635623, acc.: 66.41%] [G loss: 2.621891]\n",
            "18120 [D loss: 0.733346, acc.: 60.55%] [G loss: 2.343013]\n",
            "18140 [D loss: 0.769231, acc.: 57.03%] [G loss: 2.262064]\n",
            "18160 [D loss: 0.589929, acc.: 73.44%] [G loss: 2.606108]\n",
            "18180 [D loss: 0.695130, acc.: 60.55%] [G loss: 2.438402]\n",
            "18200 [D loss: 0.693445, acc.: 59.77%] [G loss: 2.332722]\n",
            "18220 [D loss: 0.707769, acc.: 57.03%] [G loss: 2.324182]\n",
            "18240 [D loss: 0.678429, acc.: 65.23%] [G loss: 2.362583]\n",
            "18260 [D loss: 0.572679, acc.: 69.53%] [G loss: 2.500673]\n",
            "18280 [D loss: 0.708471, acc.: 57.42%] [G loss: 2.290368]\n",
            "18300 [D loss: 0.731743, acc.: 57.42%] [G loss: 2.378096]\n",
            "18320 [D loss: 0.696989, acc.: 58.59%] [G loss: 2.338337]\n",
            "18340 [D loss: 0.605087, acc.: 70.70%] [G loss: 2.668512]\n",
            "18360 [D loss: 0.540449, acc.: 72.66%] [G loss: 2.738684]\n",
            "18380 [D loss: 0.717713, acc.: 57.42%] [G loss: 2.188789]\n",
            "18400 [D loss: 0.589509, acc.: 67.19%] [G loss: 2.702072]\n",
            "18420 [D loss: 0.628398, acc.: 65.23%] [G loss: 2.511513]\n",
            "18440 [D loss: 0.643171, acc.: 65.23%] [G loss: 2.499477]\n",
            "18460 [D loss: 0.547076, acc.: 73.83%] [G loss: 2.706311]\n",
            "18480 [D loss: 0.599757, acc.: 68.75%] [G loss: 2.576262]\n",
            "18500 [D loss: 0.605665, acc.: 65.23%] [G loss: 2.712691]\n",
            "18520 [D loss: 0.606192, acc.: 67.19%] [G loss: 2.466415]\n",
            "18540 [D loss: 0.585125, acc.: 70.31%] [G loss: 2.583102]\n",
            "18560 [D loss: 0.615529, acc.: 67.58%] [G loss: 2.625475]\n",
            "18580 [D loss: 0.570208, acc.: 75.39%] [G loss: 2.761086]\n",
            "18600 [D loss: 0.611694, acc.: 69.92%] [G loss: 2.578129]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}