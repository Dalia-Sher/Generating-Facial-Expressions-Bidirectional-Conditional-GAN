{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Final_Model_BiCoGAN_Shear.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS516rLy1LYQ",
        "outputId": "b55172af-1e05-4327-c8c2-eebb212217b5"
      },
      "source": [
        "!unzip fer2013.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  fer2013.zip\n",
            "  inflating: fer2013.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jF8QfAYPg_71",
        "outputId": "422083d5-4fc6-4940-f785-c9d6af3579c9"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "b7938252-e606-42c3-bdd9-dcb923a5c7c5"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn5xEsrnIB0t"
      },
      "source": [
        "dic = {0:'Angry', 1:'disgust' , 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRPqlqU1IG94"
      },
      "source": [
        "The emotion disgust has too few samples, therefore we won't use it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHR9swxs5P0w",
        "outputId": "b31d666e-84aa-4a41-e2a3-f6f3ae57c3e9"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkSAij3uINfw"
      },
      "source": [
        "We will redefine the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "4bcd4d77-7781-4f0c-c718-6a3cc330938a"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg76WOkUexa0",
        "outputId": "66cbc6df-ebee-4327-8539-8656aef05350"
      },
      "source": [
        "epochs = X_train.shape[0]\n",
        "print(\"number of epochs:\", epochs)\n",
        "\n",
        "X_train_aug = X_train\n",
        "y_train_aug = y_train\n",
        "\n",
        "for k in range(epochs):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  contrast = iaa.Affine(shear=(0,20))\n",
        "  contrast_image = contrast.augment_image(img)\n",
        "  contrasted_images.append(contrast_image)\n",
        "\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train_aug = np.concatenate((X_train_aug, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]\n",
        "  y_train_aug = np.concatenate((y_train_aug, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration:\" , k,\", train shape:\", X_train_aug.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of epochs: 35340\n",
            "iteration: 0 , train shape: (35341, 48, 48, 1)\n",
            "iteration: 100 , train shape: (35441, 48, 48, 1)\n",
            "iteration: 200 , train shape: (35541, 48, 48, 1)\n",
            "iteration: 300 , train shape: (35641, 48, 48, 1)\n",
            "iteration: 400 , train shape: (35741, 48, 48, 1)\n",
            "iteration: 500 , train shape: (35841, 48, 48, 1)\n",
            "iteration: 600 , train shape: (35941, 48, 48, 1)\n",
            "iteration: 700 , train shape: (36041, 48, 48, 1)\n",
            "iteration: 800 , train shape: (36141, 48, 48, 1)\n",
            "iteration: 900 , train shape: (36241, 48, 48, 1)\n",
            "iteration: 1000 , train shape: (36341, 48, 48, 1)\n",
            "iteration: 1100 , train shape: (36441, 48, 48, 1)\n",
            "iteration: 1200 , train shape: (36541, 48, 48, 1)\n",
            "iteration: 1300 , train shape: (36641, 48, 48, 1)\n",
            "iteration: 1400 , train shape: (36741, 48, 48, 1)\n",
            "iteration: 1500 , train shape: (36841, 48, 48, 1)\n",
            "iteration: 1600 , train shape: (36941, 48, 48, 1)\n",
            "iteration: 1700 , train shape: (37041, 48, 48, 1)\n",
            "iteration: 1800 , train shape: (37141, 48, 48, 1)\n",
            "iteration: 1900 , train shape: (37241, 48, 48, 1)\n",
            "iteration: 2000 , train shape: (37341, 48, 48, 1)\n",
            "iteration: 2100 , train shape: (37441, 48, 48, 1)\n",
            "iteration: 2200 , train shape: (37541, 48, 48, 1)\n",
            "iteration: 2300 , train shape: (37641, 48, 48, 1)\n",
            "iteration: 2400 , train shape: (37741, 48, 48, 1)\n",
            "iteration: 2500 , train shape: (37841, 48, 48, 1)\n",
            "iteration: 2600 , train shape: (37941, 48, 48, 1)\n",
            "iteration: 2700 , train shape: (38041, 48, 48, 1)\n",
            "iteration: 2800 , train shape: (38141, 48, 48, 1)\n",
            "iteration: 2900 , train shape: (38241, 48, 48, 1)\n",
            "iteration: 3000 , train shape: (38341, 48, 48, 1)\n",
            "iteration: 3100 , train shape: (38441, 48, 48, 1)\n",
            "iteration: 3200 , train shape: (38541, 48, 48, 1)\n",
            "iteration: 3300 , train shape: (38641, 48, 48, 1)\n",
            "iteration: 3400 , train shape: (38741, 48, 48, 1)\n",
            "iteration: 3500 , train shape: (38841, 48, 48, 1)\n",
            "iteration: 3600 , train shape: (38941, 48, 48, 1)\n",
            "iteration: 3700 , train shape: (39041, 48, 48, 1)\n",
            "iteration: 3800 , train shape: (39141, 48, 48, 1)\n",
            "iteration: 3900 , train shape: (39241, 48, 48, 1)\n",
            "iteration: 4000 , train shape: (39341, 48, 48, 1)\n",
            "iteration: 4100 , train shape: (39441, 48, 48, 1)\n",
            "iteration: 4200 , train shape: (39541, 48, 48, 1)\n",
            "iteration: 4300 , train shape: (39641, 48, 48, 1)\n",
            "iteration: 4400 , train shape: (39741, 48, 48, 1)\n",
            "iteration: 4500 , train shape: (39841, 48, 48, 1)\n",
            "iteration: 4600 , train shape: (39941, 48, 48, 1)\n",
            "iteration: 4700 , train shape: (40041, 48, 48, 1)\n",
            "iteration: 4800 , train shape: (40141, 48, 48, 1)\n",
            "iteration: 4900 , train shape: (40241, 48, 48, 1)\n",
            "iteration: 5000 , train shape: (40341, 48, 48, 1)\n",
            "iteration: 5100 , train shape: (40441, 48, 48, 1)\n",
            "iteration: 5200 , train shape: (40541, 48, 48, 1)\n",
            "iteration: 5300 , train shape: (40641, 48, 48, 1)\n",
            "iteration: 5400 , train shape: (40741, 48, 48, 1)\n",
            "iteration: 5500 , train shape: (40841, 48, 48, 1)\n",
            "iteration: 5600 , train shape: (40941, 48, 48, 1)\n",
            "iteration: 5700 , train shape: (41041, 48, 48, 1)\n",
            "iteration: 5800 , train shape: (41141, 48, 48, 1)\n",
            "iteration: 5900 , train shape: (41241, 48, 48, 1)\n",
            "iteration: 6000 , train shape: (41341, 48, 48, 1)\n",
            "iteration: 6100 , train shape: (41441, 48, 48, 1)\n",
            "iteration: 6200 , train shape: (41541, 48, 48, 1)\n",
            "iteration: 6300 , train shape: (41641, 48, 48, 1)\n",
            "iteration: 6400 , train shape: (41741, 48, 48, 1)\n",
            "iteration: 6500 , train shape: (41841, 48, 48, 1)\n",
            "iteration: 6600 , train shape: (41941, 48, 48, 1)\n",
            "iteration: 6700 , train shape: (42041, 48, 48, 1)\n",
            "iteration: 6800 , train shape: (42141, 48, 48, 1)\n",
            "iteration: 6900 , train shape: (42241, 48, 48, 1)\n",
            "iteration: 7000 , train shape: (42341, 48, 48, 1)\n",
            "iteration: 7100 , train shape: (42441, 48, 48, 1)\n",
            "iteration: 7200 , train shape: (42541, 48, 48, 1)\n",
            "iteration: 7300 , train shape: (42641, 48, 48, 1)\n",
            "iteration: 7400 , train shape: (42741, 48, 48, 1)\n",
            "iteration: 7500 , train shape: (42841, 48, 48, 1)\n",
            "iteration: 7600 , train shape: (42941, 48, 48, 1)\n",
            "iteration: 7700 , train shape: (43041, 48, 48, 1)\n",
            "iteration: 7800 , train shape: (43141, 48, 48, 1)\n",
            "iteration: 7900 , train shape: (43241, 48, 48, 1)\n",
            "iteration: 8000 , train shape: (43341, 48, 48, 1)\n",
            "iteration: 8100 , train shape: (43441, 48, 48, 1)\n",
            "iteration: 8200 , train shape: (43541, 48, 48, 1)\n",
            "iteration: 8300 , train shape: (43641, 48, 48, 1)\n",
            "iteration: 8400 , train shape: (43741, 48, 48, 1)\n",
            "iteration: 8500 , train shape: (43841, 48, 48, 1)\n",
            "iteration: 8600 , train shape: (43941, 48, 48, 1)\n",
            "iteration: 8700 , train shape: (44041, 48, 48, 1)\n",
            "iteration: 8800 , train shape: (44141, 48, 48, 1)\n",
            "iteration: 8900 , train shape: (44241, 48, 48, 1)\n",
            "iteration: 9000 , train shape: (44341, 48, 48, 1)\n",
            "iteration: 9100 , train shape: (44441, 48, 48, 1)\n",
            "iteration: 9200 , train shape: (44541, 48, 48, 1)\n",
            "iteration: 9300 , train shape: (44641, 48, 48, 1)\n",
            "iteration: 9400 , train shape: (44741, 48, 48, 1)\n",
            "iteration: 9500 , train shape: (44841, 48, 48, 1)\n",
            "iteration: 9600 , train shape: (44941, 48, 48, 1)\n",
            "iteration: 9700 , train shape: (45041, 48, 48, 1)\n",
            "iteration: 9800 , train shape: (45141, 48, 48, 1)\n",
            "iteration: 9900 , train shape: (45241, 48, 48, 1)\n",
            "iteration: 10000 , train shape: (45341, 48, 48, 1)\n",
            "iteration: 10100 , train shape: (45441, 48, 48, 1)\n",
            "iteration: 10200 , train shape: (45541, 48, 48, 1)\n",
            "iteration: 10300 , train shape: (45641, 48, 48, 1)\n",
            "iteration: 10400 , train shape: (45741, 48, 48, 1)\n",
            "iteration: 10500 , train shape: (45841, 48, 48, 1)\n",
            "iteration: 10600 , train shape: (45941, 48, 48, 1)\n",
            "iteration: 10700 , train shape: (46041, 48, 48, 1)\n",
            "iteration: 10800 , train shape: (46141, 48, 48, 1)\n",
            "iteration: 10900 , train shape: (46241, 48, 48, 1)\n",
            "iteration: 11000 , train shape: (46341, 48, 48, 1)\n",
            "iteration: 11100 , train shape: (46441, 48, 48, 1)\n",
            "iteration: 11200 , train shape: (46541, 48, 48, 1)\n",
            "iteration: 11300 , train shape: (46641, 48, 48, 1)\n",
            "iteration: 11400 , train shape: (46741, 48, 48, 1)\n",
            "iteration: 11500 , train shape: (46841, 48, 48, 1)\n",
            "iteration: 11600 , train shape: (46941, 48, 48, 1)\n",
            "iteration: 11700 , train shape: (47041, 48, 48, 1)\n",
            "iteration: 11800 , train shape: (47141, 48, 48, 1)\n",
            "iteration: 11900 , train shape: (47241, 48, 48, 1)\n",
            "iteration: 12000 , train shape: (47341, 48, 48, 1)\n",
            "iteration: 12100 , train shape: (47441, 48, 48, 1)\n",
            "iteration: 12200 , train shape: (47541, 48, 48, 1)\n",
            "iteration: 12300 , train shape: (47641, 48, 48, 1)\n",
            "iteration: 12400 , train shape: (47741, 48, 48, 1)\n",
            "iteration: 12500 , train shape: (47841, 48, 48, 1)\n",
            "iteration: 12600 , train shape: (47941, 48, 48, 1)\n",
            "iteration: 12700 , train shape: (48041, 48, 48, 1)\n",
            "iteration: 12800 , train shape: (48141, 48, 48, 1)\n",
            "iteration: 12900 , train shape: (48241, 48, 48, 1)\n",
            "iteration: 13000 , train shape: (48341, 48, 48, 1)\n",
            "iteration: 13100 , train shape: (48441, 48, 48, 1)\n",
            "iteration: 13200 , train shape: (48541, 48, 48, 1)\n",
            "iteration: 13300 , train shape: (48641, 48, 48, 1)\n",
            "iteration: 13400 , train shape: (48741, 48, 48, 1)\n",
            "iteration: 13500 , train shape: (48841, 48, 48, 1)\n",
            "iteration: 13600 , train shape: (48941, 48, 48, 1)\n",
            "iteration: 13700 , train shape: (49041, 48, 48, 1)\n",
            "iteration: 13800 , train shape: (49141, 48, 48, 1)\n",
            "iteration: 13900 , train shape: (49241, 48, 48, 1)\n",
            "iteration: 14000 , train shape: (49341, 48, 48, 1)\n",
            "iteration: 14100 , train shape: (49441, 48, 48, 1)\n",
            "iteration: 14200 , train shape: (49541, 48, 48, 1)\n",
            "iteration: 14300 , train shape: (49641, 48, 48, 1)\n",
            "iteration: 14400 , train shape: (49741, 48, 48, 1)\n",
            "iteration: 14500 , train shape: (49841, 48, 48, 1)\n",
            "iteration: 14600 , train shape: (49941, 48, 48, 1)\n",
            "iteration: 14700 , train shape: (50041, 48, 48, 1)\n",
            "iteration: 14800 , train shape: (50141, 48, 48, 1)\n",
            "iteration: 14900 , train shape: (50241, 48, 48, 1)\n",
            "iteration: 15000 , train shape: (50341, 48, 48, 1)\n",
            "iteration: 15100 , train shape: (50441, 48, 48, 1)\n",
            "iteration: 15200 , train shape: (50541, 48, 48, 1)\n",
            "iteration: 15300 , train shape: (50641, 48, 48, 1)\n",
            "iteration: 15400 , train shape: (50741, 48, 48, 1)\n",
            "iteration: 15500 , train shape: (50841, 48, 48, 1)\n",
            "iteration: 15600 , train shape: (50941, 48, 48, 1)\n",
            "iteration: 15700 , train shape: (51041, 48, 48, 1)\n",
            "iteration: 15800 , train shape: (51141, 48, 48, 1)\n",
            "iteration: 15900 , train shape: (51241, 48, 48, 1)\n",
            "iteration: 16000 , train shape: (51341, 48, 48, 1)\n",
            "iteration: 16100 , train shape: (51441, 48, 48, 1)\n",
            "iteration: 16200 , train shape: (51541, 48, 48, 1)\n",
            "iteration: 16300 , train shape: (51641, 48, 48, 1)\n",
            "iteration: 16400 , train shape: (51741, 48, 48, 1)\n",
            "iteration: 16500 , train shape: (51841, 48, 48, 1)\n",
            "iteration: 16600 , train shape: (51941, 48, 48, 1)\n",
            "iteration: 16700 , train shape: (52041, 48, 48, 1)\n",
            "iteration: 16800 , train shape: (52141, 48, 48, 1)\n",
            "iteration: 16900 , train shape: (52241, 48, 48, 1)\n",
            "iteration: 17000 , train shape: (52341, 48, 48, 1)\n",
            "iteration: 17100 , train shape: (52441, 48, 48, 1)\n",
            "iteration: 17200 , train shape: (52541, 48, 48, 1)\n",
            "iteration: 17300 , train shape: (52641, 48, 48, 1)\n",
            "iteration: 17400 , train shape: (52741, 48, 48, 1)\n",
            "iteration: 17500 , train shape: (52841, 48, 48, 1)\n",
            "iteration: 17600 , train shape: (52941, 48, 48, 1)\n",
            "iteration: 17700 , train shape: (53041, 48, 48, 1)\n",
            "iteration: 17800 , train shape: (53141, 48, 48, 1)\n",
            "iteration: 17900 , train shape: (53241, 48, 48, 1)\n",
            "iteration: 18000 , train shape: (53341, 48, 48, 1)\n",
            "iteration: 18100 , train shape: (53441, 48, 48, 1)\n",
            "iteration: 18200 , train shape: (53541, 48, 48, 1)\n",
            "iteration: 18300 , train shape: (53641, 48, 48, 1)\n",
            "iteration: 18400 , train shape: (53741, 48, 48, 1)\n",
            "iteration: 18500 , train shape: (53841, 48, 48, 1)\n",
            "iteration: 18600 , train shape: (53941, 48, 48, 1)\n",
            "iteration: 18700 , train shape: (54041, 48, 48, 1)\n",
            "iteration: 18800 , train shape: (54141, 48, 48, 1)\n",
            "iteration: 18900 , train shape: (54241, 48, 48, 1)\n",
            "iteration: 19000 , train shape: (54341, 48, 48, 1)\n",
            "iteration: 19100 , train shape: (54441, 48, 48, 1)\n",
            "iteration: 19200 , train shape: (54541, 48, 48, 1)\n",
            "iteration: 19300 , train shape: (54641, 48, 48, 1)\n",
            "iteration: 19400 , train shape: (54741, 48, 48, 1)\n",
            "iteration: 19500 , train shape: (54841, 48, 48, 1)\n",
            "iteration: 19600 , train shape: (54941, 48, 48, 1)\n",
            "iteration: 19700 , train shape: (55041, 48, 48, 1)\n",
            "iteration: 19800 , train shape: (55141, 48, 48, 1)\n",
            "iteration: 19900 , train shape: (55241, 48, 48, 1)\n",
            "iteration: 20000 , train shape: (55341, 48, 48, 1)\n",
            "iteration: 20100 , train shape: (55441, 48, 48, 1)\n",
            "iteration: 20200 , train shape: (55541, 48, 48, 1)\n",
            "iteration: 20300 , train shape: (55641, 48, 48, 1)\n",
            "iteration: 20400 , train shape: (55741, 48, 48, 1)\n",
            "iteration: 20500 , train shape: (55841, 48, 48, 1)\n",
            "iteration: 20600 , train shape: (55941, 48, 48, 1)\n",
            "iteration: 20700 , train shape: (56041, 48, 48, 1)\n",
            "iteration: 20800 , train shape: (56141, 48, 48, 1)\n",
            "iteration: 20900 , train shape: (56241, 48, 48, 1)\n",
            "iteration: 21000 , train shape: (56341, 48, 48, 1)\n",
            "iteration: 21100 , train shape: (56441, 48, 48, 1)\n",
            "iteration: 21200 , train shape: (56541, 48, 48, 1)\n",
            "iteration: 21300 , train shape: (56641, 48, 48, 1)\n",
            "iteration: 21400 , train shape: (56741, 48, 48, 1)\n",
            "iteration: 21500 , train shape: (56841, 48, 48, 1)\n",
            "iteration: 21600 , train shape: (56941, 48, 48, 1)\n",
            "iteration: 21700 , train shape: (57041, 48, 48, 1)\n",
            "iteration: 21800 , train shape: (57141, 48, 48, 1)\n",
            "iteration: 21900 , train shape: (57241, 48, 48, 1)\n",
            "iteration: 22000 , train shape: (57341, 48, 48, 1)\n",
            "iteration: 22100 , train shape: (57441, 48, 48, 1)\n",
            "iteration: 22200 , train shape: (57541, 48, 48, 1)\n",
            "iteration: 22300 , train shape: (57641, 48, 48, 1)\n",
            "iteration: 22400 , train shape: (57741, 48, 48, 1)\n",
            "iteration: 22500 , train shape: (57841, 48, 48, 1)\n",
            "iteration: 22600 , train shape: (57941, 48, 48, 1)\n",
            "iteration: 22700 , train shape: (58041, 48, 48, 1)\n",
            "iteration: 22800 , train shape: (58141, 48, 48, 1)\n",
            "iteration: 22900 , train shape: (58241, 48, 48, 1)\n",
            "iteration: 23000 , train shape: (58341, 48, 48, 1)\n",
            "iteration: 23100 , train shape: (58441, 48, 48, 1)\n",
            "iteration: 23200 , train shape: (58541, 48, 48, 1)\n",
            "iteration: 23300 , train shape: (58641, 48, 48, 1)\n",
            "iteration: 23400 , train shape: (58741, 48, 48, 1)\n",
            "iteration: 23500 , train shape: (58841, 48, 48, 1)\n",
            "iteration: 23600 , train shape: (58941, 48, 48, 1)\n",
            "iteration: 23700 , train shape: (59041, 48, 48, 1)\n",
            "iteration: 23800 , train shape: (59141, 48, 48, 1)\n",
            "iteration: 23900 , train shape: (59241, 48, 48, 1)\n",
            "iteration: 24000 , train shape: (59341, 48, 48, 1)\n",
            "iteration: 24100 , train shape: (59441, 48, 48, 1)\n",
            "iteration: 24200 , train shape: (59541, 48, 48, 1)\n",
            "iteration: 24300 , train shape: (59641, 48, 48, 1)\n",
            "iteration: 24400 , train shape: (59741, 48, 48, 1)\n",
            "iteration: 24500 , train shape: (59841, 48, 48, 1)\n",
            "iteration: 24600 , train shape: (59941, 48, 48, 1)\n",
            "iteration: 24700 , train shape: (60041, 48, 48, 1)\n",
            "iteration: 24800 , train shape: (60141, 48, 48, 1)\n",
            "iteration: 24900 , train shape: (60241, 48, 48, 1)\n",
            "iteration: 25000 , train shape: (60341, 48, 48, 1)\n",
            "iteration: 25100 , train shape: (60441, 48, 48, 1)\n",
            "iteration: 25200 , train shape: (60541, 48, 48, 1)\n",
            "iteration: 25300 , train shape: (60641, 48, 48, 1)\n",
            "iteration: 25400 , train shape: (60741, 48, 48, 1)\n",
            "iteration: 25500 , train shape: (60841, 48, 48, 1)\n",
            "iteration: 25600 , train shape: (60941, 48, 48, 1)\n",
            "iteration: 25700 , train shape: (61041, 48, 48, 1)\n",
            "iteration: 25800 , train shape: (61141, 48, 48, 1)\n",
            "iteration: 25900 , train shape: (61241, 48, 48, 1)\n",
            "iteration: 26000 , train shape: (61341, 48, 48, 1)\n",
            "iteration: 26100 , train shape: (61441, 48, 48, 1)\n",
            "iteration: 26200 , train shape: (61541, 48, 48, 1)\n",
            "iteration: 26300 , train shape: (61641, 48, 48, 1)\n",
            "iteration: 26400 , train shape: (61741, 48, 48, 1)\n",
            "iteration: 26500 , train shape: (61841, 48, 48, 1)\n",
            "iteration: 26600 , train shape: (61941, 48, 48, 1)\n",
            "iteration: 26700 , train shape: (62041, 48, 48, 1)\n",
            "iteration: 26800 , train shape: (62141, 48, 48, 1)\n",
            "iteration: 26900 , train shape: (62241, 48, 48, 1)\n",
            "iteration: 27000 , train shape: (62341, 48, 48, 1)\n",
            "iteration: 27100 , train shape: (62441, 48, 48, 1)\n",
            "iteration: 27200 , train shape: (62541, 48, 48, 1)\n",
            "iteration: 27300 , train shape: (62641, 48, 48, 1)\n",
            "iteration: 27400 , train shape: (62741, 48, 48, 1)\n",
            "iteration: 27500 , train shape: (62841, 48, 48, 1)\n",
            "iteration: 27600 , train shape: (62941, 48, 48, 1)\n",
            "iteration: 27700 , train shape: (63041, 48, 48, 1)\n",
            "iteration: 27800 , train shape: (63141, 48, 48, 1)\n",
            "iteration: 27900 , train shape: (63241, 48, 48, 1)\n",
            "iteration: 28000 , train shape: (63341, 48, 48, 1)\n",
            "iteration: 28100 , train shape: (63441, 48, 48, 1)\n",
            "iteration: 28200 , train shape: (63541, 48, 48, 1)\n",
            "iteration: 28300 , train shape: (63641, 48, 48, 1)\n",
            "iteration: 28400 , train shape: (63741, 48, 48, 1)\n",
            "iteration: 28500 , train shape: (63841, 48, 48, 1)\n",
            "iteration: 28600 , train shape: (63941, 48, 48, 1)\n",
            "iteration: 28700 , train shape: (64041, 48, 48, 1)\n",
            "iteration: 28800 , train shape: (64141, 48, 48, 1)\n",
            "iteration: 28900 , train shape: (64241, 48, 48, 1)\n",
            "iteration: 29000 , train shape: (64341, 48, 48, 1)\n",
            "iteration: 29100 , train shape: (64441, 48, 48, 1)\n",
            "iteration: 29200 , train shape: (64541, 48, 48, 1)\n",
            "iteration: 29300 , train shape: (64641, 48, 48, 1)\n",
            "iteration: 29400 , train shape: (64741, 48, 48, 1)\n",
            "iteration: 29500 , train shape: (64841, 48, 48, 1)\n",
            "iteration: 29600 , train shape: (64941, 48, 48, 1)\n",
            "iteration: 29700 , train shape: (65041, 48, 48, 1)\n",
            "iteration: 29800 , train shape: (65141, 48, 48, 1)\n",
            "iteration: 29900 , train shape: (65241, 48, 48, 1)\n",
            "iteration: 30000 , train shape: (65341, 48, 48, 1)\n",
            "iteration: 30100 , train shape: (65441, 48, 48, 1)\n",
            "iteration: 30200 , train shape: (65541, 48, 48, 1)\n",
            "iteration: 30300 , train shape: (65641, 48, 48, 1)\n",
            "iteration: 30400 , train shape: (65741, 48, 48, 1)\n",
            "iteration: 30500 , train shape: (65841, 48, 48, 1)\n",
            "iteration: 30600 , train shape: (65941, 48, 48, 1)\n",
            "iteration: 30700 , train shape: (66041, 48, 48, 1)\n",
            "iteration: 30800 , train shape: (66141, 48, 48, 1)\n",
            "iteration: 30900 , train shape: (66241, 48, 48, 1)\n",
            "iteration: 31000 , train shape: (66341, 48, 48, 1)\n",
            "iteration: 31100 , train shape: (66441, 48, 48, 1)\n",
            "iteration: 31200 , train shape: (66541, 48, 48, 1)\n",
            "iteration: 31300 , train shape: (66641, 48, 48, 1)\n",
            "iteration: 31400 , train shape: (66741, 48, 48, 1)\n",
            "iteration: 31500 , train shape: (66841, 48, 48, 1)\n",
            "iteration: 31600 , train shape: (66941, 48, 48, 1)\n",
            "iteration: 31700 , train shape: (67041, 48, 48, 1)\n",
            "iteration: 31800 , train shape: (67141, 48, 48, 1)\n",
            "iteration: 31900 , train shape: (67241, 48, 48, 1)\n",
            "iteration: 32000 , train shape: (67341, 48, 48, 1)\n",
            "iteration: 32100 , train shape: (67441, 48, 48, 1)\n",
            "iteration: 32200 , train shape: (67541, 48, 48, 1)\n",
            "iteration: 32300 , train shape: (67641, 48, 48, 1)\n",
            "iteration: 32400 , train shape: (67741, 48, 48, 1)\n",
            "iteration: 32500 , train shape: (67841, 48, 48, 1)\n",
            "iteration: 32600 , train shape: (67941, 48, 48, 1)\n",
            "iteration: 32700 , train shape: (68041, 48, 48, 1)\n",
            "iteration: 32800 , train shape: (68141, 48, 48, 1)\n",
            "iteration: 32900 , train shape: (68241, 48, 48, 1)\n",
            "iteration: 33000 , train shape: (68341, 48, 48, 1)\n",
            "iteration: 33100 , train shape: (68441, 48, 48, 1)\n",
            "iteration: 33200 , train shape: (68541, 48, 48, 1)\n",
            "iteration: 33300 , train shape: (68641, 48, 48, 1)\n",
            "iteration: 33400 , train shape: (68741, 48, 48, 1)\n",
            "iteration: 33500 , train shape: (68841, 48, 48, 1)\n",
            "iteration: 33600 , train shape: (68941, 48, 48, 1)\n",
            "iteration: 33700 , train shape: (69041, 48, 48, 1)\n",
            "iteration: 33800 , train shape: (69141, 48, 48, 1)\n",
            "iteration: 33900 , train shape: (69241, 48, 48, 1)\n",
            "iteration: 34000 , train shape: (69341, 48, 48, 1)\n",
            "iteration: 34100 , train shape: (69441, 48, 48, 1)\n",
            "iteration: 34200 , train shape: (69541, 48, 48, 1)\n",
            "iteration: 34300 , train shape: (69641, 48, 48, 1)\n",
            "iteration: 34400 , train shape: (69741, 48, 48, 1)\n",
            "iteration: 34500 , train shape: (69841, 48, 48, 1)\n",
            "iteration: 34600 , train shape: (69941, 48, 48, 1)\n",
            "iteration: 34700 , train shape: (70041, 48, 48, 1)\n",
            "iteration: 34800 , train shape: (70141, 48, 48, 1)\n",
            "iteration: 34900 , train shape: (70241, 48, 48, 1)\n",
            "iteration: 35000 , train shape: (70341, 48, 48, 1)\n",
            "iteration: 35100 , train shape: (70441, 48, 48, 1)\n",
            "iteration: 35200 , train shape: (70541, 48, 48, 1)\n",
            "iteration: 35300 , train shape: (70641, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWsEbPyuvHO8",
        "outputId": "afdedfeb-b36d-461c-e3f8-dec22e7b8847"
      },
      "source": [
        "print(\"X_train_aug shape:\", X_train_aug.shape)\n",
        "print(\"y_train_aug shape:\", y_train_aug.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_aug shape: (70680, 48, 48, 1)\n",
            "y_train_aug shape: (70680, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding image of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "        zn = Flatten()(zi)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train_aug.shape[0], batch_size)\n",
        "            imgs, labels = X_train_aug[idx], y_train_aug[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wG29wyj1kJd",
        "outputId": "0ac78baf-febc-428d-b923-49022c10a1ed"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=18610, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.710260, acc.: 36.33%] [G loss: 1.416152]\n",
            "20 [D loss: 1.357919, acc.: 8.59%] [G loss: 0.673803]\n",
            "40 [D loss: 2.778759, acc.: 44.14%] [G loss: 0.438685]\n",
            "60 [D loss: 1.002571, acc.: 46.88%] [G loss: 6.002392]\n",
            "80 [D loss: 0.689819, acc.: 51.56%] [G loss: 2.879420]\n",
            "100 [D loss: 0.560641, acc.: 76.17%] [G loss: 3.936252]\n",
            "120 [D loss: 0.655989, acc.: 48.05%] [G loss: 3.689402]\n",
            "140 [D loss: 0.859260, acc.: 39.45%] [G loss: 3.874625]\n",
            "160 [D loss: 0.341282, acc.: 88.28%] [G loss: 6.455709]\n",
            "180 [D loss: 0.749014, acc.: 59.38%] [G loss: 4.353196]\n",
            "200 [D loss: 0.510762, acc.: 78.12%] [G loss: 3.413136]\n",
            "220 [D loss: 0.418352, acc.: 80.08%] [G loss: 5.962391]\n",
            "240 [D loss: 0.294843, acc.: 85.16%] [G loss: 6.519878]\n",
            "260 [D loss: 0.427301, acc.: 88.28%] [G loss: 4.220860]\n",
            "280 [D loss: 1.060287, acc.: 39.84%] [G loss: 2.491427]\n",
            "300 [D loss: 0.312092, acc.: 91.02%] [G loss: 5.023406]\n",
            "320 [D loss: 1.490266, acc.: 55.47%] [G loss: 3.188958]\n",
            "340 [D loss: 0.320381, acc.: 91.80%] [G loss: 3.949071]\n",
            "360 [D loss: 0.398119, acc.: 84.77%] [G loss: 3.911855]\n",
            "380 [D loss: 0.614311, acc.: 69.53%] [G loss: 3.787537]\n",
            "400 [D loss: 0.474752, acc.: 81.25%] [G loss: 4.987549]\n",
            "420 [D loss: 0.621422, acc.: 61.72%] [G loss: 2.419485]\n",
            "440 [D loss: 0.666983, acc.: 64.45%] [G loss: 3.351705]\n",
            "460 [D loss: 0.536325, acc.: 70.31%] [G loss: 3.690858]\n",
            "480 [D loss: 0.627144, acc.: 68.36%] [G loss: 2.092335]\n",
            "500 [D loss: 0.458047, acc.: 83.98%] [G loss: 3.731731]\n",
            "520 [D loss: 0.312370, acc.: 89.45%] [G loss: 4.717635]\n",
            "540 [D loss: 0.525537, acc.: 75.78%] [G loss: 2.833516]\n",
            "560 [D loss: 0.687011, acc.: 55.86%] [G loss: 2.528634]\n",
            "580 [D loss: 0.641115, acc.: 66.80%] [G loss: 3.193852]\n",
            "600 [D loss: 0.419133, acc.: 77.34%] [G loss: 4.267668]\n",
            "620 [D loss: 0.747504, acc.: 57.42%] [G loss: 2.620968]\n",
            "640 [D loss: 0.454611, acc.: 81.64%] [G loss: 2.798015]\n",
            "660 [D loss: 0.631621, acc.: 64.84%] [G loss: 3.755096]\n",
            "680 [D loss: 0.739409, acc.: 57.42%] [G loss: 3.314746]\n",
            "700 [D loss: 0.459263, acc.: 81.64%] [G loss: 3.124313]\n",
            "720 [D loss: 0.482141, acc.: 82.42%] [G loss: 3.854061]\n",
            "740 [D loss: 0.576035, acc.: 66.80%] [G loss: 3.413374]\n",
            "760 [D loss: 0.690368, acc.: 57.81%] [G loss: 2.779897]\n",
            "780 [D loss: 0.513158, acc.: 73.44%] [G loss: 2.713050]\n",
            "800 [D loss: 0.470422, acc.: 81.64%] [G loss: 3.375484]\n",
            "820 [D loss: 0.708320, acc.: 54.69%] [G loss: 2.610667]\n",
            "840 [D loss: 0.370991, acc.: 90.62%] [G loss: 4.297818]\n",
            "860 [D loss: 0.487508, acc.: 76.17%] [G loss: 3.547424]\n",
            "880 [D loss: 0.576464, acc.: 71.09%] [G loss: 3.708636]\n",
            "900 [D loss: 0.513822, acc.: 76.56%] [G loss: 2.211231]\n",
            "920 [D loss: 0.566843, acc.: 69.92%] [G loss: 3.842015]\n",
            "940 [D loss: 0.590224, acc.: 73.44%] [G loss: 3.354333]\n",
            "960 [D loss: 0.632342, acc.: 62.50%] [G loss: 2.590875]\n",
            "980 [D loss: 0.473722, acc.: 80.08%] [G loss: 4.229438]\n",
            "1000 [D loss: 0.528791, acc.: 70.31%] [G loss: 3.316165]\n",
            "1020 [D loss: 0.565230, acc.: 71.88%] [G loss: 3.097115]\n",
            "1040 [D loss: 0.504484, acc.: 75.39%] [G loss: 3.637143]\n",
            "1060 [D loss: 0.617013, acc.: 67.58%] [G loss: 2.082272]\n",
            "1080 [D loss: 0.575606, acc.: 71.48%] [G loss: 2.450809]\n",
            "1100 [D loss: 0.560622, acc.: 73.44%] [G loss: 2.794036]\n",
            "1120 [D loss: 0.575539, acc.: 73.05%] [G loss: 2.651076]\n",
            "1140 [D loss: 0.451634, acc.: 82.03%] [G loss: 3.416709]\n",
            "1160 [D loss: 0.583575, acc.: 69.92%] [G loss: 3.124660]\n",
            "1180 [D loss: 0.491452, acc.: 80.47%] [G loss: 3.273798]\n",
            "1200 [D loss: 0.467880, acc.: 76.17%] [G loss: 3.592752]\n",
            "1220 [D loss: 0.536124, acc.: 75.39%] [G loss: 2.850089]\n",
            "1240 [D loss: 0.496131, acc.: 77.34%] [G loss: 3.152973]\n",
            "1260 [D loss: 0.491813, acc.: 76.95%] [G loss: 3.381973]\n",
            "1280 [D loss: 0.465077, acc.: 82.03%] [G loss: 3.473475]\n",
            "1300 [D loss: 0.741821, acc.: 53.91%] [G loss: 2.959513]\n",
            "1320 [D loss: 0.539066, acc.: 76.56%] [G loss: 3.105767]\n",
            "1340 [D loss: 0.541670, acc.: 73.83%] [G loss: 2.937165]\n",
            "1360 [D loss: 0.438199, acc.: 82.81%] [G loss: 3.390210]\n",
            "1380 [D loss: 0.524681, acc.: 75.00%] [G loss: 3.153097]\n",
            "1400 [D loss: 0.466932, acc.: 80.47%] [G loss: 3.256874]\n",
            "1420 [D loss: 0.579702, acc.: 71.48%] [G loss: 2.853162]\n",
            "1440 [D loss: 0.505392, acc.: 73.05%] [G loss: 3.269550]\n",
            "1460 [D loss: 0.466422, acc.: 78.91%] [G loss: 3.144807]\n",
            "1480 [D loss: 0.596798, acc.: 66.80%] [G loss: 2.757254]\n",
            "1500 [D loss: 0.505007, acc.: 75.39%] [G loss: 2.991458]\n",
            "1520 [D loss: 0.507522, acc.: 75.39%] [G loss: 3.057176]\n",
            "1540 [D loss: 0.481785, acc.: 77.73%] [G loss: 2.716793]\n",
            "1560 [D loss: 1.011225, acc.: 37.89%] [G loss: 2.073658]\n",
            "1580 [D loss: 0.618609, acc.: 68.75%] [G loss: 2.895234]\n",
            "1600 [D loss: 0.530200, acc.: 71.48%] [G loss: 3.423589]\n",
            "1620 [D loss: 0.472288, acc.: 77.34%] [G loss: 3.496489]\n",
            "1640 [D loss: 0.597534, acc.: 67.58%] [G loss: 2.979145]\n",
            "1660 [D loss: 0.492141, acc.: 77.34%] [G loss: 3.244034]\n",
            "1680 [D loss: 0.527920, acc.: 78.12%] [G loss: 2.979808]\n",
            "1700 [D loss: 0.624951, acc.: 64.45%] [G loss: 1.967320]\n",
            "1720 [D loss: 0.425952, acc.: 81.64%] [G loss: 2.813652]\n",
            "1740 [D loss: 0.467839, acc.: 77.73%] [G loss: 3.757411]\n",
            "1760 [D loss: 0.527315, acc.: 73.05%] [G loss: 3.504933]\n",
            "1780 [D loss: 0.524264, acc.: 74.22%] [G loss: 3.002683]\n",
            "1800 [D loss: 0.479650, acc.: 77.34%] [G loss: 3.484860]\n",
            "1820 [D loss: 0.541793, acc.: 74.61%] [G loss: 3.260476]\n",
            "1840 [D loss: 0.493446, acc.: 75.39%] [G loss: 2.917166]\n",
            "1860 [D loss: 0.496907, acc.: 76.56%] [G loss: 3.179990]\n",
            "1880 [D loss: 0.471320, acc.: 80.86%] [G loss: 3.195848]\n",
            "1900 [D loss: 0.410750, acc.: 84.77%] [G loss: 3.454078]\n",
            "1920 [D loss: 0.592596, acc.: 69.53%] [G loss: 3.325547]\n",
            "1940 [D loss: 0.550194, acc.: 73.05%] [G loss: 3.231031]\n",
            "1960 [D loss: 0.667666, acc.: 61.72%] [G loss: 2.614059]\n",
            "1980 [D loss: 0.593115, acc.: 67.19%] [G loss: 3.055716]\n",
            "2000 [D loss: 0.516250, acc.: 75.78%] [G loss: 3.477292]\n",
            "2020 [D loss: 0.478241, acc.: 76.17%] [G loss: 3.609112]\n",
            "2040 [D loss: 0.485032, acc.: 75.00%] [G loss: 3.497555]\n",
            "2060 [D loss: 0.426027, acc.: 81.64%] [G loss: 3.703837]\n",
            "2080 [D loss: 0.491656, acc.: 74.61%] [G loss: 2.739392]\n",
            "2100 [D loss: 0.455898, acc.: 79.69%] [G loss: 3.387691]\n",
            "2120 [D loss: 0.606919, acc.: 68.75%] [G loss: 3.443930]\n",
            "2140 [D loss: 0.430739, acc.: 81.64%] [G loss: 3.794590]\n",
            "2160 [D loss: 0.526374, acc.: 73.83%] [G loss: 3.453128]\n",
            "2180 [D loss: 0.570052, acc.: 69.92%] [G loss: 3.187270]\n",
            "2200 [D loss: 0.507167, acc.: 76.17%] [G loss: 3.876026]\n",
            "2220 [D loss: 0.449910, acc.: 78.52%] [G loss: 3.518467]\n",
            "2240 [D loss: 0.586473, acc.: 70.31%] [G loss: 2.588861]\n",
            "2260 [D loss: 0.474974, acc.: 81.64%] [G loss: 3.586117]\n",
            "2280 [D loss: 0.322484, acc.: 91.02%] [G loss: 3.221619]\n",
            "2300 [D loss: 0.585018, acc.: 70.31%] [G loss: 3.171933]\n",
            "2320 [D loss: 0.475161, acc.: 78.52%] [G loss: 3.509600]\n",
            "2340 [D loss: 0.636903, acc.: 66.02%] [G loss: 2.811513]\n",
            "2360 [D loss: 0.508324, acc.: 74.22%] [G loss: 3.282962]\n",
            "2380 [D loss: 0.489524, acc.: 76.95%] [G loss: 3.228951]\n",
            "2400 [D loss: 0.609558, acc.: 67.19%] [G loss: 3.276315]\n",
            "2420 [D loss: 0.905122, acc.: 45.31%] [G loss: 2.489725]\n",
            "2440 [D loss: 0.446512, acc.: 81.25%] [G loss: 3.716121]\n",
            "2460 [D loss: 0.802311, acc.: 57.03%] [G loss: 3.039660]\n",
            "2480 [D loss: 0.619377, acc.: 64.84%] [G loss: 3.126548]\n",
            "2500 [D loss: 0.493476, acc.: 76.95%] [G loss: 3.875785]\n",
            "2520 [D loss: 0.562576, acc.: 69.53%] [G loss: 3.158243]\n",
            "2540 [D loss: 0.559572, acc.: 71.48%] [G loss: 3.349903]\n",
            "2560 [D loss: 0.525576, acc.: 74.61%] [G loss: 3.416574]\n",
            "2580 [D loss: 0.517011, acc.: 74.22%] [G loss: 2.882266]\n",
            "2600 [D loss: 0.782585, acc.: 53.52%] [G loss: 2.239908]\n",
            "2620 [D loss: 0.816228, acc.: 52.34%] [G loss: 3.246364]\n",
            "2640 [D loss: 0.642859, acc.: 64.45%] [G loss: 3.219778]\n",
            "2660 [D loss: 0.514516, acc.: 75.00%] [G loss: 3.301069]\n",
            "2680 [D loss: 0.549415, acc.: 69.14%] [G loss: 2.849778]\n",
            "2700 [D loss: 0.523755, acc.: 71.88%] [G loss: 2.863355]\n",
            "2720 [D loss: 0.464047, acc.: 79.30%] [G loss: 3.367445]\n",
            "2740 [D loss: 0.644024, acc.: 66.41%] [G loss: 2.980033]\n",
            "2760 [D loss: 0.647435, acc.: 63.28%] [G loss: 3.279043]\n",
            "2780 [D loss: 0.486506, acc.: 78.52%] [G loss: 3.402388]\n",
            "2800 [D loss: 0.543091, acc.: 71.48%] [G loss: 2.909946]\n",
            "2820 [D loss: 0.508262, acc.: 72.66%] [G loss: 3.420103]\n",
            "2840 [D loss: 0.538968, acc.: 70.31%] [G loss: 3.110762]\n",
            "2860 [D loss: 0.476995, acc.: 80.08%] [G loss: 3.051413]\n",
            "2880 [D loss: 0.554742, acc.: 69.92%] [G loss: 3.158223]\n",
            "2900 [D loss: 0.502526, acc.: 75.39%] [G loss: 3.148245]\n",
            "2920 [D loss: 0.562026, acc.: 70.31%] [G loss: 3.230793]\n",
            "2940 [D loss: 0.503137, acc.: 78.12%] [G loss: 3.251144]\n",
            "2960 [D loss: 0.517945, acc.: 75.78%] [G loss: 3.111605]\n",
            "2980 [D loss: 0.549527, acc.: 76.17%] [G loss: 3.036159]\n",
            "3000 [D loss: 0.562349, acc.: 70.70%] [G loss: 3.091965]\n",
            "3020 [D loss: 0.496627, acc.: 78.12%] [G loss: 3.346113]\n",
            "3040 [D loss: 0.717338, acc.: 60.55%] [G loss: 2.263040]\n",
            "3060 [D loss: 0.463500, acc.: 82.81%] [G loss: 3.106876]\n",
            "3080 [D loss: 0.535587, acc.: 72.66%] [G loss: 3.086285]\n",
            "3100 [D loss: 0.457043, acc.: 81.25%] [G loss: 3.229020]\n",
            "3120 [D loss: 0.563451, acc.: 67.58%] [G loss: 2.927115]\n",
            "3140 [D loss: 0.557681, acc.: 73.05%] [G loss: 2.896378]\n",
            "3160 [D loss: 0.489975, acc.: 75.39%] [G loss: 3.004402]\n",
            "3180 [D loss: 0.536517, acc.: 71.88%] [G loss: 2.946744]\n",
            "3200 [D loss: 0.559646, acc.: 68.75%] [G loss: 3.064740]\n",
            "3220 [D loss: 0.572867, acc.: 69.14%] [G loss: 2.707333]\n",
            "3240 [D loss: 0.532678, acc.: 73.44%] [G loss: 2.981828]\n",
            "3260 [D loss: 0.417463, acc.: 84.77%] [G loss: 2.760824]\n",
            "3280 [D loss: 0.599494, acc.: 67.19%] [G loss: 2.360887]\n",
            "3300 [D loss: 1.038328, acc.: 42.19%] [G loss: 2.502956]\n",
            "3320 [D loss: 0.876864, acc.: 53.12%] [G loss: 1.762927]\n",
            "3340 [D loss: 0.561687, acc.: 72.66%] [G loss: 3.895833]\n",
            "3360 [D loss: 0.586532, acc.: 67.58%] [G loss: 2.959604]\n",
            "3380 [D loss: 0.570998, acc.: 67.97%] [G loss: 3.050964]\n",
            "3400 [D loss: 0.522729, acc.: 71.09%] [G loss: 3.378548]\n",
            "3420 [D loss: 0.476381, acc.: 80.86%] [G loss: 3.345104]\n",
            "3440 [D loss: 0.553664, acc.: 72.27%] [G loss: 3.087070]\n",
            "3460 [D loss: 0.561508, acc.: 71.48%] [G loss: 3.233429]\n",
            "3480 [D loss: 0.560207, acc.: 70.70%] [G loss: 2.904788]\n",
            "3500 [D loss: 0.569181, acc.: 71.09%] [G loss: 2.965158]\n",
            "3520 [D loss: 0.580781, acc.: 68.75%] [G loss: 2.987999]\n",
            "3540 [D loss: 0.537515, acc.: 72.27%] [G loss: 3.366392]\n",
            "3560 [D loss: 0.528717, acc.: 76.56%] [G loss: 2.718813]\n",
            "3580 [D loss: 0.536476, acc.: 71.88%] [G loss: 3.064352]\n",
            "3600 [D loss: 0.458596, acc.: 78.91%] [G loss: 3.507504]\n",
            "3620 [D loss: 0.494907, acc.: 79.30%] [G loss: 3.281046]\n",
            "3640 [D loss: 0.544919, acc.: 69.53%] [G loss: 3.269770]\n",
            "3660 [D loss: 0.557006, acc.: 73.05%] [G loss: 2.990306]\n",
            "3680 [D loss: 0.514868, acc.: 74.61%] [G loss: 3.300422]\n",
            "3700 [D loss: 0.506905, acc.: 76.56%] [G loss: 3.124773]\n",
            "3720 [D loss: 0.571190, acc.: 67.97%] [G loss: 2.650088]\n",
            "3740 [D loss: 0.455226, acc.: 79.69%] [G loss: 3.133634]\n",
            "3760 [D loss: 0.526610, acc.: 76.56%] [G loss: 2.865294]\n",
            "3780 [D loss: 0.524897, acc.: 74.61%] [G loss: 3.170534]\n",
            "3800 [D loss: 0.491799, acc.: 78.12%] [G loss: 3.339634]\n",
            "3820 [D loss: 0.560576, acc.: 70.70%] [G loss: 3.117676]\n",
            "3840 [D loss: 0.578877, acc.: 72.27%] [G loss: 2.810009]\n",
            "3860 [D loss: 0.552230, acc.: 73.83%] [G loss: 2.956745]\n",
            "3880 [D loss: 0.504786, acc.: 76.56%] [G loss: 3.075501]\n",
            "3900 [D loss: 0.593342, acc.: 66.02%] [G loss: 3.118804]\n",
            "3920 [D loss: 0.487029, acc.: 78.91%] [G loss: 3.214804]\n",
            "3940 [D loss: 0.493339, acc.: 78.12%] [G loss: 3.292909]\n",
            "3960 [D loss: 0.460392, acc.: 80.47%] [G loss: 3.710582]\n",
            "3980 [D loss: 0.543800, acc.: 72.66%] [G loss: 3.049579]\n",
            "4000 [D loss: 0.524740, acc.: 72.27%] [G loss: 3.082396]\n",
            "4020 [D loss: 0.480063, acc.: 78.12%] [G loss: 3.172369]\n",
            "4040 [D loss: 0.555403, acc.: 70.31%] [G loss: 2.922776]\n",
            "4060 [D loss: 0.551093, acc.: 72.66%] [G loss: 2.869089]\n",
            "4080 [D loss: 0.451700, acc.: 80.08%] [G loss: 3.029275]\n",
            "4100 [D loss: 0.811448, acc.: 55.08%] [G loss: 2.201079]\n",
            "4120 [D loss: 0.654525, acc.: 64.84%] [G loss: 2.388626]\n",
            "4140 [D loss: 0.591263, acc.: 69.53%] [G loss: 3.137215]\n",
            "4160 [D loss: 0.798824, acc.: 57.03%] [G loss: 3.185284]\n",
            "4180 [D loss: 0.549387, acc.: 72.27%] [G loss: 3.236987]\n",
            "4200 [D loss: 0.427285, acc.: 83.20%] [G loss: 2.927959]\n",
            "4220 [D loss: 0.515692, acc.: 76.56%] [G loss: 3.541252]\n",
            "4240 [D loss: 0.517038, acc.: 76.95%] [G loss: 3.248871]\n",
            "4260 [D loss: 0.422654, acc.: 85.94%] [G loss: 3.503576]\n",
            "4280 [D loss: 0.484569, acc.: 76.17%] [G loss: 3.833575]\n",
            "4300 [D loss: 0.512147, acc.: 75.39%] [G loss: 3.474666]\n",
            "4320 [D loss: 0.589094, acc.: 70.70%] [G loss: 3.188220]\n",
            "4340 [D loss: 0.416960, acc.: 84.77%] [G loss: 3.422866]\n",
            "4360 [D loss: 0.660818, acc.: 66.80%] [G loss: 3.292030]\n",
            "4380 [D loss: 0.444592, acc.: 81.25%] [G loss: 3.499207]\n",
            "4400 [D loss: 0.583944, acc.: 72.27%] [G loss: 3.119348]\n",
            "4420 [D loss: 0.472822, acc.: 77.73%] [G loss: 3.380364]\n",
            "4440 [D loss: 0.559913, acc.: 71.88%] [G loss: 3.075472]\n",
            "4460 [D loss: 0.471441, acc.: 79.30%] [G loss: 3.523241]\n",
            "4480 [D loss: 0.533775, acc.: 74.61%] [G loss: 2.952334]\n",
            "4500 [D loss: 0.523394, acc.: 71.09%] [G loss: 2.969262]\n",
            "4520 [D loss: 0.516017, acc.: 73.83%] [G loss: 3.238518]\n",
            "4540 [D loss: 0.463346, acc.: 76.17%] [G loss: 3.323464]\n",
            "4560 [D loss: 0.630780, acc.: 62.50%] [G loss: 3.044315]\n",
            "4580 [D loss: 0.467954, acc.: 78.12%] [G loss: 3.298549]\n",
            "4600 [D loss: 0.540624, acc.: 73.05%] [G loss: 3.100092]\n",
            "4620 [D loss: 0.423435, acc.: 82.03%] [G loss: 3.360936]\n",
            "4640 [D loss: 0.650891, acc.: 62.11%] [G loss: 2.981700]\n",
            "4660 [D loss: 0.446666, acc.: 80.08%] [G loss: 3.251518]\n",
            "4680 [D loss: 0.503993, acc.: 73.83%] [G loss: 3.621746]\n",
            "4700 [D loss: 0.502271, acc.: 75.00%] [G loss: 3.422806]\n",
            "4720 [D loss: 0.571335, acc.: 68.36%] [G loss: 3.234104]\n",
            "4740 [D loss: 0.565421, acc.: 69.14%] [G loss: 2.978418]\n",
            "4760 [D loss: 0.482859, acc.: 75.78%] [G loss: 3.513195]\n",
            "4780 [D loss: 0.594339, acc.: 69.92%] [G loss: 2.904874]\n",
            "4800 [D loss: 0.468676, acc.: 80.08%] [G loss: 3.432984]\n",
            "4820 [D loss: 0.563394, acc.: 71.09%] [G loss: 2.960764]\n",
            "4840 [D loss: 0.488138, acc.: 77.34%] [G loss: 3.742984]\n",
            "4860 [D loss: 0.507198, acc.: 74.61%] [G loss: 3.106636]\n",
            "4880 [D loss: 0.484929, acc.: 76.17%] [G loss: 3.689362]\n",
            "4900 [D loss: 0.554059, acc.: 70.31%] [G loss: 2.920754]\n",
            "4920 [D loss: 0.483960, acc.: 77.73%] [G loss: 3.430101]\n",
            "4940 [D loss: 0.557834, acc.: 71.48%] [G loss: 3.053149]\n",
            "4960 [D loss: 0.531479, acc.: 71.88%] [G loss: 3.072433]\n",
            "4980 [D loss: 0.544616, acc.: 74.61%] [G loss: 3.092199]\n",
            "5000 [D loss: 0.513951, acc.: 75.00%] [G loss: 3.619761]\n",
            "5020 [D loss: 0.511869, acc.: 76.17%] [G loss: 3.503060]\n",
            "5040 [D loss: 0.558463, acc.: 76.56%] [G loss: 3.285044]\n",
            "5060 [D loss: 0.530601, acc.: 75.78%] [G loss: 2.952735]\n",
            "5080 [D loss: 0.527157, acc.: 75.00%] [G loss: 2.770480]\n",
            "5100 [D loss: 0.558676, acc.: 69.53%] [G loss: 3.114792]\n",
            "5120 [D loss: 0.448735, acc.: 78.52%] [G loss: 3.571841]\n",
            "5140 [D loss: 0.580476, acc.: 69.53%] [G loss: 3.079362]\n",
            "5160 [D loss: 0.481335, acc.: 77.73%] [G loss: 3.553673]\n",
            "5180 [D loss: 0.466493, acc.: 80.47%] [G loss: 3.819465]\n",
            "5200 [D loss: 0.444441, acc.: 80.86%] [G loss: 3.440459]\n",
            "5220 [D loss: 0.474658, acc.: 80.47%] [G loss: 3.734339]\n",
            "5240 [D loss: 0.481448, acc.: 78.52%] [G loss: 3.400472]\n",
            "5260 [D loss: 0.570414, acc.: 72.27%] [G loss: 3.247128]\n",
            "5280 [D loss: 0.567208, acc.: 69.14%] [G loss: 3.130587]\n",
            "5300 [D loss: 0.450934, acc.: 81.25%] [G loss: 3.501558]\n",
            "5320 [D loss: 0.530462, acc.: 71.48%] [G loss: 3.314427]\n",
            "5340 [D loss: 0.448162, acc.: 78.52%] [G loss: 3.558949]\n",
            "5360 [D loss: 0.483490, acc.: 75.78%] [G loss: 3.447317]\n",
            "5380 [D loss: 0.520442, acc.: 73.44%] [G loss: 3.291790]\n",
            "5400 [D loss: 0.493692, acc.: 75.78%] [G loss: 3.433820]\n",
            "5420 [D loss: 0.457484, acc.: 77.73%] [G loss: 3.447636]\n",
            "5440 [D loss: 0.473388, acc.: 77.73%] [G loss: 3.360976]\n",
            "5460 [D loss: 0.466694, acc.: 82.42%] [G loss: 3.910313]\n",
            "5480 [D loss: 0.546470, acc.: 71.88%] [G loss: 3.463203]\n",
            "5500 [D loss: 0.426222, acc.: 83.59%] [G loss: 3.648299]\n",
            "5520 [D loss: 0.539093, acc.: 69.92%] [G loss: 3.299890]\n",
            "5540 [D loss: 0.420513, acc.: 80.47%] [G loss: 3.629441]\n",
            "5560 [D loss: 0.577845, acc.: 66.02%] [G loss: 3.016695]\n",
            "5580 [D loss: 0.537675, acc.: 72.66%] [G loss: 3.232382]\n",
            "5600 [D loss: 0.588902, acc.: 66.41%] [G loss: 2.759596]\n",
            "5620 [D loss: 0.602976, acc.: 69.53%] [G loss: 3.528293]\n",
            "5640 [D loss: 0.683910, acc.: 62.11%] [G loss: 3.295850]\n",
            "5660 [D loss: 0.725734, acc.: 55.86%] [G loss: 3.257893]\n",
            "5680 [D loss: 0.605322, acc.: 69.92%] [G loss: 3.190001]\n",
            "5700 [D loss: 0.451750, acc.: 79.30%] [G loss: 3.542181]\n",
            "5720 [D loss: 0.490385, acc.: 76.95%] [G loss: 3.538736]\n",
            "5740 [D loss: 0.475969, acc.: 76.56%] [G loss: 3.504826]\n",
            "5760 [D loss: 0.539719, acc.: 72.66%] [G loss: 3.610505]\n",
            "5780 [D loss: 0.453488, acc.: 77.34%] [G loss: 3.698030]\n",
            "5800 [D loss: 0.537151, acc.: 72.66%] [G loss: 3.465993]\n",
            "5820 [D loss: 0.469420, acc.: 80.08%] [G loss: 3.694297]\n",
            "5840 [D loss: 0.517736, acc.: 73.44%] [G loss: 3.241175]\n",
            "5860 [D loss: 0.504483, acc.: 77.34%] [G loss: 3.271514]\n",
            "5880 [D loss: 0.469874, acc.: 78.12%] [G loss: 3.506625]\n",
            "5900 [D loss: 0.446244, acc.: 76.17%] [G loss: 3.870871]\n",
            "5920 [D loss: 0.550448, acc.: 71.88%] [G loss: 3.435033]\n",
            "5940 [D loss: 0.402971, acc.: 81.64%] [G loss: 3.881002]\n",
            "5960 [D loss: 0.480300, acc.: 76.56%] [G loss: 3.818545]\n",
            "5980 [D loss: 0.412465, acc.: 83.20%] [G loss: 3.752712]\n",
            "6000 [D loss: 0.459814, acc.: 78.52%] [G loss: 3.862697]\n",
            "6020 [D loss: 0.481092, acc.: 80.08%] [G loss: 3.481373]\n",
            "6040 [D loss: 0.552975, acc.: 71.48%] [G loss: 3.567985]\n",
            "6060 [D loss: 0.524117, acc.: 73.83%] [G loss: 3.571294]\n",
            "6080 [D loss: 0.563527, acc.: 73.83%] [G loss: 3.385383]\n",
            "6100 [D loss: 0.478244, acc.: 76.17%] [G loss: 3.707503]\n",
            "6120 [D loss: 0.545321, acc.: 72.27%] [G loss: 3.173449]\n",
            "6140 [D loss: 0.451676, acc.: 79.30%] [G loss: 2.901587]\n",
            "6160 [D loss: 0.413359, acc.: 82.81%] [G loss: 3.988617]\n",
            "6180 [D loss: 0.636940, acc.: 67.58%] [G loss: 3.771585]\n",
            "6200 [D loss: 0.981542, acc.: 52.73%] [G loss: 3.307635]\n",
            "6220 [D loss: 0.927394, acc.: 51.56%] [G loss: 2.958165]\n",
            "6240 [D loss: 0.777677, acc.: 57.42%] [G loss: 3.551065]\n",
            "6260 [D loss: 0.514232, acc.: 73.83%] [G loss: 3.668361]\n",
            "6280 [D loss: 0.546859, acc.: 75.00%] [G loss: 3.467279]\n",
            "6300 [D loss: 0.585995, acc.: 69.92%] [G loss: 3.336834]\n",
            "6320 [D loss: 0.583526, acc.: 68.75%] [G loss: 3.416874]\n",
            "6340 [D loss: 0.499025, acc.: 76.56%] [G loss: 3.363140]\n",
            "6360 [D loss: 0.440518, acc.: 77.73%] [G loss: 3.948761]\n",
            "6380 [D loss: 0.552107, acc.: 71.09%] [G loss: 3.221695]\n",
            "6400 [D loss: 0.500006, acc.: 77.34%] [G loss: 3.252166]\n",
            "6420 [D loss: 0.459216, acc.: 75.78%] [G loss: 3.665156]\n",
            "6440 [D loss: 0.449500, acc.: 84.77%] [G loss: 3.394371]\n",
            "6460 [D loss: 0.430640, acc.: 79.30%] [G loss: 3.679919]\n",
            "6480 [D loss: 0.598792, acc.: 69.53%] [G loss: 3.501137]\n",
            "6500 [D loss: 0.547730, acc.: 70.70%] [G loss: 3.507357]\n",
            "6520 [D loss: 0.542241, acc.: 72.27%] [G loss: 3.370029]\n",
            "6540 [D loss: 0.474264, acc.: 75.78%] [G loss: 3.797682]\n",
            "6560 [D loss: 0.459019, acc.: 80.86%] [G loss: 3.425645]\n",
            "6580 [D loss: 0.518373, acc.: 75.39%] [G loss: 3.379378]\n",
            "6600 [D loss: 0.497392, acc.: 73.44%] [G loss: 3.572865]\n",
            "6620 [D loss: 0.492295, acc.: 74.61%] [G loss: 3.604057]\n",
            "6640 [D loss: 0.457939, acc.: 79.30%] [G loss: 3.575547]\n",
            "6660 [D loss: 0.489527, acc.: 76.17%] [G loss: 3.555368]\n",
            "6680 [D loss: 0.540767, acc.: 73.44%] [G loss: 3.519600]\n",
            "6700 [D loss: 0.450015, acc.: 79.69%] [G loss: 3.754031]\n",
            "6720 [D loss: 0.419325, acc.: 80.47%] [G loss: 4.163191]\n",
            "6740 [D loss: 0.422192, acc.: 82.42%] [G loss: 3.798739]\n",
            "6760 [D loss: 0.502848, acc.: 75.78%] [G loss: 3.476979]\n",
            "6780 [D loss: 0.457028, acc.: 78.52%] [G loss: 3.785395]\n",
            "6800 [D loss: 0.451231, acc.: 79.30%] [G loss: 3.733246]\n",
            "6820 [D loss: 0.574890, acc.: 67.19%] [G loss: 3.841513]\n",
            "6840 [D loss: 0.421259, acc.: 80.86%] [G loss: 4.158456]\n",
            "6860 [D loss: 0.528594, acc.: 74.61%] [G loss: 3.442700]\n",
            "6880 [D loss: 0.486722, acc.: 77.34%] [G loss: 3.251799]\n",
            "6900 [D loss: 0.395182, acc.: 82.03%] [G loss: 3.973816]\n",
            "6920 [D loss: 0.388095, acc.: 85.16%] [G loss: 4.137388]\n",
            "6940 [D loss: 0.501357, acc.: 76.17%] [G loss: 5.177505]\n",
            "6960 [D loss: 0.455170, acc.: 78.52%] [G loss: 4.621369]\n",
            "6980 [D loss: 0.575739, acc.: 71.09%] [G loss: 2.724644]\n",
            "7000 [D loss: 0.776999, acc.: 64.06%] [G loss: 3.552659]\n",
            "7020 [D loss: 0.364272, acc.: 85.16%] [G loss: 4.295144]\n",
            "7040 [D loss: 0.731431, acc.: 56.64%] [G loss: 3.276895]\n",
            "7060 [D loss: 0.461449, acc.: 78.91%] [G loss: 4.220211]\n",
            "7080 [D loss: 0.392316, acc.: 85.55%] [G loss: 3.986393]\n",
            "7100 [D loss: 0.534391, acc.: 74.22%] [G loss: 3.547950]\n",
            "7120 [D loss: 0.492523, acc.: 77.73%] [G loss: 3.901191]\n",
            "7140 [D loss: 0.559690, acc.: 74.61%] [G loss: 4.109842]\n",
            "7160 [D loss: 0.421881, acc.: 83.20%] [G loss: 4.192199]\n",
            "7180 [D loss: 0.502210, acc.: 76.95%] [G loss: 3.887055]\n",
            "7200 [D loss: 0.463637, acc.: 78.91%] [G loss: 3.829731]\n",
            "7220 [D loss: 0.512437, acc.: 72.66%] [G loss: 3.574010]\n",
            "7240 [D loss: 0.414554, acc.: 80.86%] [G loss: 4.077851]\n",
            "7260 [D loss: 0.500397, acc.: 71.48%] [G loss: 3.453784]\n",
            "7280 [D loss: 0.419258, acc.: 80.47%] [G loss: 4.403645]\n",
            "7300 [D loss: 0.453864, acc.: 79.69%] [G loss: 4.146120]\n",
            "7320 [D loss: 0.441597, acc.: 77.73%] [G loss: 4.227305]\n",
            "7340 [D loss: 0.462149, acc.: 77.73%] [G loss: 3.693381]\n",
            "7360 [D loss: 0.513719, acc.: 73.44%] [G loss: 4.011900]\n",
            "7380 [D loss: 0.433975, acc.: 78.52%] [G loss: 4.149827]\n",
            "7400 [D loss: 0.498282, acc.: 75.78%] [G loss: 3.993846]\n",
            "7420 [D loss: 0.433506, acc.: 79.69%] [G loss: 3.938605]\n",
            "7440 [D loss: 0.450903, acc.: 77.73%] [G loss: 3.597438]\n",
            "7460 [D loss: 0.533482, acc.: 72.66%] [G loss: 3.623457]\n",
            "7480 [D loss: 0.423019, acc.: 81.64%] [G loss: 4.155793]\n",
            "7500 [D loss: 0.461867, acc.: 77.34%] [G loss: 3.754104]\n",
            "7520 [D loss: 0.457745, acc.: 80.47%] [G loss: 3.573125]\n",
            "7540 [D loss: 0.443625, acc.: 82.42%] [G loss: 3.329301]\n",
            "7560 [D loss: 0.489358, acc.: 76.95%] [G loss: 3.298204]\n",
            "7580 [D loss: 0.493116, acc.: 75.00%] [G loss: 3.842081]\n",
            "7600 [D loss: 0.498078, acc.: 76.56%] [G loss: 4.637199]\n",
            "7620 [D loss: 0.546334, acc.: 71.09%] [G loss: 3.979396]\n",
            "7640 [D loss: 0.570576, acc.: 72.27%] [G loss: 3.780999]\n",
            "7660 [D loss: 0.458725, acc.: 79.30%] [G loss: 3.933461]\n",
            "7680 [D loss: 0.538247, acc.: 69.14%] [G loss: 4.129254]\n",
            "7700 [D loss: 0.514020, acc.: 77.34%] [G loss: 3.854862]\n",
            "7720 [D loss: 0.524728, acc.: 71.48%] [G loss: 3.909982]\n",
            "7740 [D loss: 0.405787, acc.: 81.25%] [G loss: 4.210751]\n",
            "7760 [D loss: 0.398828, acc.: 84.77%] [G loss: 4.271526]\n",
            "7780 [D loss: 0.506767, acc.: 74.61%] [G loss: 4.185315]\n",
            "7800 [D loss: 0.451963, acc.: 83.20%] [G loss: 3.558832]\n",
            "7820 [D loss: 0.401638, acc.: 82.03%] [G loss: 4.460553]\n",
            "7840 [D loss: 0.468685, acc.: 77.73%] [G loss: 3.600395]\n",
            "7860 [D loss: 0.537464, acc.: 73.05%] [G loss: 4.276027]\n",
            "7880 [D loss: 0.487967, acc.: 78.91%] [G loss: 3.967825]\n",
            "7900 [D loss: 0.512192, acc.: 73.44%] [G loss: 3.488130]\n",
            "7920 [D loss: 0.520534, acc.: 72.27%] [G loss: 3.700341]\n",
            "7940 [D loss: 0.440603, acc.: 82.42%] [G loss: 4.244824]\n",
            "7960 [D loss: 0.422815, acc.: 82.81%] [G loss: 4.191713]\n",
            "7980 [D loss: 0.467465, acc.: 79.30%] [G loss: 4.117273]\n",
            "8000 [D loss: 0.566660, acc.: 72.66%] [G loss: 3.772017]\n",
            "8020 [D loss: 0.474482, acc.: 77.34%] [G loss: 3.675737]\n",
            "8040 [D loss: 0.471815, acc.: 75.78%] [G loss: 4.429235]\n",
            "8060 [D loss: 0.496431, acc.: 75.39%] [G loss: 3.938218]\n",
            "8080 [D loss: 0.453153, acc.: 76.95%] [G loss: 3.694494]\n",
            "8100 [D loss: 0.555329, acc.: 71.88%] [G loss: 3.577150]\n",
            "8120 [D loss: 0.569591, acc.: 70.70%] [G loss: 3.925045]\n",
            "8140 [D loss: 0.465498, acc.: 78.12%] [G loss: 3.965322]\n",
            "8160 [D loss: 0.524663, acc.: 73.44%] [G loss: 3.867432]\n",
            "8180 [D loss: 0.444718, acc.: 80.08%] [G loss: 4.429407]\n",
            "8200 [D loss: 0.486229, acc.: 74.22%] [G loss: 3.691734]\n",
            "8220 [D loss: 0.414270, acc.: 80.08%] [G loss: 4.032076]\n",
            "8240 [D loss: 0.470868, acc.: 77.34%] [G loss: 3.631079]\n",
            "8260 [D loss: 0.472835, acc.: 76.95%] [G loss: 3.932525]\n",
            "8280 [D loss: 0.341785, acc.: 86.72%] [G loss: 2.769193]\n",
            "8300 [D loss: 0.672963, acc.: 62.89%] [G loss: 4.683963]\n",
            "8320 [D loss: 72.122307, acc.: 50.39%] [G loss: 373.987091]\n",
            "8340 [D loss: 0.794202, acc.: 65.62%] [G loss: 4.997966]\n",
            "8360 [D loss: 0.602518, acc.: 73.83%] [G loss: 6.839397]\n",
            "8380 [D loss: 0.831233, acc.: 51.95%] [G loss: 3.314082]\n",
            "8400 [D loss: 0.809120, acc.: 50.39%] [G loss: 2.226515]\n",
            "8420 [D loss: 0.799642, acc.: 44.53%] [G loss: 2.192557]\n",
            "8440 [D loss: 0.613574, acc.: 64.84%] [G loss: 2.491436]\n",
            "8460 [D loss: 0.806444, acc.: 46.09%] [G loss: 1.949556]\n",
            "8480 [D loss: 0.590314, acc.: 67.19%] [G loss: 2.346189]\n",
            "8500 [D loss: 0.643185, acc.: 65.62%] [G loss: 2.279612]\n",
            "8520 [D loss: 0.618919, acc.: 68.36%] [G loss: 2.200202]\n",
            "8540 [D loss: 0.644080, acc.: 64.84%] [G loss: 2.172367]\n",
            "8560 [D loss: 0.596814, acc.: 70.70%] [G loss: 2.374727]\n",
            "8580 [D loss: 0.619861, acc.: 67.58%] [G loss: 2.305846]\n",
            "8600 [D loss: 0.568174, acc.: 69.92%] [G loss: 2.498259]\n",
            "8620 [D loss: 0.645827, acc.: 64.06%] [G loss: 2.314277]\n",
            "8640 [D loss: 0.620797, acc.: 65.62%] [G loss: 2.297992]\n",
            "8660 [D loss: 0.677354, acc.: 59.38%] [G loss: 2.113703]\n",
            "8680 [D loss: 0.641548, acc.: 64.84%] [G loss: 2.222541]\n",
            "8700 [D loss: 0.717945, acc.: 57.42%] [G loss: 2.094729]\n",
            "8720 [D loss: 0.612792, acc.: 68.75%] [G loss: 2.305540]\n",
            "8740 [D loss: 0.638628, acc.: 66.80%] [G loss: 2.225149]\n",
            "8760 [D loss: 0.616407, acc.: 64.84%] [G loss: 2.334047]\n",
            "8780 [D loss: 0.640217, acc.: 64.06%] [G loss: 1.964836]\n",
            "8800 [D loss: 0.692386, acc.: 56.25%] [G loss: 1.473563]\n",
            "8820 [D loss: 0.657882, acc.: 62.50%] [G loss: 1.985277]\n",
            "8840 [D loss: 0.730355, acc.: 55.86%] [G loss: 2.455966]\n",
            "8860 [D loss: 0.542990, acc.: 73.05%] [G loss: 2.512937]\n",
            "8880 [D loss: 0.778937, acc.: 54.30%] [G loss: 2.242557]\n",
            "8900 [D loss: 0.633906, acc.: 66.80%] [G loss: 2.328954]\n",
            "8920 [D loss: 0.536425, acc.: 73.83%] [G loss: 2.579773]\n",
            "8940 [D loss: 0.599353, acc.: 68.75%] [G loss: 2.533347]\n",
            "8960 [D loss: 0.550015, acc.: 73.05%] [G loss: 2.595593]\n",
            "8980 [D loss: 0.545450, acc.: 73.44%] [G loss: 2.717762]\n",
            "9000 [D loss: 0.518917, acc.: 75.00%] [G loss: 2.657612]\n",
            "9020 [D loss: 0.661931, acc.: 64.45%] [G loss: 2.588296]\n",
            "9040 [D loss: 0.543942, acc.: 75.00%] [G loss: 2.471290]\n",
            "9060 [D loss: 0.533876, acc.: 75.39%] [G loss: 2.431813]\n",
            "9080 [D loss: 0.625511, acc.: 68.36%] [G loss: 2.195432]\n",
            "9100 [D loss: 0.628235, acc.: 64.06%] [G loss: 2.691813]\n",
            "9120 [D loss: 0.575087, acc.: 67.97%] [G loss: 2.879287]\n",
            "9140 [D loss: 0.653073, acc.: 64.84%] [G loss: 2.491741]\n",
            "9160 [D loss: 0.495771, acc.: 75.00%] [G loss: 2.891720]\n",
            "9180 [D loss: 0.550894, acc.: 69.92%] [G loss: 2.640645]\n",
            "9200 [D loss: 0.498762, acc.: 76.56%] [G loss: 2.911379]\n",
            "9220 [D loss: 0.553786, acc.: 73.05%] [G loss: 2.635305]\n",
            "9240 [D loss: 0.590500, acc.: 69.53%] [G loss: 2.621256]\n",
            "9260 [D loss: 0.457589, acc.: 80.47%] [G loss: 2.957234]\n",
            "9280 [D loss: 0.608389, acc.: 67.58%] [G loss: 2.704990]\n",
            "9300 [D loss: 0.593292, acc.: 69.14%] [G loss: 2.716147]\n",
            "9320 [D loss: 0.572749, acc.: 67.19%] [G loss: 2.737156]\n",
            "9340 [D loss: 0.514661, acc.: 75.39%] [G loss: 2.922564]\n",
            "9360 [D loss: 0.519479, acc.: 73.44%] [G loss: 3.057450]\n",
            "9380 [D loss: 0.547001, acc.: 72.27%] [G loss: 2.731160]\n",
            "9400 [D loss: 0.427523, acc.: 84.38%] [G loss: 3.308687]\n",
            "9420 [D loss: 0.534572, acc.: 74.61%] [G loss: 2.999674]\n",
            "9440 [D loss: 0.574809, acc.: 69.14%] [G loss: 2.994148]\n",
            "9460 [D loss: 0.546179, acc.: 72.66%] [G loss: 2.836938]\n",
            "9480 [D loss: 0.505812, acc.: 76.95%] [G loss: 3.194911]\n",
            "9500 [D loss: 0.597237, acc.: 71.48%] [G loss: 2.887781]\n",
            "9520 [D loss: 0.457736, acc.: 79.30%] [G loss: 3.062497]\n",
            "9540 [D loss: 0.574186, acc.: 70.31%] [G loss: 2.891257]\n",
            "9560 [D loss: 0.490848, acc.: 73.83%] [G loss: 3.182863]\n",
            "9580 [D loss: 0.488965, acc.: 78.12%] [G loss: 3.123629]\n",
            "9600 [D loss: 0.501019, acc.: 75.00%] [G loss: 3.165445]\n",
            "9620 [D loss: 0.486468, acc.: 78.12%] [G loss: 3.167414]\n",
            "9640 [D loss: 0.482623, acc.: 80.47%] [G loss: 3.086595]\n",
            "9660 [D loss: 0.416005, acc.: 82.42%] [G loss: 3.620413]\n",
            "9680 [D loss: 0.549038, acc.: 75.00%] [G loss: 3.034129]\n",
            "9700 [D loss: 0.500679, acc.: 75.00%] [G loss: 3.180938]\n",
            "9720 [D loss: 0.492620, acc.: 75.78%] [G loss: 3.285434]\n",
            "9740 [D loss: 0.482591, acc.: 78.52%] [G loss: 3.282046]\n",
            "9760 [D loss: 0.530563, acc.: 71.88%] [G loss: 3.141133]\n",
            "9780 [D loss: 0.503451, acc.: 78.91%] [G loss: 3.423969]\n",
            "9800 [D loss: 0.446246, acc.: 82.81%] [G loss: 3.384996]\n",
            "9820 [D loss: 0.526119, acc.: 75.78%] [G loss: 3.001754]\n",
            "9840 [D loss: 0.508658, acc.: 75.78%] [G loss: 3.017762]\n",
            "9860 [D loss: 0.478166, acc.: 79.30%] [G loss: 3.481020]\n",
            "9880 [D loss: 0.484129, acc.: 76.95%] [G loss: 3.172013]\n",
            "9900 [D loss: 0.538721, acc.: 73.83%] [G loss: 2.699309]\n",
            "9920 [D loss: 0.424591, acc.: 79.69%] [G loss: 3.497353]\n",
            "9940 [D loss: 0.558238, acc.: 68.36%] [G loss: 2.185728]\n",
            "9960 [D loss: 0.561702, acc.: 70.31%] [G loss: 3.182554]\n",
            "9980 [D loss: 0.458299, acc.: 80.08%] [G loss: 2.803051]\n",
            "10000 [D loss: 0.527606, acc.: 72.66%] [G loss: 3.088439]\n",
            "10020 [D loss: 0.904937, acc.: 49.22%] [G loss: 4.772345]\n",
            "10040 [D loss: 0.575409, acc.: 73.44%] [G loss: 3.791032]\n",
            "10060 [D loss: 0.434919, acc.: 78.91%] [G loss: 3.893264]\n",
            "10080 [D loss: 0.523624, acc.: 78.52%] [G loss: 3.196717]\n",
            "10100 [D loss: 0.549960, acc.: 72.27%] [G loss: 3.032032]\n",
            "10120 [D loss: 0.450630, acc.: 80.86%] [G loss: 3.473791]\n",
            "10140 [D loss: 0.640385, acc.: 61.72%] [G loss: 3.051750]\n",
            "10160 [D loss: 0.577206, acc.: 68.75%] [G loss: 3.536800]\n",
            "10180 [D loss: 0.445652, acc.: 81.25%] [G loss: 3.573038]\n",
            "10200 [D loss: 0.542200, acc.: 71.48%] [G loss: 3.384488]\n",
            "10220 [D loss: 0.426150, acc.: 82.42%] [G loss: 4.316420]\n",
            "10240 [D loss: 0.438587, acc.: 80.47%] [G loss: 3.724081]\n",
            "10260 [D loss: 0.627343, acc.: 67.19%] [G loss: 3.121490]\n",
            "10280 [D loss: 0.520940, acc.: 74.61%] [G loss: 3.078668]\n",
            "10300 [D loss: 0.503698, acc.: 76.95%] [G loss: 3.055548]\n",
            "10320 [D loss: 0.443204, acc.: 80.47%] [G loss: 3.567256]\n",
            "10340 [D loss: 0.504916, acc.: 74.61%] [G loss: 3.449942]\n",
            "10360 [D loss: 0.594007, acc.: 66.41%] [G loss: 2.838433]\n",
            "10380 [D loss: 0.554345, acc.: 71.48%] [G loss: 3.055371]\n",
            "10400 [D loss: 0.435234, acc.: 80.08%] [G loss: 3.715314]\n",
            "10420 [D loss: 0.455010, acc.: 77.73%] [G loss: 3.842149]\n",
            "10440 [D loss: 0.527783, acc.: 73.05%] [G loss: 3.432030]\n",
            "10460 [D loss: 0.458760, acc.: 79.30%] [G loss: 3.581835]\n",
            "10480 [D loss: 0.526480, acc.: 75.78%] [G loss: 3.506851]\n",
            "10500 [D loss: 0.575430, acc.: 73.05%] [G loss: 3.100848]\n",
            "10520 [D loss: 0.467616, acc.: 78.91%] [G loss: 3.475038]\n",
            "10540 [D loss: 0.483266, acc.: 78.91%] [G loss: 3.360613]\n",
            "10560 [D loss: 0.501377, acc.: 76.56%] [G loss: 3.309927]\n",
            "10580 [D loss: 0.395846, acc.: 83.59%] [G loss: 3.564491]\n",
            "10600 [D loss: 0.597365, acc.: 71.09%] [G loss: 3.424257]\n",
            "10620 [D loss: 0.386338, acc.: 83.98%] [G loss: 4.123731]\n",
            "10640 [D loss: 0.514252, acc.: 75.39%] [G loss: 3.316526]\n",
            "10660 [D loss: 0.498048, acc.: 77.34%] [G loss: 3.552812]\n",
            "10680 [D loss: 0.540324, acc.: 73.44%] [G loss: 3.536655]\n",
            "10700 [D loss: 0.449889, acc.: 80.47%] [G loss: 3.801307]\n",
            "10720 [D loss: 0.500939, acc.: 73.83%] [G loss: 3.746833]\n",
            "10740 [D loss: 0.425969, acc.: 82.03%] [G loss: 3.682046]\n",
            "10760 [D loss: 0.500080, acc.: 74.61%] [G loss: 3.637811]\n",
            "10780 [D loss: 0.446000, acc.: 80.08%] [G loss: 3.887088]\n",
            "10800 [D loss: 0.435349, acc.: 78.91%] [G loss: 3.424177]\n",
            "10820 [D loss: 0.470281, acc.: 77.73%] [G loss: 3.892372]\n",
            "10840 [D loss: 0.422143, acc.: 84.77%] [G loss: 3.639438]\n",
            "10860 [D loss: 0.442995, acc.: 82.03%] [G loss: 3.729689]\n",
            "10880 [D loss: 0.488632, acc.: 75.39%] [G loss: 3.933532]\n",
            "10900 [D loss: 0.439753, acc.: 79.69%] [G loss: 3.949774]\n",
            "10920 [D loss: 0.393937, acc.: 82.42%] [G loss: 3.904796]\n",
            "10940 [D loss: 0.537655, acc.: 76.95%] [G loss: 3.602857]\n",
            "10960 [D loss: 0.394352, acc.: 84.38%] [G loss: 4.258386]\n",
            "10980 [D loss: 0.524026, acc.: 74.61%] [G loss: 3.497081]\n",
            "11000 [D loss: 0.411573, acc.: 80.47%] [G loss: 3.780954]\n",
            "11020 [D loss: 0.519719, acc.: 73.44%] [G loss: 3.665623]\n",
            "11040 [D loss: 0.448806, acc.: 82.42%] [G loss: 3.710539]\n",
            "11060 [D loss: 0.606482, acc.: 67.58%] [G loss: 3.790088]\n",
            "11080 [D loss: 0.385137, acc.: 84.77%] [G loss: 3.880587]\n",
            "11100 [D loss: 0.467045, acc.: 76.56%] [G loss: 4.070587]\n",
            "11120 [D loss: 0.512670, acc.: 76.17%] [G loss: 3.900054]\n",
            "11140 [D loss: 0.512775, acc.: 74.22%] [G loss: 3.538016]\n",
            "11160 [D loss: 0.491065, acc.: 75.39%] [G loss: 4.699265]\n",
            "11180 [D loss: 0.493546, acc.: 74.61%] [G loss: 3.629382]\n",
            "11200 [D loss: 0.416141, acc.: 83.98%] [G loss: 4.038415]\n",
            "11220 [D loss: 0.571074, acc.: 69.14%] [G loss: 3.249165]\n",
            "11240 [D loss: 0.365633, acc.: 84.77%] [G loss: 4.117696]\n",
            "11260 [D loss: 0.470293, acc.: 76.17%] [G loss: 4.121777]\n",
            "11280 [D loss: 0.445639, acc.: 80.08%] [G loss: 3.984321]\n",
            "11300 [D loss: 0.499541, acc.: 75.78%] [G loss: 3.662227]\n",
            "11320 [D loss: 0.538895, acc.: 71.48%] [G loss: 3.943852]\n",
            "11340 [D loss: 0.387305, acc.: 82.42%] [G loss: 4.296959]\n",
            "11360 [D loss: 0.447077, acc.: 78.52%] [G loss: 3.519859]\n",
            "11380 [D loss: 0.528253, acc.: 75.00%] [G loss: 3.693024]\n",
            "11400 [D loss: 0.408892, acc.: 80.86%] [G loss: 4.520503]\n",
            "11420 [D loss: 0.413298, acc.: 83.59%] [G loss: 4.233447]\n",
            "11440 [D loss: 0.498160, acc.: 76.17%] [G loss: 3.764095]\n",
            "11460 [D loss: 0.457299, acc.: 78.52%] [G loss: 4.047290]\n",
            "11480 [D loss: 0.436005, acc.: 81.25%] [G loss: 4.333815]\n",
            "11500 [D loss: 0.441943, acc.: 82.42%] [G loss: 4.170405]\n",
            "11520 [D loss: 0.425698, acc.: 83.20%] [G loss: 3.989794]\n",
            "11540 [D loss: 0.432133, acc.: 78.52%] [G loss: 4.067553]\n",
            "11560 [D loss: 0.468011, acc.: 77.73%] [G loss: 4.185304]\n",
            "11580 [D loss: 0.462955, acc.: 78.52%] [G loss: 3.810024]\n",
            "11600 [D loss: 0.437901, acc.: 76.95%] [G loss: 4.032003]\n",
            "11620 [D loss: 0.439361, acc.: 82.42%] [G loss: 4.235739]\n",
            "11640 [D loss: 0.373947, acc.: 85.94%] [G loss: 4.093066]\n",
            "11660 [D loss: 0.482495, acc.: 75.78%] [G loss: 3.789939]\n",
            "11680 [D loss: 0.412311, acc.: 81.64%] [G loss: 3.777103]\n",
            "11700 [D loss: 0.505452, acc.: 74.61%] [G loss: 3.617141]\n",
            "11720 [D loss: 0.505694, acc.: 76.56%] [G loss: 3.677025]\n",
            "11740 [D loss: 0.453278, acc.: 80.86%] [G loss: 3.768736]\n",
            "11760 [D loss: 0.504726, acc.: 77.34%] [G loss: 4.612066]\n",
            "11780 [D loss: 0.577701, acc.: 69.53%] [G loss: 4.533728]\n",
            "11800 [D loss: 0.540733, acc.: 73.05%] [G loss: 3.936740]\n",
            "11820 [D loss: 0.428634, acc.: 83.59%] [G loss: 3.992885]\n",
            "11840 [D loss: 0.304274, acc.: 89.45%] [G loss: 2.557489]\n",
            "11860 [D loss: 0.580850, acc.: 71.88%] [G loss: 2.456997]\n",
            "11880 [D loss: 1.152386, acc.: 50.39%] [G loss: 5.488003]\n",
            "11900 [D loss: 0.806350, acc.: 59.77%] [G loss: 4.025605]\n",
            "11920 [D loss: 0.437473, acc.: 82.03%] [G loss: 4.298112]\n",
            "11940 [D loss: 0.391915, acc.: 82.42%] [G loss: 4.460935]\n",
            "11960 [D loss: 0.438467, acc.: 78.52%] [G loss: 4.417439]\n",
            "11980 [D loss: 0.489068, acc.: 76.56%] [G loss: 3.929702]\n",
            "12000 [D loss: 0.457627, acc.: 80.08%] [G loss: 4.122962]\n",
            "12020 [D loss: 0.435022, acc.: 80.86%] [G loss: 4.187171]\n",
            "12040 [D loss: 0.377410, acc.: 83.98%] [G loss: 4.692807]\n",
            "12060 [D loss: 0.424887, acc.: 80.47%] [G loss: 4.750044]\n",
            "12080 [D loss: 0.422927, acc.: 82.42%] [G loss: 4.250822]\n",
            "12100 [D loss: 0.404593, acc.: 81.25%] [G loss: 4.583338]\n",
            "12120 [D loss: 0.432212, acc.: 80.08%] [G loss: 4.330485]\n",
            "12140 [D loss: 0.434554, acc.: 79.30%] [G loss: 4.562164]\n",
            "12160 [D loss: 0.482020, acc.: 77.34%] [G loss: 3.794831]\n",
            "12180 [D loss: 0.431183, acc.: 81.25%] [G loss: 4.682524]\n",
            "12200 [D loss: 0.426538, acc.: 79.30%] [G loss: 4.297180]\n",
            "12220 [D loss: 0.490858, acc.: 75.78%] [G loss: 4.518388]\n",
            "12240 [D loss: 0.461950, acc.: 81.25%] [G loss: 4.336947]\n",
            "12260 [D loss: 0.452377, acc.: 78.52%] [G loss: 4.605940]\n",
            "12280 [D loss: 0.474562, acc.: 80.47%] [G loss: 4.195315]\n",
            "12300 [D loss: 0.389624, acc.: 82.42%] [G loss: 4.798237]\n",
            "12320 [D loss: 0.446291, acc.: 80.08%] [G loss: 4.790896]\n",
            "12340 [D loss: 0.493798, acc.: 78.12%] [G loss: 3.855075]\n",
            "12360 [D loss: 0.467718, acc.: 77.73%] [G loss: 3.907128]\n",
            "12380 [D loss: 0.484419, acc.: 75.00%] [G loss: 4.121822]\n",
            "12400 [D loss: 0.368664, acc.: 83.59%] [G loss: 4.480712]\n",
            "12420 [D loss: 0.430511, acc.: 79.69%] [G loss: 4.505017]\n",
            "12440 [D loss: 0.502439, acc.: 77.34%] [G loss: 4.445452]\n",
            "12460 [D loss: 0.451202, acc.: 78.52%] [G loss: 4.232288]\n",
            "12480 [D loss: 0.393344, acc.: 83.59%] [G loss: 4.225490]\n",
            "12500 [D loss: 0.396902, acc.: 85.16%] [G loss: 4.692410]\n",
            "12520 [D loss: 0.432207, acc.: 78.12%] [G loss: 4.461965]\n",
            "12540 [D loss: 0.404918, acc.: 82.42%] [G loss: 4.316086]\n",
            "12560 [D loss: 0.497038, acc.: 75.00%] [G loss: 4.636095]\n",
            "12580 [D loss: 0.412490, acc.: 80.47%] [G loss: 4.699271]\n",
            "12600 [D loss: 0.460372, acc.: 77.34%] [G loss: 4.119112]\n",
            "12620 [D loss: 0.418877, acc.: 82.42%] [G loss: 4.243959]\n",
            "12640 [D loss: 0.447622, acc.: 80.08%] [G loss: 4.474877]\n",
            "12660 [D loss: 0.423139, acc.: 80.47%] [G loss: 4.654431]\n",
            "12680 [D loss: 0.376025, acc.: 84.38%] [G loss: 4.449094]\n",
            "12700 [D loss: 0.535777, acc.: 73.83%] [G loss: 4.506266]\n",
            "12720 [D loss: 0.442858, acc.: 76.95%] [G loss: 4.459638]\n",
            "12740 [D loss: 0.416997, acc.: 82.42%] [G loss: 5.060258]\n",
            "12760 [D loss: 0.393506, acc.: 80.08%] [G loss: 4.045355]\n",
            "12780 [D loss: 0.443793, acc.: 78.12%] [G loss: 4.073487]\n",
            "12800 [D loss: 0.328183, acc.: 85.94%] [G loss: 4.138739]\n",
            "12820 [D loss: 0.435829, acc.: 81.25%] [G loss: 3.101720]\n",
            "12840 [D loss: 10.264312, acc.: 47.66%] [G loss: 22.319000]\n",
            "12860 [D loss: 1.262783, acc.: 31.25%] [G loss: 1.520308]\n",
            "12880 [D loss: 1.037254, acc.: 36.72%] [G loss: 1.806651]\n",
            "12900 [D loss: 0.989001, acc.: 31.25%] [G loss: 1.347418]\n",
            "12920 [D loss: 0.856767, acc.: 44.14%] [G loss: 1.946435]\n",
            "12940 [D loss: 0.693056, acc.: 57.42%] [G loss: 3.000854]\n",
            "12960 [D loss: 0.637451, acc.: 66.41%] [G loss: 3.058228]\n",
            "12980 [D loss: 0.602230, acc.: 67.58%] [G loss: 3.073028]\n",
            "13000 [D loss: 0.534464, acc.: 73.44%] [G loss: 2.856998]\n",
            "13020 [D loss: 0.574847, acc.: 71.48%] [G loss: 3.010094]\n",
            "13040 [D loss: 0.529710, acc.: 72.66%] [G loss: 2.824281]\n",
            "13060 [D loss: 0.570502, acc.: 73.05%] [G loss: 3.519435]\n",
            "13080 [D loss: 0.478163, acc.: 77.73%] [G loss: 3.034051]\n",
            "13100 [D loss: 0.588792, acc.: 66.02%] [G loss: 3.268360]\n",
            "13120 [D loss: 0.508862, acc.: 75.78%] [G loss: 3.248541]\n",
            "13140 [D loss: 0.488383, acc.: 77.34%] [G loss: 3.268679]\n",
            "13160 [D loss: 0.476150, acc.: 78.91%] [G loss: 3.577334]\n",
            "13180 [D loss: 0.427486, acc.: 83.98%] [G loss: 3.799532]\n",
            "13200 [D loss: 0.500814, acc.: 75.00%] [G loss: 3.414482]\n",
            "13220 [D loss: 0.468680, acc.: 77.73%] [G loss: 3.541378]\n",
            "13240 [D loss: 0.478767, acc.: 75.78%] [G loss: 3.510837]\n",
            "13260 [D loss: 0.452381, acc.: 78.52%] [G loss: 3.903347]\n",
            "13280 [D loss: 0.417862, acc.: 81.64%] [G loss: 4.036277]\n",
            "13300 [D loss: 0.465578, acc.: 79.69%] [G loss: 3.910944]\n",
            "13320 [D loss: 0.413661, acc.: 80.47%] [G loss: 3.770388]\n",
            "13340 [D loss: 0.505034, acc.: 76.17%] [G loss: 3.764079]\n",
            "13360 [D loss: 0.464155, acc.: 75.78%] [G loss: 3.752258]\n",
            "13380 [D loss: 0.482165, acc.: 75.39%] [G loss: 3.915007]\n",
            "13400 [D loss: 0.456136, acc.: 77.34%] [G loss: 3.467968]\n",
            "13420 [D loss: 0.469799, acc.: 80.47%] [G loss: 3.807907]\n",
            "13440 [D loss: 0.466437, acc.: 79.69%] [G loss: 3.827528]\n",
            "13460 [D loss: 0.508204, acc.: 72.66%] [G loss: 3.950356]\n",
            "13480 [D loss: 0.396062, acc.: 82.81%] [G loss: 4.232464]\n",
            "13500 [D loss: 0.504670, acc.: 72.27%] [G loss: 3.793425]\n",
            "13520 [D loss: 0.511214, acc.: 74.22%] [G loss: 3.548928]\n",
            "13540 [D loss: 0.503650, acc.: 72.66%] [G loss: 3.665053]\n",
            "13560 [D loss: 0.445843, acc.: 79.30%] [G loss: 3.983719]\n",
            "13580 [D loss: 0.374127, acc.: 84.77%] [G loss: 4.075280]\n",
            "13600 [D loss: 0.462068, acc.: 77.73%] [G loss: 3.513188]\n",
            "13620 [D loss: 0.566114, acc.: 69.14%] [G loss: 3.541970]\n",
            "13640 [D loss: 0.440158, acc.: 77.73%] [G loss: 4.130300]\n",
            "13660 [D loss: 0.471887, acc.: 76.17%] [G loss: 4.149559]\n",
            "13680 [D loss: 0.473451, acc.: 75.78%] [G loss: 3.807356]\n",
            "13700 [D loss: 0.449295, acc.: 78.91%] [G loss: 4.332154]\n",
            "13720 [D loss: 0.533457, acc.: 72.27%] [G loss: 3.862200]\n",
            "13740 [D loss: 0.517517, acc.: 75.78%] [G loss: 4.270423]\n",
            "13760 [D loss: 0.575485, acc.: 70.31%] [G loss: 3.891304]\n",
            "13780 [D loss: 0.439222, acc.: 83.20%] [G loss: 3.764770]\n",
            "13800 [D loss: 0.424440, acc.: 78.12%] [G loss: 4.354916]\n",
            "13820 [D loss: 0.425363, acc.: 80.47%] [G loss: 3.847892]\n",
            "13840 [D loss: 0.431784, acc.: 79.69%] [G loss: 3.875170]\n",
            "13860 [D loss: 0.425849, acc.: 80.86%] [G loss: 4.296571]\n",
            "13880 [D loss: 0.463216, acc.: 77.73%] [G loss: 4.541390]\n",
            "13900 [D loss: 0.445557, acc.: 79.69%] [G loss: 4.221807]\n",
            "13920 [D loss: 0.434964, acc.: 81.64%] [G loss: 4.100104]\n",
            "13940 [D loss: 0.405571, acc.: 81.64%] [G loss: 4.514895]\n",
            "13960 [D loss: 0.416799, acc.: 82.81%] [G loss: 4.372655]\n",
            "13980 [D loss: 0.455277, acc.: 78.91%] [G loss: 4.024486]\n",
            "14000 [D loss: 0.412139, acc.: 82.03%] [G loss: 4.561580]\n",
            "14020 [D loss: 0.496761, acc.: 74.22%] [G loss: 4.213759]\n",
            "14040 [D loss: 0.421769, acc.: 83.98%] [G loss: 4.229169]\n",
            "14060 [D loss: 0.448840, acc.: 78.91%] [G loss: 4.246661]\n",
            "14080 [D loss: 0.421247, acc.: 81.25%] [G loss: 4.499720]\n",
            "14100 [D loss: 0.467858, acc.: 76.56%] [G loss: 4.202550]\n",
            "14120 [D loss: 0.439565, acc.: 78.91%] [G loss: 4.415467]\n",
            "14140 [D loss: 0.446314, acc.: 81.64%] [G loss: 4.647425]\n",
            "14160 [D loss: 0.386395, acc.: 81.64%] [G loss: 4.429204]\n",
            "14180 [D loss: 0.445401, acc.: 80.86%] [G loss: 4.484643]\n",
            "14200 [D loss: 0.412988, acc.: 81.25%] [G loss: 4.073558]\n",
            "14220 [D loss: 0.484466, acc.: 76.17%] [G loss: 4.170240]\n",
            "14240 [D loss: 0.168308, acc.: 96.88%] [G loss: 2.948014]\n",
            "14260 [D loss: 0.299979, acc.: 88.67%] [G loss: 4.638995]\n",
            "14280 [D loss: 0.163927, acc.: 97.66%] [G loss: 4.588977]\n",
            "14300 [D loss: 0.623553, acc.: 67.58%] [G loss: 4.122127]\n",
            "14320 [D loss: 0.530975, acc.: 73.83%] [G loss: 3.814707]\n",
            "14340 [D loss: 0.622219, acc.: 65.62%] [G loss: 2.947031]\n",
            "14360 [D loss: 0.532918, acc.: 73.83%] [G loss: 3.182976]\n",
            "14380 [D loss: 0.456624, acc.: 80.47%] [G loss: 4.761460]\n",
            "14400 [D loss: 0.437850, acc.: 76.56%] [G loss: 4.608951]\n",
            "14420 [D loss: 0.344282, acc.: 85.94%] [G loss: 5.214165]\n",
            "14440 [D loss: 0.358637, acc.: 87.11%] [G loss: 4.557674]\n",
            "14460 [D loss: 0.506259, acc.: 76.56%] [G loss: 4.197336]\n",
            "14480 [D loss: 0.503735, acc.: 76.95%] [G loss: 4.533824]\n",
            "14500 [D loss: 0.455130, acc.: 76.95%] [G loss: 4.252011]\n",
            "14520 [D loss: 0.441757, acc.: 80.08%] [G loss: 4.384326]\n",
            "14540 [D loss: 0.502747, acc.: 75.00%] [G loss: 4.284564]\n",
            "14560 [D loss: 0.320901, acc.: 89.06%] [G loss: 4.967340]\n",
            "14580 [D loss: 0.499286, acc.: 76.17%] [G loss: 4.350848]\n",
            "14600 [D loss: 0.333724, acc.: 86.33%] [G loss: 4.714267]\n",
            "14620 [D loss: 0.483025, acc.: 75.78%] [G loss: 4.975262]\n",
            "14640 [D loss: 0.386078, acc.: 82.42%] [G loss: 5.087942]\n",
            "14660 [D loss: 0.463300, acc.: 74.61%] [G loss: 4.224627]\n",
            "14680 [D loss: 0.425283, acc.: 81.64%] [G loss: 4.536847]\n",
            "14700 [D loss: 0.428862, acc.: 78.91%] [G loss: 4.862581]\n",
            "14720 [D loss: 0.357433, acc.: 85.55%] [G loss: 4.769269]\n",
            "14740 [D loss: 0.414593, acc.: 82.81%] [G loss: 4.288194]\n",
            "14760 [D loss: 0.428621, acc.: 81.25%] [G loss: 4.708740]\n",
            "14780 [D loss: 0.447697, acc.: 77.34%] [G loss: 4.066862]\n",
            "14800 [D loss: 0.415725, acc.: 80.86%] [G loss: 4.435352]\n",
            "14820 [D loss: 0.422259, acc.: 81.25%] [G loss: 4.938560]\n",
            "14840 [D loss: 0.587412, acc.: 71.88%] [G loss: 3.856583]\n",
            "14860 [D loss: 0.369252, acc.: 84.38%] [G loss: 4.773142]\n",
            "14880 [D loss: 0.376669, acc.: 85.16%] [G loss: 4.817186]\n",
            "14900 [D loss: 0.457475, acc.: 79.30%] [G loss: 4.949731]\n",
            "14920 [D loss: 0.399982, acc.: 81.64%] [G loss: 5.095596]\n",
            "14940 [D loss: 0.431449, acc.: 79.69%] [G loss: 4.892741]\n",
            "14960 [D loss: 0.481150, acc.: 78.52%] [G loss: 4.358444]\n",
            "14980 [D loss: 0.432313, acc.: 83.20%] [G loss: 4.485386]\n",
            "15000 [D loss: 0.388431, acc.: 81.64%] [G loss: 4.844778]\n",
            "15020 [D loss: 0.389804, acc.: 84.38%] [G loss: 5.270637]\n",
            "15040 [D loss: 0.345452, acc.: 86.33%] [G loss: 4.781682]\n",
            "15060 [D loss: 0.524956, acc.: 74.22%] [G loss: 4.236223]\n",
            "15080 [D loss: 0.452788, acc.: 82.81%] [G loss: 4.680569]\n",
            "15100 [D loss: 0.355840, acc.: 85.94%] [G loss: 5.062317]\n",
            "15120 [D loss: 0.374681, acc.: 83.59%] [G loss: 4.652081]\n",
            "15140 [D loss: 0.330382, acc.: 85.55%] [G loss: 5.112494]\n",
            "15160 [D loss: 0.408163, acc.: 81.25%] [G loss: 5.116651]\n",
            "15180 [D loss: 0.384524, acc.: 81.64%] [G loss: 5.351439]\n",
            "15200 [D loss: 0.369046, acc.: 81.64%] [G loss: 4.861406]\n",
            "15220 [D loss: 0.458264, acc.: 78.52%] [G loss: 5.287457]\n",
            "15240 [D loss: 0.343074, acc.: 83.59%] [G loss: 4.881019]\n",
            "15260 [D loss: 0.386525, acc.: 82.81%] [G loss: 5.122472]\n",
            "15280 [D loss: 0.407499, acc.: 82.03%] [G loss: 4.204903]\n",
            "15300 [D loss: 0.429789, acc.: 79.69%] [G loss: 4.550998]\n",
            "15320 [D loss: 0.249610, acc.: 91.41%] [G loss: 3.185671]\n",
            "15340 [D loss: 1.389140, acc.: 35.16%] [G loss: 5.657907]\n",
            "15360 [D loss: 0.401227, acc.: 81.64%] [G loss: 5.697423]\n",
            "15380 [D loss: 0.478136, acc.: 76.56%] [G loss: 5.659923]\n",
            "15400 [D loss: 0.626557, acc.: 67.19%] [G loss: 4.747446]\n",
            "15420 [D loss: 0.491902, acc.: 78.52%] [G loss: 4.899268]\n",
            "15440 [D loss: 0.480774, acc.: 78.12%] [G loss: 5.077878]\n",
            "15460 [D loss: 0.400881, acc.: 82.03%] [G loss: 5.121024]\n",
            "15480 [D loss: 0.365936, acc.: 85.94%] [G loss: 5.499012]\n",
            "15500 [D loss: 0.507754, acc.: 75.78%] [G loss: 4.707586]\n",
            "15520 [D loss: 0.372847, acc.: 83.59%] [G loss: 4.942164]\n",
            "15540 [D loss: 0.398325, acc.: 82.81%] [G loss: 5.173062]\n",
            "15560 [D loss: 0.356605, acc.: 83.98%] [G loss: 5.363036]\n",
            "15580 [D loss: 0.372826, acc.: 83.59%] [G loss: 5.311263]\n",
            "15600 [D loss: 0.415373, acc.: 81.64%] [G loss: 4.968049]\n",
            "15620 [D loss: 0.384195, acc.: 83.59%] [G loss: 5.006286]\n",
            "15640 [D loss: 0.320715, acc.: 87.11%] [G loss: 4.896445]\n",
            "15660 [D loss: 0.404512, acc.: 82.42%] [G loss: 5.149398]\n",
            "15680 [D loss: 0.368933, acc.: 84.77%] [G loss: 5.569808]\n",
            "15700 [D loss: 0.497163, acc.: 73.44%] [G loss: 5.260941]\n",
            "15720 [D loss: 0.380025, acc.: 82.81%] [G loss: 5.800910]\n",
            "15740 [D loss: 0.453489, acc.: 79.30%] [G loss: 4.780568]\n",
            "15760 [D loss: 0.344889, acc.: 85.16%] [G loss: 5.648249]\n",
            "15780 [D loss: 0.428950, acc.: 79.69%] [G loss: 5.076117]\n",
            "15800 [D loss: 0.423479, acc.: 80.47%] [G loss: 5.736635]\n",
            "15820 [D loss: 0.377173, acc.: 82.81%] [G loss: 4.659647]\n",
            "15840 [D loss: 0.363508, acc.: 85.94%] [G loss: 5.442510]\n",
            "15860 [D loss: 0.312887, acc.: 85.94%] [G loss: 5.192987]\n",
            "15880 [D loss: 0.424028, acc.: 80.86%] [G loss: 4.906164]\n",
            "15900 [D loss: 0.394545, acc.: 82.81%] [G loss: 5.556522]\n",
            "15920 [D loss: 0.318082, acc.: 86.72%] [G loss: 6.259025]\n",
            "15940 [D loss: 0.342154, acc.: 87.11%] [G loss: 5.349727]\n",
            "15960 [D loss: 0.401833, acc.: 82.42%] [G loss: 4.800699]\n",
            "15980 [D loss: 0.340436, acc.: 85.16%] [G loss: 5.456253]\n",
            "16000 [D loss: 0.357872, acc.: 85.16%] [G loss: 5.351392]\n",
            "16020 [D loss: 0.417920, acc.: 81.25%] [G loss: 5.045147]\n",
            "16040 [D loss: 0.391986, acc.: 82.03%] [G loss: 6.331852]\n",
            "16060 [D loss: 0.334913, acc.: 85.55%] [G loss: 5.525968]\n",
            "16080 [D loss: 0.450610, acc.: 77.73%] [G loss: 5.037412]\n",
            "16100 [D loss: 0.364298, acc.: 83.98%] [G loss: 6.177034]\n",
            "16120 [D loss: 0.310719, acc.: 89.45%] [G loss: 5.807567]\n",
            "16140 [D loss: 0.359695, acc.: 83.98%] [G loss: 5.291653]\n",
            "16160 [D loss: 0.447151, acc.: 80.08%] [G loss: 5.012240]\n",
            "16180 [D loss: 0.329657, acc.: 85.16%] [G loss: 5.150499]\n",
            "16200 [D loss: 0.453048, acc.: 80.08%] [G loss: 5.547553]\n",
            "16220 [D loss: 0.342814, acc.: 84.77%] [G loss: 6.258939]\n",
            "16240 [D loss: 0.345565, acc.: 85.94%] [G loss: 5.764729]\n",
            "16260 [D loss: 0.337350, acc.: 85.16%] [G loss: 5.424223]\n",
            "16280 [D loss: 0.332370, acc.: 85.94%] [G loss: 5.234813]\n",
            "16300 [D loss: 0.320994, acc.: 87.11%] [G loss: 3.715562]\n",
            "16320 [D loss: 0.440200, acc.: 78.52%] [G loss: 3.902296]\n",
            "16340 [D loss: 0.743514, acc.: 48.83%] [G loss: 2.986101]\n",
            "16360 [D loss: 1.002304, acc.: 47.27%] [G loss: 2.501052]\n",
            "16380 [D loss: 0.675705, acc.: 65.23%] [G loss: 3.977003]\n",
            "16400 [D loss: 0.497885, acc.: 78.12%] [G loss: 4.900868]\n",
            "16420 [D loss: 0.405893, acc.: 80.47%] [G loss: 4.621168]\n",
            "16440 [D loss: 0.487513, acc.: 77.34%] [G loss: 4.340921]\n",
            "16460 [D loss: 0.330208, acc.: 87.11%] [G loss: 5.247740]\n",
            "16480 [D loss: 0.413476, acc.: 80.86%] [G loss: 5.013721]\n",
            "16500 [D loss: 0.387657, acc.: 82.03%] [G loss: 4.953203]\n",
            "16520 [D loss: 0.508584, acc.: 76.17%] [G loss: 4.633538]\n",
            "16540 [D loss: 0.336511, acc.: 86.72%] [G loss: 5.041705]\n",
            "16560 [D loss: 0.455617, acc.: 79.30%] [G loss: 4.791621]\n",
            "16580 [D loss: 0.280501, acc.: 89.84%] [G loss: 5.175206]\n",
            "16600 [D loss: 0.486832, acc.: 76.95%] [G loss: 5.013504]\n",
            "16620 [D loss: 0.373238, acc.: 82.81%] [G loss: 5.254778]\n",
            "16640 [D loss: 0.440874, acc.: 80.47%] [G loss: 4.807408]\n",
            "16660 [D loss: 0.461062, acc.: 77.73%] [G loss: 4.783795]\n",
            "16680 [D loss: 0.409431, acc.: 81.64%] [G loss: 4.966967]\n",
            "16700 [D loss: 0.431414, acc.: 77.34%] [G loss: 5.241235]\n",
            "16720 [D loss: 0.359706, acc.: 84.38%] [G loss: 4.974181]\n",
            "16740 [D loss: 0.407487, acc.: 81.64%] [G loss: 5.166719]\n",
            "16760 [D loss: 0.430524, acc.: 79.30%] [G loss: 4.728244]\n",
            "16780 [D loss: 0.494477, acc.: 78.91%] [G loss: 5.597545]\n",
            "16800 [D loss: 0.397651, acc.: 81.25%] [G loss: 4.379786]\n",
            "16820 [D loss: 0.393349, acc.: 82.03%] [G loss: 5.267173]\n",
            "16840 [D loss: 0.401859, acc.: 83.59%] [G loss: 4.974369]\n",
            "16860 [D loss: 0.338209, acc.: 85.16%] [G loss: 5.664454]\n",
            "16880 [D loss: 0.456650, acc.: 76.95%] [G loss: 4.633741]\n",
            "16900 [D loss: 0.373003, acc.: 85.94%] [G loss: 4.634633]\n",
            "16920 [D loss: 0.335826, acc.: 84.38%] [G loss: 5.614167]\n",
            "16940 [D loss: 0.328465, acc.: 87.50%] [G loss: 5.324001]\n",
            "16960 [D loss: 0.382558, acc.: 82.03%] [G loss: 5.345053]\n",
            "16980 [D loss: 0.317240, acc.: 87.50%] [G loss: 5.231329]\n",
            "17000 [D loss: 0.477039, acc.: 76.95%] [G loss: 4.972672]\n",
            "17020 [D loss: 0.388873, acc.: 82.81%] [G loss: 5.209314]\n",
            "17040 [D loss: 0.402799, acc.: 80.86%] [G loss: 4.886915]\n",
            "17060 [D loss: 0.415352, acc.: 80.08%] [G loss: 5.640355]\n",
            "17080 [D loss: 0.426049, acc.: 81.25%] [G loss: 5.689845]\n",
            "17100 [D loss: 0.433109, acc.: 78.91%] [G loss: 4.914865]\n",
            "17120 [D loss: 0.396254, acc.: 82.03%] [G loss: 4.937297]\n",
            "17140 [D loss: 0.412717, acc.: 78.91%] [G loss: 5.020500]\n",
            "17160 [D loss: 0.324656, acc.: 89.06%] [G loss: 6.360015]\n",
            "17180 [D loss: 0.420809, acc.: 80.47%] [G loss: 5.391016]\n",
            "17200 [D loss: 0.383854, acc.: 81.25%] [G loss: 5.253289]\n",
            "17220 [D loss: 0.431688, acc.: 80.47%] [G loss: 4.735560]\n",
            "17240 [D loss: 0.401414, acc.: 79.69%] [G loss: 5.086798]\n",
            "17260 [D loss: 0.336876, acc.: 86.72%] [G loss: 6.185205]\n",
            "17280 [D loss: 0.394941, acc.: 84.38%] [G loss: 4.908179]\n",
            "17300 [D loss: 0.316634, acc.: 87.50%] [G loss: 5.541538]\n",
            "17320 [D loss: 0.308675, acc.: 88.28%] [G loss: 5.777006]\n",
            "17340 [D loss: 0.486083, acc.: 77.34%] [G loss: 5.614432]\n",
            "17360 [D loss: 0.354236, acc.: 83.59%] [G loss: 5.176650]\n",
            "17380 [D loss: 0.285809, acc.: 88.67%] [G loss: 6.295000]\n",
            "17400 [D loss: 0.410442, acc.: 83.20%] [G loss: 5.750517]\n",
            "17420 [D loss: 0.349784, acc.: 87.11%] [G loss: 5.652696]\n",
            "17440 [D loss: 0.327412, acc.: 86.72%] [G loss: 6.282148]\n",
            "17460 [D loss: 0.335625, acc.: 86.33%] [G loss: 5.525354]\n",
            "17480 [D loss: 0.318836, acc.: 87.11%] [G loss: 5.564485]\n",
            "17500 [D loss: 0.389656, acc.: 82.03%] [G loss: 5.938044]\n",
            "17520 [D loss: 0.462249, acc.: 79.30%] [G loss: 5.997227]\n",
            "17540 [D loss: 0.365276, acc.: 84.38%] [G loss: 4.974575]\n",
            "17560 [D loss: 0.385862, acc.: 85.94%] [G loss: 5.622503]\n",
            "17580 [D loss: 0.430547, acc.: 79.30%] [G loss: 5.474545]\n",
            "17600 [D loss: 0.306609, acc.: 86.72%] [G loss: 6.328410]\n",
            "17620 [D loss: 0.323739, acc.: 88.28%] [G loss: 5.950774]\n",
            "17640 [D loss: 0.438836, acc.: 79.69%] [G loss: 5.273666]\n",
            "17660 [D loss: 0.324792, acc.: 82.03%] [G loss: 6.314446]\n",
            "17680 [D loss: 0.242578, acc.: 89.45%] [G loss: 4.234013]\n",
            "17700 [D loss: 0.516396, acc.: 76.95%] [G loss: 3.426879]\n",
            "17720 [D loss: 0.710506, acc.: 65.62%] [G loss: 5.662370]\n",
            "17740 [D loss: 0.432368, acc.: 82.81%] [G loss: 6.105488]\n",
            "17760 [D loss: 0.441444, acc.: 79.30%] [G loss: 5.405694]\n",
            "17780 [D loss: 0.344424, acc.: 82.81%] [G loss: 6.206056]\n",
            "17800 [D loss: 0.394133, acc.: 82.03%] [G loss: 5.694659]\n",
            "17820 [D loss: 0.324662, acc.: 85.94%] [G loss: 7.318079]\n",
            "17840 [D loss: 0.381732, acc.: 83.59%] [G loss: 5.820994]\n",
            "17860 [D loss: 0.296153, acc.: 87.50%] [G loss: 6.826048]\n",
            "17880 [D loss: 0.388860, acc.: 82.81%] [G loss: 5.374186]\n",
            "17900 [D loss: 0.356173, acc.: 85.16%] [G loss: 5.837128]\n",
            "17920 [D loss: 0.320687, acc.: 87.89%] [G loss: 5.707166]\n",
            "17940 [D loss: 0.340729, acc.: 85.94%] [G loss: 6.182121]\n",
            "17960 [D loss: 0.297170, acc.: 87.89%] [G loss: 6.692943]\n",
            "17980 [D loss: 0.448421, acc.: 78.91%] [G loss: 5.547339]\n",
            "18000 [D loss: 0.331833, acc.: 86.72%] [G loss: 6.006695]\n",
            "18020 [D loss: 0.348783, acc.: 84.77%] [G loss: 5.948410]\n",
            "18040 [D loss: 0.363815, acc.: 85.55%] [G loss: 5.445758]\n",
            "18060 [D loss: 0.348519, acc.: 85.94%] [G loss: 6.037502]\n",
            "18080 [D loss: 0.389844, acc.: 83.98%] [G loss: 5.939655]\n",
            "18100 [D loss: 0.325624, acc.: 87.89%] [G loss: 6.091106]\n",
            "18120 [D loss: 0.377292, acc.: 83.98%] [G loss: 5.330450]\n",
            "18140 [D loss: 0.383470, acc.: 82.42%] [G loss: 5.158847]\n",
            "18160 [D loss: 0.260402, acc.: 92.19%] [G loss: 6.694026]\n",
            "18180 [D loss: 0.433467, acc.: 80.86%] [G loss: 6.413772]\n",
            "18200 [D loss: 0.353264, acc.: 82.42%] [G loss: 5.184613]\n",
            "18220 [D loss: 0.336166, acc.: 86.33%] [G loss: 6.344069]\n",
            "18240 [D loss: 0.388624, acc.: 82.42%] [G loss: 6.782049]\n",
            "18260 [D loss: 0.301595, acc.: 88.28%] [G loss: 5.809802]\n",
            "18280 [D loss: 0.337688, acc.: 84.77%] [G loss: 5.947719]\n",
            "18300 [D loss: 0.317893, acc.: 85.94%] [G loss: 6.751332]\n",
            "18320 [D loss: 0.404192, acc.: 80.08%] [G loss: 6.559566]\n",
            "18340 [D loss: 0.374496, acc.: 85.55%] [G loss: 5.840132]\n",
            "18360 [D loss: 0.399106, acc.: 83.98%] [G loss: 5.352736]\n",
            "18380 [D loss: 0.325394, acc.: 84.38%] [G loss: 6.272518]\n",
            "18400 [D loss: 0.403595, acc.: 82.42%] [G loss: 5.830152]\n",
            "18420 [D loss: 0.463820, acc.: 80.47%] [G loss: 5.394187]\n",
            "18440 [D loss: 0.358702, acc.: 84.38%] [G loss: 6.004645]\n",
            "18460 [D loss: 0.387689, acc.: 83.59%] [G loss: 6.191020]\n",
            "18480 [D loss: 0.380126, acc.: 82.42%] [G loss: 6.396157]\n",
            "18500 [D loss: 0.370296, acc.: 84.77%] [G loss: 6.407322]\n",
            "18520 [D loss: 0.405846, acc.: 81.64%] [G loss: 6.281465]\n",
            "18540 [D loss: 0.372342, acc.: 82.03%] [G loss: 5.923351]\n",
            "18560 [D loss: 0.405341, acc.: 81.25%] [G loss: 6.319572]\n",
            "18580 [D loss: 0.244556, acc.: 90.62%] [G loss: 7.483626]\n",
            "18600 [D loss: 0.168813, acc.: 93.75%] [G loss: 3.891478]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}