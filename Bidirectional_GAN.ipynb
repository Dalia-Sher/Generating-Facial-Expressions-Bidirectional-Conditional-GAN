{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Keras_BiGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/bigan/bigan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\r\n",
        "\r\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\r\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\r\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose\r\n",
        "from keras.layers.advanced_activations import LeakyReLU\r\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras import losses\r\n",
        "from keras.utils import to_categorical\r\n",
        "import keras.backend as K\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "49a142c2-7763-47a8-ebd7-4dc1153b8150"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\r\n",
        "data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f7a5e4-6de0-46f0-e22d-28acf4ee92ac"
      },
      "source": [
        "data.Usage.value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training       28709\n",
              "PublicTest      3589\n",
              "PrivateTest     3589\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 7\r\n",
        "img_width = 48\r\n",
        "img_height = 48"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "b3c76ba0-3127-45a7-e6bf-4418ab7412cf"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35887, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuCiGR9EiNwu"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m_u8eq-iPfb"
      },
      "source": [
        "class BIGAN():\r\n",
        "    def __init__(self):\r\n",
        "        self.img_rows = 48\r\n",
        "        self.img_cols = 48\r\n",
        "        self.channels = 1\r\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\r\n",
        "        self.latent_dim = 100\r\n",
        "\r\n",
        "        optimizer = Adam(0.0002, 0.5)\r\n",
        "\r\n",
        "        # Build and compile the discriminator\r\n",
        "        self.discriminator = self.build_discriminator()\r\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\r\n",
        "            optimizer=optimizer,\r\n",
        "            metrics=['accuracy'])\r\n",
        "\r\n",
        "        # Build the generator\r\n",
        "        self.generator = self.build_generator()\r\n",
        "\r\n",
        "        # Build the encoder\r\n",
        "        self.encoder = self.build_encoder()\r\n",
        "\r\n",
        "        # The part of the bigan that trains the discriminator and encoder\r\n",
        "        self.discriminator.trainable = False\r\n",
        "\r\n",
        "        # Generate image from sampled noise\r\n",
        "        z = Input(shape=(self.latent_dim, ))\r\n",
        "        img_ = self.generator(z)\r\n",
        "\r\n",
        "        # Encode image\r\n",
        "        img = Input(shape=self.img_shape)\r\n",
        "        z_ = self.encoder(img)\r\n",
        "\r\n",
        "        # Latent -> img is fake, and img -> latent is valid\r\n",
        "        fake = self.discriminator([z, img_])\r\n",
        "        valid = self.discriminator([z_, img])\r\n",
        "\r\n",
        "        # Set up and compile the combined model\r\n",
        "        # Trains generator to fool the discriminator\r\n",
        "        self.bigan_generator = Model([z, img], [fake, valid])\r\n",
        "        self.bigan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\r\n",
        "            optimizer=optimizer)\r\n",
        "\r\n",
        "\r\n",
        "    def build_encoder(self):\r\n",
        "        model = Sequential()\r\n",
        "\r\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\r\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\r\n",
        "        model.add(BatchNormalization(momentum=0.9))\r\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\r\n",
        "        model.add(BatchNormalization(momentum=0.9))\r\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\r\n",
        "        model.add(BatchNormalization(momentum=0.9))\r\n",
        "        model.add(Flatten())\r\n",
        "        model.add(Dense(self.latent_dim))\r\n",
        "\r\n",
        "\r\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\r\n",
        "        # model.add(Dense(512))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(BatchNormalization(momentum=0.8))\r\n",
        "        # model.add(Dense(512))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(BatchNormalization(momentum=0.8))\r\n",
        "        # model.add(Dense(self.latent_dim))\r\n",
        "\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "        img = Input(shape=self.img_shape)\r\n",
        "        z = model(img)\r\n",
        "\r\n",
        "        return Model(img, z)\r\n",
        "\r\n",
        "    def build_generator(self):\r\n",
        "        model = Sequential()\r\n",
        "\r\n",
        "        # model.add(Dense(512, input_dim=self.latent_dim))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(BatchNormalization(momentum=0.8))\r\n",
        "        # model.add(Dense(512))\r\n",
        "        # model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # model.add(BatchNormalization(momentum=0.8))\r\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\r\n",
        "        # model.add(Reshape(self.img_shape))\r\n",
        "\r\n",
        "        # foundation for 12x12 image\r\n",
        "        n_nodes = 128 * 12 * 12\r\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\r\n",
        "        model.add(LeakyReLU(alpha=0.2))\r\n",
        "        model.add(Reshape((12, 12, 128)))\r\n",
        "        # upsample to 24x24\r\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\r\n",
        "        model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # upsample to 48x48\r\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\r\n",
        "        model.add(LeakyReLU(alpha=0.2))\r\n",
        "        # generate\r\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\r\n",
        "\r\n",
        "        model.summary()\r\n",
        "\r\n",
        "        z = Input(shape=(self.latent_dim,))\r\n",
        "        gen_img = model(z)\r\n",
        "\r\n",
        "        return Model(z, gen_img)\r\n",
        "\r\n",
        "    def build_discriminator(self):\r\n",
        "\r\n",
        "        # xi = Input(shape=self.img_shape)\r\n",
        "        # zi = Input(self.latent_dim,)\r\n",
        "        # xn = Conv2D(64, (5,5), strides=(2,2), activation='relu')(xi)\r\n",
        "        # xn = Conv2D(128, (5,5), strides=(2,2))(xn)\r\n",
        "        # xn = BatchNormalization(momentum=0.9)(xn)\r\n",
        "        # xn = Dropout(0.2)(xn)\r\n",
        "        # xn = Conv2D(256, (5,5), strides=(2,2))(xn)\r\n",
        "        # xn = BatchNormalization(momentum=0.9)(xn)\r\n",
        "        # xn = Dropout(0.2)(xn)\r\n",
        "        # xn = Conv2D(512, (5,5), strides=(2,2))(xn)\r\n",
        "        # xn = BatchNormalization(momentum=0.9)(xn)\r\n",
        "        # xn = Dropout(0.2)(xn)\r\n",
        "        # xn = Flatten()(xn)\r\n",
        "        # zn = Flatten()(zi)\r\n",
        "        # zn = Dense(512, activation='relu')(zn)\r\n",
        "        # zn = Dropout(0.2)(zn)\r\n",
        "        # nn = concatenate()([xn, zn])\r\n",
        "        # nn = Dense(1, activation=\"sigmoid\")(nn)\r\n",
        "\r\n",
        "        # return Model(inputs=[xi, zi], outputs=nn, name='discriminator')\r\n",
        "\r\n",
        "        z = Input(shape=(self.latent_dim, ))\r\n",
        "        img = Input(shape=self.img_shape)\r\n",
        "        d_in = concatenate([z, Flatten()(img)])\r\n",
        "\r\n",
        "        model = Dense(1024)(d_in)\r\n",
        "        model = LeakyReLU(alpha=0.2)(model)\r\n",
        "        model = Dropout(0.5)(model)\r\n",
        "        model = Dense(1024)(model)\r\n",
        "        model = LeakyReLU(alpha=0.2)(model)\r\n",
        "        model = Dropout(0.5)(model)\r\n",
        "        model = Dense(1024)(model)\r\n",
        "        model = LeakyReLU(alpha=0.2)(model)\r\n",
        "        model = Dropout(0.5)(model)\r\n",
        "        validity = Dense(1, activation=\"sigmoid\")(model)\r\n",
        "\r\n",
        "        return Model([z, img], validity)\r\n",
        "\r\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\r\n",
        "\r\n",
        "        # # Load the dataset\r\n",
        "        # (X_train, _), (_, _) = mnist.load_data()\r\n",
        "\r\n",
        "        # # Rescale -1 to 1\r\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\r\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\r\n",
        "\r\n",
        "        # Adversarial ground truths\r\n",
        "        valid = np.ones((batch_size, 1))\r\n",
        "        fake = np.zeros((batch_size, 1))\r\n",
        "\r\n",
        "        for epoch in range(epochs):\r\n",
        "\r\n",
        "            # ---------------------\r\n",
        "            #  Train Discriminator\r\n",
        "            # ---------------------\r\n",
        "\r\n",
        "            # Sample noise and generate img\r\n",
        "            z = np.random.normal(size=(batch_size, self.latent_dim))\r\n",
        "            imgs_ = self.generator.predict(z)\r\n",
        "\r\n",
        "            # Select a random batch of images and encode\r\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\r\n",
        "            imgs = X_train[idx]\r\n",
        "            z_ = self.encoder.predict(imgs)\r\n",
        "\r\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\r\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs], valid)\r\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_], fake)\r\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\r\n",
        "\r\n",
        "            # ---------------------\r\n",
        "            #  Train Generator\r\n",
        "            # ---------------------\r\n",
        "\r\n",
        "            # Train the generator (z -> img is valid and img -> z is is invalid)\r\n",
        "            g_loss = self.bigan_generator.train_on_batch([z, imgs], [valid, fake])\r\n",
        "\r\n",
        "            # Plot the progress\r\n",
        "            if epoch%20 == 0:\r\n",
        "              print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\r\n",
        "\r\n",
        "            # If at save interval => save generated image samples\r\n",
        "            if epoch % sample_interval == 0:\r\n",
        "                self.sample_interval(epoch)\r\n",
        "\r\n",
        "    def sample_interval(self, epoch):\r\n",
        "        r, c = 5, 5\r\n",
        "        z = np.random.normal(size=(25, self.latent_dim))\r\n",
        "        gen_imgs = self.generator.predict(z)\r\n",
        "\r\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\r\n",
        "\r\n",
        "        fig, axs = plt.subplots(r, c)\r\n",
        "        cnt = 0\r\n",
        "        for i in range(r):\r\n",
        "            for j in range(c):\r\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\r\n",
        "                axs[i,j].axis('off')\r\n",
        "                cnt += 1\r\n",
        "        fig.savefig(\"images/epoch_%d.png\" % epoch)\r\n",
        "        plt.close()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHManCpgiUbr",
        "outputId": "3f7c6b9f-7cd8-4bc2-fcd4-31f2cf2de1cf"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    bigan = BIGAN()\r\n",
        "    bigan.train(epochs=20000, batch_size=128, sample_interval=400)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.901296, acc: 35.94%] [G loss: 5.721769]\n",
            "20 [D loss: 0.651714, acc: 50.00%] [G loss: 8.229768]\n",
            "40 [D loss: 0.233653, acc: 96.09%] [G loss: 1.265795]\n",
            "60 [D loss: 0.620010, acc: 49.22%] [G loss: 3.042512]\n",
            "80 [D loss: 0.747481, acc: 39.45%] [G loss: 1.552697]\n",
            "100 [D loss: 0.739168, acc: 37.11%] [G loss: 1.474075]\n",
            "120 [D loss: 0.734626, acc: 36.33%] [G loss: 1.450811]\n",
            "140 [D loss: 0.744575, acc: 35.55%] [G loss: 1.422553]\n",
            "160 [D loss: 0.740270, acc: 37.11%] [G loss: 1.383372]\n",
            "180 [D loss: 0.730728, acc: 41.02%] [G loss: 1.420498]\n",
            "200 [D loss: 0.710691, acc: 43.36%] [G loss: 1.434185]\n",
            "220 [D loss: 0.728968, acc: 41.02%] [G loss: 1.417056]\n",
            "240 [D loss: 0.725502, acc: 39.45%] [G loss: 1.416327]\n",
            "260 [D loss: 0.722364, acc: 39.45%] [G loss: 1.412235]\n",
            "280 [D loss: 0.718387, acc: 40.62%] [G loss: 1.419825]\n",
            "300 [D loss: 0.718318, acc: 44.14%] [G loss: 1.402555]\n",
            "320 [D loss: 0.716093, acc: 41.41%] [G loss: 1.405378]\n",
            "340 [D loss: 0.725663, acc: 37.89%] [G loss: 1.403540]\n",
            "360 [D loss: 0.711217, acc: 44.14%] [G loss: 1.397174]\n",
            "380 [D loss: 0.709818, acc: 41.41%] [G loss: 1.409033]\n",
            "400 [D loss: 0.705788, acc: 42.58%] [G loss: 1.397791]\n",
            "420 [D loss: 0.719759, acc: 36.33%] [G loss: 1.392136]\n",
            "440 [D loss: 0.711268, acc: 47.66%] [G loss: 1.413738]\n",
            "460 [D loss: 0.714427, acc: 36.72%] [G loss: 1.388101]\n",
            "480 [D loss: 0.708935, acc: 42.19%] [G loss: 1.412439]\n",
            "500 [D loss: 0.717584, acc: 41.80%] [G loss: 1.401772]\n",
            "520 [D loss: 0.706259, acc: 44.92%] [G loss: 1.397003]\n",
            "540 [D loss: 0.711032, acc: 41.02%] [G loss: 1.402137]\n",
            "560 [D loss: 0.704129, acc: 44.14%] [G loss: 1.417344]\n",
            "580 [D loss: 0.705617, acc: 48.05%] [G loss: 1.399537]\n",
            "600 [D loss: 0.721810, acc: 42.97%] [G loss: 1.444855]\n",
            "620 [D loss: 0.703191, acc: 49.22%] [G loss: 1.396419]\n",
            "640 [D loss: 0.724649, acc: 41.80%] [G loss: 1.396462]\n",
            "660 [D loss: 0.719755, acc: 39.45%] [G loss: 1.411993]\n",
            "680 [D loss: 0.710416, acc: 38.28%] [G loss: 1.400452]\n",
            "700 [D loss: 0.712998, acc: 38.67%] [G loss: 1.396604]\n",
            "720 [D loss: 0.710667, acc: 42.58%] [G loss: 1.394663]\n",
            "740 [D loss: 0.709840, acc: 37.50%] [G loss: 1.389471]\n",
            "760 [D loss: 0.709450, acc: 39.45%] [G loss: 1.390059]\n",
            "780 [D loss: 0.713713, acc: 37.11%] [G loss: 1.390249]\n",
            "800 [D loss: 0.708831, acc: 42.97%] [G loss: 1.407414]\n",
            "820 [D loss: 0.705853, acc: 38.67%] [G loss: 1.387371]\n",
            "840 [D loss: 0.704442, acc: 42.97%] [G loss: 1.383926]\n",
            "860 [D loss: 0.707603, acc: 42.19%] [G loss: 1.388813]\n",
            "880 [D loss: 0.702782, acc: 45.70%] [G loss: 1.403659]\n",
            "900 [D loss: 0.697033, acc: 45.70%] [G loss: 1.395142]\n",
            "920 [D loss: 0.705897, acc: 37.89%] [G loss: 1.399627]\n",
            "940 [D loss: 0.706346, acc: 41.02%] [G loss: 1.381712]\n",
            "960 [D loss: 0.704510, acc: 40.62%] [G loss: 1.412845]\n",
            "980 [D loss: 0.705525, acc: 44.14%] [G loss: 1.409043]\n",
            "1000 [D loss: 0.701105, acc: 45.70%] [G loss: 1.401330]\n",
            "1020 [D loss: 0.710003, acc: 38.28%] [G loss: 1.406746]\n",
            "1040 [D loss: 0.703678, acc: 45.31%] [G loss: 1.403688]\n",
            "1060 [D loss: 0.705111, acc: 46.48%] [G loss: 1.400099]\n",
            "1080 [D loss: 0.698319, acc: 41.41%] [G loss: 1.393212]\n",
            "1100 [D loss: 0.702850, acc: 42.19%] [G loss: 1.401659]\n",
            "1120 [D loss: 0.697284, acc: 46.88%] [G loss: 1.387964]\n",
            "1140 [D loss: 0.707071, acc: 39.84%] [G loss: 1.393430]\n",
            "1160 [D loss: 0.698807, acc: 41.80%] [G loss: 1.388088]\n",
            "1180 [D loss: 0.703273, acc: 39.06%] [G loss: 1.391614]\n",
            "1200 [D loss: 0.706704, acc: 34.38%] [G loss: 1.380789]\n",
            "1220 [D loss: 0.700207, acc: 41.41%] [G loss: 1.390817]\n",
            "1240 [D loss: 0.701860, acc: 45.31%] [G loss: 1.401870]\n",
            "1260 [D loss: 0.703096, acc: 41.41%] [G loss: 1.393271]\n",
            "1280 [D loss: 0.702206, acc: 42.58%] [G loss: 1.395366]\n",
            "1300 [D loss: 0.703106, acc: 42.97%] [G loss: 1.395327]\n",
            "1320 [D loss: 0.706028, acc: 39.06%] [G loss: 1.388909]\n",
            "1340 [D loss: 0.702896, acc: 40.62%] [G loss: 1.398961]\n",
            "1360 [D loss: 0.707112, acc: 42.19%] [G loss: 1.405730]\n",
            "1380 [D loss: 0.702986, acc: 42.97%] [G loss: 1.397968]\n",
            "1400 [D loss: 0.700959, acc: 37.89%] [G loss: 1.393263]\n",
            "1420 [D loss: 0.701964, acc: 41.41%] [G loss: 1.401013]\n",
            "1440 [D loss: 0.700287, acc: 43.75%] [G loss: 1.398497]\n",
            "1460 [D loss: 0.703619, acc: 35.55%] [G loss: 1.391407]\n",
            "1480 [D loss: 0.702737, acc: 42.97%] [G loss: 1.397560]\n",
            "1500 [D loss: 0.708466, acc: 42.58%] [G loss: 1.410892]\n",
            "1520 [D loss: 0.699320, acc: 46.09%] [G loss: 1.411016]\n",
            "1540 [D loss: 0.701970, acc: 46.88%] [G loss: 1.392193]\n",
            "1560 [D loss: 0.707868, acc: 41.41%] [G loss: 1.395361]\n",
            "1580 [D loss: 0.703577, acc: 40.62%] [G loss: 1.403817]\n",
            "1600 [D loss: 0.703555, acc: 39.06%] [G loss: 1.398812]\n",
            "1620 [D loss: 0.691963, acc: 47.66%] [G loss: 1.401156]\n",
            "1640 [D loss: 0.693304, acc: 51.56%] [G loss: 1.406790]\n",
            "1660 [D loss: 0.701315, acc: 45.31%] [G loss: 1.393850]\n",
            "1680 [D loss: 0.704293, acc: 42.97%] [G loss: 1.400491]\n",
            "1700 [D loss: 0.699066, acc: 45.70%] [G loss: 1.386250]\n",
            "1720 [D loss: 0.701417, acc: 41.02%] [G loss: 1.395948]\n",
            "1740 [D loss: 0.705927, acc: 39.45%] [G loss: 1.388510]\n",
            "1760 [D loss: 0.708423, acc: 38.67%] [G loss: 1.389613]\n",
            "1780 [D loss: 0.704310, acc: 38.67%] [G loss: 1.386972]\n",
            "1800 [D loss: 0.696517, acc: 44.92%] [G loss: 1.401018]\n",
            "1820 [D loss: 0.701321, acc: 43.36%] [G loss: 1.394606]\n",
            "1840 [D loss: 0.703875, acc: 38.67%] [G loss: 1.403037]\n",
            "1860 [D loss: 0.703481, acc: 38.67%] [G loss: 1.386621]\n",
            "1880 [D loss: 0.700771, acc: 42.97%] [G loss: 1.389219]\n",
            "1900 [D loss: 0.706344, acc: 34.77%] [G loss: 1.396820]\n",
            "1920 [D loss: 0.704811, acc: 39.06%] [G loss: 1.389149]\n",
            "1940 [D loss: 0.700145, acc: 44.53%] [G loss: 1.393566]\n",
            "1960 [D loss: 0.702072, acc: 42.19%] [G loss: 1.391102]\n",
            "1980 [D loss: 0.699849, acc: 42.19%] [G loss: 1.395984]\n",
            "2000 [D loss: 0.697665, acc: 45.31%] [G loss: 1.396664]\n",
            "2020 [D loss: 0.711319, acc: 38.28%] [G loss: 1.389560]\n",
            "2040 [D loss: 0.700904, acc: 43.36%] [G loss: 1.395210]\n",
            "2060 [D loss: 0.698968, acc: 44.53%] [G loss: 1.400446]\n",
            "2080 [D loss: 0.699268, acc: 43.36%] [G loss: 1.396117]\n",
            "2100 [D loss: 0.700252, acc: 42.97%] [G loss: 1.390992]\n",
            "2120 [D loss: 0.703122, acc: 38.67%] [G loss: 1.393461]\n",
            "2140 [D loss: 0.702751, acc: 42.19%] [G loss: 1.381843]\n",
            "2160 [D loss: 0.699202, acc: 42.58%] [G loss: 1.398880]\n",
            "2180 [D loss: 0.701801, acc: 44.92%] [G loss: 1.399616]\n",
            "2200 [D loss: 0.697942, acc: 43.36%] [G loss: 1.395996]\n",
            "2220 [D loss: 0.700711, acc: 41.80%] [G loss: 1.390937]\n",
            "2240 [D loss: 0.707517, acc: 33.20%] [G loss: 1.394877]\n",
            "2260 [D loss: 0.698190, acc: 47.27%] [G loss: 1.393349]\n",
            "2280 [D loss: 0.703226, acc: 35.55%] [G loss: 1.395934]\n",
            "2300 [D loss: 0.700985, acc: 40.23%] [G loss: 1.393967]\n",
            "2320 [D loss: 0.700447, acc: 39.84%] [G loss: 1.390908]\n",
            "2340 [D loss: 0.699411, acc: 44.14%] [G loss: 1.388987]\n",
            "2360 [D loss: 0.701729, acc: 38.67%] [G loss: 1.388572]\n",
            "2380 [D loss: 0.705497, acc: 36.72%] [G loss: 1.396438]\n",
            "2400 [D loss: 0.703364, acc: 41.41%] [G loss: 1.393982]\n",
            "2420 [D loss: 0.704846, acc: 37.50%] [G loss: 1.390226]\n",
            "2440 [D loss: 0.704133, acc: 38.28%] [G loss: 1.394939]\n",
            "2460 [D loss: 0.704639, acc: 38.28%] [G loss: 1.393966]\n",
            "2480 [D loss: 0.707631, acc: 36.33%] [G loss: 1.394771]\n",
            "2500 [D loss: 0.700035, acc: 43.36%] [G loss: 1.388288]\n",
            "2520 [D loss: 0.698946, acc: 46.48%] [G loss: 1.396260]\n",
            "2540 [D loss: 0.699359, acc: 45.31%] [G loss: 1.389623]\n",
            "2560 [D loss: 0.700567, acc: 44.14%] [G loss: 1.392822]\n",
            "2580 [D loss: 0.701373, acc: 44.14%] [G loss: 1.393264]\n",
            "2600 [D loss: 0.705799, acc: 40.62%] [G loss: 1.392800]\n",
            "2620 [D loss: 0.701118, acc: 45.31%] [G loss: 1.392192]\n",
            "2640 [D loss: 0.698135, acc: 43.36%] [G loss: 1.393207]\n",
            "2660 [D loss: 0.702266, acc: 41.41%] [G loss: 1.394226]\n",
            "2680 [D loss: 0.697149, acc: 45.70%] [G loss: 1.398498]\n",
            "2700 [D loss: 0.704535, acc: 41.02%] [G loss: 1.399049]\n",
            "2720 [D loss: 0.701703, acc: 45.31%] [G loss: 1.388712]\n",
            "2740 [D loss: 0.700483, acc: 41.41%] [G loss: 1.391663]\n",
            "2760 [D loss: 0.700830, acc: 45.70%] [G loss: 1.396034]\n",
            "2780 [D loss: 0.698921, acc: 46.88%] [G loss: 1.399839]\n",
            "2800 [D loss: 0.704610, acc: 40.62%] [G loss: 1.398297]\n",
            "2820 [D loss: 0.705345, acc: 39.45%] [G loss: 1.395066]\n",
            "2840 [D loss: 0.699747, acc: 43.36%] [G loss: 1.406503]\n",
            "2860 [D loss: 0.693988, acc: 44.53%] [G loss: 1.390440]\n",
            "2880 [D loss: 0.703151, acc: 38.28%] [G loss: 1.391624]\n",
            "2900 [D loss: 0.694844, acc: 49.61%] [G loss: 1.391781]\n",
            "2920 [D loss: 0.701384, acc: 48.44%] [G loss: 1.375673]\n",
            "2940 [D loss: 0.702598, acc: 40.62%] [G loss: 1.390405]\n",
            "2960 [D loss: 0.701636, acc: 37.89%] [G loss: 1.392799]\n",
            "2980 [D loss: 0.699350, acc: 41.80%] [G loss: 1.389359]\n",
            "3000 [D loss: 0.700978, acc: 42.97%] [G loss: 1.393655]\n",
            "3020 [D loss: 0.695600, acc: 46.48%] [G loss: 1.397409]\n",
            "3040 [D loss: 0.695959, acc: 44.53%] [G loss: 1.395326]\n",
            "3060 [D loss: 0.697550, acc: 46.88%] [G loss: 1.389455]\n",
            "3080 [D loss: 0.704024, acc: 37.11%] [G loss: 1.388512]\n",
            "3100 [D loss: 0.697040, acc: 45.31%] [G loss: 1.384216]\n",
            "3120 [D loss: 0.702774, acc: 36.72%] [G loss: 1.389456]\n",
            "3140 [D loss: 0.698732, acc: 46.48%] [G loss: 1.394914]\n",
            "3160 [D loss: 0.701829, acc: 41.80%] [G loss: 1.391416]\n",
            "3180 [D loss: 0.701661, acc: 39.84%] [G loss: 1.386659]\n",
            "3200 [D loss: 0.699837, acc: 39.45%] [G loss: 1.398450]\n",
            "3220 [D loss: 0.701402, acc: 42.58%] [G loss: 1.392522]\n",
            "3240 [D loss: 0.704747, acc: 40.23%] [G loss: 1.393495]\n",
            "3260 [D loss: 0.702100, acc: 39.06%] [G loss: 1.387796]\n",
            "3280 [D loss: 0.700068, acc: 39.84%] [G loss: 1.389746]\n",
            "3300 [D loss: 0.701812, acc: 38.67%] [G loss: 1.391126]\n",
            "3320 [D loss: 0.701720, acc: 43.75%] [G loss: 1.387427]\n",
            "3340 [D loss: 0.701861, acc: 40.62%] [G loss: 1.386968]\n",
            "3360 [D loss: 0.701825, acc: 41.41%] [G loss: 1.392388]\n",
            "3380 [D loss: 0.697063, acc: 45.70%] [G loss: 1.388208]\n",
            "3400 [D loss: 0.700259, acc: 42.19%] [G loss: 1.392057]\n",
            "3420 [D loss: 0.701851, acc: 39.84%] [G loss: 1.386181]\n",
            "3440 [D loss: 0.703534, acc: 39.84%] [G loss: 1.394613]\n",
            "3460 [D loss: 0.705145, acc: 39.45%] [G loss: 1.391842]\n",
            "3480 [D loss: 0.699325, acc: 41.41%] [G loss: 1.403645]\n",
            "3500 [D loss: 0.701166, acc: 44.53%] [G loss: 1.397536]\n",
            "3520 [D loss: 0.701823, acc: 36.72%] [G loss: 1.391231]\n",
            "3540 [D loss: 0.702050, acc: 38.67%] [G loss: 1.389985]\n",
            "3560 [D loss: 0.699384, acc: 42.97%] [G loss: 1.387166]\n",
            "3580 [D loss: 0.703220, acc: 35.16%] [G loss: 1.388829]\n",
            "3600 [D loss: 0.702278, acc: 40.23%] [G loss: 1.393159]\n",
            "3620 [D loss: 0.701018, acc: 41.41%] [G loss: 1.394453]\n",
            "3640 [D loss: 0.700529, acc: 40.23%] [G loss: 1.399850]\n",
            "3660 [D loss: 0.700257, acc: 47.27%] [G loss: 1.387875]\n",
            "3680 [D loss: 0.703686, acc: 41.41%] [G loss: 1.392218]\n",
            "3700 [D loss: 0.702696, acc: 38.67%] [G loss: 1.397127]\n",
            "3720 [D loss: 0.698378, acc: 47.27%] [G loss: 1.391422]\n",
            "3740 [D loss: 0.702012, acc: 41.02%] [G loss: 1.388245]\n",
            "3760 [D loss: 0.699173, acc: 42.58%] [G loss: 1.392468]\n",
            "3780 [D loss: 0.698504, acc: 40.23%] [G loss: 1.391411]\n",
            "3800 [D loss: 0.700493, acc: 39.84%] [G loss: 1.397949]\n",
            "3820 [D loss: 0.701760, acc: 39.06%] [G loss: 1.391119]\n",
            "3840 [D loss: 0.700087, acc: 39.45%] [G loss: 1.396123]\n",
            "3860 [D loss: 0.698782, acc: 41.80%] [G loss: 1.397139]\n",
            "3880 [D loss: 0.701267, acc: 41.02%] [G loss: 1.385228]\n",
            "3900 [D loss: 0.700316, acc: 36.33%] [G loss: 1.396306]\n",
            "3920 [D loss: 0.702457, acc: 37.89%] [G loss: 1.392451]\n",
            "3940 [D loss: 0.700605, acc: 42.97%] [G loss: 1.383378]\n",
            "3960 [D loss: 0.696337, acc: 44.14%] [G loss: 1.394791]\n",
            "3980 [D loss: 0.700484, acc: 41.41%] [G loss: 1.388933]\n",
            "4000 [D loss: 0.701318, acc: 40.23%] [G loss: 1.388542]\n",
            "4020 [D loss: 0.700885, acc: 43.75%] [G loss: 1.390787]\n",
            "4040 [D loss: 0.698406, acc: 44.14%] [G loss: 1.391207]\n",
            "4060 [D loss: 0.694993, acc: 39.45%] [G loss: 1.387899]\n",
            "4080 [D loss: 0.699556, acc: 41.41%] [G loss: 1.396590]\n",
            "4100 [D loss: 0.701608, acc: 42.58%] [G loss: 1.393354]\n",
            "4120 [D loss: 0.699716, acc: 42.58%] [G loss: 1.399524]\n",
            "4140 [D loss: 0.699053, acc: 43.36%] [G loss: 1.400152]\n",
            "4160 [D loss: 0.702013, acc: 39.45%] [G loss: 1.388499]\n",
            "4180 [D loss: 0.701812, acc: 37.89%] [G loss: 1.388558]\n",
            "4200 [D loss: 0.698366, acc: 44.92%] [G loss: 1.389932]\n",
            "4220 [D loss: 0.698697, acc: 39.45%] [G loss: 1.392655]\n",
            "4240 [D loss: 0.700917, acc: 41.80%] [G loss: 1.392007]\n",
            "4260 [D loss: 0.699523, acc: 43.36%] [G loss: 1.386783]\n",
            "4280 [D loss: 0.700751, acc: 45.70%] [G loss: 1.391673]\n",
            "4300 [D loss: 0.698093, acc: 44.53%] [G loss: 1.389576]\n",
            "4320 [D loss: 0.699975, acc: 39.84%] [G loss: 1.393202]\n",
            "4340 [D loss: 0.703321, acc: 44.14%] [G loss: 1.394272]\n",
            "4360 [D loss: 0.701476, acc: 43.36%] [G loss: 1.386905]\n",
            "4380 [D loss: 0.701739, acc: 38.67%] [G loss: 1.391613]\n",
            "4400 [D loss: 0.700655, acc: 38.28%] [G loss: 1.384878]\n",
            "4420 [D loss: 0.702881, acc: 37.11%] [G loss: 1.389598]\n",
            "4440 [D loss: 0.697453, acc: 43.36%] [G loss: 1.394881]\n",
            "4460 [D loss: 0.699439, acc: 42.19%] [G loss: 1.405849]\n",
            "4480 [D loss: 0.695336, acc: 48.44%] [G loss: 1.396243]\n",
            "4500 [D loss: 0.699362, acc: 42.58%] [G loss: 1.394806]\n",
            "4520 [D loss: 0.702370, acc: 40.62%] [G loss: 1.391933]\n",
            "4540 [D loss: 0.697831, acc: 48.05%] [G loss: 1.390133]\n",
            "4560 [D loss: 0.702148, acc: 39.06%] [G loss: 1.396869]\n",
            "4580 [D loss: 0.698742, acc: 41.41%] [G loss: 1.393790]\n",
            "4600 [D loss: 0.697284, acc: 47.66%] [G loss: 1.389048]\n",
            "4620 [D loss: 0.701948, acc: 41.41%] [G loss: 1.396988]\n",
            "4640 [D loss: 0.698197, acc: 44.92%] [G loss: 1.393279]\n",
            "4660 [D loss: 0.695918, acc: 45.31%] [G loss: 1.386894]\n",
            "4680 [D loss: 0.697494, acc: 45.70%] [G loss: 1.389903]\n",
            "4700 [D loss: 0.703844, acc: 37.89%] [G loss: 1.390434]\n",
            "4720 [D loss: 0.700947, acc: 40.23%] [G loss: 1.388906]\n",
            "4740 [D loss: 0.699812, acc: 39.45%] [G loss: 1.388933]\n",
            "4760 [D loss: 0.703539, acc: 35.16%] [G loss: 1.396233]\n",
            "4780 [D loss: 0.699897, acc: 41.02%] [G loss: 1.390771]\n",
            "4800 [D loss: 0.699088, acc: 39.84%] [G loss: 1.387958]\n",
            "4820 [D loss: 0.699418, acc: 43.36%] [G loss: 1.394610]\n",
            "4840 [D loss: 0.698640, acc: 44.14%] [G loss: 1.403737]\n",
            "4860 [D loss: 0.701531, acc: 34.77%] [G loss: 1.390886]\n",
            "4880 [D loss: 0.699485, acc: 41.02%] [G loss: 1.387690]\n",
            "4900 [D loss: 0.697378, acc: 44.14%] [G loss: 1.397805]\n",
            "4920 [D loss: 0.705670, acc: 37.11%] [G loss: 1.401038]\n",
            "4940 [D loss: 0.699329, acc: 42.19%] [G loss: 1.386103]\n",
            "4960 [D loss: 0.701966, acc: 40.23%] [G loss: 1.394032]\n",
            "4980 [D loss: 0.705105, acc: 32.81%] [G loss: 1.389849]\n",
            "5000 [D loss: 0.697995, acc: 44.92%] [G loss: 1.385721]\n",
            "5020 [D loss: 0.694815, acc: 43.36%] [G loss: 1.398603]\n",
            "5040 [D loss: 0.698181, acc: 44.92%] [G loss: 1.398484]\n",
            "5060 [D loss: 0.696731, acc: 48.83%] [G loss: 1.393850]\n",
            "5080 [D loss: 0.697739, acc: 39.84%] [G loss: 1.399009]\n",
            "5100 [D loss: 0.697971, acc: 42.58%] [G loss: 1.395311]\n",
            "5120 [D loss: 0.697062, acc: 44.92%] [G loss: 1.393587]\n",
            "5140 [D loss: 0.695784, acc: 42.58%] [G loss: 1.404197]\n",
            "5160 [D loss: 0.696174, acc: 41.02%] [G loss: 1.391579]\n",
            "5180 [D loss: 0.698177, acc: 42.58%] [G loss: 1.384865]\n",
            "5200 [D loss: 0.699822, acc: 38.67%] [G loss: 1.388312]\n",
            "5220 [D loss: 0.702582, acc: 42.97%] [G loss: 1.380040]\n",
            "5240 [D loss: 0.701013, acc: 41.41%] [G loss: 1.386951]\n",
            "5260 [D loss: 0.699409, acc: 44.92%] [G loss: 1.392558]\n",
            "5280 [D loss: 0.695582, acc: 42.97%] [G loss: 1.396441]\n",
            "5300 [D loss: 0.697259, acc: 41.80%] [G loss: 1.397173]\n",
            "5320 [D loss: 0.698717, acc: 41.80%] [G loss: 1.397144]\n",
            "5340 [D loss: 0.695936, acc: 49.22%] [G loss: 1.393483]\n",
            "5360 [D loss: 0.699023, acc: 42.19%] [G loss: 1.393164]\n",
            "5380 [D loss: 0.699289, acc: 46.88%] [G loss: 1.388252]\n",
            "5400 [D loss: 0.696760, acc: 46.09%] [G loss: 1.399884]\n",
            "5420 [D loss: 0.699193, acc: 40.23%] [G loss: 1.389961]\n",
            "5440 [D loss: 0.697815, acc: 45.31%] [G loss: 1.390960]\n",
            "5460 [D loss: 0.700058, acc: 40.23%] [G loss: 1.384074]\n",
            "5480 [D loss: 0.699360, acc: 42.19%] [G loss: 1.387544]\n",
            "5500 [D loss: 0.700749, acc: 42.19%] [G loss: 1.387971]\n",
            "5520 [D loss: 0.698616, acc: 43.36%] [G loss: 1.394937]\n",
            "5540 [D loss: 0.698276, acc: 42.58%] [G loss: 1.389883]\n",
            "5560 [D loss: 0.699736, acc: 42.19%] [G loss: 1.392909]\n",
            "5580 [D loss: 0.700095, acc: 45.31%] [G loss: 1.388195]\n",
            "5600 [D loss: 1.050589, acc: 73.44%] [G loss: 7.483686]\n",
            "5620 [D loss: 1.134697, acc: 47.27%] [G loss: 3.470668]\n",
            "5640 [D loss: 1.031423, acc: 46.48%] [G loss: 2.857363]\n",
            "5660 [D loss: 1.065826, acc: 41.80%] [G loss: 2.338554]\n",
            "5680 [D loss: 0.832539, acc: 50.00%] [G loss: 2.296362]\n",
            "5700 [D loss: 0.939199, acc: 50.78%] [G loss: 2.285428]\n",
            "5720 [D loss: 0.897361, acc: 46.48%] [G loss: 2.175857]\n",
            "5740 [D loss: 0.856614, acc: 48.83%] [G loss: 1.866068]\n",
            "5760 [D loss: 0.959544, acc: 46.88%] [G loss: 1.868628]\n",
            "5780 [D loss: 0.889096, acc: 46.88%] [G loss: 1.644984]\n",
            "5800 [D loss: 0.860415, acc: 50.78%] [G loss: 1.830389]\n",
            "5820 [D loss: 0.808518, acc: 49.22%] [G loss: 1.763650]\n",
            "5840 [D loss: 0.852626, acc: 44.53%] [G loss: 1.727783]\n",
            "5860 [D loss: 0.801201, acc: 46.09%] [G loss: 1.551765]\n",
            "5880 [D loss: 0.863518, acc: 45.70%] [G loss: 1.698072]\n",
            "5900 [D loss: 0.812210, acc: 51.95%] [G loss: 1.567446]\n",
            "5920 [D loss: 0.783953, acc: 49.61%] [G loss: 1.835632]\n",
            "5940 [D loss: 0.832860, acc: 45.70%] [G loss: 1.638766]\n",
            "5960 [D loss: 0.801228, acc: 46.88%] [G loss: 1.463499]\n",
            "5980 [D loss: 0.799769, acc: 48.44%] [G loss: 1.500624]\n",
            "6000 [D loss: 0.780516, acc: 51.17%] [G loss: 1.544451]\n",
            "6020 [D loss: 0.765514, acc: 52.34%] [G loss: 1.545433]\n",
            "6040 [D loss: 0.770213, acc: 46.88%] [G loss: 1.594628]\n",
            "6060 [D loss: 0.833839, acc: 43.36%] [G loss: 1.572944]\n",
            "6080 [D loss: 0.742587, acc: 51.56%] [G loss: 1.545426]\n",
            "6100 [D loss: 0.759506, acc: 46.88%] [G loss: 1.551704]\n",
            "6120 [D loss: 0.792394, acc: 45.70%] [G loss: 1.462799]\n",
            "6140 [D loss: 0.764152, acc: 47.66%] [G loss: 1.553686]\n",
            "6160 [D loss: 0.831054, acc: 41.41%] [G loss: 1.455667]\n",
            "6180 [D loss: 0.767147, acc: 46.88%] [G loss: 1.514049]\n",
            "6200 [D loss: 0.734886, acc: 48.83%] [G loss: 1.460184]\n",
            "6220 [D loss: 0.779273, acc: 48.05%] [G loss: 1.583347]\n",
            "6240 [D loss: 0.736696, acc: 47.66%] [G loss: 1.506561]\n",
            "6260 [D loss: 0.826555, acc: 44.92%] [G loss: 1.500106]\n",
            "6280 [D loss: 0.767121, acc: 49.22%] [G loss: 1.520574]\n",
            "6300 [D loss: 0.804251, acc: 41.80%] [G loss: 1.464317]\n",
            "6320 [D loss: 0.716252, acc: 50.39%] [G loss: 1.505134]\n",
            "6340 [D loss: 0.787919, acc: 41.02%] [G loss: 1.401156]\n",
            "6360 [D loss: 0.732048, acc: 48.05%] [G loss: 1.451977]\n",
            "6380 [D loss: 0.752190, acc: 48.44%] [G loss: 1.467764]\n",
            "6400 [D loss: 0.752781, acc: 44.92%] [G loss: 1.463686]\n",
            "6420 [D loss: 0.771133, acc: 43.36%] [G loss: 1.492536]\n",
            "6440 [D loss: 0.738685, acc: 46.48%] [G loss: 1.428788]\n",
            "6460 [D loss: 0.742203, acc: 44.92%] [G loss: 1.442978]\n",
            "6480 [D loss: 0.772447, acc: 41.41%] [G loss: 1.481427]\n",
            "6500 [D loss: 0.743265, acc: 48.44%] [G loss: 1.435808]\n",
            "6520 [D loss: 0.718966, acc: 45.70%] [G loss: 1.427877]\n",
            "6540 [D loss: 0.744072, acc: 46.09%] [G loss: 1.480813]\n",
            "6560 [D loss: 0.734644, acc: 46.88%] [G loss: 1.445516]\n",
            "6580 [D loss: 0.740332, acc: 44.53%] [G loss: 1.458553]\n",
            "6600 [D loss: 0.738846, acc: 44.53%] [G loss: 1.487016]\n",
            "6620 [D loss: 0.733865, acc: 45.31%] [G loss: 1.429709]\n",
            "6640 [D loss: 0.748050, acc: 42.19%] [G loss: 1.428452]\n",
            "6660 [D loss: 0.740390, acc: 43.75%] [G loss: 1.404577]\n",
            "6680 [D loss: 0.731079, acc: 48.44%] [G loss: 1.428233]\n",
            "6700 [D loss: 0.703379, acc: 52.73%] [G loss: 1.442016]\n",
            "6720 [D loss: 0.726823, acc: 48.44%] [G loss: 1.454689]\n",
            "6740 [D loss: 0.703712, acc: 49.22%] [G loss: 1.418900]\n",
            "6760 [D loss: 0.728133, acc: 48.44%] [G loss: 1.416880]\n",
            "6780 [D loss: 0.735982, acc: 37.89%] [G loss: 1.405231]\n",
            "6800 [D loss: 0.721156, acc: 45.70%] [G loss: 1.398946]\n",
            "6820 [D loss: 0.741699, acc: 40.23%] [G loss: 1.432983]\n",
            "6840 [D loss: 0.728775, acc: 41.80%] [G loss: 1.435222]\n",
            "6860 [D loss: 0.731342, acc: 43.36%] [G loss: 1.412943]\n",
            "6880 [D loss: 0.726122, acc: 44.92%] [G loss: 1.379346]\n",
            "6900 [D loss: 0.720776, acc: 47.27%] [G loss: 1.445342]\n",
            "6920 [D loss: 0.730780, acc: 43.36%] [G loss: 1.406773]\n",
            "6940 [D loss: 0.726643, acc: 46.88%] [G loss: 1.435222]\n",
            "6960 [D loss: 0.720901, acc: 46.48%] [G loss: 1.415220]\n",
            "6980 [D loss: 0.720028, acc: 44.14%] [G loss: 1.394911]\n",
            "7000 [D loss: 0.710074, acc: 43.36%] [G loss: 1.415535]\n",
            "7020 [D loss: 0.714248, acc: 46.48%] [G loss: 1.380897]\n",
            "7040 [D loss: 0.690431, acc: 57.03%] [G loss: 1.496330]\n",
            "7060 [D loss: 0.713263, acc: 47.66%] [G loss: 1.458662]\n",
            "7080 [D loss: 0.706815, acc: 48.83%] [G loss: 1.386963]\n",
            "7100 [D loss: 0.712212, acc: 46.88%] [G loss: 1.393740]\n",
            "7120 [D loss: 0.711431, acc: 45.70%] [G loss: 1.411185]\n",
            "7140 [D loss: 0.734985, acc: 41.02%] [G loss: 1.410330]\n",
            "7160 [D loss: 0.705719, acc: 46.48%] [G loss: 1.405355]\n",
            "7180 [D loss: 0.713936, acc: 45.70%] [G loss: 1.407329]\n",
            "7200 [D loss: 0.709879, acc: 46.88%] [G loss: 1.423583]\n",
            "7220 [D loss: 0.699547, acc: 50.39%] [G loss: 1.403541]\n",
            "7240 [D loss: 0.704672, acc: 48.83%] [G loss: 1.430713]\n",
            "7260 [D loss: 0.706194, acc: 46.48%] [G loss: 1.377164]\n",
            "7280 [D loss: 0.698153, acc: 54.30%] [G loss: 1.396158]\n",
            "7300 [D loss: 0.702772, acc: 49.22%] [G loss: 1.406029]\n",
            "7320 [D loss: 0.711130, acc: 45.70%] [G loss: 1.415400]\n",
            "7340 [D loss: 0.700456, acc: 49.22%] [G loss: 1.410841]\n",
            "7360 [D loss: 0.719118, acc: 45.31%] [G loss: 1.416542]\n",
            "7380 [D loss: 0.712992, acc: 46.09%] [G loss: 1.401296]\n",
            "7400 [D loss: 0.718274, acc: 42.97%] [G loss: 1.389513]\n",
            "7420 [D loss: 0.717462, acc: 44.53%] [G loss: 1.390724]\n",
            "7440 [D loss: 0.695057, acc: 51.17%] [G loss: 1.396656]\n",
            "7460 [D loss: 0.713559, acc: 45.70%] [G loss: 1.401518]\n",
            "7480 [D loss: 0.701872, acc: 47.27%] [G loss: 1.393741]\n",
            "7500 [D loss: 0.708051, acc: 43.75%] [G loss: 1.378255]\n",
            "7520 [D loss: 0.704105, acc: 46.09%] [G loss: 1.401780]\n",
            "7540 [D loss: 0.716191, acc: 43.75%] [G loss: 1.410371]\n",
            "7560 [D loss: 0.701567, acc: 46.88%] [G loss: 1.398791]\n",
            "7580 [D loss: 0.710555, acc: 47.27%] [G loss: 1.395697]\n",
            "7600 [D loss: 0.685136, acc: 53.12%] [G loss: 1.548317]\n",
            "7620 [D loss: 0.709787, acc: 46.48%] [G loss: 1.392236]\n",
            "7640 [D loss: 0.707398, acc: 44.14%] [G loss: 1.402847]\n",
            "7660 [D loss: 0.697335, acc: 53.12%] [G loss: 1.394188]\n",
            "7680 [D loss: 0.702555, acc: 44.14%] [G loss: 1.390167]\n",
            "7700 [D loss: 0.699811, acc: 48.05%] [G loss: 1.420194]\n",
            "7720 [D loss: 0.703320, acc: 47.27%] [G loss: 1.388322]\n",
            "7740 [D loss: 0.705044, acc: 46.09%] [G loss: 1.392083]\n",
            "7760 [D loss: 0.699598, acc: 49.61%] [G loss: 1.406473]\n",
            "7780 [D loss: 0.701549, acc: 49.22%] [G loss: 1.392240]\n",
            "7800 [D loss: 0.706040, acc: 44.14%] [G loss: 1.400862]\n",
            "7820 [D loss: 0.707983, acc: 44.14%] [G loss: 1.391711]\n",
            "7840 [D loss: 0.701970, acc: 48.83%] [G loss: 1.406689]\n",
            "7860 [D loss: 0.702213, acc: 46.88%] [G loss: 1.406653]\n",
            "7880 [D loss: 0.693917, acc: 53.91%] [G loss: 1.408709]\n",
            "7900 [D loss: 0.702048, acc: 48.83%] [G loss: 1.410681]\n",
            "7920 [D loss: 0.618094, acc: 63.67%] [G loss: 1.982400]\n",
            "7940 [D loss: 0.709950, acc: 48.44%] [G loss: 1.416033]\n",
            "7960 [D loss: 0.703261, acc: 46.88%] [G loss: 1.402363]\n",
            "7980 [D loss: 0.697557, acc: 49.22%] [G loss: 1.372513]\n",
            "8000 [D loss: 0.695436, acc: 50.78%] [G loss: 1.400564]\n",
            "8020 [D loss: 0.705355, acc: 47.66%] [G loss: 1.395414]\n",
            "8040 [D loss: 0.695073, acc: 49.61%] [G loss: 1.394620]\n",
            "8060 [D loss: 0.708773, acc: 42.97%] [G loss: 1.401715]\n",
            "8080 [D loss: 0.706856, acc: 50.00%] [G loss: 1.384795]\n",
            "8100 [D loss: 0.696894, acc: 46.88%] [G loss: 1.411603]\n",
            "8120 [D loss: 0.818865, acc: 59.38%] [G loss: 2.962366]\n",
            "8140 [D loss: 0.780119, acc: 43.36%] [G loss: 1.458504]\n",
            "8160 [D loss: 0.711434, acc: 46.09%] [G loss: 1.414560]\n",
            "8180 [D loss: 0.709809, acc: 46.88%] [G loss: 1.414253]\n",
            "8200 [D loss: 0.714417, acc: 44.14%] [G loss: 1.388663]\n",
            "8220 [D loss: 0.709156, acc: 42.97%] [G loss: 1.417096]\n",
            "8240 [D loss: 0.706237, acc: 44.92%] [G loss: 1.394935]\n",
            "8260 [D loss: 0.701845, acc: 48.05%] [G loss: 1.374937]\n",
            "8280 [D loss: 0.702603, acc: 44.92%] [G loss: 1.397663]\n",
            "8300 [D loss: 0.696355, acc: 49.22%] [G loss: 1.405213]\n",
            "8320 [D loss: 0.698446, acc: 49.61%] [G loss: 1.385447]\n",
            "8340 [D loss: 0.700195, acc: 48.05%] [G loss: 1.409266]\n",
            "8360 [D loss: 0.697438, acc: 47.27%] [G loss: 1.386397]\n",
            "8380 [D loss: 0.705878, acc: 48.44%] [G loss: 1.396880]\n",
            "8400 [D loss: 0.692440, acc: 52.73%] [G loss: 1.470229]\n",
            "8420 [D loss: 0.697846, acc: 47.27%] [G loss: 1.378325]\n",
            "8440 [D loss: 0.701036, acc: 43.75%] [G loss: 1.395482]\n",
            "8460 [D loss: 0.696205, acc: 50.00%] [G loss: 1.386346]\n",
            "8480 [D loss: 0.695886, acc: 49.22%] [G loss: 1.393741]\n",
            "8500 [D loss: 0.702785, acc: 46.09%] [G loss: 1.402672]\n",
            "8520 [D loss: 0.698282, acc: 46.48%] [G loss: 1.400788]\n",
            "8540 [D loss: 0.698781, acc: 49.22%] [G loss: 1.403211]\n",
            "8560 [D loss: 0.697332, acc: 46.09%] [G loss: 1.380472]\n",
            "8580 [D loss: 0.698111, acc: 49.61%] [G loss: 1.393615]\n",
            "8600 [D loss: 0.705493, acc: 39.84%] [G loss: 1.396782]\n",
            "8620 [D loss: 0.717898, acc: 41.80%] [G loss: 1.412862]\n",
            "8640 [D loss: 0.700841, acc: 47.27%] [G loss: 1.387784]\n",
            "8660 [D loss: 0.703626, acc: 42.58%] [G loss: 1.385221]\n",
            "8680 [D loss: 0.718067, acc: 43.75%] [G loss: 1.375402]\n",
            "8700 [D loss: 0.701614, acc: 47.66%] [G loss: 1.393435]\n",
            "8720 [D loss: 0.705420, acc: 40.62%] [G loss: 1.386048]\n",
            "8740 [D loss: 0.695162, acc: 48.44%] [G loss: 1.386013]\n",
            "8760 [D loss: 0.696754, acc: 48.44%] [G loss: 1.382574]\n",
            "8780 [D loss: 0.717885, acc: 46.09%] [G loss: 1.412228]\n",
            "8800 [D loss: 0.705491, acc: 42.97%] [G loss: 1.397816]\n",
            "8820 [D loss: 0.700897, acc: 48.05%] [G loss: 1.394929]\n",
            "8840 [D loss: 0.696522, acc: 49.61%] [G loss: 1.401807]\n",
            "8860 [D loss: 0.696355, acc: 49.22%] [G loss: 1.393223]\n",
            "8880 [D loss: 0.692898, acc: 50.78%] [G loss: 1.394501]\n",
            "8900 [D loss: 0.702881, acc: 44.53%] [G loss: 1.391823]\n",
            "8920 [D loss: 0.694348, acc: 51.56%] [G loss: 1.413374]\n",
            "8940 [D loss: 0.698436, acc: 47.27%] [G loss: 1.394930]\n",
            "8960 [D loss: 0.696699, acc: 53.91%] [G loss: 1.381251]\n",
            "8980 [D loss: 0.695579, acc: 48.83%] [G loss: 1.396661]\n",
            "9000 [D loss: 0.708796, acc: 42.97%] [G loss: 1.396927]\n",
            "9020 [D loss: 0.695187, acc: 48.83%] [G loss: 1.396678]\n",
            "9040 [D loss: 0.696593, acc: 47.66%] [G loss: 1.392871]\n",
            "9060 [D loss: 0.697844, acc: 44.92%] [G loss: 1.397083]\n",
            "9080 [D loss: 0.698640, acc: 44.53%] [G loss: 1.394329]\n",
            "9100 [D loss: 0.699132, acc: 48.05%] [G loss: 1.396597]\n",
            "9120 [D loss: 0.695928, acc: 50.00%] [G loss: 1.397929]\n",
            "9140 [D loss: 0.700501, acc: 43.75%] [G loss: 1.394352]\n",
            "9160 [D loss: 0.698991, acc: 47.66%] [G loss: 1.390461]\n",
            "9180 [D loss: 0.701179, acc: 45.31%] [G loss: 1.388238]\n",
            "9200 [D loss: 0.696235, acc: 49.61%] [G loss: 1.396402]\n",
            "9220 [D loss: 0.696740, acc: 50.39%] [G loss: 1.404562]\n",
            "9240 [D loss: 0.695299, acc: 49.61%] [G loss: 1.400445]\n",
            "9260 [D loss: 0.696488, acc: 45.31%] [G loss: 1.391697]\n",
            "9280 [D loss: 0.703540, acc: 43.75%] [G loss: 1.387187]\n",
            "9300 [D loss: 0.701139, acc: 46.09%] [G loss: 1.401868]\n",
            "9320 [D loss: 0.694560, acc: 51.17%] [G loss: 1.383876]\n",
            "9340 [D loss: 0.691535, acc: 50.00%] [G loss: 1.400494]\n",
            "9360 [D loss: 0.700724, acc: 46.09%] [G loss: 1.387792]\n",
            "9380 [D loss: 0.694327, acc: 49.22%] [G loss: 1.386031]\n",
            "9400 [D loss: 0.735973, acc: 49.61%] [G loss: 1.406922]\n",
            "9420 [D loss: 0.700288, acc: 46.88%] [G loss: 1.392952]\n",
            "9440 [D loss: 0.694255, acc: 48.44%] [G loss: 1.391889]\n",
            "9460 [D loss: 0.804263, acc: 39.06%] [G loss: 1.512069]\n",
            "9480 [D loss: 0.705744, acc: 46.09%] [G loss: 1.406349]\n",
            "9500 [D loss: 0.730994, acc: 49.22%] [G loss: 1.392425]\n",
            "9520 [D loss: 0.736789, acc: 41.80%] [G loss: 1.426805]\n",
            "9540 [D loss: 0.707508, acc: 41.41%] [G loss: 1.404909]\n",
            "9560 [D loss: 0.694009, acc: 51.56%] [G loss: 1.395297]\n",
            "9580 [D loss: 0.697846, acc: 44.14%] [G loss: 1.380668]\n",
            "9600 [D loss: 0.706832, acc: 41.41%] [G loss: 1.387038]\n",
            "9620 [D loss: 0.700673, acc: 46.88%] [G loss: 1.418723]\n",
            "9640 [D loss: 0.701814, acc: 44.92%] [G loss: 1.394129]\n",
            "9660 [D loss: 0.694758, acc: 47.66%] [G loss: 1.408175]\n",
            "9680 [D loss: 0.693714, acc: 50.00%] [G loss: 1.399145]\n",
            "9700 [D loss: 0.694459, acc: 46.88%] [G loss: 1.396380]\n",
            "9720 [D loss: 0.702367, acc: 41.02%] [G loss: 1.389230]\n",
            "9740 [D loss: 0.701129, acc: 43.36%] [G loss: 1.393244]\n",
            "9760 [D loss: 0.698766, acc: 44.92%] [G loss: 1.391882]\n",
            "9780 [D loss: 0.692445, acc: 52.73%] [G loss: 1.397025]\n",
            "9800 [D loss: 0.701768, acc: 46.48%] [G loss: 1.380418]\n",
            "9820 [D loss: 0.695255, acc: 51.17%] [G loss: 1.391512]\n",
            "9840 [D loss: 0.700738, acc: 41.41%] [G loss: 1.387588]\n",
            "9860 [D loss: 0.695515, acc: 50.39%] [G loss: 1.393852]\n",
            "9880 [D loss: 0.698513, acc: 45.31%] [G loss: 1.394606]\n",
            "9900 [D loss: 0.701560, acc: 44.14%] [G loss: 1.380055]\n",
            "9920 [D loss: 0.701843, acc: 45.31%] [G loss: 1.385154]\n",
            "9940 [D loss: 0.697981, acc: 46.09%] [G loss: 1.386593]\n",
            "9960 [D loss: 0.697205, acc: 48.05%] [G loss: 1.393810]\n",
            "9980 [D loss: 0.694444, acc: 50.39%] [G loss: 1.387525]\n",
            "10000 [D loss: 0.697952, acc: 43.75%] [G loss: 1.388004]\n",
            "10020 [D loss: 0.699222, acc: 44.92%] [G loss: 1.399125]\n",
            "10040 [D loss: 0.708389, acc: 36.33%] [G loss: 1.390657]\n",
            "10060 [D loss: 0.701348, acc: 50.00%] [G loss: 1.396301]\n",
            "10080 [D loss: 0.700799, acc: 41.80%] [G loss: 1.387338]\n",
            "10100 [D loss: 0.697825, acc: 46.48%] [G loss: 1.396404]\n",
            "10120 [D loss: 0.717262, acc: 40.23%] [G loss: 1.385130]\n",
            "10140 [D loss: 0.701188, acc: 47.27%] [G loss: 1.384079]\n",
            "10160 [D loss: 0.697678, acc: 42.97%] [G loss: 1.388765]\n",
            "10180 [D loss: 0.694021, acc: 49.22%] [G loss: 1.393576]\n",
            "10200 [D loss: 0.694161, acc: 48.83%] [G loss: 1.390934]\n",
            "10220 [D loss: 0.698777, acc: 43.75%] [G loss: 1.394608]\n",
            "10240 [D loss: 0.808237, acc: 41.02%] [G loss: 1.875466]\n",
            "10260 [D loss: 0.754498, acc: 44.92%] [G loss: 1.542306]\n",
            "10280 [D loss: 0.724787, acc: 46.88%] [G loss: 1.460880]\n",
            "10300 [D loss: 0.734570, acc: 41.80%] [G loss: 1.467437]\n",
            "10320 [D loss: 0.716259, acc: 45.31%] [G loss: 1.443000]\n",
            "10340 [D loss: 0.724231, acc: 43.75%] [G loss: 1.411722]\n",
            "10360 [D loss: 0.702750, acc: 50.00%] [G loss: 1.442160]\n",
            "10380 [D loss: 0.700741, acc: 47.66%] [G loss: 1.424971]\n",
            "10400 [D loss: 0.728491, acc: 38.28%] [G loss: 1.411342]\n",
            "10420 [D loss: 0.721917, acc: 41.02%] [G loss: 1.395700]\n",
            "10440 [D loss: 0.707746, acc: 45.70%] [G loss: 1.410642]\n",
            "10460 [D loss: 0.713922, acc: 41.80%] [G loss: 1.397780]\n",
            "10480 [D loss: 0.713005, acc: 45.70%] [G loss: 1.407548]\n",
            "10500 [D loss: 0.712032, acc: 42.19%] [G loss: 1.394986]\n",
            "10520 [D loss: 0.705400, acc: 47.27%] [G loss: 1.398283]\n",
            "10540 [D loss: 0.713039, acc: 43.36%] [G loss: 1.407226]\n",
            "10560 [D loss: 0.706539, acc: 45.31%] [G loss: 1.400814]\n",
            "10580 [D loss: 0.698333, acc: 45.70%] [G loss: 1.395849]\n",
            "10600 [D loss: 0.704760, acc: 43.75%] [G loss: 1.395391]\n",
            "10620 [D loss: 0.703292, acc: 45.70%] [G loss: 1.391940]\n",
            "10640 [D loss: 0.702226, acc: 44.92%] [G loss: 1.383307]\n",
            "10660 [D loss: 0.702438, acc: 44.92%] [G loss: 1.388349]\n",
            "10680 [D loss: 0.701258, acc: 47.66%] [G loss: 1.390599]\n",
            "10700 [D loss: 0.701158, acc: 46.09%] [G loss: 1.397273]\n",
            "10720 [D loss: 0.701247, acc: 46.09%] [G loss: 1.396348]\n",
            "10740 [D loss: 0.703728, acc: 48.05%] [G loss: 1.389283]\n",
            "10760 [D loss: 0.700170, acc: 50.00%] [G loss: 1.419275]\n",
            "10780 [D loss: 0.708857, acc: 42.19%] [G loss: 1.397586]\n",
            "10800 [D loss: 0.697596, acc: 52.34%] [G loss: 1.395420]\n",
            "10820 [D loss: 0.704002, acc: 42.19%] [G loss: 1.394539]\n",
            "10840 [D loss: 0.697913, acc: 48.05%] [G loss: 1.388965]\n",
            "10860 [D loss: 0.701374, acc: 46.09%] [G loss: 1.396715]\n",
            "10880 [D loss: 0.697447, acc: 47.27%] [G loss: 1.397938]\n",
            "10900 [D loss: 0.696356, acc: 50.78%] [G loss: 1.391453]\n",
            "10920 [D loss: 0.694553, acc: 44.92%] [G loss: 1.403613]\n",
            "10940 [D loss: 0.695872, acc: 46.48%] [G loss: 1.397800]\n",
            "10960 [D loss: 0.700882, acc: 43.75%] [G loss: 1.397851]\n",
            "10980 [D loss: 0.698720, acc: 48.44%] [G loss: 1.389962]\n",
            "11000 [D loss: 0.696832, acc: 52.73%] [G loss: 1.401066]\n",
            "11020 [D loss: 0.700879, acc: 48.05%] [G loss: 1.391254]\n",
            "11040 [D loss: 0.702686, acc: 47.66%] [G loss: 1.393748]\n",
            "11060 [D loss: 0.701982, acc: 42.19%] [G loss: 1.401140]\n",
            "11080 [D loss: 0.689376, acc: 53.12%] [G loss: 1.396762]\n",
            "11100 [D loss: 0.702174, acc: 48.05%] [G loss: 1.384270]\n",
            "11120 [D loss: 0.695336, acc: 48.05%] [G loss: 1.389761]\n",
            "11140 [D loss: 0.693712, acc: 46.48%] [G loss: 1.396380]\n",
            "11160 [D loss: 0.698972, acc: 47.27%] [G loss: 1.379241]\n",
            "11180 [D loss: 0.702167, acc: 47.66%] [G loss: 1.385317]\n",
            "11200 [D loss: 0.697229, acc: 48.44%] [G loss: 1.379604]\n",
            "11220 [D loss: 0.695360, acc: 50.78%] [G loss: 1.405114]\n",
            "11240 [D loss: 0.700082, acc: 44.92%] [G loss: 1.393892]\n",
            "11260 [D loss: 0.692497, acc: 50.39%] [G loss: 1.393592]\n",
            "11280 [D loss: 0.697755, acc: 48.44%] [G loss: 1.395503]\n",
            "11300 [D loss: 0.696326, acc: 46.88%] [G loss: 1.395800]\n",
            "11320 [D loss: 0.701196, acc: 42.97%] [G loss: 1.391422]\n",
            "11340 [D loss: 0.691213, acc: 53.52%] [G loss: 1.390470]\n",
            "11360 [D loss: 0.696598, acc: 48.05%] [G loss: 1.381474]\n",
            "11380 [D loss: 0.694536, acc: 50.78%] [G loss: 1.402830]\n",
            "11400 [D loss: 0.696366, acc: 48.44%] [G loss: 1.393563]\n",
            "11420 [D loss: 0.702710, acc: 41.02%] [G loss: 1.386028]\n",
            "11440 [D loss: 0.700201, acc: 41.41%] [G loss: 1.394846]\n",
            "11460 [D loss: 0.699304, acc: 42.97%] [G loss: 1.394538]\n",
            "11480 [D loss: 0.704165, acc: 44.92%] [G loss: 1.388405]\n",
            "11500 [D loss: 0.700302, acc: 44.92%] [G loss: 1.389202]\n",
            "11520 [D loss: 0.698328, acc: 47.27%] [G loss: 1.395204]\n",
            "11540 [D loss: 0.698544, acc: 46.09%] [G loss: 1.392834]\n",
            "11560 [D loss: 0.695312, acc: 46.09%] [G loss: 1.392195]\n",
            "11580 [D loss: 0.698662, acc: 44.53%] [G loss: 1.389476]\n",
            "11600 [D loss: 0.695925, acc: 50.00%] [G loss: 1.393031]\n",
            "11620 [D loss: 0.695967, acc: 46.48%] [G loss: 1.385694]\n",
            "11640 [D loss: 0.698999, acc: 44.92%] [G loss: 1.385721]\n",
            "11660 [D loss: 0.700877, acc: 43.75%] [G loss: 1.385072]\n",
            "11680 [D loss: 0.699692, acc: 44.53%] [G loss: 1.394048]\n",
            "11700 [D loss: 0.695676, acc: 49.61%] [G loss: 1.384003]\n",
            "11720 [D loss: 0.694390, acc: 48.44%] [G loss: 1.389819]\n",
            "11740 [D loss: 0.699410, acc: 44.14%] [G loss: 1.385535]\n",
            "11760 [D loss: 0.692602, acc: 49.61%] [G loss: 1.392770]\n",
            "11780 [D loss: 0.694024, acc: 51.95%] [G loss: 1.388855]\n",
            "11800 [D loss: 0.696169, acc: 46.88%] [G loss: 1.381485]\n",
            "11820 [D loss: 0.700022, acc: 43.75%] [G loss: 1.395918]\n",
            "11840 [D loss: 0.701612, acc: 39.06%] [G loss: 1.395573]\n",
            "11860 [D loss: 0.693334, acc: 51.56%] [G loss: 1.388242]\n",
            "11880 [D loss: 0.698562, acc: 46.88%] [G loss: 1.385448]\n",
            "11900 [D loss: 0.699257, acc: 44.14%] [G loss: 1.384377]\n",
            "11920 [D loss: 0.698091, acc: 43.36%] [G loss: 1.395306]\n",
            "11940 [D loss: 0.693186, acc: 49.61%] [G loss: 1.394538]\n",
            "11960 [D loss: 0.694467, acc: 49.61%] [G loss: 1.389479]\n",
            "11980 [D loss: 0.697643, acc: 43.75%] [G loss: 1.388091]\n",
            "12000 [D loss: 0.696534, acc: 46.88%] [G loss: 1.393485]\n",
            "12020 [D loss: 0.695193, acc: 44.14%] [G loss: 1.376846]\n",
            "12040 [D loss: 0.699802, acc: 41.41%] [G loss: 1.387500]\n",
            "12060 [D loss: 0.696176, acc: 46.48%] [G loss: 1.388738]\n",
            "12080 [D loss: 0.695067, acc: 48.83%] [G loss: 1.391193]\n",
            "12100 [D loss: 0.698012, acc: 46.88%] [G loss: 1.386442]\n",
            "12120 [D loss: 0.700543, acc: 44.53%] [G loss: 1.383322]\n",
            "12140 [D loss: 0.694064, acc: 48.05%] [G loss: 1.388987]\n",
            "12160 [D loss: 0.694401, acc: 48.44%] [G loss: 1.392480]\n",
            "12180 [D loss: 0.699194, acc: 37.11%] [G loss: 1.385832]\n",
            "12200 [D loss: 0.697534, acc: 44.14%] [G loss: 1.392863]\n",
            "12220 [D loss: 0.694463, acc: 46.48%] [G loss: 1.384839]\n",
            "12240 [D loss: 0.693930, acc: 46.48%] [G loss: 1.390956]\n",
            "12260 [D loss: 0.695245, acc: 48.83%] [G loss: 1.391536]\n",
            "12280 [D loss: 0.697625, acc: 44.92%] [G loss: 1.385728]\n",
            "12300 [D loss: 0.693172, acc: 53.12%] [G loss: 1.386253]\n",
            "12320 [D loss: 0.697426, acc: 43.36%] [G loss: 1.391673]\n",
            "12340 [D loss: 0.695522, acc: 46.88%] [G loss: 1.395440]\n",
            "12360 [D loss: 0.693307, acc: 50.78%] [G loss: 1.397268]\n",
            "12380 [D loss: 0.697973, acc: 44.92%] [G loss: 1.393326]\n",
            "12400 [D loss: 0.694376, acc: 48.83%] [G loss: 1.388747]\n",
            "12420 [D loss: 0.695540, acc: 50.00%] [G loss: 1.379657]\n",
            "12440 [D loss: 0.694788, acc: 46.09%] [G loss: 1.389481]\n",
            "12460 [D loss: 0.698180, acc: 46.09%] [G loss: 1.390275]\n",
            "12480 [D loss: 0.702287, acc: 35.94%] [G loss: 1.384638]\n",
            "12500 [D loss: 0.694812, acc: 50.78%] [G loss: 1.392821]\n",
            "12520 [D loss: 0.694555, acc: 50.00%] [G loss: 1.386980]\n",
            "12540 [D loss: 0.697452, acc: 44.92%] [G loss: 1.388188]\n",
            "12560 [D loss: 0.697430, acc: 42.97%] [G loss: 1.381798]\n",
            "12580 [D loss: 0.695853, acc: 47.27%] [G loss: 1.389896]\n",
            "12600 [D loss: 0.691830, acc: 51.95%] [G loss: 1.386745]\n",
            "12620 [D loss: 0.696218, acc: 49.61%] [G loss: 1.391582]\n",
            "12640 [D loss: 0.696241, acc: 42.58%] [G loss: 1.390027]\n",
            "12660 [D loss: 0.696870, acc: 47.27%] [G loss: 1.396888]\n",
            "12680 [D loss: 0.697025, acc: 44.14%] [G loss: 1.388694]\n",
            "12700 [D loss: 0.695275, acc: 48.05%] [G loss: 1.387546]\n",
            "12720 [D loss: 0.695837, acc: 50.39%] [G loss: 1.393849]\n",
            "12740 [D loss: 0.699105, acc: 43.75%] [G loss: 1.387469]\n",
            "12760 [D loss: 0.696321, acc: 45.70%] [G loss: 1.389573]\n",
            "12780 [D loss: 0.697239, acc: 45.70%] [G loss: 1.390011]\n",
            "12800 [D loss: 0.694989, acc: 45.31%] [G loss: 1.390081]\n",
            "12820 [D loss: 0.695220, acc: 43.75%] [G loss: 1.395841]\n",
            "12840 [D loss: 0.697970, acc: 43.36%] [G loss: 1.393013]\n",
            "12860 [D loss: 0.697999, acc: 44.53%] [G loss: 1.396616]\n",
            "12880 [D loss: 0.691649, acc: 52.73%] [G loss: 1.388693]\n",
            "12900 [D loss: 0.695328, acc: 45.70%] [G loss: 1.392442]\n",
            "12920 [D loss: 0.700166, acc: 41.02%] [G loss: 1.390614]\n",
            "12940 [D loss: 0.696089, acc: 44.14%] [G loss: 1.391769]\n",
            "12960 [D loss: 0.698630, acc: 39.06%] [G loss: 1.393254]\n",
            "12980 [D loss: 0.694528, acc: 49.61%] [G loss: 1.386251]\n",
            "13000 [D loss: 0.693296, acc: 50.00%] [G loss: 1.391753]\n",
            "13020 [D loss: 0.696285, acc: 47.27%] [G loss: 1.391080]\n",
            "13040 [D loss: 0.696089, acc: 48.83%] [G loss: 1.384987]\n",
            "13060 [D loss: 0.696614, acc: 44.92%] [G loss: 1.387872]\n",
            "13080 [D loss: 0.695602, acc: 46.88%] [G loss: 1.393874]\n",
            "13100 [D loss: 0.692922, acc: 50.00%] [G loss: 1.388457]\n",
            "13120 [D loss: 0.691485, acc: 51.95%] [G loss: 1.385224]\n",
            "13140 [D loss: 0.693261, acc: 45.70%] [G loss: 1.394195]\n",
            "13160 [D loss: 0.694380, acc: 46.48%] [G loss: 1.394940]\n",
            "13180 [D loss: 0.694779, acc: 46.09%] [G loss: 1.390781]\n",
            "13200 [D loss: 0.699602, acc: 39.06%] [G loss: 1.397763]\n",
            "13220 [D loss: 0.695107, acc: 47.66%] [G loss: 1.394569]\n",
            "13240 [D loss: 0.696449, acc: 46.48%] [G loss: 1.395959]\n",
            "13260 [D loss: 0.697427, acc: 43.36%] [G loss: 1.390307]\n",
            "13280 [D loss: 0.697602, acc: 47.66%] [G loss: 1.388087]\n",
            "13300 [D loss: 0.696843, acc: 42.58%] [G loss: 1.390447]\n",
            "13320 [D loss: 0.695555, acc: 47.27%] [G loss: 1.385855]\n",
            "13340 [D loss: 0.697619, acc: 42.19%] [G loss: 1.385114]\n",
            "13360 [D loss: 0.694336, acc: 46.88%] [G loss: 1.390984]\n",
            "13380 [D loss: 0.693889, acc: 48.83%] [G loss: 1.387780]\n",
            "13400 [D loss: 0.695408, acc: 51.56%] [G loss: 1.389631]\n",
            "13420 [D loss: 0.694859, acc: 47.66%] [G loss: 1.394720]\n",
            "13440 [D loss: 0.697046, acc: 45.70%] [G loss: 1.409801]\n",
            "13460 [D loss: 0.695270, acc: 42.97%] [G loss: 1.391870]\n",
            "13480 [D loss: 0.696345, acc: 43.36%] [G loss: 1.388217]\n",
            "13500 [D loss: 0.708975, acc: 45.70%] [G loss: 1.448634]\n",
            "13520 [D loss: 0.701068, acc: 43.75%] [G loss: 1.398372]\n",
            "13540 [D loss: 0.700074, acc: 39.84%] [G loss: 1.396510]\n",
            "13560 [D loss: 0.698899, acc: 38.67%] [G loss: 1.386934]\n",
            "13580 [D loss: 0.697532, acc: 46.09%] [G loss: 1.387794]\n",
            "13600 [D loss: 0.696504, acc: 41.41%] [G loss: 1.386946]\n",
            "13620 [D loss: 0.696920, acc: 41.80%] [G loss: 1.383126]\n",
            "13640 [D loss: 0.695441, acc: 50.00%] [G loss: 1.389786]\n",
            "13660 [D loss: 0.698194, acc: 48.44%] [G loss: 1.382051]\n",
            "13680 [D loss: 0.693659, acc: 45.70%] [G loss: 1.388131]\n",
            "13700 [D loss: 0.696244, acc: 45.70%] [G loss: 1.385980]\n",
            "13720 [D loss: 0.693734, acc: 48.83%] [G loss: 1.389609]\n",
            "13740 [D loss: 0.695840, acc: 45.31%] [G loss: 1.392127]\n",
            "13760 [D loss: 0.696216, acc: 48.05%] [G loss: 1.390305]\n",
            "13780 [D loss: 0.696916, acc: 46.88%] [G loss: 1.394950]\n",
            "13800 [D loss: 0.696082, acc: 48.05%] [G loss: 1.389555]\n",
            "13820 [D loss: 0.694693, acc: 44.53%] [G loss: 1.386952]\n",
            "13840 [D loss: 0.695579, acc: 45.70%] [G loss: 1.386873]\n",
            "13860 [D loss: 0.696571, acc: 48.44%] [G loss: 1.390077]\n",
            "13880 [D loss: 0.696769, acc: 44.92%] [G loss: 1.393476]\n",
            "13900 [D loss: 0.698100, acc: 43.75%] [G loss: 1.390824]\n",
            "13920 [D loss: 0.697221, acc: 38.67%] [G loss: 1.384203]\n",
            "13940 [D loss: 0.698458, acc: 42.58%] [G loss: 1.392384]\n",
            "13960 [D loss: 0.694434, acc: 45.70%] [G loss: 1.388583]\n",
            "13980 [D loss: 0.695089, acc: 49.61%] [G loss: 1.395859]\n",
            "14000 [D loss: 0.697731, acc: 44.14%] [G loss: 1.389433]\n",
            "14020 [D loss: 0.693814, acc: 49.22%] [G loss: 1.391491]\n",
            "14040 [D loss: 0.696009, acc: 46.88%] [G loss: 1.397629]\n",
            "14060 [D loss: 0.693666, acc: 50.00%] [G loss: 1.395832]\n",
            "14080 [D loss: 0.694137, acc: 50.78%] [G loss: 1.387049]\n",
            "14100 [D loss: 0.698030, acc: 44.53%] [G loss: 1.395072]\n",
            "14120 [D loss: 0.693784, acc: 43.75%] [G loss: 1.392398]\n",
            "14140 [D loss: 0.696119, acc: 48.44%] [G loss: 1.392954]\n",
            "14160 [D loss: 0.696286, acc: 41.80%] [G loss: 1.386809]\n",
            "14180 [D loss: 0.692788, acc: 47.66%] [G loss: 1.399354]\n",
            "14200 [D loss: 0.693084, acc: 51.56%] [G loss: 1.387900]\n",
            "14220 [D loss: 0.695092, acc: 47.66%] [G loss: 1.390198]\n",
            "14240 [D loss: 0.692715, acc: 50.00%] [G loss: 1.392884]\n",
            "14260 [D loss: 0.696584, acc: 41.02%] [G loss: 1.397968]\n",
            "14280 [D loss: 0.693137, acc: 47.27%] [G loss: 1.393090]\n",
            "14300 [D loss: 0.696114, acc: 44.92%] [G loss: 1.380974]\n",
            "14320 [D loss: 0.695509, acc: 45.31%] [G loss: 1.392064]\n",
            "14340 [D loss: 0.696393, acc: 45.70%] [G loss: 1.391222]\n",
            "14360 [D loss: 0.697989, acc: 42.19%] [G loss: 1.391418]\n",
            "14380 [D loss: 0.640699, acc: 58.59%] [G loss: 2.107584]\n",
            "14400 [D loss: 0.715626, acc: 47.27%] [G loss: 1.543835]\n",
            "14420 [D loss: 0.700130, acc: 49.22%] [G loss: 1.476583]\n",
            "14440 [D loss: 0.704599, acc: 40.23%] [G loss: 1.396616]\n",
            "14460 [D loss: 0.700422, acc: 42.58%] [G loss: 1.389928]\n",
            "14480 [D loss: 0.699533, acc: 41.41%] [G loss: 1.390488]\n",
            "14500 [D loss: 0.698176, acc: 40.62%] [G loss: 1.385675]\n",
            "14520 [D loss: 0.696608, acc: 44.53%] [G loss: 1.397529]\n",
            "14540 [D loss: 0.697636, acc: 46.88%] [G loss: 1.389760]\n",
            "14560 [D loss: 0.698850, acc: 42.58%] [G loss: 1.394189]\n",
            "14580 [D loss: 0.695987, acc: 46.09%] [G loss: 1.399894]\n",
            "14600 [D loss: 0.696820, acc: 42.58%] [G loss: 1.399331]\n",
            "14620 [D loss: 0.696037, acc: 44.53%] [G loss: 1.398587]\n",
            "14640 [D loss: 0.696214, acc: 46.88%] [G loss: 1.388074]\n",
            "14660 [D loss: 0.698581, acc: 42.97%] [G loss: 1.388862]\n",
            "14680 [D loss: 0.696385, acc: 42.19%] [G loss: 1.391831]\n",
            "14700 [D loss: 0.696573, acc: 46.48%] [G loss: 1.389490]\n",
            "14720 [D loss: 0.693543, acc: 51.17%] [G loss: 1.391607]\n",
            "14740 [D loss: 0.694091, acc: 50.78%] [G loss: 1.390618]\n",
            "14760 [D loss: 0.695698, acc: 46.09%] [G loss: 1.393259]\n",
            "14780 [D loss: 0.695689, acc: 43.75%] [G loss: 1.385543]\n",
            "14800 [D loss: 0.699026, acc: 39.06%] [G loss: 1.388615]\n",
            "14820 [D loss: 0.694816, acc: 48.05%] [G loss: 1.392961]\n",
            "14840 [D loss: 0.695488, acc: 43.75%] [G loss: 1.388508]\n",
            "14860 [D loss: 0.693396, acc: 46.09%] [G loss: 1.388156]\n",
            "14880 [D loss: 0.696343, acc: 43.36%] [G loss: 1.391435]\n",
            "14900 [D loss: 0.694303, acc: 46.48%] [G loss: 1.389219]\n",
            "14920 [D loss: 0.693498, acc: 46.88%] [G loss: 1.388097]\n",
            "14940 [D loss: 0.693439, acc: 50.00%] [G loss: 1.386031]\n",
            "14960 [D loss: 0.697974, acc: 44.53%] [G loss: 1.383845]\n",
            "14980 [D loss: 0.694668, acc: 45.70%] [G loss: 1.390656]\n",
            "15000 [D loss: 0.694924, acc: 46.48%] [G loss: 1.386826]\n",
            "15020 [D loss: 0.695230, acc: 44.14%] [G loss: 1.384455]\n",
            "15040 [D loss: 0.695063, acc: 43.75%] [G loss: 1.388628]\n",
            "15060 [D loss: 0.694065, acc: 45.31%] [G loss: 1.391517]\n",
            "15080 [D loss: 0.697149, acc: 42.97%] [G loss: 1.385040]\n",
            "15100 [D loss: 0.694217, acc: 47.27%] [G loss: 1.392219]\n",
            "15120 [D loss: 0.695084, acc: 50.00%] [G loss: 1.390409]\n",
            "15140 [D loss: 0.693757, acc: 51.56%] [G loss: 1.394928]\n",
            "15160 [D loss: 0.695618, acc: 43.36%] [G loss: 1.389054]\n",
            "15180 [D loss: 0.696234, acc: 44.92%] [G loss: 1.388000]\n",
            "15200 [D loss: 0.692922, acc: 48.44%] [G loss: 1.391147]\n",
            "15220 [D loss: 0.693087, acc: 49.22%] [G loss: 1.388641]\n",
            "15240 [D loss: 0.697825, acc: 45.70%] [G loss: 1.386262]\n",
            "15260 [D loss: 0.695225, acc: 48.44%] [G loss: 1.390188]\n",
            "15280 [D loss: 0.693544, acc: 48.83%] [G loss: 1.385615]\n",
            "15300 [D loss: 0.693492, acc: 49.61%] [G loss: 1.387343]\n",
            "15320 [D loss: 0.695573, acc: 48.83%] [G loss: 1.388698]\n",
            "15340 [D loss: 0.694521, acc: 49.61%] [G loss: 1.392192]\n",
            "15360 [D loss: 0.694694, acc: 44.92%] [G loss: 1.391733]\n",
            "15380 [D loss: 0.695370, acc: 42.58%] [G loss: 1.382409]\n",
            "15400 [D loss: 0.695111, acc: 48.44%] [G loss: 1.388776]\n",
            "15420 [D loss: 0.695961, acc: 42.97%] [G loss: 1.393737]\n",
            "15440 [D loss: 0.698073, acc: 41.80%] [G loss: 1.391570]\n",
            "15460 [D loss: 0.695288, acc: 42.19%] [G loss: 1.386838]\n",
            "15480 [D loss: 0.695044, acc: 43.75%] [G loss: 1.386709]\n",
            "15500 [D loss: 0.695911, acc: 42.97%] [G loss: 1.386500]\n",
            "15520 [D loss: 0.692734, acc: 50.00%] [G loss: 1.389323]\n",
            "15540 [D loss: 0.695679, acc: 41.80%] [G loss: 1.390733]\n",
            "15560 [D loss: 0.693871, acc: 49.22%] [G loss: 1.393907]\n",
            "15580 [D loss: 0.692212, acc: 50.78%] [G loss: 1.389584]\n",
            "15600 [D loss: 0.693248, acc: 51.95%] [G loss: 1.390080]\n",
            "15620 [D loss: 0.695989, acc: 42.19%] [G loss: 1.387951]\n",
            "15640 [D loss: 0.693726, acc: 48.83%] [G loss: 1.390092]\n",
            "15660 [D loss: 0.693577, acc: 48.05%] [G loss: 1.387882]\n",
            "15680 [D loss: 0.695025, acc: 42.97%] [G loss: 1.393509]\n",
            "15700 [D loss: 0.695338, acc: 43.75%] [G loss: 1.397700]\n",
            "15720 [D loss: 0.694489, acc: 45.70%] [G loss: 1.389511]\n",
            "15740 [D loss: 0.696395, acc: 41.80%] [G loss: 1.389723]\n",
            "15760 [D loss: 0.692056, acc: 46.48%] [G loss: 1.405541]\n",
            "15780 [D loss: 0.694780, acc: 48.05%] [G loss: 1.391070]\n",
            "15800 [D loss: 0.691581, acc: 49.61%] [G loss: 1.392053]\n",
            "15820 [D loss: 0.970697, acc: 59.38%] [G loss: 1.865178]\n",
            "15840 [D loss: 0.862947, acc: 42.58%] [G loss: 1.750333]\n",
            "15860 [D loss: 0.708518, acc: 52.34%] [G loss: 1.753131]\n",
            "15880 [D loss: 0.734240, acc: 44.92%] [G loss: 1.499303]\n",
            "15900 [D loss: 0.726337, acc: 42.58%] [G loss: 1.467930]\n",
            "15920 [D loss: 0.741721, acc: 34.38%] [G loss: 1.413317]\n",
            "15940 [D loss: 0.720112, acc: 40.62%] [G loss: 1.392678]\n",
            "15960 [D loss: 0.722828, acc: 41.02%] [G loss: 1.404014]\n",
            "15980 [D loss: 0.725652, acc: 37.89%] [G loss: 1.405605]\n",
            "16000 [D loss: 0.716861, acc: 39.06%] [G loss: 1.392472]\n",
            "16020 [D loss: 0.706091, acc: 42.97%] [G loss: 1.412670]\n",
            "16040 [D loss: 0.711580, acc: 41.02%] [G loss: 1.392106]\n",
            "16060 [D loss: 0.706727, acc: 43.75%] [G loss: 1.400824]\n",
            "16080 [D loss: 0.707315, acc: 40.62%] [G loss: 1.395436]\n",
            "16100 [D loss: 0.704156, acc: 41.80%] [G loss: 1.399236]\n",
            "16120 [D loss: 0.710332, acc: 41.41%] [G loss: 1.391586]\n",
            "16140 [D loss: 0.698983, acc: 47.27%] [G loss: 1.399818]\n",
            "16160 [D loss: 0.701340, acc: 42.58%] [G loss: 1.387330]\n",
            "16180 [D loss: 0.711399, acc: 38.28%] [G loss: 1.392768]\n",
            "16200 [D loss: 0.699249, acc: 43.75%] [G loss: 1.399235]\n",
            "16220 [D loss: 0.705406, acc: 42.97%] [G loss: 1.394752]\n",
            "16240 [D loss: 0.702226, acc: 43.36%] [G loss: 1.405688]\n",
            "16260 [D loss: 0.699066, acc: 44.53%] [G loss: 1.392927]\n",
            "16280 [D loss: 0.704076, acc: 40.62%] [G loss: 1.402648]\n",
            "16300 [D loss: 0.707416, acc: 39.84%] [G loss: 1.395976]\n",
            "16320 [D loss: 0.703458, acc: 40.62%] [G loss: 1.396775]\n",
            "16340 [D loss: 0.696273, acc: 46.88%] [G loss: 1.393969]\n",
            "16360 [D loss: 0.697630, acc: 50.78%] [G loss: 1.399883]\n",
            "16380 [D loss: 0.702629, acc: 44.53%] [G loss: 1.404285]\n",
            "16400 [D loss: 0.699394, acc: 42.97%] [G loss: 1.388319]\n",
            "16420 [D loss: 0.701331, acc: 41.02%] [G loss: 1.391128]\n",
            "16440 [D loss: 0.698234, acc: 48.05%] [G loss: 1.385195]\n",
            "16460 [D loss: 0.696459, acc: 47.66%] [G loss: 1.391600]\n",
            "16480 [D loss: 0.702520, acc: 40.62%] [G loss: 1.388496]\n",
            "16500 [D loss: 0.696411, acc: 44.92%] [G loss: 1.395237]\n",
            "16520 [D loss: 0.699760, acc: 45.31%] [G loss: 1.394999]\n",
            "16540 [D loss: 0.704404, acc: 38.28%] [G loss: 1.394161]\n",
            "16560 [D loss: 0.695582, acc: 48.83%] [G loss: 1.389612]\n",
            "16580 [D loss: 0.701079, acc: 41.02%] [G loss: 1.389617]\n",
            "16600 [D loss: 0.700906, acc: 43.36%] [G loss: 1.395993]\n",
            "16620 [D loss: 0.698835, acc: 44.92%] [G loss: 1.389227]\n",
            "16640 [D loss: 0.695651, acc: 46.88%] [G loss: 1.386930]\n",
            "16660 [D loss: 0.697784, acc: 47.66%] [G loss: 1.391045]\n",
            "16680 [D loss: 0.694782, acc: 46.88%] [G loss: 1.392104]\n",
            "16700 [D loss: 0.701876, acc: 37.11%] [G loss: 1.389742]\n",
            "16720 [D loss: 0.695739, acc: 46.48%] [G loss: 1.386902]\n",
            "16740 [D loss: 0.695220, acc: 50.00%] [G loss: 1.386804]\n",
            "16760 [D loss: 0.695770, acc: 46.09%] [G loss: 1.385907]\n",
            "16780 [D loss: 0.693959, acc: 47.66%] [G loss: 1.385634]\n",
            "16800 [D loss: 0.698745, acc: 41.02%] [G loss: 1.387795]\n",
            "16820 [D loss: 0.697384, acc: 44.14%] [G loss: 1.388370]\n",
            "16840 [D loss: 0.697920, acc: 43.36%] [G loss: 1.387168]\n",
            "16860 [D loss: 0.696695, acc: 44.14%] [G loss: 1.391030]\n",
            "16880 [D loss: 0.695324, acc: 49.61%] [G loss: 1.392841]\n",
            "16900 [D loss: 0.696258, acc: 48.05%] [G loss: 1.396451]\n",
            "16920 [D loss: 0.712875, acc: 43.36%] [G loss: 1.381891]\n",
            "16940 [D loss: 0.697602, acc: 43.36%] [G loss: 1.389568]\n",
            "16960 [D loss: 0.695148, acc: 51.17%] [G loss: 1.387446]\n",
            "16980 [D loss: 0.697949, acc: 44.14%] [G loss: 1.385411]\n",
            "17000 [D loss: 0.699110, acc: 45.70%] [G loss: 1.389910]\n",
            "17020 [D loss: 0.695077, acc: 46.09%] [G loss: 1.392305]\n",
            "17040 [D loss: 0.695648, acc: 46.09%] [G loss: 1.393473]\n",
            "17060 [D loss: 0.697410, acc: 43.36%] [G loss: 1.390273]\n",
            "17080 [D loss: 0.695124, acc: 46.09%] [G loss: 1.393784]\n",
            "17100 [D loss: 0.694941, acc: 48.05%] [G loss: 1.391623]\n",
            "17120 [D loss: 0.696874, acc: 45.31%] [G loss: 1.390966]\n",
            "17140 [D loss: 0.697466, acc: 44.14%] [G loss: 1.395265]\n",
            "17160 [D loss: 0.697130, acc: 46.88%] [G loss: 1.387051]\n",
            "17180 [D loss: 0.695688, acc: 45.31%] [G loss: 1.386196]\n",
            "17200 [D loss: 0.695103, acc: 46.48%] [G loss: 1.386169]\n",
            "17220 [D loss: 0.695914, acc: 48.44%] [G loss: 1.394449]\n",
            "17240 [D loss: 0.696747, acc: 47.27%] [G loss: 1.388442]\n",
            "17260 [D loss: 0.696034, acc: 43.36%] [G loss: 1.385311]\n",
            "17280 [D loss: 0.696709, acc: 46.09%] [G loss: 1.389900]\n",
            "17300 [D loss: 0.696788, acc: 46.48%] [G loss: 1.392075]\n",
            "17320 [D loss: 0.696530, acc: 44.92%] [G loss: 1.392338]\n",
            "17340 [D loss: 0.694676, acc: 48.44%] [G loss: 1.388005]\n",
            "17360 [D loss: 0.694048, acc: 46.88%] [G loss: 1.391176]\n",
            "17380 [D loss: 0.697082, acc: 43.36%] [G loss: 1.392897]\n",
            "17400 [D loss: 0.696208, acc: 42.97%] [G loss: 1.392400]\n",
            "17420 [D loss: 0.695627, acc: 41.80%] [G loss: 1.389637]\n",
            "17440 [D loss: 0.693637, acc: 45.31%] [G loss: 1.399056]\n",
            "17460 [D loss: 0.695562, acc: 46.09%] [G loss: 1.388926]\n",
            "17480 [D loss: 0.693759, acc: 43.36%] [G loss: 1.392155]\n",
            "17500 [D loss: 0.693863, acc: 46.48%] [G loss: 1.390537]\n",
            "17520 [D loss: 0.693224, acc: 51.95%] [G loss: 1.393734]\n",
            "17540 [D loss: 0.695145, acc: 46.88%] [G loss: 1.388664]\n",
            "17560 [D loss: 0.694284, acc: 46.48%] [G loss: 1.389147]\n",
            "17580 [D loss: 0.694304, acc: 45.70%] [G loss: 1.383531]\n",
            "17600 [D loss: 0.696818, acc: 39.06%] [G loss: 1.388849]\n",
            "17620 [D loss: 0.694081, acc: 46.09%] [G loss: 1.388694]\n",
            "17640 [D loss: 0.693670, acc: 48.44%] [G loss: 1.396574]\n",
            "17660 [D loss: 0.695462, acc: 43.75%] [G loss: 1.392148]\n",
            "17680 [D loss: 0.697837, acc: 40.62%] [G loss: 1.389846]\n",
            "17700 [D loss: 0.697415, acc: 42.19%] [G loss: 1.388712]\n",
            "17720 [D loss: 0.694712, acc: 46.88%] [G loss: 1.386785]\n",
            "17740 [D loss: 0.694001, acc: 47.27%] [G loss: 1.393687]\n",
            "17760 [D loss: 0.694237, acc: 47.27%] [G loss: 1.388214]\n",
            "17780 [D loss: 0.695993, acc: 44.92%] [G loss: 1.390791]\n",
            "17800 [D loss: 0.693431, acc: 50.00%] [G loss: 1.387884]\n",
            "17820 [D loss: 0.696185, acc: 46.09%] [G loss: 1.396310]\n",
            "17840 [D loss: 0.693144, acc: 52.73%] [G loss: 1.391254]\n",
            "17860 [D loss: 0.692694, acc: 51.17%] [G loss: 1.393116]\n",
            "17880 [D loss: 0.694712, acc: 45.31%] [G loss: 1.383380]\n",
            "17900 [D loss: 0.694089, acc: 50.00%] [G loss: 1.390856]\n",
            "17920 [D loss: 0.692835, acc: 53.91%] [G loss: 1.387661]\n",
            "17940 [D loss: 0.693515, acc: 49.61%] [G loss: 1.389904]\n",
            "17960 [D loss: 0.695099, acc: 45.31%] [G loss: 1.387307]\n",
            "17980 [D loss: 0.694643, acc: 44.92%] [G loss: 1.391607]\n",
            "18000 [D loss: 0.695156, acc: 46.09%] [G loss: 1.393102]\n",
            "18020 [D loss: 0.698420, acc: 41.80%] [G loss: 1.389104]\n",
            "18040 [D loss: 0.695929, acc: 44.53%] [G loss: 1.391211]\n",
            "18060 [D loss: 0.696570, acc: 47.27%] [G loss: 1.390738]\n",
            "18080 [D loss: 0.695913, acc: 42.19%] [G loss: 1.391050]\n",
            "18100 [D loss: 0.692359, acc: 47.27%] [G loss: 1.389867]\n",
            "18120 [D loss: 0.692935, acc: 49.22%] [G loss: 1.391453]\n",
            "18140 [D loss: 0.694512, acc: 48.05%] [G loss: 1.387645]\n",
            "18160 [D loss: 0.696474, acc: 42.97%] [G loss: 1.389364]\n",
            "18180 [D loss: 0.693434, acc: 50.00%] [G loss: 1.390486]\n",
            "18200 [D loss: 0.720047, acc: 33.59%] [G loss: 1.589212]\n",
            "18220 [D loss: 0.700039, acc: 39.06%] [G loss: 1.386319]\n",
            "18240 [D loss: 0.696221, acc: 44.92%] [G loss: 1.389980]\n",
            "18260 [D loss: 0.693985, acc: 44.53%] [G loss: 1.392266]\n",
            "18280 [D loss: 0.693307, acc: 50.00%] [G loss: 1.391373]\n",
            "18300 [D loss: 0.697809, acc: 44.14%] [G loss: 1.392464]\n",
            "18320 [D loss: 0.693823, acc: 46.48%] [G loss: 1.387154]\n",
            "18340 [D loss: 0.695488, acc: 48.05%] [G loss: 1.389155]\n",
            "18360 [D loss: 0.696743, acc: 42.97%] [G loss: 1.388368]\n",
            "18380 [D loss: 0.694778, acc: 49.22%] [G loss: 1.389082]\n",
            "18400 [D loss: 0.695828, acc: 48.44%] [G loss: 1.391756]\n",
            "18420 [D loss: 0.693711, acc: 45.70%] [G loss: 1.391632]\n",
            "18440 [D loss: 0.692551, acc: 54.69%] [G loss: 1.393047]\n",
            "18460 [D loss: 0.693391, acc: 50.00%] [G loss: 1.387608]\n",
            "18480 [D loss: 0.694687, acc: 48.83%] [G loss: 1.389280]\n",
            "18500 [D loss: 0.694337, acc: 48.05%] [G loss: 1.394304]\n",
            "18520 [D loss: 0.703250, acc: 37.11%] [G loss: 1.438381]\n",
            "18540 [D loss: 0.695056, acc: 46.88%] [G loss: 1.407071]\n",
            "18560 [D loss: 0.697512, acc: 47.27%] [G loss: 1.389815]\n",
            "18580 [D loss: 0.693247, acc: 50.39%] [G loss: 1.393380]\n",
            "18600 [D loss: 0.694440, acc: 44.92%] [G loss: 1.398508]\n",
            "18620 [D loss: 0.694814, acc: 44.53%] [G loss: 1.390262]\n",
            "18640 [D loss: 0.695042, acc: 46.09%] [G loss: 1.390512]\n",
            "18660 [D loss: 0.695953, acc: 43.75%] [G loss: 1.391382]\n",
            "18680 [D loss: 0.695116, acc: 46.09%] [G loss: 1.398422]\n",
            "18700 [D loss: 0.693298, acc: 50.00%] [G loss: 1.389542]\n",
            "18720 [D loss: 0.693437, acc: 46.88%] [G loss: 1.391524]\n",
            "18740 [D loss: 0.697583, acc: 40.23%] [G loss: 1.392980]\n",
            "18760 [D loss: 0.699536, acc: 35.16%] [G loss: 1.383385]\n",
            "18780 [D loss: 0.695409, acc: 43.36%] [G loss: 1.391065]\n",
            "18800 [D loss: 0.694528, acc: 43.75%] [G loss: 1.390548]\n",
            "18820 [D loss: 0.696621, acc: 43.36%] [G loss: 1.404555]\n",
            "18840 [D loss: 0.696032, acc: 41.80%] [G loss: 1.384155]\n",
            "18860 [D loss: 0.695660, acc: 44.14%] [G loss: 1.391242]\n",
            "18880 [D loss: 0.692103, acc: 50.00%] [G loss: 1.393153]\n",
            "18900 [D loss: 0.691721, acc: 51.17%] [G loss: 1.396611]\n",
            "18920 [D loss: 0.693839, acc: 50.39%] [G loss: 1.394274]\n",
            "18940 [D loss: 0.695096, acc: 45.31%] [G loss: 1.394959]\n",
            "18960 [D loss: 0.693300, acc: 45.70%] [G loss: 1.395235]\n",
            "18980 [D loss: 0.694920, acc: 48.05%] [G loss: 1.391058]\n",
            "19000 [D loss: 0.694264, acc: 47.66%] [G loss: 1.390845]\n",
            "19020 [D loss: 0.694736, acc: 48.05%] [G loss: 1.391271]\n",
            "19040 [D loss: 0.692171, acc: 50.39%] [G loss: 1.406416]\n",
            "19060 [D loss: 0.695137, acc: 42.58%] [G loss: 1.398225]\n",
            "19080 [D loss: 0.694491, acc: 49.22%] [G loss: 1.411113]\n",
            "19100 [D loss: 0.695894, acc: 43.75%] [G loss: 1.400412]\n",
            "19120 [D loss: 0.696001, acc: 42.58%] [G loss: 1.393259]\n",
            "19140 [D loss: 0.692370, acc: 48.83%] [G loss: 1.393270]\n",
            "19160 [D loss: 0.696756, acc: 46.09%] [G loss: 1.388342]\n",
            "19180 [D loss: 0.691083, acc: 51.17%] [G loss: 1.395565]\n",
            "19200 [D loss: 0.696535, acc: 46.48%] [G loss: 1.399199]\n",
            "19220 [D loss: 0.695138, acc: 44.14%] [G loss: 1.390032]\n",
            "19240 [D loss: 0.696201, acc: 46.88%] [G loss: 1.391314]\n",
            "19260 [D loss: 0.691712, acc: 53.12%] [G loss: 1.396599]\n",
            "19280 [D loss: 0.696071, acc: 45.70%] [G loss: 1.391579]\n",
            "19300 [D loss: 0.724822, acc: 30.08%] [G loss: 1.415143]\n",
            "19320 [D loss: 0.693076, acc: 53.52%] [G loss: 1.391414]\n",
            "19340 [D loss: 0.695026, acc: 47.27%] [G loss: 1.388983]\n",
            "19360 [D loss: 0.692267, acc: 53.91%] [G loss: 1.398155]\n",
            "19380 [D loss: 0.692394, acc: 50.39%] [G loss: 1.393067]\n",
            "19400 [D loss: 0.698141, acc: 41.41%] [G loss: 1.396297]\n",
            "19420 [D loss: 0.692943, acc: 52.73%] [G loss: 1.394073]\n",
            "19440 [D loss: 0.698090, acc: 42.58%] [G loss: 1.389284]\n",
            "19460 [D loss: 0.696038, acc: 43.36%] [G loss: 1.394187]\n",
            "19480 [D loss: 0.695973, acc: 41.02%] [G loss: 1.393928]\n",
            "19500 [D loss: 0.694563, acc: 48.05%] [G loss: 1.402330]\n",
            "19520 [D loss: 0.696907, acc: 45.70%] [G loss: 1.398960]\n",
            "19540 [D loss: 0.697616, acc: 41.41%] [G loss: 1.392418]\n",
            "19560 [D loss: 0.691548, acc: 58.59%] [G loss: 1.397442]\n",
            "19580 [D loss: 0.694319, acc: 45.31%] [G loss: 1.392289]\n",
            "19600 [D loss: 0.692373, acc: 51.17%] [G loss: 1.397326]\n",
            "19620 [D loss: 0.694017, acc: 50.00%] [G loss: 1.394009]\n",
            "19640 [D loss: 0.695609, acc: 47.66%] [G loss: 1.399871]\n",
            "19660 [D loss: 0.694245, acc: 44.92%] [G loss: 1.398404]\n",
            "19680 [D loss: 0.692603, acc: 45.31%] [G loss: 1.399723]\n",
            "19700 [D loss: 0.693460, acc: 48.05%] [G loss: 1.400137]\n",
            "19720 [D loss: 0.693944, acc: 47.27%] [G loss: 1.400085]\n",
            "19740 [D loss: 0.692788, acc: 43.75%] [G loss: 1.401022]\n",
            "19760 [D loss: 0.693533, acc: 41.80%] [G loss: 1.398281]\n",
            "19780 [D loss: 0.691451, acc: 55.08%] [G loss: 1.405580]\n",
            "19800 [D loss: 0.696091, acc: 42.97%] [G loss: 1.394391]\n",
            "19820 [D loss: 0.694352, acc: 50.39%] [G loss: 1.394621]\n",
            "19840 [D loss: 0.697589, acc: 42.19%] [G loss: 1.397058]\n",
            "19860 [D loss: 0.694628, acc: 41.80%] [G loss: 1.393677]\n",
            "19880 [D loss: 0.695839, acc: 41.41%] [G loss: 1.390137]\n",
            "19900 [D loss: 0.693019, acc: 46.48%] [G loss: 1.391362]\n",
            "19920 [D loss: 0.694492, acc: 50.00%] [G loss: 1.392477]\n",
            "19940 [D loss: 0.694563, acc: 46.48%] [G loss: 1.398371]\n",
            "19960 [D loss: 0.694891, acc: 50.00%] [G loss: 1.399169]\n",
            "19980 [D loss: 0.693512, acc: 50.00%] [G loss: 1.398799]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
