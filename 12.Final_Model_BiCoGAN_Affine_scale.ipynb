{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "12.Final_Model_BiCoGAN_aug_Affine.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "2de87ae5-8ee1-4a23-dd5e-ee811c35c04c"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UobTEvVkClTQ",
        "outputId": "0fd22469-ed5f-4004-e8e5-142d2a0247ad"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHR9swxs5P0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bca5fcd-49e3-49f7-a64f-a16d7c53e4f6"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "20b52d5a-3fca-4482-d63e-376314010f69"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6KZfZ4z86g0"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG7mZac9cJlA"
      },
      "source": [
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPxELXghdnUV",
        "outputId": "2ee02b7a-d6b5-410d-bc13-9de9b18bc591"
      },
      "source": [
        "for k in range(len(X_train)):\n",
        "  img = X_train[k]\n",
        "  emotion = y_train[k]\n",
        "  contrasted_images = []\n",
        "  emotions_list = []\n",
        "\n",
        "  for i in range(2):\n",
        "    contrast = iaa.Affine(scale={\"x\": (1.5, 1.0), \"y\": (1.5, 1.0)})\n",
        "    contrast_image =contrast.augment_image(img)\n",
        "    contrasted_images.append(contrast_image)\n",
        "\n",
        "  contrasted_images = np.array(contrasted_images)\n",
        "  emotions_list = np.array(emotions_list)\n",
        "  X_train = np.concatenate((X_train, contrasted_images), axis=0)\n",
        "  emotions_list = [emotion]*2\n",
        "  y_train = np.concatenate((y_train, emotions_list), axis=0)\n",
        "\n",
        "  if k % 100 == 0:\n",
        "    print (\"iteration: %d train shape: %s\" % (k, X_train.shape))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration: 0 train shape: (35342, 48, 48, 1)\n",
            "iteration: 100 train shape: (35542, 48, 48, 1)\n",
            "iteration: 200 train shape: (35742, 48, 48, 1)\n",
            "iteration: 300 train shape: (35942, 48, 48, 1)\n",
            "iteration: 400 train shape: (36142, 48, 48, 1)\n",
            "iteration: 500 train shape: (36342, 48, 48, 1)\n",
            "iteration: 600 train shape: (36542, 48, 48, 1)\n",
            "iteration: 700 train shape: (36742, 48, 48, 1)\n",
            "iteration: 800 train shape: (36942, 48, 48, 1)\n",
            "iteration: 900 train shape: (37142, 48, 48, 1)\n",
            "iteration: 1000 train shape: (37342, 48, 48, 1)\n",
            "iteration: 1100 train shape: (37542, 48, 48, 1)\n",
            "iteration: 1200 train shape: (37742, 48, 48, 1)\n",
            "iteration: 1300 train shape: (37942, 48, 48, 1)\n",
            "iteration: 1400 train shape: (38142, 48, 48, 1)\n",
            "iteration: 1500 train shape: (38342, 48, 48, 1)\n",
            "iteration: 1600 train shape: (38542, 48, 48, 1)\n",
            "iteration: 1700 train shape: (38742, 48, 48, 1)\n",
            "iteration: 1800 train shape: (38942, 48, 48, 1)\n",
            "iteration: 1900 train shape: (39142, 48, 48, 1)\n",
            "iteration: 2000 train shape: (39342, 48, 48, 1)\n",
            "iteration: 2100 train shape: (39542, 48, 48, 1)\n",
            "iteration: 2200 train shape: (39742, 48, 48, 1)\n",
            "iteration: 2300 train shape: (39942, 48, 48, 1)\n",
            "iteration: 2400 train shape: (40142, 48, 48, 1)\n",
            "iteration: 2500 train shape: (40342, 48, 48, 1)\n",
            "iteration: 2600 train shape: (40542, 48, 48, 1)\n",
            "iteration: 2700 train shape: (40742, 48, 48, 1)\n",
            "iteration: 2800 train shape: (40942, 48, 48, 1)\n",
            "iteration: 2900 train shape: (41142, 48, 48, 1)\n",
            "iteration: 3000 train shape: (41342, 48, 48, 1)\n",
            "iteration: 3100 train shape: (41542, 48, 48, 1)\n",
            "iteration: 3200 train shape: (41742, 48, 48, 1)\n",
            "iteration: 3300 train shape: (41942, 48, 48, 1)\n",
            "iteration: 3400 train shape: (42142, 48, 48, 1)\n",
            "iteration: 3500 train shape: (42342, 48, 48, 1)\n",
            "iteration: 3600 train shape: (42542, 48, 48, 1)\n",
            "iteration: 3700 train shape: (42742, 48, 48, 1)\n",
            "iteration: 3800 train shape: (42942, 48, 48, 1)\n",
            "iteration: 3900 train shape: (43142, 48, 48, 1)\n",
            "iteration: 4000 train shape: (43342, 48, 48, 1)\n",
            "iteration: 4100 train shape: (43542, 48, 48, 1)\n",
            "iteration: 4200 train shape: (43742, 48, 48, 1)\n",
            "iteration: 4300 train shape: (43942, 48, 48, 1)\n",
            "iteration: 4400 train shape: (44142, 48, 48, 1)\n",
            "iteration: 4500 train shape: (44342, 48, 48, 1)\n",
            "iteration: 4600 train shape: (44542, 48, 48, 1)\n",
            "iteration: 4700 train shape: (44742, 48, 48, 1)\n",
            "iteration: 4800 train shape: (44942, 48, 48, 1)\n",
            "iteration: 4900 train shape: (45142, 48, 48, 1)\n",
            "iteration: 5000 train shape: (45342, 48, 48, 1)\n",
            "iteration: 5100 train shape: (45542, 48, 48, 1)\n",
            "iteration: 5200 train shape: (45742, 48, 48, 1)\n",
            "iteration: 5300 train shape: (45942, 48, 48, 1)\n",
            "iteration: 5400 train shape: (46142, 48, 48, 1)\n",
            "iteration: 5500 train shape: (46342, 48, 48, 1)\n",
            "iteration: 5600 train shape: (46542, 48, 48, 1)\n",
            "iteration: 5700 train shape: (46742, 48, 48, 1)\n",
            "iteration: 5800 train shape: (46942, 48, 48, 1)\n",
            "iteration: 5900 train shape: (47142, 48, 48, 1)\n",
            "iteration: 6000 train shape: (47342, 48, 48, 1)\n",
            "iteration: 6100 train shape: (47542, 48, 48, 1)\n",
            "iteration: 6200 train shape: (47742, 48, 48, 1)\n",
            "iteration: 6300 train shape: (47942, 48, 48, 1)\n",
            "iteration: 6400 train shape: (48142, 48, 48, 1)\n",
            "iteration: 6500 train shape: (48342, 48, 48, 1)\n",
            "iteration: 6600 train shape: (48542, 48, 48, 1)\n",
            "iteration: 6700 train shape: (48742, 48, 48, 1)\n",
            "iteration: 6800 train shape: (48942, 48, 48, 1)\n",
            "iteration: 6900 train shape: (49142, 48, 48, 1)\n",
            "iteration: 7000 train shape: (49342, 48, 48, 1)\n",
            "iteration: 7100 train shape: (49542, 48, 48, 1)\n",
            "iteration: 7200 train shape: (49742, 48, 48, 1)\n",
            "iteration: 7300 train shape: (49942, 48, 48, 1)\n",
            "iteration: 7400 train shape: (50142, 48, 48, 1)\n",
            "iteration: 7500 train shape: (50342, 48, 48, 1)\n",
            "iteration: 7600 train shape: (50542, 48, 48, 1)\n",
            "iteration: 7700 train shape: (50742, 48, 48, 1)\n",
            "iteration: 7800 train shape: (50942, 48, 48, 1)\n",
            "iteration: 7900 train shape: (51142, 48, 48, 1)\n",
            "iteration: 8000 train shape: (51342, 48, 48, 1)\n",
            "iteration: 8100 train shape: (51542, 48, 48, 1)\n",
            "iteration: 8200 train shape: (51742, 48, 48, 1)\n",
            "iteration: 8300 train shape: (51942, 48, 48, 1)\n",
            "iteration: 8400 train shape: (52142, 48, 48, 1)\n",
            "iteration: 8500 train shape: (52342, 48, 48, 1)\n",
            "iteration: 8600 train shape: (52542, 48, 48, 1)\n",
            "iteration: 8700 train shape: (52742, 48, 48, 1)\n",
            "iteration: 8800 train shape: (52942, 48, 48, 1)\n",
            "iteration: 8900 train shape: (53142, 48, 48, 1)\n",
            "iteration: 9000 train shape: (53342, 48, 48, 1)\n",
            "iteration: 9100 train shape: (53542, 48, 48, 1)\n",
            "iteration: 9200 train shape: (53742, 48, 48, 1)\n",
            "iteration: 9300 train shape: (53942, 48, 48, 1)\n",
            "iteration: 9400 train shape: (54142, 48, 48, 1)\n",
            "iteration: 9500 train shape: (54342, 48, 48, 1)\n",
            "iteration: 9600 train shape: (54542, 48, 48, 1)\n",
            "iteration: 9700 train shape: (54742, 48, 48, 1)\n",
            "iteration: 9800 train shape: (54942, 48, 48, 1)\n",
            "iteration: 9900 train shape: (55142, 48, 48, 1)\n",
            "iteration: 10000 train shape: (55342, 48, 48, 1)\n",
            "iteration: 10100 train shape: (55542, 48, 48, 1)\n",
            "iteration: 10200 train shape: (55742, 48, 48, 1)\n",
            "iteration: 10300 train shape: (55942, 48, 48, 1)\n",
            "iteration: 10400 train shape: (56142, 48, 48, 1)\n",
            "iteration: 10500 train shape: (56342, 48, 48, 1)\n",
            "iteration: 10600 train shape: (56542, 48, 48, 1)\n",
            "iteration: 10700 train shape: (56742, 48, 48, 1)\n",
            "iteration: 10800 train shape: (56942, 48, 48, 1)\n",
            "iteration: 10900 train shape: (57142, 48, 48, 1)\n",
            "iteration: 11000 train shape: (57342, 48, 48, 1)\n",
            "iteration: 11100 train shape: (57542, 48, 48, 1)\n",
            "iteration: 11200 train shape: (57742, 48, 48, 1)\n",
            "iteration: 11300 train shape: (57942, 48, 48, 1)\n",
            "iteration: 11400 train shape: (58142, 48, 48, 1)\n",
            "iteration: 11500 train shape: (58342, 48, 48, 1)\n",
            "iteration: 11600 train shape: (58542, 48, 48, 1)\n",
            "iteration: 11700 train shape: (58742, 48, 48, 1)\n",
            "iteration: 11800 train shape: (58942, 48, 48, 1)\n",
            "iteration: 11900 train shape: (59142, 48, 48, 1)\n",
            "iteration: 12000 train shape: (59342, 48, 48, 1)\n",
            "iteration: 12100 train shape: (59542, 48, 48, 1)\n",
            "iteration: 12200 train shape: (59742, 48, 48, 1)\n",
            "iteration: 12300 train shape: (59942, 48, 48, 1)\n",
            "iteration: 12400 train shape: (60142, 48, 48, 1)\n",
            "iteration: 12500 train shape: (60342, 48, 48, 1)\n",
            "iteration: 12600 train shape: (60542, 48, 48, 1)\n",
            "iteration: 12700 train shape: (60742, 48, 48, 1)\n",
            "iteration: 12800 train shape: (60942, 48, 48, 1)\n",
            "iteration: 12900 train shape: (61142, 48, 48, 1)\n",
            "iteration: 13000 train shape: (61342, 48, 48, 1)\n",
            "iteration: 13100 train shape: (61542, 48, 48, 1)\n",
            "iteration: 13200 train shape: (61742, 48, 48, 1)\n",
            "iteration: 13300 train shape: (61942, 48, 48, 1)\n",
            "iteration: 13400 train shape: (62142, 48, 48, 1)\n",
            "iteration: 13500 train shape: (62342, 48, 48, 1)\n",
            "iteration: 13600 train shape: (62542, 48, 48, 1)\n",
            "iteration: 13700 train shape: (62742, 48, 48, 1)\n",
            "iteration: 13800 train shape: (62942, 48, 48, 1)\n",
            "iteration: 13900 train shape: (63142, 48, 48, 1)\n",
            "iteration: 14000 train shape: (63342, 48, 48, 1)\n",
            "iteration: 14100 train shape: (63542, 48, 48, 1)\n",
            "iteration: 14200 train shape: (63742, 48, 48, 1)\n",
            "iteration: 14300 train shape: (63942, 48, 48, 1)\n",
            "iteration: 14400 train shape: (64142, 48, 48, 1)\n",
            "iteration: 14500 train shape: (64342, 48, 48, 1)\n",
            "iteration: 14600 train shape: (64542, 48, 48, 1)\n",
            "iteration: 14700 train shape: (64742, 48, 48, 1)\n",
            "iteration: 14800 train shape: (64942, 48, 48, 1)\n",
            "iteration: 14900 train shape: (65142, 48, 48, 1)\n",
            "iteration: 15000 train shape: (65342, 48, 48, 1)\n",
            "iteration: 15100 train shape: (65542, 48, 48, 1)\n",
            "iteration: 15200 train shape: (65742, 48, 48, 1)\n",
            "iteration: 15300 train shape: (65942, 48, 48, 1)\n",
            "iteration: 15400 train shape: (66142, 48, 48, 1)\n",
            "iteration: 15500 train shape: (66342, 48, 48, 1)\n",
            "iteration: 15600 train shape: (66542, 48, 48, 1)\n",
            "iteration: 15700 train shape: (66742, 48, 48, 1)\n",
            "iteration: 15800 train shape: (66942, 48, 48, 1)\n",
            "iteration: 15900 train shape: (67142, 48, 48, 1)\n",
            "iteration: 16000 train shape: (67342, 48, 48, 1)\n",
            "iteration: 16100 train shape: (67542, 48, 48, 1)\n",
            "iteration: 16200 train shape: (67742, 48, 48, 1)\n",
            "iteration: 16300 train shape: (67942, 48, 48, 1)\n",
            "iteration: 16400 train shape: (68142, 48, 48, 1)\n",
            "iteration: 16500 train shape: (68342, 48, 48, 1)\n",
            "iteration: 16600 train shape: (68542, 48, 48, 1)\n",
            "iteration: 16700 train shape: (68742, 48, 48, 1)\n",
            "iteration: 16800 train shape: (68942, 48, 48, 1)\n",
            "iteration: 16900 train shape: (69142, 48, 48, 1)\n",
            "iteration: 17000 train shape: (69342, 48, 48, 1)\n",
            "iteration: 17100 train shape: (69542, 48, 48, 1)\n",
            "iteration: 17200 train shape: (69742, 48, 48, 1)\n",
            "iteration: 17300 train shape: (69942, 48, 48, 1)\n",
            "iteration: 17400 train shape: (70142, 48, 48, 1)\n",
            "iteration: 17500 train shape: (70342, 48, 48, 1)\n",
            "iteration: 17600 train shape: (70542, 48, 48, 1)\n",
            "iteration: 17700 train shape: (70742, 48, 48, 1)\n",
            "iteration: 17800 train shape: (70942, 48, 48, 1)\n",
            "iteration: 17900 train shape: (71142, 48, 48, 1)\n",
            "iteration: 18000 train shape: (71342, 48, 48, 1)\n",
            "iteration: 18100 train shape: (71542, 48, 48, 1)\n",
            "iteration: 18200 train shape: (71742, 48, 48, 1)\n",
            "iteration: 18300 train shape: (71942, 48, 48, 1)\n",
            "iteration: 18400 train shape: (72142, 48, 48, 1)\n",
            "iteration: 18500 train shape: (72342, 48, 48, 1)\n",
            "iteration: 18600 train shape: (72542, 48, 48, 1)\n",
            "iteration: 18700 train shape: (72742, 48, 48, 1)\n",
            "iteration: 18800 train shape: (72942, 48, 48, 1)\n",
            "iteration: 18900 train shape: (73142, 48, 48, 1)\n",
            "iteration: 19000 train shape: (73342, 48, 48, 1)\n",
            "iteration: 19100 train shape: (73542, 48, 48, 1)\n",
            "iteration: 19200 train shape: (73742, 48, 48, 1)\n",
            "iteration: 19300 train shape: (73942, 48, 48, 1)\n",
            "iteration: 19400 train shape: (74142, 48, 48, 1)\n",
            "iteration: 19500 train shape: (74342, 48, 48, 1)\n",
            "iteration: 19600 train shape: (74542, 48, 48, 1)\n",
            "iteration: 19700 train shape: (74742, 48, 48, 1)\n",
            "iteration: 19800 train shape: (74942, 48, 48, 1)\n",
            "iteration: 19900 train shape: (75142, 48, 48, 1)\n",
            "iteration: 20000 train shape: (75342, 48, 48, 1)\n",
            "iteration: 20100 train shape: (75542, 48, 48, 1)\n",
            "iteration: 20200 train shape: (75742, 48, 48, 1)\n",
            "iteration: 20300 train shape: (75942, 48, 48, 1)\n",
            "iteration: 20400 train shape: (76142, 48, 48, 1)\n",
            "iteration: 20500 train shape: (76342, 48, 48, 1)\n",
            "iteration: 20600 train shape: (76542, 48, 48, 1)\n",
            "iteration: 20700 train shape: (76742, 48, 48, 1)\n",
            "iteration: 20800 train shape: (76942, 48, 48, 1)\n",
            "iteration: 20900 train shape: (77142, 48, 48, 1)\n",
            "iteration: 21000 train shape: (77342, 48, 48, 1)\n",
            "iteration: 21100 train shape: (77542, 48, 48, 1)\n",
            "iteration: 21200 train shape: (77742, 48, 48, 1)\n",
            "iteration: 21300 train shape: (77942, 48, 48, 1)\n",
            "iteration: 21400 train shape: (78142, 48, 48, 1)\n",
            "iteration: 21500 train shape: (78342, 48, 48, 1)\n",
            "iteration: 21600 train shape: (78542, 48, 48, 1)\n",
            "iteration: 21700 train shape: (78742, 48, 48, 1)\n",
            "iteration: 21800 train shape: (78942, 48, 48, 1)\n",
            "iteration: 21900 train shape: (79142, 48, 48, 1)\n",
            "iteration: 22000 train shape: (79342, 48, 48, 1)\n",
            "iteration: 22100 train shape: (79542, 48, 48, 1)\n",
            "iteration: 22200 train shape: (79742, 48, 48, 1)\n",
            "iteration: 22300 train shape: (79942, 48, 48, 1)\n",
            "iteration: 22400 train shape: (80142, 48, 48, 1)\n",
            "iteration: 22500 train shape: (80342, 48, 48, 1)\n",
            "iteration: 22600 train shape: (80542, 48, 48, 1)\n",
            "iteration: 22700 train shape: (80742, 48, 48, 1)\n",
            "iteration: 22800 train shape: (80942, 48, 48, 1)\n",
            "iteration: 22900 train shape: (81142, 48, 48, 1)\n",
            "iteration: 23000 train shape: (81342, 48, 48, 1)\n",
            "iteration: 23100 train shape: (81542, 48, 48, 1)\n",
            "iteration: 23200 train shape: (81742, 48, 48, 1)\n",
            "iteration: 23300 train shape: (81942, 48, 48, 1)\n",
            "iteration: 23400 train shape: (82142, 48, 48, 1)\n",
            "iteration: 23500 train shape: (82342, 48, 48, 1)\n",
            "iteration: 23600 train shape: (82542, 48, 48, 1)\n",
            "iteration: 23700 train shape: (82742, 48, 48, 1)\n",
            "iteration: 23800 train shape: (82942, 48, 48, 1)\n",
            "iteration: 23900 train shape: (83142, 48, 48, 1)\n",
            "iteration: 24000 train shape: (83342, 48, 48, 1)\n",
            "iteration: 24100 train shape: (83542, 48, 48, 1)\n",
            "iteration: 24200 train shape: (83742, 48, 48, 1)\n",
            "iteration: 24300 train shape: (83942, 48, 48, 1)\n",
            "iteration: 24400 train shape: (84142, 48, 48, 1)\n",
            "iteration: 24500 train shape: (84342, 48, 48, 1)\n",
            "iteration: 24600 train shape: (84542, 48, 48, 1)\n",
            "iteration: 24700 train shape: (84742, 48, 48, 1)\n",
            "iteration: 24800 train shape: (84942, 48, 48, 1)\n",
            "iteration: 24900 train shape: (85142, 48, 48, 1)\n",
            "iteration: 25000 train shape: (85342, 48, 48, 1)\n",
            "iteration: 25100 train shape: (85542, 48, 48, 1)\n",
            "iteration: 25200 train shape: (85742, 48, 48, 1)\n",
            "iteration: 25300 train shape: (85942, 48, 48, 1)\n",
            "iteration: 25400 train shape: (86142, 48, 48, 1)\n",
            "iteration: 25500 train shape: (86342, 48, 48, 1)\n",
            "iteration: 25600 train shape: (86542, 48, 48, 1)\n",
            "iteration: 25700 train shape: (86742, 48, 48, 1)\n",
            "iteration: 25800 train shape: (86942, 48, 48, 1)\n",
            "iteration: 25900 train shape: (87142, 48, 48, 1)\n",
            "iteration: 26000 train shape: (87342, 48, 48, 1)\n",
            "iteration: 26100 train shape: (87542, 48, 48, 1)\n",
            "iteration: 26200 train shape: (87742, 48, 48, 1)\n",
            "iteration: 26300 train shape: (87942, 48, 48, 1)\n",
            "iteration: 26400 train shape: (88142, 48, 48, 1)\n",
            "iteration: 26500 train shape: (88342, 48, 48, 1)\n",
            "iteration: 26600 train shape: (88542, 48, 48, 1)\n",
            "iteration: 26700 train shape: (88742, 48, 48, 1)\n",
            "iteration: 26800 train shape: (88942, 48, 48, 1)\n",
            "iteration: 26900 train shape: (89142, 48, 48, 1)\n",
            "iteration: 27000 train shape: (89342, 48, 48, 1)\n",
            "iteration: 27100 train shape: (89542, 48, 48, 1)\n",
            "iteration: 27200 train shape: (89742, 48, 48, 1)\n",
            "iteration: 27300 train shape: (89942, 48, 48, 1)\n",
            "iteration: 27400 train shape: (90142, 48, 48, 1)\n",
            "iteration: 27500 train shape: (90342, 48, 48, 1)\n",
            "iteration: 27600 train shape: (90542, 48, 48, 1)\n",
            "iteration: 27700 train shape: (90742, 48, 48, 1)\n",
            "iteration: 27800 train shape: (90942, 48, 48, 1)\n",
            "iteration: 27900 train shape: (91142, 48, 48, 1)\n",
            "iteration: 28000 train shape: (91342, 48, 48, 1)\n",
            "iteration: 28100 train shape: (91542, 48, 48, 1)\n",
            "iteration: 28200 train shape: (91742, 48, 48, 1)\n",
            "iteration: 28300 train shape: (91942, 48, 48, 1)\n",
            "iteration: 28400 train shape: (92142, 48, 48, 1)\n",
            "iteration: 28500 train shape: (92342, 48, 48, 1)\n",
            "iteration: 28600 train shape: (92542, 48, 48, 1)\n",
            "iteration: 28700 train shape: (92742, 48, 48, 1)\n",
            "iteration: 28800 train shape: (92942, 48, 48, 1)\n",
            "iteration: 28900 train shape: (93142, 48, 48, 1)\n",
            "iteration: 29000 train shape: (93342, 48, 48, 1)\n",
            "iteration: 29100 train shape: (93542, 48, 48, 1)\n",
            "iteration: 29200 train shape: (93742, 48, 48, 1)\n",
            "iteration: 29300 train shape: (93942, 48, 48, 1)\n",
            "iteration: 29400 train shape: (94142, 48, 48, 1)\n",
            "iteration: 29500 train shape: (94342, 48, 48, 1)\n",
            "iteration: 29600 train shape: (94542, 48, 48, 1)\n",
            "iteration: 29700 train shape: (94742, 48, 48, 1)\n",
            "iteration: 29800 train shape: (94942, 48, 48, 1)\n",
            "iteration: 29900 train shape: (95142, 48, 48, 1)\n",
            "iteration: 30000 train shape: (95342, 48, 48, 1)\n",
            "iteration: 30100 train shape: (95542, 48, 48, 1)\n",
            "iteration: 30200 train shape: (95742, 48, 48, 1)\n",
            "iteration: 30300 train shape: (95942, 48, 48, 1)\n",
            "iteration: 30400 train shape: (96142, 48, 48, 1)\n",
            "iteration: 30500 train shape: (96342, 48, 48, 1)\n",
            "iteration: 30600 train shape: (96542, 48, 48, 1)\n",
            "iteration: 30700 train shape: (96742, 48, 48, 1)\n",
            "iteration: 30800 train shape: (96942, 48, 48, 1)\n",
            "iteration: 30900 train shape: (97142, 48, 48, 1)\n",
            "iteration: 31000 train shape: (97342, 48, 48, 1)\n",
            "iteration: 31100 train shape: (97542, 48, 48, 1)\n",
            "iteration: 31200 train shape: (97742, 48, 48, 1)\n",
            "iteration: 31300 train shape: (97942, 48, 48, 1)\n",
            "iteration: 31400 train shape: (98142, 48, 48, 1)\n",
            "iteration: 31500 train shape: (98342, 48, 48, 1)\n",
            "iteration: 31600 train shape: (98542, 48, 48, 1)\n",
            "iteration: 31700 train shape: (98742, 48, 48, 1)\n",
            "iteration: 31800 train shape: (98942, 48, 48, 1)\n",
            "iteration: 31900 train shape: (99142, 48, 48, 1)\n",
            "iteration: 32000 train shape: (99342, 48, 48, 1)\n",
            "iteration: 32100 train shape: (99542, 48, 48, 1)\n",
            "iteration: 32200 train shape: (99742, 48, 48, 1)\n",
            "iteration: 32300 train shape: (99942, 48, 48, 1)\n",
            "iteration: 32400 train shape: (100142, 48, 48, 1)\n",
            "iteration: 32500 train shape: (100342, 48, 48, 1)\n",
            "iteration: 32600 train shape: (100542, 48, 48, 1)\n",
            "iteration: 32700 train shape: (100742, 48, 48, 1)\n",
            "iteration: 32800 train shape: (100942, 48, 48, 1)\n",
            "iteration: 32900 train shape: (101142, 48, 48, 1)\n",
            "iteration: 33000 train shape: (101342, 48, 48, 1)\n",
            "iteration: 33100 train shape: (101542, 48, 48, 1)\n",
            "iteration: 33200 train shape: (101742, 48, 48, 1)\n",
            "iteration: 33300 train shape: (101942, 48, 48, 1)\n",
            "iteration: 33400 train shape: (102142, 48, 48, 1)\n",
            "iteration: 33500 train shape: (102342, 48, 48, 1)\n",
            "iteration: 33600 train shape: (102542, 48, 48, 1)\n",
            "iteration: 33700 train shape: (102742, 48, 48, 1)\n",
            "iteration: 33800 train shape: (102942, 48, 48, 1)\n",
            "iteration: 33900 train shape: (103142, 48, 48, 1)\n",
            "iteration: 34000 train shape: (103342, 48, 48, 1)\n",
            "iteration: 34100 train shape: (103542, 48, 48, 1)\n",
            "iteration: 34200 train shape: (103742, 48, 48, 1)\n",
            "iteration: 34300 train shape: (103942, 48, 48, 1)\n",
            "iteration: 34400 train shape: (104142, 48, 48, 1)\n",
            "iteration: 34500 train shape: (104342, 48, 48, 1)\n",
            "iteration: 34600 train shape: (104542, 48, 48, 1)\n",
            "iteration: 34700 train shape: (104742, 48, 48, 1)\n",
            "iteration: 34800 train shape: (104942, 48, 48, 1)\n",
            "iteration: 34900 train shape: (105142, 48, 48, 1)\n",
            "iteration: 35000 train shape: (105342, 48, 48, 1)\n",
            "iteration: 35100 train shape: (105542, 48, 48, 1)\n",
            "iteration: 35200 train shape: (105742, 48, 48, 1)\n",
            "iteration: 35300 train shape: (105942, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UQaVvZbyNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf76c33-26cc-4d3b-8a46-278486724e86"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(106020, 48, 48, 1)\n",
            "(106020, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            # imgs = datagen.flow(X_train[idx], y_train[idx], batch_size=batch_size)\n",
        "            # labels = y_train[idx]\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aP5xXv7AJq3",
        "outputId": "dcd65a84-c43a-4561-e05e-657936821050"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=20000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 48, 48, 128)  3328        input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 48, 48, 128)  0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_11 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 1, 2304)      13824       input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 100)          0           input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_5 (Flatten)             (None, 1152)         0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 2304)         0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3556)         0           flatten_6[0][0]                  \n",
            "                                                                 flatten_5[0][0]                  \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            3557        concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)   (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTr (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)   (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_16 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb9a5751d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0 [D loss: 0.699527, acc.: 47.66%] [G loss: 1.585663]\n",
            "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb9a5751d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "20 [D loss: 1.221621, acc.: 16.41%] [G loss: 0.704532]\n",
            "40 [D loss: 1.003656, acc.: 59.77%] [G loss: 3.477149]\n",
            "60 [D loss: 0.808965, acc.: 39.84%] [G loss: 2.146442]\n",
            "80 [D loss: 0.846576, acc.: 4.69%] [G loss: 1.461323]\n",
            "100 [D loss: 0.750638, acc.: 53.91%] [G loss: 1.379888]\n",
            "120 [D loss: 0.585244, acc.: 46.88%] [G loss: 2.413634]\n",
            "140 [D loss: 0.401550, acc.: 90.62%] [G loss: 4.766757]\n",
            "160 [D loss: 0.851177, acc.: 57.03%] [G loss: 3.923627]\n",
            "180 [D loss: 0.624462, acc.: 71.88%] [G loss: 2.016839]\n",
            "200 [D loss: 0.556280, acc.: 76.56%] [G loss: 2.687809]\n",
            "220 [D loss: 0.596871, acc.: 70.31%] [G loss: 2.082612]\n",
            "240 [D loss: 0.673126, acc.: 42.58%] [G loss: 3.113291]\n",
            "260 [D loss: 0.673352, acc.: 64.45%] [G loss: 1.891034]\n",
            "280 [D loss: 0.308957, acc.: 89.45%] [G loss: 6.203260]\n",
            "300 [D loss: 0.809106, acc.: 54.30%] [G loss: 1.497399]\n",
            "320 [D loss: 0.886121, acc.: 43.36%] [G loss: 2.106707]\n",
            "340 [D loss: 0.413637, acc.: 88.28%] [G loss: 3.229646]\n",
            "360 [D loss: 0.505026, acc.: 80.86%] [G loss: 3.873034]\n",
            "380 [D loss: 0.400719, acc.: 84.38%] [G loss: 3.655046]\n",
            "400 [D loss: 0.443138, acc.: 84.77%] [G loss: 2.880270]\n",
            "420 [D loss: 0.671047, acc.: 57.42%] [G loss: 2.158563]\n",
            "440 [D loss: 0.515895, acc.: 76.17%] [G loss: 3.435327]\n",
            "460 [D loss: 0.620328, acc.: 67.19%] [G loss: 2.458146]\n",
            "480 [D loss: 0.524826, acc.: 76.56%] [G loss: 2.603099]\n",
            "500 [D loss: 0.569076, acc.: 73.83%] [G loss: 2.306409]\n",
            "520 [D loss: 0.611153, acc.: 62.89%] [G loss: 2.637910]\n",
            "540 [D loss: 0.555910, acc.: 72.27%] [G loss: 2.792122]\n",
            "560 [D loss: 0.561731, acc.: 75.00%] [G loss: 2.243770]\n",
            "580 [D loss: 0.581578, acc.: 66.02%] [G loss: 2.437503]\n",
            "600 [D loss: 0.583187, acc.: 73.83%] [G loss: 2.271822]\n",
            "620 [D loss: 0.528163, acc.: 81.25%] [G loss: 2.409734]\n",
            "640 [D loss: 0.682878, acc.: 57.03%] [G loss: 1.787976]\n",
            "660 [D loss: 0.639617, acc.: 59.77%] [G loss: 3.110284]\n",
            "680 [D loss: 0.490710, acc.: 79.69%] [G loss: 3.206672]\n",
            "700 [D loss: 0.475134, acc.: 85.94%] [G loss: 1.689641]\n",
            "720 [D loss: 0.715965, acc.: 54.30%] [G loss: 1.273632]\n",
            "740 [D loss: 0.943485, acc.: 28.52%] [G loss: 1.664033]\n",
            "760 [D loss: 0.749086, acc.: 51.95%] [G loss: 2.018267]\n",
            "780 [D loss: 0.642070, acc.: 61.33%] [G loss: 2.091253]\n",
            "800 [D loss: 0.729704, acc.: 62.89%] [G loss: 2.321696]\n",
            "820 [D loss: 0.641539, acc.: 61.33%] [G loss: 2.248370]\n",
            "840 [D loss: 0.535998, acc.: 75.78%] [G loss: 2.822905]\n",
            "860 [D loss: 0.621738, acc.: 64.06%] [G loss: 2.425650]\n",
            "880 [D loss: 0.516758, acc.: 73.44%] [G loss: 2.505689]\n",
            "900 [D loss: 0.538908, acc.: 72.27%] [G loss: 2.913590]\n",
            "920 [D loss: 0.659217, acc.: 63.67%] [G loss: 2.148681]\n",
            "940 [D loss: 0.530283, acc.: 72.27%] [G loss: 2.648431]\n",
            "960 [D loss: 0.648806, acc.: 64.84%] [G loss: 2.783085]\n",
            "980 [D loss: 0.595104, acc.: 70.70%] [G loss: 2.443833]\n",
            "1000 [D loss: 0.716993, acc.: 54.69%] [G loss: 2.292506]\n",
            "1020 [D loss: 0.593361, acc.: 69.92%] [G loss: 2.297209]\n",
            "1040 [D loss: 0.518209, acc.: 78.52%] [G loss: 2.555570]\n",
            "1060 [D loss: 0.654518, acc.: 60.55%] [G loss: 2.099865]\n",
            "1080 [D loss: 0.651959, acc.: 61.72%] [G loss: 2.082789]\n",
            "1100 [D loss: 0.539661, acc.: 78.52%] [G loss: 2.461058]\n",
            "1120 [D loss: 0.563082, acc.: 72.27%] [G loss: 2.385711]\n",
            "1140 [D loss: 0.612306, acc.: 66.80%] [G loss: 2.337581]\n",
            "1160 [D loss: 0.619189, acc.: 65.23%] [G loss: 2.235897]\n",
            "1180 [D loss: 0.608547, acc.: 66.80%] [G loss: 2.420858]\n",
            "1200 [D loss: 0.525710, acc.: 78.12%] [G loss: 2.169075]\n",
            "1220 [D loss: 0.718158, acc.: 53.12%] [G loss: 1.821944]\n",
            "1240 [D loss: 0.644986, acc.: 62.89%] [G loss: 2.525318]\n",
            "1260 [D loss: 0.627926, acc.: 64.45%] [G loss: 2.844867]\n",
            "1280 [D loss: 0.592372, acc.: 69.53%] [G loss: 2.841997]\n",
            "1300 [D loss: 0.599880, acc.: 69.14%] [G loss: 2.510057]\n",
            "1320 [D loss: 0.641058, acc.: 66.02%] [G loss: 2.545147]\n",
            "1340 [D loss: 0.554332, acc.: 74.22%] [G loss: 2.480507]\n",
            "1360 [D loss: 0.626740, acc.: 59.38%] [G loss: 2.481944]\n",
            "1380 [D loss: 0.605669, acc.: 67.19%] [G loss: 2.558933]\n",
            "1400 [D loss: 0.562398, acc.: 73.83%] [G loss: 2.621421]\n",
            "1420 [D loss: 0.917127, acc.: 43.36%] [G loss: 2.985917]\n",
            "1440 [D loss: 0.679748, acc.: 57.03%] [G loss: 2.119084]\n",
            "1460 [D loss: 0.562550, acc.: 69.92%] [G loss: 2.390296]\n",
            "1480 [D loss: 0.586589, acc.: 69.53%] [G loss: 2.341506]\n",
            "1500 [D loss: 0.609599, acc.: 66.02%] [G loss: 2.352860]\n",
            "1520 [D loss: 0.615688, acc.: 66.80%] [G loss: 2.371607]\n",
            "1540 [D loss: 0.519483, acc.: 78.91%] [G loss: 2.638971]\n",
            "1560 [D loss: 0.545613, acc.: 72.27%] [G loss: 2.602082]\n",
            "1580 [D loss: 0.508812, acc.: 77.73%] [G loss: 2.834377]\n",
            "1600 [D loss: 0.633857, acc.: 62.89%] [G loss: 2.452868]\n",
            "1620 [D loss: 0.602648, acc.: 69.14%] [G loss: 2.332870]\n",
            "1640 [D loss: 0.618043, acc.: 69.14%] [G loss: 2.419996]\n",
            "1660 [D loss: 0.634238, acc.: 64.84%] [G loss: 2.088017]\n",
            "1680 [D loss: 0.571457, acc.: 71.88%] [G loss: 2.323599]\n",
            "1700 [D loss: 0.585903, acc.: 67.97%] [G loss: 2.313067]\n",
            "1720 [D loss: 0.617426, acc.: 64.84%] [G loss: 2.301580]\n",
            "1740 [D loss: 0.528351, acc.: 77.73%] [G loss: 2.312152]\n",
            "1760 [D loss: 0.645975, acc.: 60.94%] [G loss: 2.122539]\n",
            "1780 [D loss: 0.634807, acc.: 61.72%] [G loss: 2.061074]\n",
            "1800 [D loss: 0.579210, acc.: 71.88%] [G loss: 2.289332]\n",
            "1820 [D loss: 0.570281, acc.: 73.44%] [G loss: 2.268555]\n",
            "1840 [D loss: 0.609361, acc.: 65.23%] [G loss: 2.065413]\n",
            "1860 [D loss: 0.651135, acc.: 64.84%] [G loss: 2.057703]\n",
            "1880 [D loss: 0.696003, acc.: 56.64%] [G loss: 2.102122]\n",
            "1900 [D loss: 0.612560, acc.: 67.19%] [G loss: 2.198493]\n",
            "1920 [D loss: 0.634477, acc.: 63.67%] [G loss: 2.052451]\n",
            "1940 [D loss: 0.604796, acc.: 66.41%] [G loss: 2.032338]\n",
            "1960 [D loss: 0.666293, acc.: 59.77%] [G loss: 1.973600]\n",
            "1980 [D loss: 0.669867, acc.: 57.03%] [G loss: 2.017293]\n",
            "2000 [D loss: 0.589071, acc.: 67.19%] [G loss: 2.180258]\n",
            "2020 [D loss: 0.619617, acc.: 64.06%] [G loss: 2.133373]\n",
            "2040 [D loss: 0.589157, acc.: 70.31%] [G loss: 2.287665]\n",
            "2060 [D loss: 0.655665, acc.: 57.81%] [G loss: 2.145711]\n",
            "2080 [D loss: 0.618495, acc.: 66.41%] [G loss: 2.159709]\n",
            "2100 [D loss: 0.580525, acc.: 72.27%] [G loss: 2.254100]\n",
            "2120 [D loss: 0.639638, acc.: 60.16%] [G loss: 2.195107]\n",
            "2140 [D loss: 0.571926, acc.: 73.44%] [G loss: 2.265462]\n",
            "2160 [D loss: 0.616542, acc.: 68.36%] [G loss: 2.249888]\n",
            "2180 [D loss: 0.560964, acc.: 75.78%] [G loss: 2.213082]\n",
            "2200 [D loss: 0.594746, acc.: 70.70%] [G loss: 2.376112]\n",
            "2220 [D loss: 0.668249, acc.: 63.28%] [G loss: 1.987456]\n",
            "2240 [D loss: 0.545857, acc.: 73.44%] [G loss: 2.343707]\n",
            "2260 [D loss: 0.659737, acc.: 58.59%] [G loss: 2.035346]\n",
            "2280 [D loss: 0.584997, acc.: 70.70%] [G loss: 2.189589]\n",
            "2300 [D loss: 0.336272, acc.: 94.53%] [G loss: 1.705487]\n",
            "2320 [D loss: 0.802637, acc.: 48.44%] [G loss: 3.257769]\n",
            "2340 [D loss: 0.902079, acc.: 32.03%] [G loss: 1.585559]\n",
            "2360 [D loss: 0.669604, acc.: 58.59%] [G loss: 1.657806]\n",
            "2380 [D loss: 0.691828, acc.: 57.81%] [G loss: 1.813518]\n",
            "2400 [D loss: 0.623890, acc.: 62.50%] [G loss: 2.236742]\n",
            "2420 [D loss: 0.655535, acc.: 59.38%] [G loss: 2.637642]\n",
            "2440 [D loss: 0.640472, acc.: 64.45%] [G loss: 2.352642]\n",
            "2460 [D loss: 0.605225, acc.: 65.23%] [G loss: 2.548223]\n",
            "2480 [D loss: 0.633548, acc.: 64.06%] [G loss: 2.215864]\n",
            "2500 [D loss: 0.568975, acc.: 72.27%] [G loss: 2.469963]\n",
            "2520 [D loss: 0.642865, acc.: 60.55%] [G loss: 2.319598]\n",
            "2540 [D loss: 0.628446, acc.: 63.28%] [G loss: 2.251623]\n",
            "2560 [D loss: 0.592166, acc.: 69.14%] [G loss: 2.399417]\n",
            "2580 [D loss: 0.560770, acc.: 71.09%] [G loss: 2.394264]\n",
            "2600 [D loss: 0.622938, acc.: 66.80%] [G loss: 2.355158]\n",
            "2620 [D loss: 0.632347, acc.: 65.62%] [G loss: 2.161324]\n",
            "2640 [D loss: 0.593186, acc.: 69.92%] [G loss: 2.278090]\n",
            "2660 [D loss: 0.596088, acc.: 70.70%] [G loss: 2.277235]\n",
            "2680 [D loss: 0.583621, acc.: 71.48%] [G loss: 2.326283]\n",
            "2700 [D loss: 0.592159, acc.: 66.80%] [G loss: 2.412143]\n",
            "2720 [D loss: 0.576136, acc.: 67.19%] [G loss: 2.254532]\n",
            "2740 [D loss: 0.561973, acc.: 70.31%] [G loss: 2.333660]\n",
            "2760 [D loss: 0.643119, acc.: 62.50%] [G loss: 2.276248]\n",
            "2780 [D loss: 0.589239, acc.: 69.53%] [G loss: 2.438964]\n",
            "2800 [D loss: 0.648518, acc.: 63.28%] [G loss: 2.288075]\n",
            "2820 [D loss: 0.637396, acc.: 65.23%] [G loss: 2.267151]\n",
            "2840 [D loss: 0.499397, acc.: 80.86%] [G loss: 2.637731]\n",
            "2860 [D loss: 0.625290, acc.: 65.23%] [G loss: 2.283590]\n",
            "2880 [D loss: 0.642635, acc.: 66.41%] [G loss: 2.282707]\n",
            "2900 [D loss: 0.592663, acc.: 65.62%] [G loss: 2.341260]\n",
            "2920 [D loss: 0.561002, acc.: 73.44%] [G loss: 2.417572]\n",
            "2940 [D loss: 0.557415, acc.: 71.48%] [G loss: 2.534248]\n",
            "2960 [D loss: 0.593477, acc.: 69.92%] [G loss: 2.382520]\n",
            "2980 [D loss: 0.572638, acc.: 69.92%] [G loss: 2.457179]\n",
            "3000 [D loss: 0.572795, acc.: 74.22%] [G loss: 2.507362]\n",
            "3020 [D loss: 0.525409, acc.: 76.56%] [G loss: 2.686415]\n",
            "3040 [D loss: 0.545582, acc.: 74.22%] [G loss: 2.504221]\n",
            "3060 [D loss: 0.554589, acc.: 71.88%] [G loss: 2.506631]\n",
            "3080 [D loss: 0.599934, acc.: 67.19%] [G loss: 2.397494]\n",
            "3100 [D loss: 0.625551, acc.: 64.45%] [G loss: 2.372025]\n",
            "3120 [D loss: 0.608781, acc.: 72.66%] [G loss: 2.468114]\n",
            "3140 [D loss: 0.601836, acc.: 65.23%] [G loss: 2.228412]\n",
            "3160 [D loss: 0.609426, acc.: 71.48%] [G loss: 2.323255]\n",
            "3180 [D loss: 0.607908, acc.: 67.97%] [G loss: 2.350555]\n",
            "3200 [D loss: 0.560193, acc.: 72.66%] [G loss: 2.505677]\n",
            "3220 [D loss: 0.614019, acc.: 63.28%] [G loss: 2.390750]\n",
            "3240 [D loss: 0.616365, acc.: 69.92%] [G loss: 2.439845]\n",
            "3260 [D loss: 0.570018, acc.: 68.36%] [G loss: 2.575455]\n",
            "3280 [D loss: 0.603993, acc.: 67.19%] [G loss: 2.496021]\n",
            "3300 [D loss: 0.547872, acc.: 75.39%] [G loss: 2.558887]\n",
            "3320 [D loss: 0.621175, acc.: 66.80%] [G loss: 2.472893]\n",
            "3340 [D loss: 0.580380, acc.: 69.53%] [G loss: 2.366895]\n",
            "3360 [D loss: 0.568464, acc.: 69.14%] [G loss: 2.483781]\n",
            "3380 [D loss: 0.594048, acc.: 67.97%] [G loss: 2.516682]\n",
            "3400 [D loss: 0.550688, acc.: 75.78%] [G loss: 2.502211]\n",
            "3420 [D loss: 0.570909, acc.: 70.31%] [G loss: 2.523766]\n",
            "3440 [D loss: 0.546868, acc.: 72.66%] [G loss: 2.601794]\n",
            "3460 [D loss: 0.581250, acc.: 69.92%] [G loss: 2.447514]\n",
            "3480 [D loss: 0.575152, acc.: 68.75%] [G loss: 2.620964]\n",
            "3500 [D loss: 0.586185, acc.: 68.75%] [G loss: 2.472528]\n",
            "3520 [D loss: 0.556741, acc.: 68.75%] [G loss: 2.653461]\n",
            "3540 [D loss: 0.587009, acc.: 69.92%] [G loss: 2.518313]\n",
            "3560 [D loss: 0.584443, acc.: 66.80%] [G loss: 2.507606]\n",
            "3580 [D loss: 0.582123, acc.: 71.09%] [G loss: 2.463976]\n",
            "3600 [D loss: 0.610645, acc.: 65.62%] [G loss: 2.535803]\n",
            "3620 [D loss: 0.636143, acc.: 65.23%] [G loss: 2.306882]\n",
            "3640 [D loss: 0.578519, acc.: 69.92%] [G loss: 2.500947]\n",
            "3660 [D loss: 0.641445, acc.: 63.67%] [G loss: 2.236241]\n",
            "3680 [D loss: 0.536897, acc.: 73.44%] [G loss: 2.763207]\n",
            "3700 [D loss: 0.666964, acc.: 58.59%] [G loss: 2.456647]\n",
            "3720 [D loss: 0.582933, acc.: 66.80%] [G loss: 2.493226]\n",
            "3740 [D loss: 0.610917, acc.: 65.23%] [G loss: 2.401187]\n",
            "3760 [D loss: 0.607774, acc.: 66.02%] [G loss: 2.397459]\n",
            "3780 [D loss: 0.554024, acc.: 72.27%] [G loss: 2.554247]\n",
            "3800 [D loss: 0.560210, acc.: 66.80%] [G loss: 2.763619]\n",
            "3820 [D loss: 0.593350, acc.: 68.75%] [G loss: 2.425364]\n",
            "3840 [D loss: 0.592654, acc.: 70.31%] [G loss: 2.345855]\n",
            "3860 [D loss: 0.598227, acc.: 68.75%] [G loss: 2.418698]\n",
            "3880 [D loss: 0.568166, acc.: 74.61%] [G loss: 2.384850]\n",
            "3900 [D loss: 0.578285, acc.: 68.36%] [G loss: 2.431780]\n",
            "3920 [D loss: 0.581186, acc.: 69.53%] [G loss: 2.582702]\n",
            "3940 [D loss: 0.540626, acc.: 74.22%] [G loss: 2.517509]\n",
            "3960 [D loss: 0.578209, acc.: 68.36%] [G loss: 2.538160]\n",
            "3980 [D loss: 0.617211, acc.: 67.58%] [G loss: 2.472643]\n",
            "4000 [D loss: 0.617346, acc.: 66.80%] [G loss: 2.347776]\n",
            "4020 [D loss: 0.620028, acc.: 69.92%] [G loss: 2.480977]\n",
            "4040 [D loss: 0.585539, acc.: 71.48%] [G loss: 2.690382]\n",
            "4060 [D loss: 0.556441, acc.: 69.53%] [G loss: 2.571239]\n",
            "4080 [D loss: 0.574592, acc.: 68.36%] [G loss: 2.436368]\n",
            "4100 [D loss: 0.607254, acc.: 67.58%] [G loss: 2.570415]\n",
            "4120 [D loss: 0.541913, acc.: 74.22%] [G loss: 2.635850]\n",
            "4140 [D loss: 0.546118, acc.: 72.27%] [G loss: 2.413266]\n",
            "4160 [D loss: 0.586402, acc.: 68.36%] [G loss: 2.440905]\n",
            "4180 [D loss: 0.590692, acc.: 66.41%] [G loss: 2.497142]\n",
            "4200 [D loss: 0.621506, acc.: 65.23%] [G loss: 2.377339]\n",
            "4220 [D loss: 0.620341, acc.: 67.58%] [G loss: 2.571432]\n",
            "4240 [D loss: 0.598325, acc.: 65.23%] [G loss: 2.431204]\n",
            "4260 [D loss: 0.570117, acc.: 67.19%] [G loss: 2.549478]\n",
            "4280 [D loss: 0.570747, acc.: 69.53%] [G loss: 2.612960]\n",
            "4300 [D loss: 0.542925, acc.: 75.00%] [G loss: 2.555734]\n",
            "4320 [D loss: 0.574122, acc.: 69.14%] [G loss: 2.552458]\n",
            "4340 [D loss: 0.601984, acc.: 68.75%] [G loss: 2.566367]\n",
            "4360 [D loss: 0.566079, acc.: 67.97%] [G loss: 2.724037]\n",
            "4380 [D loss: 0.545332, acc.: 75.00%] [G loss: 2.810011]\n",
            "4400 [D loss: 0.520142, acc.: 74.61%] [G loss: 2.724182]\n",
            "4420 [D loss: 0.568664, acc.: 71.48%] [G loss: 2.701161]\n",
            "4440 [D loss: 0.528536, acc.: 73.83%] [G loss: 2.688395]\n",
            "4460 [D loss: 0.551293, acc.: 73.05%] [G loss: 2.491332]\n",
            "4480 [D loss: 0.517417, acc.: 75.78%] [G loss: 2.748962]\n",
            "4500 [D loss: 0.576124, acc.: 68.36%] [G loss: 2.655929]\n",
            "4520 [D loss: 0.528418, acc.: 70.31%] [G loss: 2.740712]\n",
            "4540 [D loss: 0.605834, acc.: 64.84%] [G loss: 2.612738]\n",
            "4560 [D loss: 0.547708, acc.: 70.70%] [G loss: 2.780703]\n",
            "4580 [D loss: 0.523338, acc.: 79.69%] [G loss: 2.887965]\n",
            "4600 [D loss: 0.564031, acc.: 69.53%] [G loss: 2.707251]\n",
            "4620 [D loss: 0.553866, acc.: 73.44%] [G loss: 2.690882]\n",
            "4640 [D loss: 0.530195, acc.: 71.88%] [G loss: 2.794910]\n",
            "4660 [D loss: 0.599652, acc.: 67.19%] [G loss: 2.534236]\n",
            "4680 [D loss: 0.565034, acc.: 69.53%] [G loss: 2.689815]\n",
            "4700 [D loss: 0.597918, acc.: 66.80%] [G loss: 2.432948]\n",
            "4720 [D loss: 0.755653, acc.: 52.73%] [G loss: 2.465723]\n",
            "4740 [D loss: 0.755508, acc.: 55.08%] [G loss: 2.581339]\n",
            "4760 [D loss: 0.600987, acc.: 66.41%] [G loss: 2.312401]\n",
            "4780 [D loss: 0.484651, acc.: 80.47%] [G loss: 2.209334]\n",
            "4800 [D loss: 0.097304, acc.: 100.00%] [G loss: 2.411499]\n",
            "4820 [D loss: 0.105599, acc.: 98.44%] [G loss: 2.609133]\n",
            "4840 [D loss: 0.221562, acc.: 93.75%] [G loss: 2.173710]\n",
            "4860 [D loss: 1.930490, acc.: 23.05%] [G loss: 1.514883]\n",
            "4880 [D loss: 2.634090, acc.: 61.33%] [G loss: 8.731325]\n",
            "4900 [D loss: 0.486473, acc.: 76.95%] [G loss: 3.271441]\n",
            "4920 [D loss: 0.662367, acc.: 66.02%] [G loss: 3.631433]\n",
            "4940 [D loss: 0.738785, acc.: 53.12%] [G loss: 2.914921]\n",
            "4960 [D loss: 0.780147, acc.: 52.73%] [G loss: 2.391663]\n",
            "4980 [D loss: 0.565516, acc.: 71.09%] [G loss: 2.596578]\n",
            "5000 [D loss: 0.786598, acc.: 53.12%] [G loss: 2.103435]\n",
            "5020 [D loss: 0.644970, acc.: 64.06%] [G loss: 2.325059]\n",
            "5040 [D loss: 0.655941, acc.: 63.28%] [G loss: 2.125927]\n",
            "5060 [D loss: 0.589191, acc.: 65.62%] [G loss: 2.332802]\n",
            "5080 [D loss: 0.594739, acc.: 68.36%] [G loss: 2.140189]\n",
            "5100 [D loss: 0.592806, acc.: 64.84%] [G loss: 2.251918]\n",
            "5120 [D loss: 0.570488, acc.: 71.09%] [G loss: 2.255017]\n",
            "5140 [D loss: 0.646544, acc.: 64.06%] [G loss: 2.035904]\n",
            "5160 [D loss: 0.592863, acc.: 67.97%] [G loss: 2.204178]\n",
            "5180 [D loss: 0.606084, acc.: 63.28%] [G loss: 2.162238]\n",
            "5200 [D loss: 0.620917, acc.: 64.06%] [G loss: 2.106249]\n",
            "5220 [D loss: 0.582026, acc.: 68.75%] [G loss: 2.116880]\n",
            "5240 [D loss: 0.565112, acc.: 69.53%] [G loss: 2.286629]\n",
            "5260 [D loss: 0.572575, acc.: 71.88%] [G loss: 2.140444]\n",
            "5280 [D loss: 0.608324, acc.: 65.62%] [G loss: 2.030846]\n",
            "5300 [D loss: 0.619922, acc.: 64.06%] [G loss: 2.102196]\n",
            "5320 [D loss: 0.599722, acc.: 69.14%] [G loss: 2.246067]\n",
            "5340 [D loss: 0.585422, acc.: 69.92%] [G loss: 2.155722]\n",
            "5360 [D loss: 0.610596, acc.: 65.62%] [G loss: 2.166081]\n",
            "5380 [D loss: 0.566199, acc.: 75.00%] [G loss: 2.275042]\n",
            "5400 [D loss: 0.625958, acc.: 64.84%] [G loss: 2.161099]\n",
            "5420 [D loss: 0.556870, acc.: 74.22%] [G loss: 2.280111]\n",
            "5440 [D loss: 0.520571, acc.: 79.30%] [G loss: 2.361561]\n",
            "5460 [D loss: 0.639543, acc.: 62.89%] [G loss: 2.073150]\n",
            "5480 [D loss: 0.590772, acc.: 71.09%] [G loss: 2.316096]\n",
            "5500 [D loss: 0.649119, acc.: 60.94%] [G loss: 2.088904]\n",
            "5520 [D loss: 0.583034, acc.: 66.80%] [G loss: 2.252358]\n",
            "5540 [D loss: 0.571259, acc.: 71.48%] [G loss: 2.300208]\n",
            "5560 [D loss: 0.568838, acc.: 69.14%] [G loss: 2.276640]\n",
            "5580 [D loss: 0.548473, acc.: 74.61%] [G loss: 2.447303]\n",
            "5600 [D loss: 0.572309, acc.: 69.14%] [G loss: 2.334867]\n",
            "5620 [D loss: 0.611086, acc.: 66.41%] [G loss: 2.265058]\n",
            "5640 [D loss: 0.596819, acc.: 66.80%] [G loss: 2.193667]\n",
            "5660 [D loss: 0.608211, acc.: 67.19%] [G loss: 2.232588]\n",
            "5680 [D loss: 0.596633, acc.: 69.14%] [G loss: 2.201167]\n",
            "5700 [D loss: 0.638028, acc.: 64.45%] [G loss: 2.170130]\n",
            "5720 [D loss: 0.578224, acc.: 67.97%] [G loss: 2.158344]\n",
            "5740 [D loss: 0.549150, acc.: 71.48%] [G loss: 2.375335]\n",
            "5760 [D loss: 0.569653, acc.: 69.14%] [G loss: 2.291678]\n",
            "5780 [D loss: 0.599246, acc.: 71.88%] [G loss: 2.214702]\n",
            "5800 [D loss: 0.610914, acc.: 66.80%] [G loss: 2.183947]\n",
            "5820 [D loss: 0.616741, acc.: 64.45%] [G loss: 2.144380]\n",
            "5840 [D loss: 0.566554, acc.: 70.70%] [G loss: 2.451427]\n",
            "5860 [D loss: 0.599805, acc.: 70.70%] [G loss: 2.273426]\n",
            "5880 [D loss: 0.570472, acc.: 70.70%] [G loss: 2.295147]\n",
            "5900 [D loss: 0.598738, acc.: 66.02%] [G loss: 2.185112]\n",
            "5920 [D loss: 0.617499, acc.: 67.19%] [G loss: 2.270339]\n",
            "5940 [D loss: 0.587138, acc.: 67.58%] [G loss: 2.201591]\n",
            "5960 [D loss: 0.588174, acc.: 67.97%] [G loss: 2.313644]\n",
            "5980 [D loss: 0.641312, acc.: 62.11%] [G loss: 2.255511]\n",
            "6000 [D loss: 0.608999, acc.: 64.06%] [G loss: 2.289798]\n",
            "6020 [D loss: 0.577603, acc.: 73.44%] [G loss: 2.270305]\n",
            "6040 [D loss: 0.606558, acc.: 66.02%] [G loss: 2.250344]\n",
            "6060 [D loss: 0.574298, acc.: 69.14%] [G loss: 2.478012]\n",
            "6080 [D loss: 0.614187, acc.: 65.62%] [G loss: 2.309986]\n",
            "6100 [D loss: 0.538319, acc.: 73.83%] [G loss: 2.514520]\n",
            "6120 [D loss: 0.581778, acc.: 69.92%] [G loss: 2.307775]\n",
            "6140 [D loss: 0.572413, acc.: 73.05%] [G loss: 2.414598]\n",
            "6160 [D loss: 0.609543, acc.: 62.89%] [G loss: 2.232793]\n",
            "6180 [D loss: 0.562662, acc.: 73.05%] [G loss: 2.365583]\n",
            "6200 [D loss: 0.603002, acc.: 65.23%] [G loss: 2.335910]\n",
            "6220 [D loss: 0.598916, acc.: 65.23%] [G loss: 2.502887]\n",
            "6240 [D loss: 0.574844, acc.: 71.09%] [G loss: 2.393115]\n",
            "6260 [D loss: 0.542190, acc.: 72.66%] [G loss: 2.521758]\n",
            "6280 [D loss: 0.580111, acc.: 68.75%] [G loss: 2.333228]\n",
            "6300 [D loss: 0.625302, acc.: 63.67%] [G loss: 2.335385]\n",
            "6320 [D loss: 0.554907, acc.: 73.44%] [G loss: 2.407695]\n",
            "6340 [D loss: 0.605047, acc.: 67.19%] [G loss: 2.417512]\n",
            "6360 [D loss: 0.540616, acc.: 73.83%] [G loss: 2.390337]\n",
            "6380 [D loss: 0.367516, acc.: 91.80%] [G loss: 1.844261]\n",
            "6400 [D loss: 1.067966, acc.: 38.28%] [G loss: 2.418283]\n",
            "6420 [D loss: 0.792532, acc.: 48.83%] [G loss: 2.568889]\n",
            "6440 [D loss: 0.643565, acc.: 62.11%] [G loss: 2.867280]\n",
            "6460 [D loss: 0.611157, acc.: 67.58%] [G loss: 2.326576]\n",
            "6480 [D loss: 0.471768, acc.: 80.47%] [G loss: 2.899420]\n",
            "6500 [D loss: 0.835922, acc.: 46.09%] [G loss: 2.293893]\n",
            "6520 [D loss: 0.617552, acc.: 67.19%] [G loss: 2.571822]\n",
            "6540 [D loss: 0.786148, acc.: 48.83%] [G loss: 2.154997]\n",
            "6560 [D loss: 0.633932, acc.: 64.45%] [G loss: 2.441601]\n",
            "6580 [D loss: 0.639698, acc.: 60.94%] [G loss: 2.493722]\n",
            "6600 [D loss: 0.571854, acc.: 71.09%] [G loss: 2.657177]\n",
            "6620 [D loss: 0.582973, acc.: 68.75%] [G loss: 2.694302]\n",
            "6640 [D loss: 0.508315, acc.: 77.73%] [G loss: 2.800662]\n",
            "6660 [D loss: 0.655541, acc.: 66.41%] [G loss: 2.217862]\n",
            "6680 [D loss: 0.667975, acc.: 66.41%] [G loss: 2.557356]\n",
            "6700 [D loss: 0.559459, acc.: 72.27%] [G loss: 2.507347]\n",
            "6720 [D loss: 0.645255, acc.: 61.33%] [G loss: 2.514466]\n",
            "6740 [D loss: 0.554363, acc.: 72.27%] [G loss: 2.455574]\n",
            "6760 [D loss: 0.664217, acc.: 60.16%] [G loss: 2.446515]\n",
            "6780 [D loss: 0.544609, acc.: 74.22%] [G loss: 2.762530]\n",
            "6800 [D loss: 0.580563, acc.: 69.53%] [G loss: 2.687639]\n",
            "6820 [D loss: 0.577680, acc.: 69.14%] [G loss: 2.551333]\n",
            "6840 [D loss: 0.610687, acc.: 68.36%] [G loss: 2.489554]\n",
            "6860 [D loss: 0.534598, acc.: 73.83%] [G loss: 2.571406]\n",
            "6880 [D loss: 0.527800, acc.: 73.83%] [G loss: 2.697032]\n",
            "6900 [D loss: 0.561954, acc.: 74.61%] [G loss: 2.674748]\n",
            "6920 [D loss: 0.565458, acc.: 71.09%] [G loss: 2.656049]\n",
            "6940 [D loss: 0.582539, acc.: 70.31%] [G loss: 2.602776]\n",
            "6960 [D loss: 0.580936, acc.: 66.41%] [G loss: 2.699267]\n",
            "6980 [D loss: 0.539598, acc.: 73.44%] [G loss: 2.708122]\n",
            "7000 [D loss: 0.583879, acc.: 69.14%] [G loss: 2.563887]\n",
            "7020 [D loss: 0.481523, acc.: 78.91%] [G loss: 2.923038]\n",
            "7040 [D loss: 0.619023, acc.: 66.41%] [G loss: 2.475661]\n",
            "7060 [D loss: 0.558820, acc.: 66.80%] [G loss: 2.715513]\n",
            "7080 [D loss: 0.612415, acc.: 66.02%] [G loss: 2.627918]\n",
            "7100 [D loss: 0.489746, acc.: 77.34%] [G loss: 3.004678]\n",
            "7120 [D loss: 0.527944, acc.: 73.44%] [G loss: 2.714587]\n",
            "7140 [D loss: 0.557966, acc.: 69.92%] [G loss: 2.675908]\n",
            "7160 [D loss: 0.634190, acc.: 63.67%] [G loss: 2.535988]\n",
            "7180 [D loss: 0.493623, acc.: 78.52%] [G loss: 2.727592]\n",
            "7200 [D loss: 0.593742, acc.: 69.14%] [G loss: 2.502205]\n",
            "7220 [D loss: 0.519057, acc.: 75.00%] [G loss: 2.794750]\n",
            "7240 [D loss: 0.563719, acc.: 68.36%] [G loss: 2.833635]\n",
            "7260 [D loss: 0.608832, acc.: 66.41%] [G loss: 2.575768]\n",
            "7280 [D loss: 0.503061, acc.: 76.17%] [G loss: 2.690936]\n",
            "7300 [D loss: 0.711596, acc.: 58.20%] [G loss: 2.357253]\n",
            "7320 [D loss: 0.589651, acc.: 69.53%] [G loss: 2.941016]\n",
            "7340 [D loss: 0.589207, acc.: 68.75%] [G loss: 2.712922]\n",
            "7360 [D loss: 0.440919, acc.: 80.47%] [G loss: 3.203233]\n",
            "7380 [D loss: 0.700814, acc.: 60.94%] [G loss: 2.597030]\n",
            "7400 [D loss: 0.243151, acc.: 97.66%] [G loss: 2.097991]\n",
            "7420 [D loss: 1.126529, acc.: 34.38%] [G loss: 2.608278]\n",
            "7440 [D loss: 0.457008, acc.: 83.20%] [G loss: 2.165377]\n",
            "7460 [D loss: 0.675976, acc.: 61.33%] [G loss: 2.813496]\n",
            "7480 [D loss: 0.743723, acc.: 52.73%] [G loss: 1.892002]\n",
            "7500 [D loss: 0.513870, acc.: 76.56%] [G loss: 3.378853]\n",
            "7520 [D loss: 0.555100, acc.: 72.66%] [G loss: 2.769718]\n",
            "7540 [D loss: 0.513450, acc.: 77.34%] [G loss: 3.008196]\n",
            "7560 [D loss: 0.513476, acc.: 73.05%] [G loss: 2.942784]\n",
            "7580 [D loss: 0.619129, acc.: 69.92%] [G loss: 2.659744]\n",
            "7600 [D loss: 0.539349, acc.: 72.66%] [G loss: 2.788921]\n",
            "7620 [D loss: 0.582408, acc.: 71.88%] [G loss: 2.813473]\n",
            "7640 [D loss: 0.555779, acc.: 71.09%] [G loss: 2.847777]\n",
            "7660 [D loss: 0.523514, acc.: 74.22%] [G loss: 2.757383]\n",
            "7680 [D loss: 0.587015, acc.: 67.97%] [G loss: 2.849306]\n",
            "7700 [D loss: 0.494719, acc.: 78.12%] [G loss: 2.800997]\n",
            "7720 [D loss: 0.487563, acc.: 78.52%] [G loss: 3.120764]\n",
            "7740 [D loss: 0.561612, acc.: 68.36%] [G loss: 2.804378]\n",
            "7760 [D loss: 0.506737, acc.: 73.44%] [G loss: 2.951881]\n",
            "7780 [D loss: 0.530793, acc.: 74.22%] [G loss: 2.995718]\n",
            "7800 [D loss: 0.632101, acc.: 66.02%] [G loss: 2.710356]\n",
            "7820 [D loss: 0.529218, acc.: 74.22%] [G loss: 2.850848]\n",
            "7840 [D loss: 0.579014, acc.: 70.31%] [G loss: 2.920801]\n",
            "7860 [D loss: 0.633987, acc.: 61.72%] [G loss: 2.535602]\n",
            "7880 [D loss: 0.494278, acc.: 79.69%] [G loss: 2.924584]\n",
            "7900 [D loss: 0.615997, acc.: 67.19%] [G loss: 2.678575]\n",
            "7920 [D loss: 0.468908, acc.: 79.30%] [G loss: 3.125421]\n",
            "7940 [D loss: 0.564922, acc.: 72.27%] [G loss: 2.852299]\n",
            "7960 [D loss: 0.543497, acc.: 75.00%] [G loss: 2.782030]\n",
            "7980 [D loss: 0.499468, acc.: 75.39%] [G loss: 3.048119]\n",
            "8000 [D loss: 0.512204, acc.: 76.17%] [G loss: 2.849272]\n",
            "8020 [D loss: 0.551377, acc.: 72.66%] [G loss: 2.768265]\n",
            "8040 [D loss: 0.547109, acc.: 71.88%] [G loss: 2.835856]\n",
            "8060 [D loss: 0.544941, acc.: 69.14%] [G loss: 2.905836]\n",
            "8080 [D loss: 0.599773, acc.: 64.06%] [G loss: 2.958672]\n",
            "8100 [D loss: 0.498417, acc.: 77.73%] [G loss: 2.990978]\n",
            "8120 [D loss: 0.602585, acc.: 66.02%] [G loss: 2.865334]\n",
            "8140 [D loss: 0.475569, acc.: 79.30%] [G loss: 3.325473]\n",
            "8160 [D loss: 0.506730, acc.: 76.95%] [G loss: 3.048767]\n",
            "8180 [D loss: 0.528152, acc.: 74.22%] [G loss: 3.120921]\n",
            "8200 [D loss: 0.518585, acc.: 73.44%] [G loss: 2.904085]\n",
            "8220 [D loss: 0.539913, acc.: 73.05%] [G loss: 2.912723]\n",
            "8240 [D loss: 0.538299, acc.: 71.88%] [G loss: 3.123675]\n",
            "8260 [D loss: 0.570538, acc.: 67.97%] [G loss: 2.775516]\n",
            "8280 [D loss: 0.559400, acc.: 69.92%] [G loss: 2.944664]\n",
            "8300 [D loss: 0.541508, acc.: 71.09%] [G loss: 2.899379]\n",
            "8320 [D loss: 0.606280, acc.: 66.80%] [G loss: 2.619802]\n",
            "8340 [D loss: 0.594186, acc.: 69.92%] [G loss: 2.899035]\n",
            "8360 [D loss: 0.493542, acc.: 75.00%] [G loss: 3.010409]\n",
            "8380 [D loss: 0.573838, acc.: 71.09%] [G loss: 2.792324]\n",
            "8400 [D loss: 0.570766, acc.: 69.53%] [G loss: 2.811886]\n",
            "8420 [D loss: 0.579527, acc.: 68.36%] [G loss: 2.666187]\n",
            "8440 [D loss: 0.642343, acc.: 61.72%] [G loss: 2.694676]\n",
            "8460 [D loss: 0.499970, acc.: 78.12%] [G loss: 3.055489]\n",
            "8480 [D loss: 0.556227, acc.: 69.92%] [G loss: 2.960016]\n",
            "8500 [D loss: 0.586016, acc.: 70.31%] [G loss: 2.930554]\n",
            "8520 [D loss: 0.521797, acc.: 79.30%] [G loss: 2.907741]\n",
            "8540 [D loss: 0.561687, acc.: 72.66%] [G loss: 2.897965]\n",
            "8560 [D loss: 0.535442, acc.: 73.44%] [G loss: 2.800331]\n",
            "8580 [D loss: 0.498451, acc.: 77.34%] [G loss: 3.058115]\n",
            "8600 [D loss: 0.475891, acc.: 79.69%] [G loss: 3.062596]\n",
            "8620 [D loss: 0.550741, acc.: 72.27%] [G loss: 2.884334]\n",
            "8640 [D loss: 0.607596, acc.: 65.62%] [G loss: 2.900235]\n",
            "8660 [D loss: 0.574229, acc.: 67.58%] [G loss: 3.124969]\n",
            "8680 [D loss: 0.506711, acc.: 74.61%] [G loss: 3.003493]\n",
            "8700 [D loss: 0.481558, acc.: 79.30%] [G loss: 3.040518]\n",
            "8720 [D loss: 0.531117, acc.: 73.83%] [G loss: 3.008736]\n",
            "8740 [D loss: 0.509203, acc.: 75.78%] [G loss: 3.205104]\n",
            "8760 [D loss: 0.441298, acc.: 80.86%] [G loss: 3.208410]\n",
            "8780 [D loss: 0.530329, acc.: 72.66%] [G loss: 3.118389]\n",
            "8800 [D loss: 0.526314, acc.: 72.66%] [G loss: 3.283861]\n",
            "8820 [D loss: 0.553829, acc.: 72.27%] [G loss: 2.957509]\n",
            "8840 [D loss: 0.444962, acc.: 81.64%] [G loss: 3.594648]\n",
            "8860 [D loss: 0.581269, acc.: 69.92%] [G loss: 2.892034]\n",
            "8880 [D loss: 0.546528, acc.: 72.27%] [G loss: 3.170641]\n",
            "8900 [D loss: 0.554908, acc.: 73.44%] [G loss: 2.923546]\n",
            "8920 [D loss: 0.541419, acc.: 73.44%] [G loss: 3.111263]\n",
            "8940 [D loss: 0.534766, acc.: 73.83%] [G loss: 3.264385]\n",
            "8960 [D loss: 0.489458, acc.: 77.34%] [G loss: 3.077024]\n",
            "8980 [D loss: 0.546790, acc.: 73.83%] [G loss: 3.433616]\n",
            "9000 [D loss: 0.733580, acc.: 57.81%] [G loss: 2.993194]\n",
            "9020 [D loss: 0.544270, acc.: 71.88%] [G loss: 3.091386]\n",
            "9040 [D loss: 0.594031, acc.: 68.36%] [G loss: 2.808616]\n",
            "9060 [D loss: 0.440969, acc.: 82.81%] [G loss: 3.149122]\n",
            "9080 [D loss: 0.774467, acc.: 54.30%] [G loss: 3.014414]\n",
            "9100 [D loss: 0.598265, acc.: 65.62%] [G loss: 3.066607]\n",
            "9120 [D loss: 0.656024, acc.: 60.94%] [G loss: 2.874701]\n",
            "9140 [D loss: 0.552097, acc.: 69.92%] [G loss: 3.096615]\n",
            "9160 [D loss: 0.482424, acc.: 80.47%] [G loss: 3.126379]\n",
            "9180 [D loss: 0.542775, acc.: 73.05%] [G loss: 3.094050]\n",
            "9200 [D loss: 0.611341, acc.: 70.31%] [G loss: 2.991625]\n",
            "9220 [D loss: 0.533598, acc.: 71.48%] [G loss: 3.153100]\n",
            "9240 [D loss: 0.466352, acc.: 80.47%] [G loss: 3.392658]\n",
            "9260 [D loss: 0.515808, acc.: 73.83%] [G loss: 3.122930]\n",
            "9280 [D loss: 0.457522, acc.: 78.12%] [G loss: 3.149112]\n",
            "9300 [D loss: 0.527038, acc.: 74.22%] [G loss: 3.343176]\n",
            "9320 [D loss: 0.586150, acc.: 69.14%] [G loss: 3.013674]\n",
            "9340 [D loss: 0.494044, acc.: 76.56%] [G loss: 3.345043]\n",
            "9360 [D loss: 0.534241, acc.: 72.66%] [G loss: 3.144023]\n",
            "9380 [D loss: 0.516763, acc.: 71.88%] [G loss: 3.220051]\n",
            "9400 [D loss: 0.541572, acc.: 76.17%] [G loss: 3.411367]\n",
            "9420 [D loss: 0.482005, acc.: 75.78%] [G loss: 3.286994]\n",
            "9440 [D loss: 0.506986, acc.: 75.78%] [G loss: 3.474696]\n",
            "9460 [D loss: 0.541515, acc.: 73.44%] [G loss: 3.291359]\n",
            "9480 [D loss: 0.475351, acc.: 76.56%] [G loss: 3.169610]\n",
            "9500 [D loss: 0.518210, acc.: 76.17%] [G loss: 3.222515]\n",
            "9520 [D loss: 0.547135, acc.: 74.22%] [G loss: 3.023944]\n",
            "9540 [D loss: 0.525517, acc.: 71.09%] [G loss: 3.075209]\n",
            "9560 [D loss: 0.520986, acc.: 74.61%] [G loss: 3.102575]\n",
            "9580 [D loss: 0.456430, acc.: 82.03%] [G loss: 3.173989]\n",
            "9600 [D loss: 0.488303, acc.: 76.95%] [G loss: 3.775417]\n",
            "9620 [D loss: 0.517668, acc.: 74.22%] [G loss: 3.174913]\n",
            "9640 [D loss: 0.496119, acc.: 78.12%] [G loss: 3.311265]\n",
            "9660 [D loss: 0.485728, acc.: 79.30%] [G loss: 3.212901]\n",
            "9680 [D loss: 0.514884, acc.: 76.56%] [G loss: 3.363701]\n",
            "9700 [D loss: 0.546290, acc.: 72.27%] [G loss: 2.978662]\n",
            "9720 [D loss: 0.476796, acc.: 79.69%] [G loss: 3.057961]\n",
            "9740 [D loss: 0.571942, acc.: 69.14%] [G loss: 3.161548]\n",
            "9760 [D loss: 0.529636, acc.: 70.70%] [G loss: 3.207560]\n",
            "9780 [D loss: 0.532898, acc.: 75.39%] [G loss: 2.900374]\n",
            "9800 [D loss: 0.368036, acc.: 87.11%] [G loss: 2.302605]\n",
            "9820 [D loss: 0.258491, acc.: 89.84%] [G loss: 2.110025]\n",
            "9840 [D loss: 0.854279, acc.: 50.78%] [G loss: 2.205760]\n",
            "9860 [D loss: 0.537033, acc.: 72.27%] [G loss: 3.299827]\n",
            "9880 [D loss: 0.812158, acc.: 55.86%] [G loss: 3.549346]\n",
            "9900 [D loss: 0.573353, acc.: 68.75%] [G loss: 2.914950]\n",
            "9920 [D loss: 0.684822, acc.: 59.77%] [G loss: 3.662986]\n",
            "9940 [D loss: 0.593451, acc.: 67.97%] [G loss: 3.512172]\n",
            "9960 [D loss: 0.477435, acc.: 78.52%] [G loss: 3.290007]\n",
            "9980 [D loss: 0.514713, acc.: 72.27%] [G loss: 3.205142]\n",
            "10000 [D loss: 0.543254, acc.: 69.92%] [G loss: 3.399670]\n",
            "10020 [D loss: 0.609555, acc.: 68.36%] [G loss: 3.091685]\n",
            "10040 [D loss: 0.444900, acc.: 81.64%] [G loss: 3.467339]\n",
            "10060 [D loss: 0.529609, acc.: 75.39%] [G loss: 3.290343]\n",
            "10080 [D loss: 0.412207, acc.: 82.42%] [G loss: 3.626420]\n",
            "10100 [D loss: 0.484021, acc.: 76.17%] [G loss: 3.368109]\n",
            "10120 [D loss: 0.477104, acc.: 79.30%] [G loss: 3.243473]\n",
            "10140 [D loss: 0.564249, acc.: 69.14%] [G loss: 3.231144]\n",
            "10160 [D loss: 0.518607, acc.: 75.78%] [G loss: 3.243737]\n",
            "10180 [D loss: 0.513219, acc.: 71.88%] [G loss: 4.035375]\n",
            "10200 [D loss: 0.524314, acc.: 73.05%] [G loss: 3.288768]\n",
            "10220 [D loss: 0.449328, acc.: 80.86%] [G loss: 3.683884]\n",
            "10240 [D loss: 0.517627, acc.: 74.22%] [G loss: 3.342978]\n",
            "10260 [D loss: 0.460103, acc.: 78.12%] [G loss: 3.365847]\n",
            "10280 [D loss: 0.587470, acc.: 71.48%] [G loss: 3.078151]\n",
            "10300 [D loss: 0.551426, acc.: 71.09%] [G loss: 3.248818]\n",
            "10320 [D loss: 0.473113, acc.: 79.30%] [G loss: 3.352407]\n",
            "10340 [D loss: 0.537743, acc.: 72.27%] [G loss: 3.317121]\n",
            "10360 [D loss: 0.588627, acc.: 67.97%] [G loss: 3.164304]\n",
            "10380 [D loss: 0.448552, acc.: 79.69%] [G loss: 3.517821]\n",
            "10400 [D loss: 0.541484, acc.: 74.61%] [G loss: 3.315915]\n",
            "10420 [D loss: 0.542993, acc.: 73.05%] [G loss: 3.216286]\n",
            "10440 [D loss: 0.517513, acc.: 75.00%] [G loss: 3.550087]\n",
            "10460 [D loss: 0.504088, acc.: 71.88%] [G loss: 3.384960]\n",
            "10480 [D loss: 0.539056, acc.: 71.48%] [G loss: 3.272975]\n",
            "10500 [D loss: 0.492815, acc.: 77.34%] [G loss: 3.711994]\n",
            "10520 [D loss: 0.554575, acc.: 71.09%] [G loss: 3.243652]\n",
            "10540 [D loss: 0.500871, acc.: 74.22%] [G loss: 3.551219]\n",
            "10560 [D loss: 0.509121, acc.: 74.22%] [G loss: 3.753717]\n",
            "10580 [D loss: 0.472896, acc.: 78.52%] [G loss: 3.680599]\n",
            "10600 [D loss: 0.515471, acc.: 75.78%] [G loss: 3.377984]\n",
            "10620 [D loss: 0.527231, acc.: 73.05%] [G loss: 3.494030]\n",
            "10640 [D loss: 0.573346, acc.: 69.92%] [G loss: 3.389130]\n",
            "10660 [D loss: 0.485159, acc.: 74.22%] [G loss: 3.292894]\n",
            "10680 [D loss: 0.508590, acc.: 77.34%] [G loss: 3.381705]\n",
            "10700 [D loss: 0.538287, acc.: 73.44%] [G loss: 3.526531]\n",
            "10720 [D loss: 0.440515, acc.: 82.42%] [G loss: 3.999088]\n",
            "10740 [D loss: 0.554780, acc.: 71.48%] [G loss: 3.023639]\n",
            "10760 [D loss: 0.446095, acc.: 79.30%] [G loss: 3.778090]\n",
            "10780 [D loss: 0.524891, acc.: 74.22%] [G loss: 3.490485]\n",
            "10800 [D loss: 0.473251, acc.: 76.95%] [G loss: 3.772117]\n",
            "10820 [D loss: 0.487014, acc.: 78.12%] [G loss: 3.334061]\n",
            "10840 [D loss: 0.523101, acc.: 73.83%] [G loss: 3.318457]\n",
            "10860 [D loss: 0.450758, acc.: 79.69%] [G loss: 3.652252]\n",
            "10880 [D loss: 0.520331, acc.: 74.22%] [G loss: 3.400944]\n",
            "10900 [D loss: 0.447037, acc.: 80.08%] [G loss: 3.830027]\n",
            "10920 [D loss: 0.422754, acc.: 81.25%] [G loss: 3.625535]\n",
            "10940 [D loss: 0.447589, acc.: 81.64%] [G loss: 3.148043]\n",
            "10960 [D loss: 0.700728, acc.: 58.98%] [G loss: 2.650975]\n",
            "10980 [D loss: 0.614031, acc.: 70.31%] [G loss: 2.122570]\n",
            "11000 [D loss: 1.276822, acc.: 35.55%] [G loss: 3.990604]\n",
            "11020 [D loss: 1.020473, acc.: 41.02%] [G loss: 1.922032]\n",
            "11040 [D loss: 1.011644, acc.: 40.23%] [G loss: 2.183817]\n",
            "11060 [D loss: 0.694202, acc.: 60.55%] [G loss: 2.398306]\n",
            "11080 [D loss: 0.762327, acc.: 57.42%] [G loss: 2.201102]\n",
            "11100 [D loss: 0.625428, acc.: 64.84%] [G loss: 2.367588]\n",
            "11120 [D loss: 0.670380, acc.: 63.28%] [G loss: 2.238606]\n",
            "11140 [D loss: 0.650026, acc.: 62.11%] [G loss: 2.371754]\n",
            "11160 [D loss: 0.619576, acc.: 66.41%] [G loss: 2.403455]\n",
            "11180 [D loss: 0.617886, acc.: 64.45%] [G loss: 2.272107]\n",
            "11200 [D loss: 0.536845, acc.: 76.17%] [G loss: 2.494151]\n",
            "11220 [D loss: 0.661309, acc.: 57.81%] [G loss: 2.234275]\n",
            "11240 [D loss: 0.551907, acc.: 72.27%] [G loss: 2.603610]\n",
            "11260 [D loss: 0.604023, acc.: 70.31%] [G loss: 2.367762]\n",
            "11280 [D loss: 0.548235, acc.: 75.39%] [G loss: 2.435651]\n",
            "11300 [D loss: 0.607261, acc.: 66.02%] [G loss: 2.370519]\n",
            "11320 [D loss: 0.505404, acc.: 77.73%] [G loss: 2.733431]\n",
            "11340 [D loss: 0.611248, acc.: 67.58%] [G loss: 2.445721]\n",
            "11360 [D loss: 0.528637, acc.: 72.66%] [G loss: 2.567325]\n",
            "11380 [D loss: 0.572976, acc.: 70.31%] [G loss: 2.440744]\n",
            "11400 [D loss: 0.478375, acc.: 78.91%] [G loss: 2.863979]\n",
            "11420 [D loss: 0.570302, acc.: 70.31%] [G loss: 2.469458]\n",
            "11440 [D loss: 0.480470, acc.: 76.95%] [G loss: 2.693382]\n",
            "11460 [D loss: 0.502100, acc.: 78.12%] [G loss: 2.764740]\n",
            "11480 [D loss: 0.647988, acc.: 61.33%] [G loss: 2.322744]\n",
            "11500 [D loss: 0.547121, acc.: 73.44%] [G loss: 2.674735]\n",
            "11520 [D loss: 0.530784, acc.: 72.27%] [G loss: 2.651298]\n",
            "11540 [D loss: 0.529937, acc.: 75.39%] [G loss: 2.717624]\n",
            "11560 [D loss: 0.535072, acc.: 71.88%] [G loss: 2.706164]\n",
            "11580 [D loss: 0.559518, acc.: 70.31%] [G loss: 2.496435]\n",
            "11600 [D loss: 0.541262, acc.: 71.09%] [G loss: 2.605942]\n",
            "11620 [D loss: 0.540037, acc.: 72.27%] [G loss: 2.676545]\n",
            "11640 [D loss: 0.583341, acc.: 69.14%] [G loss: 2.672798]\n",
            "11660 [D loss: 0.536410, acc.: 70.31%] [G loss: 2.583915]\n",
            "11680 [D loss: 0.636952, acc.: 65.23%] [G loss: 2.457157]\n",
            "11700 [D loss: 0.542288, acc.: 74.61%] [G loss: 2.686178]\n",
            "11720 [D loss: 0.548405, acc.: 72.27%] [G loss: 2.748657]\n",
            "11740 [D loss: 0.544371, acc.: 71.09%] [G loss: 2.627225]\n",
            "11760 [D loss: 0.555421, acc.: 73.83%] [G loss: 2.617417]\n",
            "11780 [D loss: 0.556547, acc.: 71.88%] [G loss: 2.667327]\n",
            "11800 [D loss: 0.539020, acc.: 73.05%] [G loss: 2.562721]\n",
            "11820 [D loss: 0.588988, acc.: 68.75%] [G loss: 2.699615]\n",
            "11840 [D loss: 0.561335, acc.: 72.27%] [G loss: 2.884151]\n",
            "11860 [D loss: 0.585834, acc.: 70.31%] [G loss: 2.575335]\n",
            "11880 [D loss: 0.579551, acc.: 70.70%] [G loss: 2.697793]\n",
            "11900 [D loss: 0.566379, acc.: 69.53%] [G loss: 2.729727]\n",
            "11920 [D loss: 0.529897, acc.: 76.95%] [G loss: 2.734351]\n",
            "11940 [D loss: 0.467717, acc.: 82.03%] [G loss: 2.962924]\n",
            "11960 [D loss: 0.501761, acc.: 74.61%] [G loss: 2.954123]\n",
            "11980 [D loss: 0.483712, acc.: 79.69%] [G loss: 2.965565]\n",
            "12000 [D loss: 0.631497, acc.: 65.23%] [G loss: 2.581028]\n",
            "12020 [D loss: 0.519785, acc.: 76.95%] [G loss: 2.821410]\n",
            "12040 [D loss: 0.565659, acc.: 68.36%] [G loss: 2.951615]\n",
            "12060 [D loss: 0.517299, acc.: 76.56%] [G loss: 2.849237]\n",
            "12080 [D loss: 0.521661, acc.: 74.61%] [G loss: 2.958178]\n",
            "12100 [D loss: 0.571238, acc.: 69.14%] [G loss: 2.722586]\n",
            "12120 [D loss: 0.542645, acc.: 71.48%] [G loss: 2.705242]\n",
            "12140 [D loss: 0.544721, acc.: 71.09%] [G loss: 2.802454]\n",
            "12160 [D loss: 0.503561, acc.: 75.39%] [G loss: 3.027832]\n",
            "12180 [D loss: 0.525652, acc.: 72.27%] [G loss: 2.878450]\n",
            "12200 [D loss: 0.560948, acc.: 69.14%] [G loss: 2.880809]\n",
            "12220 [D loss: 0.487219, acc.: 75.78%] [G loss: 3.045615]\n",
            "12240 [D loss: 0.567656, acc.: 68.36%] [G loss: 2.788461]\n",
            "12260 [D loss: 0.537036, acc.: 76.17%] [G loss: 3.069062]\n",
            "12280 [D loss: 0.507075, acc.: 75.78%] [G loss: 2.832850]\n",
            "12300 [D loss: 0.535838, acc.: 71.88%] [G loss: 2.905008]\n",
            "12320 [D loss: 0.556899, acc.: 73.44%] [G loss: 2.808980]\n",
            "12340 [D loss: 0.508283, acc.: 74.61%] [G loss: 3.053937]\n",
            "12360 [D loss: 0.545322, acc.: 72.27%] [G loss: 2.859631]\n",
            "12380 [D loss: 0.518673, acc.: 73.83%] [G loss: 2.931576]\n",
            "12400 [D loss: 0.519908, acc.: 73.05%] [G loss: 2.900983]\n",
            "12420 [D loss: 0.516243, acc.: 75.78%] [G loss: 2.996576]\n",
            "12440 [D loss: 0.527821, acc.: 73.83%] [G loss: 2.892684]\n",
            "12460 [D loss: 0.400323, acc.: 86.33%] [G loss: 2.663232]\n",
            "12480 [D loss: 0.514368, acc.: 74.22%] [G loss: 1.924700]\n",
            "12500 [D loss: 0.461000, acc.: 82.03%] [G loss: 1.828295]\n",
            "12520 [D loss: 1.655877, acc.: 27.34%] [G loss: 1.637146]\n",
            "12540 [D loss: 1.005642, acc.: 40.62%] [G loss: 3.605812]\n",
            "12560 [D loss: 0.515082, acc.: 76.17%] [G loss: 3.303570]\n",
            "12580 [D loss: 0.680801, acc.: 59.77%] [G loss: 2.726512]\n",
            "12600 [D loss: 0.529368, acc.: 74.22%] [G loss: 3.074620]\n",
            "12620 [D loss: 0.588566, acc.: 67.58%] [G loss: 2.784612]\n",
            "12640 [D loss: 0.532895, acc.: 74.22%] [G loss: 3.327515]\n",
            "12660 [D loss: 0.513714, acc.: 74.61%] [G loss: 3.119703]\n",
            "12680 [D loss: 0.563155, acc.: 70.70%] [G loss: 2.945489]\n",
            "12700 [D loss: 0.516924, acc.: 74.22%] [G loss: 3.217704]\n",
            "12720 [D loss: 0.570716, acc.: 69.53%] [G loss: 2.879184]\n",
            "12740 [D loss: 0.576770, acc.: 67.58%] [G loss: 3.006040]\n",
            "12760 [D loss: 0.509176, acc.: 75.39%] [G loss: 3.214006]\n",
            "12780 [D loss: 0.552144, acc.: 70.31%] [G loss: 3.017189]\n",
            "12800 [D loss: 0.536563, acc.: 71.09%] [G loss: 2.946986]\n",
            "12820 [D loss: 0.531583, acc.: 72.66%] [G loss: 3.068321]\n",
            "12840 [D loss: 0.470347, acc.: 78.52%] [G loss: 3.289394]\n",
            "12860 [D loss: 0.482417, acc.: 77.34%] [G loss: 3.227854]\n",
            "12880 [D loss: 0.454947, acc.: 81.64%] [G loss: 3.496037]\n",
            "12900 [D loss: 0.488847, acc.: 73.83%] [G loss: 3.294210]\n",
            "12920 [D loss: 0.566683, acc.: 68.36%] [G loss: 2.951724]\n",
            "12940 [D loss: 0.516678, acc.: 71.88%] [G loss: 3.290413]\n",
            "12960 [D loss: 0.455224, acc.: 82.03%] [G loss: 3.291854]\n",
            "12980 [D loss: 0.438863, acc.: 79.30%] [G loss: 3.535181]\n",
            "13000 [D loss: 0.539923, acc.: 72.27%] [G loss: 3.266219]\n",
            "13020 [D loss: 0.549973, acc.: 74.61%] [G loss: 3.169273]\n",
            "13040 [D loss: 0.477059, acc.: 78.12%] [G loss: 3.356713]\n",
            "13060 [D loss: 0.540135, acc.: 72.27%] [G loss: 3.392999]\n",
            "13080 [D loss: 0.556472, acc.: 71.88%] [G loss: 3.176776]\n",
            "13100 [D loss: 0.505731, acc.: 75.00%] [G loss: 3.351794]\n",
            "13120 [D loss: 0.470781, acc.: 80.86%] [G loss: 3.487991]\n",
            "13140 [D loss: 0.512443, acc.: 75.00%] [G loss: 3.363934]\n",
            "13160 [D loss: 0.552319, acc.: 71.48%] [G loss: 3.151126]\n",
            "13180 [D loss: 0.538690, acc.: 72.27%] [G loss: 2.996053]\n",
            "13200 [D loss: 0.477102, acc.: 79.30%] [G loss: 3.678388]\n",
            "13220 [D loss: 0.515859, acc.: 76.17%] [G loss: 3.179491]\n",
            "13240 [D loss: 0.516016, acc.: 72.66%] [G loss: 3.424977]\n",
            "13260 [D loss: 0.514858, acc.: 75.39%] [G loss: 3.227012]\n",
            "13280 [D loss: 0.478877, acc.: 79.69%] [G loss: 3.512269]\n",
            "13300 [D loss: 0.471060, acc.: 78.52%] [G loss: 3.429971]\n",
            "13320 [D loss: 0.527638, acc.: 73.83%] [G loss: 3.159870]\n",
            "13340 [D loss: 0.555022, acc.: 71.09%] [G loss: 3.350800]\n",
            "13360 [D loss: 0.459904, acc.: 78.91%] [G loss: 3.608495]\n",
            "13380 [D loss: 0.534414, acc.: 71.48%] [G loss: 3.004378]\n",
            "13400 [D loss: 0.470529, acc.: 75.78%] [G loss: 3.506584]\n",
            "13420 [D loss: 0.516823, acc.: 74.22%] [G loss: 3.205226]\n",
            "13440 [D loss: 0.464780, acc.: 79.30%] [G loss: 3.345053]\n",
            "13460 [D loss: 0.476090, acc.: 76.95%] [G loss: 3.757249]\n",
            "13480 [D loss: 0.487283, acc.: 74.61%] [G loss: 3.468806]\n",
            "13500 [D loss: 0.506332, acc.: 75.78%] [G loss: 3.099554]\n",
            "13520 [D loss: 0.451726, acc.: 79.30%] [G loss: 3.436162]\n",
            "13540 [D loss: 0.530954, acc.: 73.83%] [G loss: 3.261657]\n",
            "13560 [D loss: 0.543526, acc.: 72.27%] [G loss: 3.415791]\n",
            "13580 [D loss: 0.601354, acc.: 68.36%] [G loss: 3.378911]\n",
            "13600 [D loss: 0.480863, acc.: 76.95%] [G loss: 3.737319]\n",
            "13620 [D loss: 0.477150, acc.: 79.69%] [G loss: 3.488276]\n",
            "13640 [D loss: 0.572365, acc.: 67.97%] [G loss: 3.475900]\n",
            "13660 [D loss: 0.465934, acc.: 76.17%] [G loss: 3.663110]\n",
            "13680 [D loss: 0.562223, acc.: 70.31%] [G loss: 3.250568]\n",
            "13700 [D loss: 0.471381, acc.: 77.34%] [G loss: 3.350770]\n",
            "13720 [D loss: 0.539190, acc.: 72.27%] [G loss: 3.435258]\n",
            "13740 [D loss: 0.535424, acc.: 74.22%] [G loss: 3.315818]\n",
            "13760 [D loss: 0.544912, acc.: 74.61%] [G loss: 3.279789]\n",
            "13780 [D loss: 0.531884, acc.: 71.88%] [G loss: 3.303248]\n",
            "13800 [D loss: 0.466384, acc.: 81.64%] [G loss: 3.913865]\n",
            "13820 [D loss: 0.483283, acc.: 80.47%] [G loss: 3.625456]\n",
            "13840 [D loss: 0.500751, acc.: 76.95%] [G loss: 3.330792]\n",
            "13860 [D loss: 0.475901, acc.: 75.39%] [G loss: 3.609674]\n",
            "13880 [D loss: 0.514307, acc.: 74.61%] [G loss: 3.538429]\n",
            "13900 [D loss: 0.501770, acc.: 73.83%] [G loss: 3.219128]\n",
            "13920 [D loss: 0.527554, acc.: 72.27%] [G loss: 3.338284]\n",
            "13940 [D loss: 0.361135, acc.: 87.50%] [G loss: 3.837409]\n",
            "13960 [D loss: 0.249773, acc.: 94.92%] [G loss: 2.314275]\n",
            "13980 [D loss: 0.416265, acc.: 81.25%] [G loss: 2.129807]\n",
            "14000 [D loss: 0.564183, acc.: 72.66%] [G loss: 4.962333]\n",
            "14020 [D loss: 0.876268, acc.: 51.56%] [G loss: 3.306720]\n",
            "14040 [D loss: 0.802849, acc.: 58.98%] [G loss: 2.724100]\n",
            "14060 [D loss: 0.647584, acc.: 66.80%] [G loss: 3.349844]\n",
            "14080 [D loss: 0.634804, acc.: 68.36%] [G loss: 3.767342]\n",
            "14100 [D loss: 0.662368, acc.: 65.23%] [G loss: 3.933133]\n",
            "14120 [D loss: 0.590022, acc.: 67.97%] [G loss: 3.061307]\n",
            "14140 [D loss: 0.567790, acc.: 70.31%] [G loss: 3.787605]\n",
            "14160 [D loss: 0.403391, acc.: 82.81%] [G loss: 3.871509]\n",
            "14180 [D loss: 0.508371, acc.: 77.73%] [G loss: 3.712183]\n",
            "14200 [D loss: 0.580748, acc.: 72.66%] [G loss: 3.214593]\n",
            "14220 [D loss: 0.433519, acc.: 80.08%] [G loss: 3.697690]\n",
            "14240 [D loss: 0.523045, acc.: 72.66%] [G loss: 3.871648]\n",
            "14260 [D loss: 0.387334, acc.: 85.16%] [G loss: 4.204786]\n",
            "14280 [D loss: 0.466516, acc.: 77.73%] [G loss: 3.978915]\n",
            "14300 [D loss: 0.615588, acc.: 64.06%] [G loss: 3.543861]\n",
            "14320 [D loss: 0.447531, acc.: 79.30%] [G loss: 3.788499]\n",
            "14340 [D loss: 0.476791, acc.: 76.56%] [G loss: 3.535135]\n",
            "14360 [D loss: 0.477660, acc.: 75.78%] [G loss: 3.645558]\n",
            "14380 [D loss: 0.429958, acc.: 83.20%] [G loss: 3.859314]\n",
            "14400 [D loss: 0.455419, acc.: 76.17%] [G loss: 3.917985]\n",
            "14420 [D loss: 0.527009, acc.: 72.27%] [G loss: 3.523002]\n",
            "14440 [D loss: 0.422779, acc.: 83.20%] [G loss: 3.863932]\n",
            "14460 [D loss: 0.505918, acc.: 76.17%] [G loss: 3.615530]\n",
            "14480 [D loss: 0.510291, acc.: 75.39%] [G loss: 3.871922]\n",
            "14500 [D loss: 0.503213, acc.: 76.56%] [G loss: 3.659607]\n",
            "14520 [D loss: 0.529966, acc.: 70.31%] [G loss: 3.416532]\n",
            "14540 [D loss: 0.479326, acc.: 80.08%] [G loss: 4.010662]\n",
            "14560 [D loss: 0.533406, acc.: 70.70%] [G loss: 3.529429]\n",
            "14580 [D loss: 0.470678, acc.: 75.78%] [G loss: 3.928273]\n",
            "14600 [D loss: 0.477368, acc.: 75.78%] [G loss: 3.683065]\n",
            "14620 [D loss: 0.422822, acc.: 83.98%] [G loss: 3.942340]\n",
            "14640 [D loss: 0.508791, acc.: 75.39%] [G loss: 3.731540]\n",
            "14660 [D loss: 0.460957, acc.: 77.34%] [G loss: 3.667766]\n",
            "14680 [D loss: 0.485091, acc.: 75.78%] [G loss: 3.914406]\n",
            "14700 [D loss: 0.499831, acc.: 76.95%] [G loss: 3.656215]\n",
            "14720 [D loss: 0.491007, acc.: 75.00%] [G loss: 4.034770]\n",
            "14740 [D loss: 0.520038, acc.: 74.22%] [G loss: 3.799109]\n",
            "14760 [D loss: 0.405046, acc.: 84.38%] [G loss: 3.861748]\n",
            "14780 [D loss: 0.477854, acc.: 74.61%] [G loss: 3.840379]\n",
            "14800 [D loss: 0.444595, acc.: 79.69%] [G loss: 3.644909]\n",
            "14820 [D loss: 0.429566, acc.: 82.42%] [G loss: 4.028643]\n",
            "14840 [D loss: 0.419550, acc.: 82.03%] [G loss: 3.996628]\n",
            "14860 [D loss: 0.494524, acc.: 75.00%] [G loss: 3.880885]\n",
            "14880 [D loss: 0.437647, acc.: 82.42%] [G loss: 3.655240]\n",
            "14900 [D loss: 0.489100, acc.: 76.56%] [G loss: 3.998560]\n",
            "14920 [D loss: 0.431220, acc.: 78.52%] [G loss: 3.940113]\n",
            "14940 [D loss: 0.483580, acc.: 76.95%] [G loss: 3.898514]\n",
            "14960 [D loss: 0.522769, acc.: 75.78%] [G loss: 3.993024]\n",
            "14980 [D loss: 0.457025, acc.: 78.12%] [G loss: 4.041635]\n",
            "15000 [D loss: 0.413460, acc.: 82.42%] [G loss: 4.081390]\n",
            "15020 [D loss: 0.433416, acc.: 80.08%] [G loss: 4.144864]\n",
            "15040 [D loss: 0.539854, acc.: 72.27%] [G loss: 3.699645]\n",
            "15060 [D loss: 0.415216, acc.: 81.64%] [G loss: 3.860651]\n",
            "15080 [D loss: 0.464055, acc.: 75.00%] [G loss: 3.705179]\n",
            "15100 [D loss: 0.434475, acc.: 81.25%] [G loss: 3.936723]\n",
            "15120 [D loss: 0.550567, acc.: 75.78%] [G loss: 3.676489]\n",
            "15140 [D loss: 0.456978, acc.: 79.30%] [G loss: 3.433469]\n",
            "15160 [D loss: 0.578935, acc.: 70.70%] [G loss: 3.619555]\n",
            "15180 [D loss: 0.451368, acc.: 80.47%] [G loss: 3.987439]\n",
            "15200 [D loss: 0.477655, acc.: 76.56%] [G loss: 3.966389]\n",
            "15220 [D loss: 0.460447, acc.: 77.34%] [G loss: 3.678451]\n",
            "15240 [D loss: 0.414824, acc.: 81.64%] [G loss: 4.147241]\n",
            "15260 [D loss: 0.472534, acc.: 74.22%] [G loss: 4.327223]\n",
            "15280 [D loss: 0.497207, acc.: 75.78%] [G loss: 3.777789]\n",
            "15300 [D loss: 0.445820, acc.: 77.34%] [G loss: 4.233618]\n",
            "15320 [D loss: 0.440864, acc.: 75.78%] [G loss: 4.333769]\n",
            "15340 [D loss: 0.379106, acc.: 83.59%] [G loss: 4.092604]\n",
            "15360 [D loss: 0.457623, acc.: 78.52%] [G loss: 3.939475]\n",
            "15380 [D loss: 0.404205, acc.: 82.81%] [G loss: 4.146902]\n",
            "15400 [D loss: 0.495257, acc.: 75.78%] [G loss: 3.996731]\n",
            "15420 [D loss: 0.437965, acc.: 81.64%] [G loss: 3.777695]\n",
            "15440 [D loss: 0.405161, acc.: 84.77%] [G loss: 3.994419]\n",
            "15460 [D loss: 0.458802, acc.: 77.34%] [G loss: 4.091534]\n",
            "15480 [D loss: 0.430744, acc.: 80.08%] [G loss: 4.683117]\n",
            "15500 [D loss: 0.351309, acc.: 87.11%] [G loss: 3.642268]\n",
            "15520 [D loss: 0.413043, acc.: 81.25%] [G loss: 2.384441]\n",
            "15540 [D loss: 1.460486, acc.: 29.69%] [G loss: 3.131921]\n",
            "15560 [D loss: 0.654818, acc.: 62.50%] [G loss: 3.731858]\n",
            "15580 [D loss: 0.701255, acc.: 59.77%] [G loss: 3.575898]\n",
            "15600 [D loss: 0.531549, acc.: 75.39%] [G loss: 4.200634]\n",
            "15620 [D loss: 0.441348, acc.: 80.86%] [G loss: 4.224570]\n",
            "15640 [D loss: 0.502628, acc.: 74.61%] [G loss: 4.008899]\n",
            "15660 [D loss: 0.491756, acc.: 75.78%] [G loss: 4.038079]\n",
            "15680 [D loss: 0.504131, acc.: 72.66%] [G loss: 4.434848]\n",
            "15700 [D loss: 0.484981, acc.: 73.44%] [G loss: 4.292992]\n",
            "15720 [D loss: 0.491714, acc.: 78.91%] [G loss: 4.348589]\n",
            "15740 [D loss: 0.475363, acc.: 76.95%] [G loss: 4.398924]\n",
            "15760 [D loss: 0.468843, acc.: 76.56%] [G loss: 4.099914]\n",
            "15780 [D loss: 0.509836, acc.: 72.66%] [G loss: 4.061299]\n",
            "15800 [D loss: 0.489528, acc.: 76.95%] [G loss: 3.828831]\n",
            "15820 [D loss: 0.417846, acc.: 82.81%] [G loss: 4.443118]\n",
            "15840 [D loss: 0.512083, acc.: 74.61%] [G loss: 3.845281]\n",
            "15860 [D loss: 0.461684, acc.: 80.08%] [G loss: 3.894706]\n",
            "15880 [D loss: 0.491398, acc.: 75.00%] [G loss: 3.857377]\n",
            "15900 [D loss: 0.510368, acc.: 73.83%] [G loss: 3.922913]\n",
            "15920 [D loss: 0.427035, acc.: 80.08%] [G loss: 4.069708]\n",
            "15940 [D loss: 0.431362, acc.: 82.03%] [G loss: 4.060884]\n",
            "15960 [D loss: 0.408575, acc.: 82.42%] [G loss: 4.019485]\n",
            "15980 [D loss: 0.561857, acc.: 69.14%] [G loss: 3.824518]\n",
            "16000 [D loss: 0.415451, acc.: 82.81%] [G loss: 4.343228]\n",
            "16020 [D loss: 0.535959, acc.: 73.44%] [G loss: 3.998017]\n",
            "16040 [D loss: 0.447848, acc.: 76.56%] [G loss: 4.220665]\n",
            "16060 [D loss: 0.444609, acc.: 80.47%] [G loss: 4.017028]\n",
            "16080 [D loss: 0.466618, acc.: 77.73%] [G loss: 4.254573]\n",
            "16100 [D loss: 0.469322, acc.: 77.73%] [G loss: 4.177071]\n",
            "16120 [D loss: 0.462975, acc.: 78.52%] [G loss: 4.218904]\n",
            "16140 [D loss: 0.450834, acc.: 79.69%] [G loss: 4.461110]\n",
            "16160 [D loss: 0.428088, acc.: 77.73%] [G loss: 4.393669]\n",
            "16180 [D loss: 0.434908, acc.: 79.69%] [G loss: 4.271082]\n",
            "16200 [D loss: 0.400696, acc.: 84.38%] [G loss: 4.359937]\n",
            "16220 [D loss: 0.440115, acc.: 80.47%] [G loss: 4.637210]\n",
            "16240 [D loss: 0.478379, acc.: 75.39%] [G loss: 4.318015]\n",
            "16260 [D loss: 0.433988, acc.: 78.91%] [G loss: 4.268925]\n",
            "16280 [D loss: 0.460690, acc.: 76.56%] [G loss: 4.315918]\n",
            "16300 [D loss: 0.439739, acc.: 77.34%] [G loss: 4.578141]\n",
            "16320 [D loss: 0.430087, acc.: 80.86%] [G loss: 4.009703]\n",
            "16340 [D loss: 0.460041, acc.: 80.08%] [G loss: 4.264230]\n",
            "16360 [D loss: 0.439422, acc.: 78.52%] [G loss: 4.250878]\n",
            "16380 [D loss: 0.519640, acc.: 75.78%] [G loss: 4.162185]\n",
            "16400 [D loss: 0.439327, acc.: 82.42%] [G loss: 4.319679]\n",
            "16420 [D loss: 0.445256, acc.: 80.47%] [G loss: 4.307646]\n",
            "16440 [D loss: 0.463380, acc.: 78.91%] [G loss: 4.323819]\n",
            "16460 [D loss: 0.366430, acc.: 85.55%] [G loss: 4.474254]\n",
            "16480 [D loss: 0.573007, acc.: 72.66%] [G loss: 4.218464]\n",
            "16500 [D loss: 0.490587, acc.: 77.73%] [G loss: 4.543053]\n",
            "16520 [D loss: 0.431848, acc.: 80.47%] [G loss: 4.352791]\n",
            "16540 [D loss: 0.392420, acc.: 86.33%] [G loss: 4.112994]\n",
            "16560 [D loss: 0.427359, acc.: 80.08%] [G loss: 4.664361]\n",
            "16580 [D loss: 0.493197, acc.: 76.17%] [G loss: 4.362307]\n",
            "16600 [D loss: 0.507962, acc.: 74.22%] [G loss: 4.586726]\n",
            "16620 [D loss: 0.489458, acc.: 75.00%] [G loss: 4.360618]\n",
            "16640 [D loss: 0.516048, acc.: 73.05%] [G loss: 4.127778]\n",
            "16660 [D loss: 0.461432, acc.: 79.30%] [G loss: 4.339099]\n",
            "16680 [D loss: 0.500771, acc.: 75.39%] [G loss: 4.467901]\n",
            "16700 [D loss: 0.510051, acc.: 74.61%] [G loss: 4.194224]\n",
            "16720 [D loss: 0.424746, acc.: 80.47%] [G loss: 4.386207]\n",
            "16740 [D loss: 0.461779, acc.: 76.95%] [G loss: 4.773341]\n",
            "16760 [D loss: 0.418561, acc.: 80.86%] [G loss: 4.717523]\n",
            "16780 [D loss: 0.461508, acc.: 75.78%] [G loss: 4.037402]\n",
            "16800 [D loss: 0.326891, acc.: 89.84%] [G loss: 5.149434]\n",
            "16820 [D loss: 0.363826, acc.: 82.03%] [G loss: 2.931387]\n",
            "16840 [D loss: 0.339997, acc.: 84.77%] [G loss: 2.588045]\n",
            "16860 [D loss: 26.853925, acc.: 15.62%] [G loss: 2.911011]\n",
            "16880 [D loss: 0.394452, acc.: 87.11%] [G loss: 1.716462]\n",
            "16900 [D loss: 2.237671, acc.: 16.41%] [G loss: 1.712393]\n",
            "16920 [D loss: 1.458345, acc.: 30.86%] [G loss: 2.550845]\n",
            "16940 [D loss: 0.737277, acc.: 61.33%] [G loss: 3.131141]\n",
            "16960 [D loss: 0.584039, acc.: 68.36%] [G loss: 3.105033]\n",
            "16980 [D loss: 0.755096, acc.: 58.59%] [G loss: 2.752034]\n",
            "17000 [D loss: 0.581359, acc.: 69.92%] [G loss: 2.931543]\n",
            "17020 [D loss: 0.617891, acc.: 62.50%] [G loss: 2.774358]\n",
            "17040 [D loss: 0.491174, acc.: 78.12%] [G loss: 2.986320]\n",
            "17060 [D loss: 0.532627, acc.: 73.44%] [G loss: 3.065826]\n",
            "17080 [D loss: 0.529471, acc.: 72.66%] [G loss: 2.945490]\n",
            "17100 [D loss: 0.543741, acc.: 71.48%] [G loss: 3.152033]\n",
            "17120 [D loss: 0.487838, acc.: 73.05%] [G loss: 3.223122]\n",
            "17140 [D loss: 0.571102, acc.: 68.36%] [G loss: 2.954558]\n",
            "17160 [D loss: 0.494883, acc.: 74.61%] [G loss: 3.199336]\n",
            "17180 [D loss: 0.518128, acc.: 72.66%] [G loss: 2.978494]\n",
            "17200 [D loss: 0.426492, acc.: 82.03%] [G loss: 3.628711]\n",
            "17220 [D loss: 0.486083, acc.: 78.52%] [G loss: 3.192448]\n",
            "17240 [D loss: 0.499847, acc.: 75.00%] [G loss: 3.242116]\n",
            "17260 [D loss: 0.472447, acc.: 75.39%] [G loss: 3.454598]\n",
            "17280 [D loss: 0.480675, acc.: 76.56%] [G loss: 3.186957]\n",
            "17300 [D loss: 0.490625, acc.: 74.61%] [G loss: 3.383214]\n",
            "17320 [D loss: 0.505309, acc.: 77.34%] [G loss: 3.245159]\n",
            "17340 [D loss: 0.498318, acc.: 75.00%] [G loss: 3.113139]\n",
            "17360 [D loss: 0.503673, acc.: 75.00%] [G loss: 3.532748]\n",
            "17380 [D loss: 0.519021, acc.: 74.61%] [G loss: 3.291240]\n",
            "17400 [D loss: 0.445327, acc.: 81.25%] [G loss: 3.763183]\n",
            "17420 [D loss: 0.567540, acc.: 68.75%] [G loss: 2.891686]\n",
            "17440 [D loss: 0.456171, acc.: 79.30%] [G loss: 3.432354]\n",
            "17460 [D loss: 0.586194, acc.: 67.97%] [G loss: 3.189894]\n",
            "17480 [D loss: 0.402734, acc.: 82.81%] [G loss: 4.068878]\n",
            "17500 [D loss: 0.534094, acc.: 73.05%] [G loss: 3.307557]\n",
            "17520 [D loss: 0.465815, acc.: 79.30%] [G loss: 3.653864]\n",
            "17540 [D loss: 0.452841, acc.: 80.47%] [G loss: 3.527058]\n",
            "17560 [D loss: 0.536589, acc.: 75.00%] [G loss: 3.747533]\n",
            "17580 [D loss: 0.428024, acc.: 78.91%] [G loss: 3.556467]\n",
            "17600 [D loss: 0.526724, acc.: 73.05%] [G loss: 3.284200]\n",
            "17620 [D loss: 0.452610, acc.: 82.03%] [G loss: 3.742016]\n",
            "17640 [D loss: 0.485477, acc.: 74.22%] [G loss: 3.530787]\n",
            "17660 [D loss: 0.530541, acc.: 74.61%] [G loss: 3.479295]\n",
            "17680 [D loss: 0.486340, acc.: 76.56%] [G loss: 3.565885]\n",
            "17700 [D loss: 0.479505, acc.: 79.30%] [G loss: 3.577801]\n",
            "17720 [D loss: 0.529274, acc.: 75.00%] [G loss: 3.525657]\n",
            "17740 [D loss: 0.464358, acc.: 78.52%] [G loss: 3.820201]\n",
            "17760 [D loss: 0.410914, acc.: 82.81%] [G loss: 3.774330]\n",
            "17780 [D loss: 0.495847, acc.: 74.61%] [G loss: 3.623478]\n",
            "17800 [D loss: 0.518172, acc.: 73.83%] [G loss: 3.832206]\n",
            "17820 [D loss: 0.485585, acc.: 78.91%] [G loss: 4.102406]\n",
            "17840 [D loss: 0.508448, acc.: 76.17%] [G loss: 3.667813]\n",
            "17860 [D loss: 0.452739, acc.: 82.03%] [G loss: 3.886809]\n",
            "17880 [D loss: 0.475934, acc.: 76.56%] [G loss: 3.780682]\n",
            "17900 [D loss: 0.502197, acc.: 75.00%] [G loss: 3.736197]\n",
            "17920 [D loss: 0.427952, acc.: 82.42%] [G loss: 3.706923]\n",
            "17940 [D loss: 0.421095, acc.: 80.08%] [G loss: 3.769258]\n",
            "17960 [D loss: 0.477589, acc.: 76.17%] [G loss: 4.035055]\n",
            "17980 [D loss: 0.423499, acc.: 80.86%] [G loss: 4.149085]\n",
            "18000 [D loss: 0.500706, acc.: 73.44%] [G loss: 3.860730]\n",
            "18020 [D loss: 0.440855, acc.: 80.47%] [G loss: 3.931010]\n",
            "18040 [D loss: 0.474865, acc.: 75.78%] [G loss: 3.958035]\n",
            "18060 [D loss: 0.402695, acc.: 80.47%] [G loss: 4.294851]\n",
            "18080 [D loss: 0.545151, acc.: 74.61%] [G loss: 3.820875]\n",
            "18100 [D loss: 0.421179, acc.: 78.91%] [G loss: 4.015680]\n",
            "18120 [D loss: 0.534993, acc.: 74.22%] [G loss: 3.976772]\n",
            "18140 [D loss: 0.449787, acc.: 77.34%] [G loss: 3.944748]\n",
            "18160 [D loss: 0.446738, acc.: 80.47%] [G loss: 4.327723]\n",
            "18180 [D loss: 0.420069, acc.: 80.47%] [G loss: 3.960471]\n",
            "18200 [D loss: 0.444070, acc.: 83.98%] [G loss: 4.069080]\n",
            "18220 [D loss: 0.473521, acc.: 76.56%] [G loss: 3.910800]\n",
            "18240 [D loss: 0.425004, acc.: 81.64%] [G loss: 3.932204]\n",
            "18260 [D loss: 0.487726, acc.: 75.39%] [G loss: 3.959025]\n",
            "18280 [D loss: 0.460458, acc.: 78.12%] [G loss: 4.178729]\n",
            "18300 [D loss: 0.475683, acc.: 76.56%] [G loss: 3.879736]\n",
            "18320 [D loss: 0.462485, acc.: 75.39%] [G loss: 4.111789]\n",
            "18340 [D loss: 0.445274, acc.: 79.30%] [G loss: 4.139287]\n",
            "18360 [D loss: 0.447277, acc.: 76.56%] [G loss: 4.288001]\n",
            "18380 [D loss: 0.469876, acc.: 77.73%] [G loss: 3.523548]\n",
            "18400 [D loss: 0.232089, acc.: 95.70%] [G loss: 2.608773]\n",
            "18420 [D loss: 0.582167, acc.: 71.48%] [G loss: 2.768352]\n",
            "18440 [D loss: 0.651282, acc.: 64.06%] [G loss: 3.567502]\n",
            "18460 [D loss: 0.642476, acc.: 63.28%] [G loss: 3.736844]\n",
            "18480 [D loss: 0.535712, acc.: 72.27%] [G loss: 4.189948]\n",
            "18500 [D loss: 0.500594, acc.: 75.39%] [G loss: 4.313096]\n",
            "18520 [D loss: 0.315329, acc.: 87.11%] [G loss: 4.405945]\n",
            "18540 [D loss: 0.428212, acc.: 81.25%] [G loss: 4.586685]\n",
            "18560 [D loss: 0.585291, acc.: 68.36%] [G loss: 3.886494]\n",
            "18580 [D loss: 0.429317, acc.: 82.42%] [G loss: 4.361073]\n",
            "18600 [D loss: 0.489050, acc.: 76.56%] [G loss: 4.437624]\n",
            "18620 [D loss: 0.437342, acc.: 80.08%] [G loss: 4.621763]\n",
            "18640 [D loss: 0.481989, acc.: 78.52%] [G loss: 4.300341]\n",
            "18660 [D loss: 0.466595, acc.: 77.73%] [G loss: 4.309773]\n",
            "18680 [D loss: 0.434210, acc.: 78.12%] [G loss: 4.773806]\n",
            "18700 [D loss: 0.375681, acc.: 82.42%] [G loss: 4.665648]\n",
            "18720 [D loss: 0.507188, acc.: 75.78%] [G loss: 4.024816]\n",
            "18740 [D loss: 0.497357, acc.: 75.78%] [G loss: 3.908529]\n",
            "18760 [D loss: 0.489849, acc.: 77.34%] [G loss: 3.953697]\n",
            "18780 [D loss: 0.474077, acc.: 78.91%] [G loss: 4.405608]\n",
            "18800 [D loss: 0.454617, acc.: 79.30%] [G loss: 4.669702]\n",
            "18820 [D loss: 0.508933, acc.: 73.05%] [G loss: 4.001422]\n",
            "18840 [D loss: 0.489515, acc.: 76.17%] [G loss: 4.248082]\n",
            "18860 [D loss: 0.497188, acc.: 74.22%] [G loss: 4.332491]\n",
            "18880 [D loss: 0.419381, acc.: 80.08%] [G loss: 4.621839]\n",
            "18900 [D loss: 0.449521, acc.: 80.08%] [G loss: 4.417292]\n",
            "18920 [D loss: 0.447489, acc.: 80.86%] [G loss: 4.351349]\n",
            "18940 [D loss: 0.492904, acc.: 76.95%] [G loss: 4.531245]\n",
            "18960 [D loss: 0.459238, acc.: 80.47%] [G loss: 4.638504]\n",
            "18980 [D loss: 0.452056, acc.: 79.69%] [G loss: 4.201149]\n",
            "19000 [D loss: 0.569871, acc.: 69.53%] [G loss: 4.550468]\n",
            "19020 [D loss: 0.420914, acc.: 79.69%] [G loss: 4.366877]\n",
            "19040 [D loss: 0.458125, acc.: 80.47%] [G loss: 4.211919]\n",
            "19060 [D loss: 0.470161, acc.: 78.52%] [G loss: 4.462432]\n",
            "19080 [D loss: 0.397483, acc.: 82.42%] [G loss: 4.641846]\n",
            "19100 [D loss: 0.446689, acc.: 78.12%] [G loss: 4.480523]\n",
            "19120 [D loss: 0.410071, acc.: 81.64%] [G loss: 4.583220]\n",
            "19140 [D loss: 0.574562, acc.: 68.75%] [G loss: 4.278678]\n",
            "19160 [D loss: 0.451770, acc.: 80.08%] [G loss: 4.284024]\n",
            "19180 [D loss: 0.472688, acc.: 76.95%] [G loss: 4.244987]\n",
            "19200 [D loss: 0.451548, acc.: 76.95%] [G loss: 4.568344]\n",
            "19220 [D loss: 0.430456, acc.: 79.30%] [G loss: 4.406656]\n",
            "19240 [D loss: 0.461307, acc.: 77.73%] [G loss: 4.350365]\n",
            "19260 [D loss: 0.473018, acc.: 75.00%] [G loss: 4.282755]\n",
            "19280 [D loss: 0.412683, acc.: 81.64%] [G loss: 4.454531]\n",
            "19300 [D loss: 0.522233, acc.: 75.78%] [G loss: 3.888493]\n",
            "19320 [D loss: 0.385175, acc.: 82.81%] [G loss: 4.913374]\n",
            "19340 [D loss: 0.454036, acc.: 75.78%] [G loss: 4.250471]\n",
            "19360 [D loss: 0.401104, acc.: 81.25%] [G loss: 4.720685]\n",
            "19380 [D loss: 0.373720, acc.: 85.16%] [G loss: 4.765467]\n",
            "19400 [D loss: 0.490565, acc.: 77.73%] [G loss: 4.296782]\n",
            "19420 [D loss: 0.413326, acc.: 81.64%] [G loss: 4.361929]\n",
            "19440 [D loss: 0.408161, acc.: 84.38%] [G loss: 5.022882]\n",
            "19460 [D loss: 0.471397, acc.: 79.69%] [G loss: 4.631384]\n",
            "19480 [D loss: 0.417590, acc.: 80.08%] [G loss: 4.632797]\n",
            "19500 [D loss: 0.439667, acc.: 77.73%] [G loss: 4.500723]\n",
            "19520 [D loss: 0.474521, acc.: 76.95%] [G loss: 4.962427]\n",
            "19540 [D loss: 0.406679, acc.: 80.86%] [G loss: 4.883490]\n",
            "19560 [D loss: 0.348866, acc.: 86.33%] [G loss: 4.855443]\n",
            "19580 [D loss: 0.531724, acc.: 75.78%] [G loss: 4.371767]\n",
            "19600 [D loss: 0.382449, acc.: 83.20%] [G loss: 5.072651]\n",
            "19620 [D loss: 0.499860, acc.: 76.95%] [G loss: 4.623454]\n",
            "19640 [D loss: 0.369224, acc.: 83.59%] [G loss: 4.634332]\n",
            "19660 [D loss: 0.415477, acc.: 83.59%] [G loss: 4.555026]\n",
            "19680 [D loss: 0.472010, acc.: 76.95%] [G loss: 4.475799]\n",
            "19700 [D loss: 0.491271, acc.: 76.95%] [G loss: 4.575051]\n",
            "19720 [D loss: 0.405834, acc.: 81.64%] [G loss: 4.627145]\n",
            "19740 [D loss: 0.450464, acc.: 81.25%] [G loss: 4.830544]\n",
            "19760 [D loss: 0.490285, acc.: 73.44%] [G loss: 4.871658]\n",
            "19780 [D loss: 0.327703, acc.: 87.11%] [G loss: 5.025608]\n",
            "19800 [D loss: 0.410244, acc.: 83.98%] [G loss: 4.760409]\n",
            "19820 [D loss: 0.417287, acc.: 82.42%] [G loss: 4.856427]\n",
            "19840 [D loss: 0.368922, acc.: 85.16%] [G loss: 4.619844]\n",
            "19860 [D loss: 0.439060, acc.: 80.47%] [G loss: 4.627019]\n",
            "19880 [D loss: 0.411021, acc.: 81.25%] [G loss: 4.467164]\n",
            "19900 [D loss: 0.574646, acc.: 73.44%] [G loss: 4.231258]\n",
            "19920 [D loss: 0.446797, acc.: 78.12%] [G loss: 4.849166]\n",
            "19940 [D loss: 0.394812, acc.: 84.38%] [G loss: 4.706212]\n",
            "19960 [D loss: 0.419905, acc.: 81.64%] [G loss: 5.155855]\n",
            "19980 [D loss: 0.422661, acc.: 81.64%] [G loss: 5.317084]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
