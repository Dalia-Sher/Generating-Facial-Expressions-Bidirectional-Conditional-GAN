{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "9_BiCoWGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "based on code from https://github.com/eriklindernoren/Keras-GAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13405e51-9930-44da-f8ee-d0262c6e33a6"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "037b66c2-6292-4f97-d124-7a7d4211c94c"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W1QkhYjJr1G",
        "outputId": "1a85d89f-abd9-459a-a099-25aea87cfb62"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHR9swxs5P0w"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQj2Szg75fm5",
        "outputId": "24deec8c-6476-4580-c776-a8a80029de0b"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "7b5d6221-9bf6-4222-bc43-d01de38d7db2"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "# sm = SMOTE()\n",
        "# X_train, y_train = sm.fit_sample(X_train, y_train)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoWGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoWGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "         # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        self.clip_value = 0.01\n",
        "        optimizer_c = RMSprop(lr=0.00005)\n",
        "\n",
        "        optimizer_gan = Adam(0.0002, 0.5)\n",
        "\n",
        "\n",
        "        # Build and compile the critic\n",
        "        self.critic = self.build_critic()\n",
        "        print(self.critic.summary())\n",
        "        plot_model(self.critic, show_shapes=True)\n",
        "        self.critic.compile(loss=self.wasserstein_loss,\n",
        "            optimizer=optimizer_c,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.critic.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.critic([z, img_, label])\n",
        "        valid = self.critic([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicowgan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicowgan_generator.compile(loss=[self.wasserstein_loss, self.wasserstein_loss],\n",
        "            optimizer=optimizer_c)\n",
        "        \n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 12 * 12, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "\n",
        "        # # foundation for 12x12 image\n",
        "        # n_nodes = 128 * 12 * 12\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((12, 12, 128)))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # generate\n",
        "        # model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "\n",
        "    def build_critic(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='critic')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Critic\n",
        "                # ---------------------\n",
        "\n",
        "                # Select a random batch of images and encode\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs, labels = X_train[idx], y_train[idx]\n",
        "                z_ = self.encoder.predict(imgs)\n",
        "\n",
        "                # Sample noise and generate img\n",
        "                z = np.random.normal(0, 1, (batch_size, 100))\n",
        "                imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "                # Train the critic (img -> z is valid, z -> img is fake)\n",
        "                d_loss_real = self.critic.train_on_batch([z_, imgs, labels], valid)\n",
        "                d_loss_fake = self.critic.train_on_batch([z, imgs_, labels], fake)\n",
        "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "                # Clip critic weights\n",
        "                for l in self.critic.layers:\n",
        "                    weights = l.get_weights()\n",
        "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
        "                    l.set_weights(weights)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicowgan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(\"images/%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0s06Z4lHb7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915bb9ae-9e0e-46b9-aae3-8acd07e622ce"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicowgan = BiCoWGAN()\n",
        "    bicowgan.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"critic\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_73 (InputLayer)           [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 48, 48, 128)  3328        input_73[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_58 (LeakyReLU)      (None, 48, 48, 128)  0           conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 24, 24, 128)  409728      leaky_re_lu_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_59 (LeakyReLU)      (None, 24, 24, 128)  0           conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 12, 12, 128)  409728      leaky_re_lu_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_60 (LeakyReLU)      (None, 12, 12, 128)  0           conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 6, 6, 128)    409728      leaky_re_lu_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_61 (LeakyReLU)      (None, 6, 6, 128)    0           conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 3, 3, 128)    409728      leaky_re_lu_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "input_75 (InputLayer)           [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_74 (InputLayer)           [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_62 (LeakyReLU)      (None, 3, 3, 128)    0           conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_16 (Embedding)        (None, 1, 2304)      13824       input_75[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_41 (Flatten)            (None, 100)          0           input_74[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_40 (Flatten)            (None, 1152)         0           leaky_re_lu_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_42 (Flatten)            (None, 2304)         0           embedding_16[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 3556)         0           flatten_41[0][0]                 \n",
            "                                                                 flatten_40[0][0]                 \n",
            "                                                                 flatten_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                (None, 1)            3557        concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape_8 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_4 (UpSampling2 (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_89 (Conv2D)           (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_5 (UpSampling2 (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_90 (Conv2D)           (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_91 (Conv2D)           (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_92 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_93 (Conv2D)           (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_44 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkuk6BadOwwX",
        "outputId": "c5b57276-2b4e-4a12-8918-88606e0af1dd"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=25000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.701304, acc.: 35.55%] [G loss: 1.423840]\n",
            "20 [D loss: 1.266076, acc.: 12.89%] [G loss: 0.690779]\n",
            "40 [D loss: 4.670751, acc.: 1.95%] [G loss: 73.464378]\n",
            "60 [D loss: 1.273447, acc.: 24.22%] [G loss: 1.865831]\n",
            "80 [D loss: 0.582444, acc.: 72.27%] [G loss: 2.191918]\n",
            "100 [D loss: 0.460353, acc.: 92.58%] [G loss: 3.069184]\n",
            "120 [D loss: 0.733006, acc.: 53.12%] [G loss: 2.198898]\n",
            "140 [D loss: 0.540820, acc.: 46.88%] [G loss: 3.090268]\n",
            "160 [D loss: 0.197886, acc.: 97.27%] [G loss: 9.550190]\n",
            "180 [D loss: 0.904006, acc.: 32.42%] [G loss: 3.277715]\n",
            "200 [D loss: 0.571875, acc.: 80.08%] [G loss: 2.663782]\n",
            "220 [D loss: 0.181169, acc.: 95.70%] [G loss: 7.857949]\n",
            "240 [D loss: 0.424630, acc.: 81.64%] [G loss: 3.402794]\n",
            "260 [D loss: 0.400746, acc.: 87.50%] [G loss: 2.940800]\n",
            "280 [D loss: 0.404634, acc.: 93.36%] [G loss: 2.745633]\n",
            "300 [D loss: 0.732038, acc.: 42.58%] [G loss: 1.686133]\n",
            "320 [D loss: 0.600913, acc.: 79.69%] [G loss: 2.071355]\n",
            "340 [D loss: 0.724206, acc.: 56.25%] [G loss: 3.191658]\n",
            "360 [D loss: 0.492356, acc.: 80.47%] [G loss: 3.648383]\n",
            "380 [D loss: 0.368838, acc.: 82.81%] [G loss: 2.626854]\n",
            "400 [D loss: 0.357426, acc.: 85.55%] [G loss: 7.799756]\n",
            "420 [D loss: 0.679627, acc.: 60.94%] [G loss: 1.966885]\n",
            "440 [D loss: 0.492152, acc.: 75.78%] [G loss: 2.653264]\n",
            "460 [D loss: 0.637069, acc.: 66.41%] [G loss: 1.919004]\n",
            "480 [D loss: 0.542675, acc.: 79.69%] [G loss: 2.157865]\n",
            "500 [D loss: 0.546703, acc.: 71.88%] [G loss: 2.890360]\n",
            "520 [D loss: 0.533189, acc.: 76.56%] [G loss: 2.592449]\n",
            "540 [D loss: 0.491905, acc.: 79.30%] [G loss: 3.568281]\n",
            "560 [D loss: 0.548953, acc.: 72.66%] [G loss: 2.720054]\n",
            "580 [D loss: 0.582315, acc.: 72.27%] [G loss: 2.109256]\n",
            "600 [D loss: 0.686605, acc.: 55.08%] [G loss: 2.466006]\n",
            "620 [D loss: 0.579083, acc.: 67.97%] [G loss: 2.700595]\n",
            "640 [D loss: 0.509027, acc.: 78.52%] [G loss: 2.219714]\n",
            "660 [D loss: 0.629784, acc.: 69.53%] [G loss: 3.069254]\n",
            "680 [D loss: 0.829661, acc.: 43.75%] [G loss: 1.644823]\n",
            "700 [D loss: 0.771409, acc.: 47.27%] [G loss: 2.007702]\n",
            "720 [D loss: 0.619530, acc.: 63.28%] [G loss: 2.514281]\n",
            "740 [D loss: 0.595481, acc.: 70.31%] [G loss: 2.902716]\n",
            "760 [D loss: 0.664259, acc.: 58.59%] [G loss: 2.060695]\n",
            "780 [D loss: 0.482817, acc.: 82.42%] [G loss: 2.738797]\n",
            "800 [D loss: 0.565939, acc.: 73.44%] [G loss: 2.764251]\n",
            "820 [D loss: 0.638804, acc.: 61.33%] [G loss: 3.009369]\n",
            "840 [D loss: 0.581917, acc.: 70.70%] [G loss: 2.859769]\n",
            "860 [D loss: 0.692892, acc.: 57.42%] [G loss: 2.206269]\n",
            "880 [D loss: 0.480986, acc.: 78.52%] [G loss: 2.675019]\n",
            "900 [D loss: 0.503693, acc.: 76.17%] [G loss: 3.121601]\n",
            "920 [D loss: 0.561253, acc.: 71.88%] [G loss: 2.986755]\n",
            "940 [D loss: 0.540696, acc.: 74.22%] [G loss: 2.789685]\n",
            "960 [D loss: 0.681817, acc.: 61.72%] [G loss: 2.201138]\n",
            "980 [D loss: 0.629425, acc.: 63.28%] [G loss: 2.638875]\n",
            "1000 [D loss: 0.737981, acc.: 55.08%] [G loss: 2.184536]\n",
            "1020 [D loss: 0.704512, acc.: 61.33%] [G loss: 2.808267]\n",
            "1040 [D loss: 0.523102, acc.: 74.61%] [G loss: 2.909906]\n",
            "1060 [D loss: 0.451700, acc.: 83.98%] [G loss: 2.850456]\n",
            "1080 [D loss: 0.448999, acc.: 81.25%] [G loss: 3.220979]\n",
            "1100 [D loss: 0.577001, acc.: 72.66%] [G loss: 3.151368]\n",
            "1120 [D loss: 0.524854, acc.: 73.05%] [G loss: 3.159173]\n",
            "1140 [D loss: 0.466975, acc.: 82.03%] [G loss: 3.102515]\n",
            "1160 [D loss: 0.617163, acc.: 68.36%] [G loss: 2.679272]\n",
            "1180 [D loss: 0.477402, acc.: 76.17%] [G loss: 3.583383]\n",
            "1200 [D loss: 0.734036, acc.: 57.42%] [G loss: 2.675820]\n",
            "1220 [D loss: 0.524738, acc.: 76.95%] [G loss: 3.091535]\n",
            "1240 [D loss: 0.546809, acc.: 75.78%] [G loss: 3.382462]\n",
            "1260 [D loss: 0.581530, acc.: 69.92%] [G loss: 2.948097]\n",
            "1280 [D loss: 0.529968, acc.: 77.73%] [G loss: 2.698640]\n",
            "1300 [D loss: 0.483200, acc.: 81.25%] [G loss: 2.626935]\n",
            "1320 [D loss: 0.549103, acc.: 73.83%] [G loss: 3.174645]\n",
            "1340 [D loss: 0.478305, acc.: 78.91%] [G loss: 2.936454]\n",
            "1360 [D loss: 0.492329, acc.: 76.17%] [G loss: 3.005580]\n",
            "1380 [D loss: 0.527557, acc.: 76.17%] [G loss: 3.054668]\n",
            "1400 [D loss: 0.509293, acc.: 78.52%] [G loss: 3.382513]\n",
            "1420 [D loss: 0.400499, acc.: 83.20%] [G loss: 4.027691]\n",
            "1440 [D loss: 0.482706, acc.: 79.69%] [G loss: 3.542985]\n",
            "1460 [D loss: 0.696779, acc.: 58.20%] [G loss: 2.916883]\n",
            "1480 [D loss: 0.477661, acc.: 78.91%] [G loss: 3.323077]\n",
            "1500 [D loss: 0.617868, acc.: 68.36%] [G loss: 2.820739]\n",
            "1520 [D loss: 0.604033, acc.: 69.14%] [G loss: 2.721321]\n",
            "1540 [D loss: 0.525753, acc.: 75.78%] [G loss: 3.086206]\n",
            "1560 [D loss: 0.549436, acc.: 70.70%] [G loss: 3.071890]\n",
            "1580 [D loss: 0.490012, acc.: 79.30%] [G loss: 3.162081]\n",
            "1600 [D loss: 0.529619, acc.: 73.83%] [G loss: 2.994212]\n",
            "1620 [D loss: 0.488622, acc.: 77.34%] [G loss: 3.139137]\n",
            "1640 [D loss: 0.534543, acc.: 73.05%] [G loss: 2.978634]\n",
            "1660 [D loss: 0.495874, acc.: 74.22%] [G loss: 3.199813]\n",
            "1680 [D loss: 0.549923, acc.: 73.83%] [G loss: 2.913361]\n",
            "1700 [D loss: 0.572734, acc.: 69.53%] [G loss: 2.966925]\n",
            "1720 [D loss: 0.520872, acc.: 75.00%] [G loss: 2.899426]\n",
            "1740 [D loss: 0.509229, acc.: 76.95%] [G loss: 2.825000]\n",
            "1760 [D loss: 0.590250, acc.: 70.70%] [G loss: 2.930942]\n",
            "1780 [D loss: 0.481645, acc.: 76.17%] [G loss: 3.043900]\n",
            "1800 [D loss: 0.491255, acc.: 78.52%] [G loss: 2.879711]\n",
            "1820 [D loss: 0.499677, acc.: 77.73%] [G loss: 3.048172]\n",
            "1840 [D loss: 0.600870, acc.: 67.58%] [G loss: 2.819824]\n",
            "1860 [D loss: 0.535577, acc.: 74.22%] [G loss: 2.948652]\n",
            "1880 [D loss: 0.521941, acc.: 73.44%] [G loss: 2.881570]\n",
            "1900 [D loss: 0.362160, acc.: 90.62%] [G loss: 2.528713]\n",
            "1920 [D loss: 0.621386, acc.: 68.75%] [G loss: 2.243778]\n",
            "1940 [D loss: 1.069244, acc.: 40.23%] [G loss: 3.402011]\n",
            "1960 [D loss: 0.549347, acc.: 71.48%] [G loss: 3.165979]\n",
            "1980 [D loss: 0.508630, acc.: 76.17%] [G loss: 3.047223]\n",
            "2000 [D loss: 0.502250, acc.: 74.61%] [G loss: 3.052085]\n",
            "2020 [D loss: 0.516851, acc.: 76.56%] [G loss: 3.199262]\n",
            "2040 [D loss: 0.603142, acc.: 69.14%] [G loss: 3.134368]\n",
            "2060 [D loss: 0.571264, acc.: 73.05%] [G loss: 2.934629]\n",
            "2080 [D loss: 0.582500, acc.: 69.53%] [G loss: 2.714746]\n",
            "2100 [D loss: 0.515383, acc.: 74.61%] [G loss: 3.206169]\n",
            "2120 [D loss: 0.513462, acc.: 75.78%] [G loss: 2.923366]\n",
            "2140 [D loss: 0.598058, acc.: 66.80%] [G loss: 2.815060]\n",
            "2160 [D loss: 0.637317, acc.: 63.67%] [G loss: 2.913659]\n",
            "2180 [D loss: 0.472213, acc.: 80.86%] [G loss: 3.074954]\n",
            "2200 [D loss: 0.554228, acc.: 71.48%] [G loss: 2.905122]\n",
            "2220 [D loss: 0.551821, acc.: 70.31%] [G loss: 2.817767]\n",
            "2240 [D loss: 0.589155, acc.: 69.53%] [G loss: 2.793106]\n",
            "2260 [D loss: 0.551383, acc.: 69.53%] [G loss: 3.139374]\n",
            "2280 [D loss: 0.492245, acc.: 76.95%] [G loss: 3.041268]\n",
            "2300 [D loss: 0.595057, acc.: 71.88%] [G loss: 2.811575]\n",
            "2320 [D loss: 0.536631, acc.: 71.88%] [G loss: 3.015552]\n",
            "2340 [D loss: 0.536093, acc.: 73.83%] [G loss: 2.778600]\n",
            "2360 [D loss: 0.504943, acc.: 76.95%] [G loss: 3.077787]\n",
            "2380 [D loss: 0.676236, acc.: 64.45%] [G loss: 1.696263]\n",
            "2400 [D loss: 0.722812, acc.: 46.09%] [G loss: 1.513677]\n",
            "2420 [D loss: 0.731258, acc.: 35.94%] [G loss: 1.397380]\n",
            "2440 [D loss: 0.682376, acc.: 55.47%] [G loss: 1.545272]\n",
            "2460 [D loss: 0.697083, acc.: 54.69%] [G loss: 1.500621]\n",
            "2480 [D loss: 0.701609, acc.: 53.91%] [G loss: 1.593110]\n",
            "2500 [D loss: 0.717272, acc.: 51.17%] [G loss: 1.609512]\n",
            "2520 [D loss: 0.670791, acc.: 57.81%] [G loss: 1.654183]\n",
            "2540 [D loss: 0.624526, acc.: 71.09%] [G loss: 1.868415]\n",
            "2560 [D loss: 0.668117, acc.: 57.03%] [G loss: 1.736463]\n",
            "2580 [D loss: 0.679681, acc.: 58.98%] [G loss: 1.741182]\n",
            "2600 [D loss: 0.672312, acc.: 59.77%] [G loss: 1.788926]\n",
            "2620 [D loss: 0.617793, acc.: 71.09%] [G loss: 1.853240]\n",
            "2640 [D loss: 0.551240, acc.: 78.12%] [G loss: 1.361349]\n",
            "2660 [D loss: 0.595520, acc.: 70.31%] [G loss: 1.260216]\n",
            "2680 [D loss: 0.832313, acc.: 43.36%] [G loss: 2.553425]\n",
            "2700 [D loss: 0.804484, acc.: 45.31%] [G loss: 2.169342]\n",
            "2720 [D loss: 0.722048, acc.: 55.86%] [G loss: 1.343315]\n",
            "2740 [D loss: 0.633138, acc.: 66.02%] [G loss: 2.903030]\n",
            "2760 [D loss: 0.610108, acc.: 69.92%] [G loss: 2.573515]\n",
            "2780 [D loss: 0.546062, acc.: 74.22%] [G loss: 2.870962]\n",
            "2800 [D loss: 0.522051, acc.: 77.73%] [G loss: 2.761539]\n",
            "2820 [D loss: 0.605835, acc.: 67.19%] [G loss: 2.702956]\n",
            "2840 [D loss: 0.615125, acc.: 68.36%] [G loss: 2.575504]\n",
            "2860 [D loss: 0.515355, acc.: 77.34%] [G loss: 2.889309]\n",
            "2880 [D loss: 0.636081, acc.: 65.62%] [G loss: 2.449385]\n",
            "2900 [D loss: 0.538407, acc.: 76.17%] [G loss: 2.593989]\n",
            "2920 [D loss: 0.596294, acc.: 67.97%] [G loss: 2.756912]\n",
            "2940 [D loss: 0.542297, acc.: 78.12%] [G loss: 2.704320]\n",
            "2960 [D loss: 0.581360, acc.: 65.62%] [G loss: 2.825998]\n",
            "2980 [D loss: 0.509037, acc.: 75.39%] [G loss: 3.151892]\n",
            "3000 [D loss: 0.520022, acc.: 73.44%] [G loss: 2.860879]\n",
            "3020 [D loss: 0.531736, acc.: 71.48%] [G loss: 3.062141]\n",
            "3040 [D loss: 0.536299, acc.: 76.95%] [G loss: 2.836114]\n",
            "3060 [D loss: 0.562976, acc.: 68.36%] [G loss: 2.797495]\n",
            "3080 [D loss: 0.502037, acc.: 74.22%] [G loss: 2.997776]\n",
            "3100 [D loss: 0.546152, acc.: 72.66%] [G loss: 2.691828]\n",
            "3120 [D loss: 0.611085, acc.: 66.80%] [G loss: 2.556175]\n",
            "3140 [D loss: 0.472615, acc.: 78.12%] [G loss: 2.790819]\n",
            "3160 [D loss: 0.561349, acc.: 72.27%] [G loss: 2.595638]\n",
            "3180 [D loss: 0.570890, acc.: 69.14%] [G loss: 2.530328]\n",
            "3200 [D loss: 0.570154, acc.: 69.53%] [G loss: 2.674651]\n",
            "3220 [D loss: 0.532901, acc.: 69.92%] [G loss: 2.935868]\n",
            "3240 [D loss: 0.549033, acc.: 69.53%] [G loss: 2.774175]\n",
            "3260 [D loss: 0.577224, acc.: 71.88%] [G loss: 2.689084]\n",
            "3280 [D loss: 0.574261, acc.: 67.58%] [G loss: 2.540749]\n",
            "3300 [D loss: 0.650002, acc.: 63.28%] [G loss: 2.431942]\n",
            "3320 [D loss: 0.572680, acc.: 68.36%] [G loss: 2.689155]\n",
            "3340 [D loss: 0.533550, acc.: 73.83%] [G loss: 2.883948]\n",
            "3360 [D loss: 0.529167, acc.: 75.39%] [G loss: 3.078577]\n",
            "3380 [D loss: 0.524056, acc.: 72.66%] [G loss: 2.845898]\n",
            "3400 [D loss: 0.539222, acc.: 71.09%] [G loss: 2.783710]\n",
            "3420 [D loss: 0.507771, acc.: 76.17%] [G loss: 2.892785]\n",
            "3440 [D loss: 0.607721, acc.: 72.66%] [G loss: 2.783684]\n",
            "3460 [D loss: 0.580687, acc.: 68.75%] [G loss: 2.687696]\n",
            "3480 [D loss: 0.557115, acc.: 74.61%] [G loss: 2.758293]\n",
            "3500 [D loss: 0.546091, acc.: 71.48%] [G loss: 2.768160]\n",
            "3520 [D loss: 0.615916, acc.: 67.19%] [G loss: 2.670376]\n",
            "3540 [D loss: 0.591908, acc.: 67.19%] [G loss: 3.063116]\n",
            "3560 [D loss: 0.521598, acc.: 73.05%] [G loss: 3.044698]\n",
            "3580 [D loss: 0.559092, acc.: 69.53%] [G loss: 2.749177]\n",
            "3600 [D loss: 0.480096, acc.: 78.12%] [G loss: 2.845303]\n",
            "3620 [D loss: 0.549253, acc.: 72.66%] [G loss: 2.741713]\n",
            "3640 [D loss: 0.556798, acc.: 71.09%] [G loss: 2.781819]\n",
            "3660 [D loss: 0.501712, acc.: 79.69%] [G loss: 2.980972]\n",
            "3680 [D loss: 0.535865, acc.: 71.48%] [G loss: 2.916289]\n",
            "3700 [D loss: 0.310261, acc.: 91.41%] [G loss: 1.911731]\n",
            "3720 [D loss: 0.730010, acc.: 64.45%] [G loss: 3.379136]\n",
            "3740 [D loss: 1.138715, acc.: 41.41%] [G loss: 2.777943]\n",
            "3760 [D loss: 0.822866, acc.: 50.78%] [G loss: 2.275154]\n",
            "3780 [D loss: 0.575357, acc.: 71.48%] [G loss: 2.720119]\n",
            "3800 [D loss: 0.852029, acc.: 47.27%] [G loss: 3.473767]\n",
            "3820 [D loss: 0.669906, acc.: 61.33%] [G loss: 2.845092]\n",
            "3840 [D loss: 0.514937, acc.: 75.00%] [G loss: 3.549900]\n",
            "3860 [D loss: 0.503264, acc.: 75.78%] [G loss: 3.281637]\n",
            "3880 [D loss: 0.512705, acc.: 76.95%] [G loss: 3.180674]\n",
            "3900 [D loss: 0.599561, acc.: 69.14%] [G loss: 2.801243]\n",
            "3920 [D loss: 0.544289, acc.: 75.78%] [G loss: 3.118453]\n",
            "3940 [D loss: 0.498395, acc.: 77.34%] [G loss: 3.283192]\n",
            "3960 [D loss: 0.632015, acc.: 64.06%] [G loss: 2.781152]\n",
            "3980 [D loss: 0.437889, acc.: 82.03%] [G loss: 3.257112]\n",
            "4000 [D loss: 0.630491, acc.: 66.41%] [G loss: 2.973758]\n",
            "4020 [D loss: 0.455692, acc.: 81.25%] [G loss: 3.074902]\n",
            "4040 [D loss: 0.583006, acc.: 69.92%] [G loss: 2.971666]\n",
            "4060 [D loss: 0.543724, acc.: 71.48%] [G loss: 2.835458]\n",
            "4080 [D loss: 0.540239, acc.: 72.66%] [G loss: 2.940527]\n",
            "4100 [D loss: 0.547748, acc.: 71.09%] [G loss: 2.746401]\n",
            "4120 [D loss: 0.559028, acc.: 71.48%] [G loss: 2.726519]\n",
            "4140 [D loss: 0.488886, acc.: 80.08%] [G loss: 3.057171]\n",
            "4160 [D loss: 0.551518, acc.: 71.09%] [G loss: 2.996392]\n",
            "4180 [D loss: 0.475498, acc.: 77.73%] [G loss: 3.107594]\n",
            "4200 [D loss: 0.524942, acc.: 75.39%] [G loss: 2.946529]\n",
            "4220 [D loss: 0.441503, acc.: 80.86%] [G loss: 3.293391]\n",
            "4240 [D loss: 0.542151, acc.: 72.27%] [G loss: 2.697888]\n",
            "4260 [D loss: 0.451870, acc.: 82.03%] [G loss: 3.385952]\n",
            "4280 [D loss: 0.560959, acc.: 72.66%] [G loss: 2.862458]\n",
            "4300 [D loss: 0.498392, acc.: 76.17%] [G loss: 3.097343]\n",
            "4320 [D loss: 0.532916, acc.: 75.78%] [G loss: 3.044103]\n",
            "4340 [D loss: 0.586692, acc.: 69.92%] [G loss: 3.246376]\n",
            "4360 [D loss: 0.553581, acc.: 70.70%] [G loss: 2.857294]\n",
            "4380 [D loss: 0.533710, acc.: 72.66%] [G loss: 3.089597]\n",
            "4400 [D loss: 0.498709, acc.: 77.34%] [G loss: 3.071715]\n",
            "4420 [D loss: 0.547846, acc.: 71.48%] [G loss: 3.094326]\n",
            "4440 [D loss: 0.498863, acc.: 76.56%] [G loss: 3.030422]\n",
            "4460 [D loss: 0.562885, acc.: 70.31%] [G loss: 2.854271]\n",
            "4480 [D loss: 0.431012, acc.: 82.03%] [G loss: 2.998319]\n",
            "4500 [D loss: 0.579693, acc.: 71.48%] [G loss: 2.797657]\n",
            "4520 [D loss: 0.603652, acc.: 68.36%] [G loss: 2.799840]\n",
            "4540 [D loss: 0.512512, acc.: 76.17%] [G loss: 3.061792]\n",
            "4560 [D loss: 0.544828, acc.: 71.48%] [G loss: 3.008157]\n",
            "4580 [D loss: 0.594551, acc.: 66.41%] [G loss: 2.608588]\n",
            "4600 [D loss: 0.480303, acc.: 77.34%] [G loss: 3.025129]\n",
            "4620 [D loss: 0.326464, acc.: 93.75%] [G loss: 2.707629]\n",
            "4640 [D loss: 0.628212, acc.: 68.36%] [G loss: 4.485128]\n",
            "4660 [D loss: 0.656818, acc.: 63.28%] [G loss: 2.917548]\n",
            "4680 [D loss: 0.515466, acc.: 77.34%] [G loss: 2.999645]\n",
            "4700 [D loss: 0.542606, acc.: 69.14%] [G loss: 3.079251]\n",
            "4720 [D loss: 0.497571, acc.: 76.56%] [G loss: 3.342772]\n",
            "4740 [D loss: 0.655433, acc.: 61.33%] [G loss: 2.725253]\n",
            "4760 [D loss: 0.495960, acc.: 78.91%] [G loss: 3.683645]\n",
            "4780 [D loss: 0.536030, acc.: 75.39%] [G loss: 2.987410]\n",
            "4800 [D loss: 0.536592, acc.: 70.31%] [G loss: 2.731952]\n",
            "4820 [D loss: 0.547202, acc.: 70.70%] [G loss: 2.959638]\n",
            "4840 [D loss: 0.507400, acc.: 74.61%] [G loss: 3.136473]\n",
            "4860 [D loss: 0.486413, acc.: 76.17%] [G loss: 3.094657]\n",
            "4880 [D loss: 0.627915, acc.: 66.80%] [G loss: 2.778571]\n",
            "4900 [D loss: 0.461181, acc.: 79.30%] [G loss: 3.452951]\n",
            "4920 [D loss: 0.544326, acc.: 74.61%] [G loss: 2.692243]\n",
            "4940 [D loss: 0.530027, acc.: 73.44%] [G loss: 2.877467]\n",
            "4960 [D loss: 0.449875, acc.: 79.69%] [G loss: 3.489448]\n",
            "4980 [D loss: 0.566104, acc.: 74.22%] [G loss: 3.037396]\n",
            "5000 [D loss: 0.576236, acc.: 67.97%] [G loss: 3.016957]\n",
            "5020 [D loss: 0.469858, acc.: 80.08%] [G loss: 3.192629]\n",
            "5040 [D loss: 0.549049, acc.: 75.78%] [G loss: 2.931938]\n",
            "5060 [D loss: 0.516486, acc.: 73.44%] [G loss: 3.191864]\n",
            "5080 [D loss: 0.520828, acc.: 73.44%] [G loss: 2.926929]\n",
            "5100 [D loss: 0.537977, acc.: 72.27%] [G loss: 3.097172]\n",
            "5120 [D loss: 0.532094, acc.: 70.70%] [G loss: 2.983793]\n",
            "5140 [D loss: 0.483628, acc.: 76.95%] [G loss: 3.263546]\n",
            "5160 [D loss: 0.492002, acc.: 75.39%] [G loss: 3.489929]\n",
            "5180 [D loss: 0.458985, acc.: 79.69%] [G loss: 3.334197]\n",
            "5200 [D loss: 0.504672, acc.: 76.56%] [G loss: 3.355224]\n",
            "5220 [D loss: 0.531203, acc.: 77.34%] [G loss: 3.265148]\n",
            "5240 [D loss: 0.525079, acc.: 73.44%] [G loss: 3.098454]\n",
            "5260 [D loss: 0.535821, acc.: 74.22%] [G loss: 3.185327]\n",
            "5280 [D loss: 0.494195, acc.: 80.08%] [G loss: 3.287323]\n",
            "5300 [D loss: 0.500780, acc.: 74.61%] [G loss: 2.567280]\n",
            "5320 [D loss: 0.590719, acc.: 70.70%] [G loss: 3.063092]\n",
            "5340 [D loss: 0.860868, acc.: 49.61%] [G loss: 3.089797]\n",
            "5360 [D loss: 0.601813, acc.: 67.97%] [G loss: 3.056518]\n",
            "5380 [D loss: 0.361652, acc.: 87.11%] [G loss: 2.383187]\n",
            "5400 [D loss: 0.331476, acc.: 88.67%] [G loss: 2.050434]\n",
            "5420 [D loss: 0.383589, acc.: 85.16%] [G loss: 1.874592]\n",
            "5440 [D loss: 1.165164, acc.: 37.11%] [G loss: 2.452987]\n",
            "5460 [D loss: 0.923181, acc.: 49.61%] [G loss: 2.969036]\n",
            "5480 [D loss: 0.570505, acc.: 73.44%] [G loss: 3.135190]\n",
            "5500 [D loss: 0.606710, acc.: 67.19%] [G loss: 3.567623]\n",
            "5520 [D loss: 0.531811, acc.: 75.00%] [G loss: 3.663039]\n",
            "5540 [D loss: 0.509582, acc.: 76.56%] [G loss: 3.298477]\n",
            "5560 [D loss: 0.550965, acc.: 72.27%] [G loss: 3.275251]\n",
            "5580 [D loss: 0.580177, acc.: 70.70%] [G loss: 3.270021]\n",
            "5600 [D loss: 0.484240, acc.: 79.30%] [G loss: 3.334895]\n",
            "5620 [D loss: 0.590033, acc.: 71.48%] [G loss: 3.337427]\n",
            "5640 [D loss: 0.450025, acc.: 80.08%] [G loss: 3.586488]\n",
            "5660 [D loss: 0.437518, acc.: 80.86%] [G loss: 3.266523]\n",
            "5680 [D loss: 0.502361, acc.: 74.22%] [G loss: 3.365832]\n",
            "5700 [D loss: 0.467616, acc.: 78.52%] [G loss: 3.371389]\n",
            "5720 [D loss: 0.552522, acc.: 71.88%] [G loss: 3.108576]\n",
            "5740 [D loss: 0.436422, acc.: 82.03%] [G loss: 3.469033]\n",
            "5760 [D loss: 0.553334, acc.: 70.70%] [G loss: 3.341945]\n",
            "5780 [D loss: 0.544839, acc.: 73.05%] [G loss: 3.173668]\n",
            "5800 [D loss: 0.496734, acc.: 76.95%] [G loss: 3.189696]\n",
            "5820 [D loss: 0.480675, acc.: 76.95%] [G loss: 3.245716]\n",
            "5840 [D loss: 0.499439, acc.: 76.17%] [G loss: 3.453490]\n",
            "5860 [D loss: 0.498700, acc.: 77.34%] [G loss: 3.299994]\n",
            "5880 [D loss: 0.520814, acc.: 73.83%] [G loss: 3.352225]\n",
            "5900 [D loss: 0.492959, acc.: 75.00%] [G loss: 3.557650]\n",
            "5920 [D loss: 0.508872, acc.: 77.34%] [G loss: 3.328711]\n",
            "5940 [D loss: 0.502548, acc.: 74.22%] [G loss: 3.306246]\n",
            "5960 [D loss: 0.501705, acc.: 74.61%] [G loss: 3.257612]\n",
            "5980 [D loss: 0.421532, acc.: 83.20%] [G loss: 3.495810]\n",
            "6000 [D loss: 0.526530, acc.: 76.95%] [G loss: 3.460682]\n",
            "6020 [D loss: 0.543985, acc.: 73.05%] [G loss: 3.187297]\n",
            "6040 [D loss: 0.524500, acc.: 75.00%] [G loss: 3.552040]\n",
            "6060 [D loss: 0.458588, acc.: 78.91%] [G loss: 3.276907]\n",
            "6080 [D loss: 0.529360, acc.: 75.00%] [G loss: 3.501544]\n",
            "6100 [D loss: 0.563832, acc.: 69.14%] [G loss: 3.287077]\n",
            "6120 [D loss: 0.250943, acc.: 95.31%] [G loss: 2.373405]\n",
            "6140 [D loss: 0.502432, acc.: 75.39%] [G loss: 2.979480]\n",
            "6160 [D loss: 2.323955, acc.: 14.45%] [G loss: 2.240222]\n",
            "6180 [D loss: 1.155580, acc.: 30.08%] [G loss: 1.050522]\n",
            "6200 [D loss: 0.920759, acc.: 34.77%] [G loss: 2.239148]\n",
            "6220 [D loss: 0.823745, acc.: 41.41%] [G loss: 2.187103]\n",
            "6240 [D loss: 0.829366, acc.: 46.09%] [G loss: 1.950516]\n",
            "6260 [D loss: 0.722156, acc.: 60.55%] [G loss: 2.134234]\n",
            "6280 [D loss: 0.759797, acc.: 54.30%] [G loss: 2.420036]\n",
            "6300 [D loss: 0.697784, acc.: 56.64%] [G loss: 1.999850]\n",
            "6320 [D loss: 0.777462, acc.: 50.00%] [G loss: 1.721941]\n",
            "6340 [D loss: 0.693530, acc.: 64.45%] [G loss: 1.898850]\n",
            "6360 [D loss: 0.694279, acc.: 60.55%] [G loss: 2.101095]\n",
            "6380 [D loss: 0.630828, acc.: 67.97%] [G loss: 2.172317]\n",
            "6400 [D loss: 0.734670, acc.: 52.34%] [G loss: 1.769998]\n",
            "6420 [D loss: 0.650045, acc.: 66.80%] [G loss: 1.996779]\n",
            "6440 [D loss: 0.661498, acc.: 64.45%] [G loss: 2.060413]\n",
            "6460 [D loss: 0.648781, acc.: 62.50%] [G loss: 1.988964]\n",
            "6480 [D loss: 0.672042, acc.: 57.03%] [G loss: 1.982731]\n",
            "6500 [D loss: 0.617692, acc.: 65.62%] [G loss: 2.027696]\n",
            "6520 [D loss: 0.665253, acc.: 61.72%] [G loss: 1.896079]\n",
            "6540 [D loss: 0.659657, acc.: 59.38%] [G loss: 1.952382]\n",
            "6560 [D loss: 0.625288, acc.: 65.62%] [G loss: 1.993025]\n",
            "6580 [D loss: 0.648776, acc.: 62.50%] [G loss: 1.986069]\n",
            "6600 [D loss: 0.667758, acc.: 59.77%] [G loss: 1.875707]\n",
            "6620 [D loss: 0.627726, acc.: 64.45%] [G loss: 2.086287]\n",
            "6640 [D loss: 0.596757, acc.: 68.75%] [G loss: 2.067978]\n",
            "6660 [D loss: 0.635084, acc.: 65.23%] [G loss: 1.993137]\n",
            "6680 [D loss: 0.586129, acc.: 67.97%] [G loss: 2.149947]\n",
            "6700 [D loss: 0.651572, acc.: 61.72%] [G loss: 1.977591]\n",
            "6720 [D loss: 0.624733, acc.: 67.19%] [G loss: 2.103397]\n",
            "6740 [D loss: 0.609717, acc.: 62.89%] [G loss: 2.083053]\n",
            "6760 [D loss: 0.631603, acc.: 65.62%] [G loss: 2.046796]\n",
            "6780 [D loss: 0.641534, acc.: 61.33%] [G loss: 2.023584]\n",
            "6800 [D loss: 0.600055, acc.: 67.97%] [G loss: 2.091638]\n",
            "6820 [D loss: 0.668956, acc.: 60.16%] [G loss: 1.934864]\n",
            "6840 [D loss: 0.663828, acc.: 58.20%] [G loss: 1.988674]\n",
            "6860 [D loss: 0.598266, acc.: 66.02%] [G loss: 2.149924]\n",
            "6880 [D loss: 0.649115, acc.: 65.23%] [G loss: 2.042296]\n",
            "6900 [D loss: 0.595871, acc.: 69.92%] [G loss: 2.120559]\n",
            "6920 [D loss: 0.629707, acc.: 64.84%] [G loss: 1.950731]\n",
            "6940 [D loss: 0.648787, acc.: 63.28%] [G loss: 2.019875]\n",
            "6960 [D loss: 0.596777, acc.: 66.41%] [G loss: 2.154530]\n",
            "6980 [D loss: 0.676324, acc.: 58.98%] [G loss: 2.110767]\n",
            "7000 [D loss: 0.610605, acc.: 68.36%] [G loss: 2.071907]\n",
            "7020 [D loss: 0.660828, acc.: 60.55%] [G loss: 1.977990]\n",
            "7040 [D loss: 0.620830, acc.: 66.80%] [G loss: 2.104814]\n",
            "7060 [D loss: 0.587373, acc.: 72.66%] [G loss: 2.049975]\n",
            "7080 [D loss: 0.585513, acc.: 71.88%] [G loss: 2.158252]\n",
            "7100 [D loss: 0.699726, acc.: 57.03%] [G loss: 2.110233]\n",
            "7120 [D loss: 0.678445, acc.: 59.38%] [G loss: 2.101861]\n",
            "7140 [D loss: 0.665130, acc.: 58.20%] [G loss: 2.189609]\n",
            "7160 [D loss: 0.651977, acc.: 60.16%] [G loss: 2.203122]\n",
            "7180 [D loss: 0.578172, acc.: 70.31%] [G loss: 2.328995]\n",
            "7200 [D loss: 0.626679, acc.: 65.62%] [G loss: 2.233999]\n",
            "7220 [D loss: 0.542751, acc.: 75.39%] [G loss: 2.452188]\n",
            "7240 [D loss: 0.639276, acc.: 65.23%] [G loss: 2.258785]\n",
            "7260 [D loss: 0.591361, acc.: 67.19%] [G loss: 2.312950]\n",
            "7280 [D loss: 0.635295, acc.: 65.62%] [G loss: 2.144130]\n",
            "7300 [D loss: 0.582893, acc.: 67.58%] [G loss: 2.147437]\n",
            "7320 [D loss: 0.581932, acc.: 72.27%] [G loss: 2.280103]\n",
            "7340 [D loss: 0.604761, acc.: 69.92%] [G loss: 2.382187]\n",
            "7360 [D loss: 0.581306, acc.: 67.58%] [G loss: 2.296571]\n",
            "7380 [D loss: 0.611732, acc.: 66.02%] [G loss: 1.885564]\n",
            "7400 [D loss: 0.521599, acc.: 76.17%] [G loss: 1.622282]\n",
            "7420 [D loss: 0.779470, acc.: 50.78%] [G loss: 1.780444]\n",
            "7440 [D loss: 0.577792, acc.: 75.00%] [G loss: 2.637575]\n",
            "7460 [D loss: 0.645830, acc.: 63.67%] [G loss: 2.378063]\n",
            "7480 [D loss: 0.662972, acc.: 64.06%] [G loss: 2.241713]\n",
            "7500 [D loss: 0.716491, acc.: 59.77%] [G loss: 1.323058]\n",
            "7520 [D loss: 0.921997, acc.: 40.23%] [G loss: 2.565077]\n",
            "7540 [D loss: 0.859356, acc.: 44.92%] [G loss: 2.167778]\n",
            "7560 [D loss: 0.729454, acc.: 57.42%] [G loss: 2.589188]\n",
            "7580 [D loss: 0.656537, acc.: 60.94%] [G loss: 2.538052]\n",
            "7600 [D loss: 0.586763, acc.: 69.14%] [G loss: 2.379553]\n",
            "7620 [D loss: 0.659397, acc.: 59.38%] [G loss: 2.261476]\n",
            "7640 [D loss: 0.539159, acc.: 77.34%] [G loss: 2.520328]\n",
            "7660 [D loss: 0.624727, acc.: 64.84%] [G loss: 2.294394]\n",
            "7680 [D loss: 0.554559, acc.: 71.48%] [G loss: 2.519812]\n",
            "7700 [D loss: 0.635868, acc.: 64.84%] [G loss: 2.344916]\n",
            "7720 [D loss: 0.570408, acc.: 73.05%] [G loss: 2.535196]\n",
            "7740 [D loss: 0.615367, acc.: 62.11%] [G loss: 2.495441]\n",
            "7760 [D loss: 0.581223, acc.: 71.48%] [G loss: 2.499008]\n",
            "7780 [D loss: 0.622443, acc.: 64.45%] [G loss: 2.411800]\n",
            "7800 [D loss: 0.510819, acc.: 78.52%] [G loss: 2.681968]\n",
            "7820 [D loss: 0.557044, acc.: 75.78%] [G loss: 2.342851]\n",
            "7840 [D loss: 0.637933, acc.: 63.28%] [G loss: 2.544615]\n",
            "7860 [D loss: 0.535149, acc.: 72.66%] [G loss: 2.565053]\n",
            "7880 [D loss: 0.559372, acc.: 71.48%] [G loss: 2.600207]\n",
            "7900 [D loss: 0.564578, acc.: 72.66%] [G loss: 2.483591]\n",
            "7920 [D loss: 0.529232, acc.: 74.22%] [G loss: 2.724263]\n",
            "7940 [D loss: 0.517066, acc.: 77.34%] [G loss: 2.731758]\n",
            "7960 [D loss: 0.573555, acc.: 68.75%] [G loss: 2.508380]\n",
            "7980 [D loss: 0.483591, acc.: 75.78%] [G loss: 2.729702]\n",
            "8000 [D loss: 0.598450, acc.: 66.02%] [G loss: 2.477621]\n",
            "8020 [D loss: 0.539408, acc.: 75.39%] [G loss: 2.652233]\n",
            "8040 [D loss: 0.566463, acc.: 72.66%] [G loss: 2.516830]\n",
            "8060 [D loss: 0.553434, acc.: 71.48%] [G loss: 2.634340]\n",
            "8080 [D loss: 0.567422, acc.: 72.66%] [G loss: 2.529952]\n",
            "8100 [D loss: 0.560809, acc.: 71.48%] [G loss: 2.524413]\n",
            "8120 [D loss: 0.554341, acc.: 71.88%] [G loss: 2.562195]\n",
            "8140 [D loss: 0.640384, acc.: 63.28%] [G loss: 2.580493]\n",
            "8160 [D loss: 0.532250, acc.: 75.39%] [G loss: 2.732190]\n",
            "8180 [D loss: 0.548040, acc.: 73.44%] [G loss: 2.705066]\n",
            "8200 [D loss: 0.579200, acc.: 72.66%] [G loss: 2.518582]\n",
            "8220 [D loss: 0.541492, acc.: 71.88%] [G loss: 2.770172]\n",
            "8240 [D loss: 0.517208, acc.: 77.73%] [G loss: 2.755811]\n",
            "8260 [D loss: 0.627825, acc.: 66.02%] [G loss: 2.613652]\n",
            "8280 [D loss: 0.537576, acc.: 72.66%] [G loss: 2.672249]\n",
            "8300 [D loss: 0.535860, acc.: 75.39%] [G loss: 2.733931]\n",
            "8320 [D loss: 0.580943, acc.: 69.14%] [G loss: 2.591676]\n",
            "8340 [D loss: 0.566269, acc.: 73.05%] [G loss: 2.676393]\n",
            "8360 [D loss: 0.563107, acc.: 71.09%] [G loss: 2.649447]\n",
            "8380 [D loss: 0.537962, acc.: 72.66%] [G loss: 2.654822]\n",
            "8400 [D loss: 0.485518, acc.: 76.95%] [G loss: 2.913832]\n",
            "8420 [D loss: 0.555149, acc.: 73.83%] [G loss: 2.695350]\n",
            "8440 [D loss: 0.560674, acc.: 75.00%] [G loss: 2.723227]\n",
            "8460 [D loss: 0.528451, acc.: 71.09%] [G loss: 2.783753]\n",
            "8480 [D loss: 0.548489, acc.: 73.44%] [G loss: 2.820539]\n",
            "8500 [D loss: 0.544382, acc.: 72.66%] [G loss: 2.894841]\n",
            "8520 [D loss: 0.516356, acc.: 73.44%] [G loss: 2.480458]\n",
            "8540 [D loss: 0.635232, acc.: 64.84%] [G loss: 2.192563]\n",
            "8560 [D loss: 0.662926, acc.: 62.11%] [G loss: 2.958608]\n",
            "8580 [D loss: 0.776528, acc.: 55.47%] [G loss: 3.229678]\n",
            "8600 [D loss: 0.710741, acc.: 59.77%] [G loss: 2.990249]\n",
            "8620 [D loss: 0.607618, acc.: 69.14%] [G loss: 2.094573]\n",
            "8640 [D loss: 0.544248, acc.: 74.61%] [G loss: 2.617053]\n",
            "8660 [D loss: 0.379066, acc.: 88.28%] [G loss: 1.959192]\n",
            "8680 [D loss: 1.219404, acc.: 28.12%] [G loss: 1.745285]\n",
            "8700 [D loss: 0.424358, acc.: 84.77%] [G loss: 3.838499]\n",
            "8720 [D loss: 0.664761, acc.: 66.02%] [G loss: 3.032892]\n",
            "8740 [D loss: 0.613042, acc.: 68.75%] [G loss: 3.061482]\n",
            "8760 [D loss: 0.523592, acc.: 73.44%] [G loss: 3.028465]\n",
            "8780 [D loss: 0.647926, acc.: 66.80%] [G loss: 2.827757]\n",
            "8800 [D loss: 0.514953, acc.: 73.44%] [G loss: 3.196986]\n",
            "8820 [D loss: 0.533848, acc.: 71.48%] [G loss: 3.072569]\n",
            "8840 [D loss: 0.566552, acc.: 70.31%] [G loss: 3.105453]\n",
            "8860 [D loss: 0.542852, acc.: 73.05%] [G loss: 3.033166]\n",
            "8880 [D loss: 0.503807, acc.: 74.22%] [G loss: 3.229363]\n",
            "8900 [D loss: 0.512170, acc.: 72.27%] [G loss: 3.114167]\n",
            "8920 [D loss: 0.478640, acc.: 77.73%] [G loss: 3.357801]\n",
            "8940 [D loss: 0.485501, acc.: 76.95%] [G loss: 3.105463]\n",
            "8960 [D loss: 0.517384, acc.: 76.17%] [G loss: 3.035922]\n",
            "8980 [D loss: 0.517173, acc.: 77.73%] [G loss: 3.196340]\n",
            "9000 [D loss: 0.565610, acc.: 72.66%] [G loss: 2.864408]\n",
            "9020 [D loss: 0.478377, acc.: 77.73%] [G loss: 3.194452]\n",
            "9040 [D loss: 0.589233, acc.: 68.75%] [G loss: 2.924530]\n",
            "9060 [D loss: 0.435662, acc.: 80.47%] [G loss: 3.195101]\n",
            "9080 [D loss: 0.525664, acc.: 72.27%] [G loss: 2.908466]\n",
            "9100 [D loss: 0.500744, acc.: 76.56%] [G loss: 3.042013]\n",
            "9120 [D loss: 0.481514, acc.: 76.17%] [G loss: 3.118252]\n",
            "9140 [D loss: 0.525897, acc.: 71.48%] [G loss: 2.911923]\n",
            "9160 [D loss: 0.524292, acc.: 75.39%] [G loss: 3.089056]\n",
            "9180 [D loss: 0.509134, acc.: 74.61%] [G loss: 3.233777]\n",
            "9200 [D loss: 0.543850, acc.: 75.78%] [G loss: 2.781223]\n",
            "9220 [D loss: 0.597418, acc.: 67.58%] [G loss: 2.894399]\n",
            "9240 [D loss: 0.542301, acc.: 73.83%] [G loss: 2.915030]\n",
            "9260 [D loss: 0.514184, acc.: 75.39%] [G loss: 3.107824]\n",
            "9280 [D loss: 0.496845, acc.: 79.69%] [G loss: 3.084638]\n",
            "9300 [D loss: 0.535879, acc.: 74.22%] [G loss: 2.991759]\n",
            "9320 [D loss: 0.492138, acc.: 76.17%] [G loss: 3.161635]\n",
            "9340 [D loss: 0.534259, acc.: 73.05%] [G loss: 3.153113]\n",
            "9360 [D loss: 0.530568, acc.: 73.44%] [G loss: 3.220362]\n",
            "9380 [D loss: 0.521957, acc.: 74.22%] [G loss: 3.155239]\n",
            "9400 [D loss: 0.438560, acc.: 81.25%] [G loss: 3.639142]\n",
            "9420 [D loss: 0.477175, acc.: 75.78%] [G loss: 3.327698]\n",
            "9440 [D loss: 0.503056, acc.: 75.78%] [G loss: 3.313310]\n",
            "9460 [D loss: 0.515748, acc.: 75.00%] [G loss: 3.224887]\n",
            "9480 [D loss: 0.473463, acc.: 76.95%] [G loss: 3.292435]\n",
            "9500 [D loss: 0.494979, acc.: 76.95%] [G loss: 3.269952]\n",
            "9520 [D loss: 0.508334, acc.: 79.30%] [G loss: 3.250947]\n",
            "9540 [D loss: 0.490135, acc.: 76.56%] [G loss: 3.485036]\n",
            "9560 [D loss: 0.526703, acc.: 71.88%] [G loss: 3.353554]\n",
            "9580 [D loss: 0.481998, acc.: 79.69%] [G loss: 3.272357]\n",
            "9600 [D loss: 0.534930, acc.: 71.09%] [G loss: 3.259816]\n",
            "9620 [D loss: 0.480360, acc.: 77.34%] [G loss: 3.163073]\n",
            "9640 [D loss: 0.467247, acc.: 79.69%] [G loss: 2.170757]\n",
            "9660 [D loss: 0.180038, acc.: 98.05%] [G loss: 2.305262]\n",
            "9680 [D loss: 0.425793, acc.: 80.08%] [G loss: 1.929758]\n",
            "9700 [D loss: 0.428700, acc.: 81.64%] [G loss: 1.941334]\n",
            "9720 [D loss: 0.956060, acc.: 37.89%] [G loss: 0.947260]\n",
            "9740 [D loss: 1.278134, acc.: 26.17%] [G loss: 1.391090]\n",
            "9760 [D loss: 0.766300, acc.: 57.42%] [G loss: 2.841336]\n",
            "9780 [D loss: 0.695150, acc.: 57.03%] [G loss: 2.838823]\n",
            "9800 [D loss: 0.628175, acc.: 66.02%] [G loss: 2.711184]\n",
            "9820 [D loss: 0.652208, acc.: 61.33%] [G loss: 2.742374]\n",
            "9840 [D loss: 0.589512, acc.: 70.31%] [G loss: 3.067405]\n",
            "9860 [D loss: 0.720771, acc.: 57.81%] [G loss: 3.060014]\n",
            "9880 [D loss: 0.537205, acc.: 74.22%] [G loss: 3.152534]\n",
            "9900 [D loss: 0.494397, acc.: 75.39%] [G loss: 3.172713]\n",
            "9920 [D loss: 0.487844, acc.: 73.44%] [G loss: 3.422647]\n",
            "9940 [D loss: 0.524557, acc.: 74.61%] [G loss: 3.232929]\n",
            "9960 [D loss: 0.634881, acc.: 64.06%] [G loss: 3.112751]\n",
            "9980 [D loss: 0.579167, acc.: 67.97%] [G loss: 3.067770]\n",
            "10000 [D loss: 0.463009, acc.: 80.08%] [G loss: 3.235129]\n",
            "10020 [D loss: 0.476391, acc.: 74.61%] [G loss: 3.217135]\n",
            "10040 [D loss: 0.522402, acc.: 76.56%] [G loss: 3.229570]\n",
            "10060 [D loss: 0.520529, acc.: 76.95%] [G loss: 3.152402]\n",
            "10080 [D loss: 0.543100, acc.: 73.05%] [G loss: 3.325416]\n",
            "10100 [D loss: 0.528522, acc.: 73.44%] [G loss: 3.207047]\n",
            "10120 [D loss: 0.495193, acc.: 79.30%] [G loss: 3.438656]\n",
            "10140 [D loss: 0.489594, acc.: 77.34%] [G loss: 3.352841]\n",
            "10160 [D loss: 0.508981, acc.: 74.61%] [G loss: 3.331890]\n",
            "10180 [D loss: 0.506110, acc.: 75.39%] [G loss: 3.374069]\n",
            "10200 [D loss: 0.499109, acc.: 77.73%] [G loss: 3.445491]\n",
            "10220 [D loss: 0.484583, acc.: 78.52%] [G loss: 3.451986]\n",
            "10240 [D loss: 0.462917, acc.: 78.91%] [G loss: 3.466992]\n",
            "10260 [D loss: 0.564489, acc.: 73.05%] [G loss: 3.407365]\n",
            "10280 [D loss: 0.509058, acc.: 74.61%] [G loss: 3.467560]\n",
            "10300 [D loss: 0.484803, acc.: 76.17%] [G loss: 3.393581]\n",
            "10320 [D loss: 0.468608, acc.: 75.78%] [G loss: 3.362370]\n",
            "10340 [D loss: 0.550224, acc.: 73.44%] [G loss: 3.118720]\n",
            "10360 [D loss: 0.466416, acc.: 80.08%] [G loss: 3.615639]\n",
            "10380 [D loss: 0.493900, acc.: 77.34%] [G loss: 3.413139]\n",
            "10400 [D loss: 0.465030, acc.: 80.08%] [G loss: 3.433688]\n",
            "10420 [D loss: 0.477510, acc.: 76.95%] [G loss: 3.322184]\n",
            "10440 [D loss: 0.542903, acc.: 71.48%] [G loss: 3.247742]\n",
            "10460 [D loss: 0.472771, acc.: 76.95%] [G loss: 3.538732]\n",
            "10480 [D loss: 0.463354, acc.: 78.52%] [G loss: 3.365281]\n",
            "10500 [D loss: 0.533067, acc.: 71.48%] [G loss: 3.227233]\n",
            "10520 [D loss: 0.448553, acc.: 80.08%] [G loss: 3.396903]\n",
            "10540 [D loss: 0.530874, acc.: 73.83%] [G loss: 3.193655]\n",
            "10560 [D loss: 0.527723, acc.: 74.22%] [G loss: 3.110812]\n",
            "10580 [D loss: 0.546893, acc.: 71.09%] [G loss: 3.419118]\n",
            "10600 [D loss: 0.516933, acc.: 75.78%] [G loss: 3.421059]\n",
            "10620 [D loss: 0.488635, acc.: 74.22%] [G loss: 3.609027]\n",
            "10640 [D loss: 0.556700, acc.: 73.83%] [G loss: 3.323818]\n",
            "10660 [D loss: 0.470210, acc.: 79.69%] [G loss: 3.669501]\n",
            "10680 [D loss: 0.411902, acc.: 81.64%] [G loss: 3.559557]\n",
            "10700 [D loss: 0.516779, acc.: 74.61%] [G loss: 3.507403]\n",
            "10720 [D loss: 0.492273, acc.: 76.17%] [G loss: 3.550575]\n",
            "10740 [D loss: 0.536439, acc.: 71.88%] [G loss: 3.569686]\n",
            "10760 [D loss: 0.459847, acc.: 81.64%] [G loss: 3.567962]\n",
            "10780 [D loss: 0.485560, acc.: 76.95%] [G loss: 3.704795]\n",
            "10800 [D loss: 0.477588, acc.: 78.12%] [G loss: 3.796070]\n",
            "10820 [D loss: 0.447926, acc.: 80.47%] [G loss: 3.934294]\n",
            "10840 [D loss: 0.477140, acc.: 79.30%] [G loss: 3.817964]\n",
            "10860 [D loss: 0.442079, acc.: 79.30%] [G loss: 3.622973]\n",
            "10880 [D loss: 0.648842, acc.: 62.89%] [G loss: 3.105603]\n",
            "10900 [D loss: 0.481598, acc.: 76.95%] [G loss: 4.071194]\n",
            "10920 [D loss: 0.133473, acc.: 98.44%] [G loss: 2.774328]\n",
            "10940 [D loss: 0.617386, acc.: 72.27%] [G loss: 4.392898]\n",
            "10960 [D loss: 0.866588, acc.: 54.30%] [G loss: 4.029865]\n",
            "10980 [D loss: 0.817687, acc.: 55.08%] [G loss: 3.740908]\n",
            "11000 [D loss: 0.592262, acc.: 71.48%] [G loss: 3.471337]\n",
            "11020 [D loss: 0.534795, acc.: 74.22%] [G loss: 3.920959]\n",
            "11040 [D loss: 0.456601, acc.: 78.52%] [G loss: 4.147064]\n",
            "11060 [D loss: 0.450247, acc.: 77.73%] [G loss: 3.634178]\n",
            "11080 [D loss: 0.380059, acc.: 84.77%] [G loss: 3.983861]\n",
            "11100 [D loss: 0.494954, acc.: 76.95%] [G loss: 3.694601]\n",
            "11120 [D loss: 0.474077, acc.: 77.34%] [G loss: 3.932493]\n",
            "11140 [D loss: 0.589971, acc.: 66.80%] [G loss: 3.360992]\n",
            "11160 [D loss: 0.444300, acc.: 78.12%] [G loss: 3.825907]\n",
            "11180 [D loss: 0.407898, acc.: 78.91%] [G loss: 3.778059]\n",
            "11200 [D loss: 0.490789, acc.: 75.39%] [G loss: 3.579124]\n",
            "11220 [D loss: 0.469969, acc.: 80.47%] [G loss: 3.635456]\n",
            "11240 [D loss: 0.450155, acc.: 78.91%] [G loss: 3.939218]\n",
            "11260 [D loss: 0.471255, acc.: 78.52%] [G loss: 3.915770]\n",
            "11280 [D loss: 0.478350, acc.: 78.52%] [G loss: 3.611583]\n",
            "11300 [D loss: 0.387848, acc.: 84.77%] [G loss: 4.101531]\n",
            "11320 [D loss: 0.483587, acc.: 74.22%] [G loss: 3.637494]\n",
            "11340 [D loss: 0.453896, acc.: 79.69%] [G loss: 3.966359]\n",
            "11360 [D loss: 0.465368, acc.: 79.30%] [G loss: 4.004762]\n",
            "11380 [D loss: 0.425527, acc.: 81.64%] [G loss: 3.790664]\n",
            "11400 [D loss: 0.407161, acc.: 83.98%] [G loss: 4.193526]\n",
            "11420 [D loss: 0.525973, acc.: 76.17%] [G loss: 3.688166]\n",
            "11440 [D loss: 0.452028, acc.: 79.30%] [G loss: 3.985892]\n",
            "11460 [D loss: 0.463073, acc.: 78.12%] [G loss: 3.923852]\n",
            "11480 [D loss: 0.493850, acc.: 77.73%] [G loss: 3.904600]\n",
            "11500 [D loss: 0.435572, acc.: 82.42%] [G loss: 4.040984]\n",
            "11520 [D loss: 0.467746, acc.: 79.30%] [G loss: 3.915230]\n",
            "11540 [D loss: 0.398445, acc.: 81.25%] [G loss: 4.289492]\n",
            "11560 [D loss: 0.455299, acc.: 77.73%] [G loss: 4.025140]\n",
            "11580 [D loss: 0.440079, acc.: 78.91%] [G loss: 4.106820]\n",
            "11600 [D loss: 0.456516, acc.: 77.34%] [G loss: 3.744514]\n",
            "11620 [D loss: 0.485034, acc.: 74.61%] [G loss: 3.750127]\n",
            "11640 [D loss: 0.442892, acc.: 79.30%] [G loss: 3.989993]\n",
            "11660 [D loss: 0.463375, acc.: 78.12%] [G loss: 3.998973]\n",
            "11680 [D loss: 0.428633, acc.: 82.42%] [G loss: 4.006311]\n",
            "11700 [D loss: 0.459955, acc.: 78.12%] [G loss: 3.961638]\n",
            "11720 [D loss: 0.400837, acc.: 82.81%] [G loss: 4.063187]\n",
            "11740 [D loss: 0.489486, acc.: 77.34%] [G loss: 3.958524]\n",
            "11760 [D loss: 0.428849, acc.: 81.25%] [G loss: 4.066579]\n",
            "11780 [D loss: 0.475022, acc.: 76.95%] [G loss: 4.048596]\n",
            "11800 [D loss: 0.409972, acc.: 80.08%] [G loss: 4.060149]\n",
            "11820 [D loss: 0.453172, acc.: 77.73%] [G loss: 4.157513]\n",
            "11840 [D loss: 0.499343, acc.: 75.00%] [G loss: 3.788950]\n",
            "11860 [D loss: 0.445036, acc.: 78.91%] [G loss: 4.068363]\n",
            "11880 [D loss: 0.481348, acc.: 79.30%] [G loss: 4.019529]\n",
            "11900 [D loss: 0.484485, acc.: 75.39%] [G loss: 3.818460]\n",
            "11920 [D loss: 0.473135, acc.: 77.73%] [G loss: 4.034264]\n",
            "11940 [D loss: 0.393836, acc.: 83.20%] [G loss: 4.272073]\n",
            "11960 [D loss: 0.476774, acc.: 76.17%] [G loss: 4.151006]\n",
            "11980 [D loss: 0.400692, acc.: 80.86%] [G loss: 4.209992]\n",
            "12000 [D loss: 0.489061, acc.: 77.73%] [G loss: 3.962571]\n",
            "12020 [D loss: 0.382847, acc.: 81.64%] [G loss: 4.548215]\n",
            "12040 [D loss: 0.506001, acc.: 75.39%] [G loss: 3.852171]\n",
            "12060 [D loss: 0.429773, acc.: 80.86%] [G loss: 3.955435]\n",
            "12080 [D loss: 0.430254, acc.: 82.03%] [G loss: 3.973526]\n",
            "12100 [D loss: 0.374157, acc.: 84.77%] [G loss: 4.356418]\n",
            "12120 [D loss: 0.583978, acc.: 69.14%] [G loss: 3.728880]\n",
            "12140 [D loss: 0.461488, acc.: 80.08%] [G loss: 3.824946]\n",
            "12160 [D loss: 0.460815, acc.: 78.91%] [G loss: 4.077789]\n",
            "12180 [D loss: 0.356433, acc.: 85.16%] [G loss: 4.291361]\n",
            "12200 [D loss: 0.434134, acc.: 77.34%] [G loss: 4.177397]\n",
            "12220 [D loss: 0.404613, acc.: 79.30%] [G loss: 4.154194]\n",
            "12240 [D loss: 0.473178, acc.: 77.73%] [G loss: 2.540153]\n",
            "12260 [D loss: 0.755175, acc.: 65.23%] [G loss: 4.246570]\n",
            "12280 [D loss: 0.477590, acc.: 77.34%] [G loss: 3.868232]\n",
            "12300 [D loss: 0.589302, acc.: 71.48%] [G loss: 3.841918]\n",
            "12320 [D loss: 0.416479, acc.: 81.64%] [G loss: 4.709984]\n",
            "12340 [D loss: 0.501544, acc.: 78.52%] [G loss: 4.509590]\n",
            "12360 [D loss: 0.467739, acc.: 79.30%] [G loss: 4.329188]\n",
            "12380 [D loss: 0.388671, acc.: 83.59%] [G loss: 4.506413]\n",
            "12400 [D loss: 0.446764, acc.: 80.47%] [G loss: 4.301947]\n",
            "12420 [D loss: 0.470305, acc.: 76.56%] [G loss: 4.348934]\n",
            "12440 [D loss: 0.412509, acc.: 81.25%] [G loss: 4.312581]\n",
            "12460 [D loss: 0.422374, acc.: 78.52%] [G loss: 4.425613]\n",
            "12480 [D loss: 0.491525, acc.: 77.34%] [G loss: 4.061432]\n",
            "12500 [D loss: 0.482526, acc.: 78.52%] [G loss: 4.195846]\n",
            "12520 [D loss: 0.418656, acc.: 83.20%] [G loss: 4.660162]\n",
            "12540 [D loss: 0.432508, acc.: 80.86%] [G loss: 4.228459]\n",
            "12560 [D loss: 0.471650, acc.: 80.86%] [G loss: 4.493207]\n",
            "12580 [D loss: 0.433500, acc.: 80.08%] [G loss: 4.409269]\n",
            "12600 [D loss: 0.385115, acc.: 83.98%] [G loss: 4.203171]\n",
            "12620 [D loss: 0.445105, acc.: 81.64%] [G loss: 4.467225]\n",
            "12640 [D loss: 0.373039, acc.: 83.20%] [G loss: 4.660304]\n",
            "12660 [D loss: 0.403283, acc.: 81.64%] [G loss: 4.270607]\n",
            "12680 [D loss: 0.403939, acc.: 79.69%] [G loss: 4.694819]\n",
            "12700 [D loss: 0.495002, acc.: 77.34%] [G loss: 4.358760]\n",
            "12720 [D loss: 0.462365, acc.: 82.03%] [G loss: 4.537499]\n",
            "12740 [D loss: 0.454413, acc.: 80.47%] [G loss: 4.090788]\n",
            "12760 [D loss: 0.398583, acc.: 84.77%] [G loss: 4.705439]\n",
            "12780 [D loss: 0.425515, acc.: 83.20%] [G loss: 4.553843]\n",
            "12800 [D loss: 0.406412, acc.: 82.03%] [G loss: 4.548645]\n",
            "12820 [D loss: 0.385517, acc.: 83.98%] [G loss: 4.851363]\n",
            "12840 [D loss: 0.486651, acc.: 77.34%] [G loss: 4.096857]\n",
            "12860 [D loss: 0.346980, acc.: 85.55%] [G loss: 4.630355]\n",
            "12880 [D loss: 0.496026, acc.: 75.78%] [G loss: 4.312039]\n",
            "12900 [D loss: 0.367153, acc.: 85.55%] [G loss: 4.650890]\n",
            "12920 [D loss: 0.494774, acc.: 74.22%] [G loss: 4.093217]\n",
            "12940 [D loss: 0.384812, acc.: 83.20%] [G loss: 4.367301]\n",
            "12960 [D loss: 0.387960, acc.: 85.55%] [G loss: 4.697811]\n",
            "12980 [D loss: 0.541594, acc.: 73.44%] [G loss: 4.345688]\n",
            "13000 [D loss: 0.416579, acc.: 82.42%] [G loss: 4.447694]\n",
            "13020 [D loss: 0.483548, acc.: 78.52%] [G loss: 4.610190]\n",
            "13040 [D loss: 0.411144, acc.: 80.86%] [G loss: 4.362617]\n",
            "13060 [D loss: 0.466919, acc.: 78.52%] [G loss: 4.797046]\n",
            "13080 [D loss: 0.344984, acc.: 85.94%] [G loss: 4.574891]\n",
            "13100 [D loss: 0.353957, acc.: 87.11%] [G loss: 4.873872]\n",
            "13120 [D loss: 0.356576, acc.: 84.77%] [G loss: 4.729154]\n",
            "13140 [D loss: 0.377946, acc.: 84.38%] [G loss: 4.939857]\n",
            "13160 [D loss: 0.414666, acc.: 81.25%] [G loss: 4.980947]\n",
            "13180 [D loss: 0.440435, acc.: 77.34%] [G loss: 4.514197]\n",
            "13200 [D loss: 0.366623, acc.: 83.59%] [G loss: 4.959686]\n",
            "13220 [D loss: 0.384542, acc.: 82.42%] [G loss: 4.865255]\n",
            "13240 [D loss: 0.306750, acc.: 89.06%] [G loss: 4.685888]\n",
            "13260 [D loss: 0.429104, acc.: 78.52%] [G loss: 4.830142]\n",
            "13280 [D loss: 0.360597, acc.: 84.77%] [G loss: 4.590719]\n",
            "13300 [D loss: 0.433398, acc.: 80.47%] [G loss: 4.721225]\n",
            "13320 [D loss: 0.394393, acc.: 83.98%] [G loss: 4.503517]\n",
            "13340 [D loss: 0.421376, acc.: 81.25%] [G loss: 4.516353]\n",
            "13360 [D loss: 0.427313, acc.: 79.30%] [G loss: 4.528166]\n",
            "13380 [D loss: 0.440030, acc.: 78.12%] [G loss: 4.578897]\n",
            "13400 [D loss: 0.411696, acc.: 81.25%] [G loss: 4.581844]\n",
            "13420 [D loss: 0.490758, acc.: 76.56%] [G loss: 4.720076]\n",
            "13440 [D loss: 0.414967, acc.: 81.25%] [G loss: 4.626144]\n",
            "13460 [D loss: 0.378264, acc.: 84.77%] [G loss: 4.567352]\n",
            "13480 [D loss: 0.393997, acc.: 83.98%] [G loss: 4.817659]\n",
            "13500 [D loss: 0.136434, acc.: 96.88%] [G loss: 3.247883]\n",
            "13520 [D loss: 0.321668, acc.: 84.77%] [G loss: 2.987608]\n",
            "13540 [D loss: 43.151072, acc.: 22.66%] [G loss: 4.302125]\n",
            "13560 [D loss: 0.437027, acc.: 85.55%] [G loss: 1.817943]\n",
            "13580 [D loss: 0.458721, acc.: 86.33%] [G loss: 1.372583]\n",
            "13600 [D loss: 1.301739, acc.: 21.88%] [G loss: 0.934593]\n",
            "13620 [D loss: 1.893352, acc.: 4.30%] [G loss: 0.954165]\n",
            "13640 [D loss: 1.015493, acc.: 43.36%] [G loss: 2.363156]\n",
            "13660 [D loss: 0.925611, acc.: 46.09%] [G loss: 2.341312]\n",
            "13680 [D loss: 0.735497, acc.: 58.98%] [G loss: 2.528474]\n",
            "13700 [D loss: 0.714926, acc.: 61.33%] [G loss: 2.796471]\n",
            "13720 [D loss: 0.489935, acc.: 75.39%] [G loss: 3.396486]\n",
            "13740 [D loss: 0.660330, acc.: 63.28%] [G loss: 3.022945]\n",
            "13760 [D loss: 0.524862, acc.: 72.27%] [G loss: 3.244027]\n",
            "13780 [D loss: 0.507436, acc.: 75.00%] [G loss: 3.027364]\n",
            "13800 [D loss: 0.510138, acc.: 73.44%] [G loss: 3.099366]\n",
            "13820 [D loss: 0.520958, acc.: 76.17%] [G loss: 3.286243]\n",
            "13840 [D loss: 0.511205, acc.: 74.22%] [G loss: 3.293339]\n",
            "13860 [D loss: 0.561657, acc.: 69.92%] [G loss: 3.377287]\n",
            "13880 [D loss: 0.480807, acc.: 80.08%] [G loss: 3.110118]\n",
            "13900 [D loss: 0.468879, acc.: 76.95%] [G loss: 3.370069]\n",
            "13920 [D loss: 0.464815, acc.: 80.08%] [G loss: 3.336648]\n",
            "13940 [D loss: 0.500176, acc.: 76.95%] [G loss: 3.155735]\n",
            "13960 [D loss: 0.594556, acc.: 70.31%] [G loss: 3.109540]\n",
            "13980 [D loss: 0.538572, acc.: 72.66%] [G loss: 3.126672]\n",
            "14000 [D loss: 0.589002, acc.: 67.19%] [G loss: 3.009085]\n",
            "14020 [D loss: 0.478020, acc.: 78.52%] [G loss: 3.365264]\n",
            "14040 [D loss: 0.431479, acc.: 80.08%] [G loss: 3.478957]\n",
            "14060 [D loss: 0.497138, acc.: 75.39%] [G loss: 3.480858]\n",
            "14080 [D loss: 0.482876, acc.: 78.12%] [G loss: 3.438637]\n",
            "14100 [D loss: 0.489398, acc.: 76.56%] [G loss: 3.421959]\n",
            "14120 [D loss: 0.556834, acc.: 72.66%] [G loss: 3.419034]\n",
            "14140 [D loss: 0.471244, acc.: 77.73%] [G loss: 3.646879]\n",
            "14160 [D loss: 0.445403, acc.: 81.64%] [G loss: 3.592345]\n",
            "14180 [D loss: 0.507347, acc.: 74.61%] [G loss: 3.536840]\n",
            "14200 [D loss: 0.509671, acc.: 75.00%] [G loss: 3.522243]\n",
            "14220 [D loss: 0.462512, acc.: 80.47%] [G loss: 3.447085]\n",
            "14240 [D loss: 0.435606, acc.: 80.08%] [G loss: 3.804571]\n",
            "14260 [D loss: 0.513165, acc.: 74.61%] [G loss: 3.437740]\n",
            "14280 [D loss: 0.486787, acc.: 77.73%] [G loss: 3.644826]\n",
            "14300 [D loss: 0.544916, acc.: 73.05%] [G loss: 3.434735]\n",
            "14320 [D loss: 0.441278, acc.: 79.30%] [G loss: 3.585064]\n",
            "14340 [D loss: 0.463210, acc.: 76.17%] [G loss: 3.878274]\n",
            "14360 [D loss: 0.488270, acc.: 76.95%] [G loss: 3.814074]\n",
            "14380 [D loss: 0.468205, acc.: 78.52%] [G loss: 3.517748]\n",
            "14400 [D loss: 0.540357, acc.: 73.05%] [G loss: 3.204419]\n",
            "14420 [D loss: 0.490557, acc.: 77.34%] [G loss: 3.733805]\n",
            "14440 [D loss: 0.494857, acc.: 74.61%] [G loss: 3.446735]\n",
            "14460 [D loss: 0.455031, acc.: 80.08%] [G loss: 3.991710]\n",
            "14480 [D loss: 0.441944, acc.: 81.64%] [G loss: 3.682742]\n",
            "14500 [D loss: 0.503404, acc.: 78.91%] [G loss: 3.399155]\n",
            "14520 [D loss: 0.496739, acc.: 77.34%] [G loss: 3.772293]\n",
            "14540 [D loss: 0.547231, acc.: 75.00%] [G loss: 3.773483]\n",
            "14560 [D loss: 0.415671, acc.: 85.16%] [G loss: 3.905868]\n",
            "14580 [D loss: 0.512875, acc.: 75.00%] [G loss: 3.810276]\n",
            "14600 [D loss: 0.401155, acc.: 81.64%] [G loss: 4.258301]\n",
            "14620 [D loss: 0.402626, acc.: 80.08%] [G loss: 4.298503]\n",
            "14640 [D loss: 0.437426, acc.: 83.59%] [G loss: 3.740632]\n",
            "14660 [D loss: 0.461449, acc.: 77.34%] [G loss: 3.678507]\n",
            "14680 [D loss: 0.431656, acc.: 80.86%] [G loss: 3.958917]\n",
            "14700 [D loss: 0.421116, acc.: 82.03%] [G loss: 3.707209]\n",
            "14720 [D loss: 0.482911, acc.: 79.69%] [G loss: 3.972618]\n",
            "14740 [D loss: 0.416668, acc.: 83.20%] [G loss: 4.141185]\n",
            "14760 [D loss: 0.473717, acc.: 79.69%] [G loss: 4.054543]\n",
            "14780 [D loss: 0.393700, acc.: 84.38%] [G loss: 4.344750]\n",
            "14800 [D loss: 0.456019, acc.: 81.64%] [G loss: 3.802157]\n",
            "14820 [D loss: 0.411577, acc.: 83.59%] [G loss: 4.149035]\n",
            "14840 [D loss: 0.401550, acc.: 81.64%] [G loss: 4.255427]\n",
            "14860 [D loss: 0.399556, acc.: 83.98%] [G loss: 4.085276]\n",
            "14880 [D loss: 0.498832, acc.: 75.39%] [G loss: 3.695047]\n",
            "14900 [D loss: 0.462474, acc.: 76.56%] [G loss: 3.864725]\n",
            "14920 [D loss: 0.481169, acc.: 79.30%] [G loss: 3.857866]\n",
            "14940 [D loss: 0.357721, acc.: 83.98%] [G loss: 4.482062]\n",
            "14960 [D loss: 0.488801, acc.: 77.73%] [G loss: 3.938093]\n",
            "14980 [D loss: 0.400950, acc.: 83.20%] [G loss: 4.342815]\n",
            "15000 [D loss: 0.429046, acc.: 82.03%] [G loss: 3.943251]\n",
            "15020 [D loss: 0.463784, acc.: 80.86%] [G loss: 4.060476]\n",
            "15040 [D loss: 0.427510, acc.: 80.47%] [G loss: 3.939546]\n",
            "15060 [D loss: 0.419878, acc.: 83.59%] [G loss: 4.137691]\n",
            "15080 [D loss: 0.380922, acc.: 83.98%] [G loss: 4.506235]\n",
            "15100 [D loss: 0.413294, acc.: 82.81%] [G loss: 4.301286]\n",
            "15120 [D loss: 0.487515, acc.: 76.17%] [G loss: 3.999916]\n",
            "15140 [D loss: 0.577823, acc.: 72.66%] [G loss: 4.017250]\n",
            "15160 [D loss: 0.500352, acc.: 75.78%] [G loss: 4.129142]\n",
            "15180 [D loss: 0.401753, acc.: 80.47%] [G loss: 4.651939]\n",
            "15200 [D loss: 0.389899, acc.: 82.81%] [G loss: 4.354352]\n",
            "15220 [D loss: 0.426926, acc.: 80.08%] [G loss: 4.034595]\n",
            "15240 [D loss: 0.435579, acc.: 80.86%] [G loss: 4.901147]\n",
            "15260 [D loss: 0.422180, acc.: 82.42%] [G loss: 4.059317]\n",
            "15280 [D loss: 0.424718, acc.: 79.30%] [G loss: 4.697741]\n",
            "15300 [D loss: 0.460145, acc.: 76.56%] [G loss: 4.340624]\n",
            "15320 [D loss: 0.498868, acc.: 76.56%] [G loss: 4.264022]\n",
            "15340 [D loss: 0.501224, acc.: 73.83%] [G loss: 4.339797]\n",
            "15360 [D loss: 0.427608, acc.: 81.25%] [G loss: 4.535197]\n",
            "15380 [D loss: 0.471456, acc.: 77.73%] [G loss: 4.428140]\n",
            "15400 [D loss: 0.430484, acc.: 82.42%] [G loss: 4.254553]\n",
            "15420 [D loss: 0.457822, acc.: 80.08%] [G loss: 4.233134]\n",
            "15440 [D loss: 0.389725, acc.: 83.59%] [G loss: 4.347857]\n",
            "15460 [D loss: 0.395798, acc.: 86.33%] [G loss: 4.405768]\n",
            "15480 [D loss: 0.196006, acc.: 93.36%] [G loss: 3.080816]\n",
            "15500 [D loss: 0.064483, acc.: 99.61%] [G loss: 3.331536]\n",
            "15520 [D loss: 0.144585, acc.: 97.27%] [G loss: 2.748930]\n",
            "15540 [D loss: 1.480425, acc.: 46.88%] [G loss: 2.681611]\n",
            "15560 [D loss: 0.452121, acc.: 81.25%] [G loss: 4.475387]\n",
            "15580 [D loss: 2.215347, acc.: 31.64%] [G loss: 2.770509]\n",
            "15600 [D loss: 0.880105, acc.: 58.20%] [G loss: 2.820915]\n",
            "15620 [D loss: 0.988674, acc.: 49.61%] [G loss: 3.320617]\n",
            "15640 [D loss: 0.323779, acc.: 89.84%] [G loss: 2.056146]\n",
            "15660 [D loss: 1.026278, acc.: 45.31%] [G loss: 2.184098]\n",
            "15680 [D loss: 0.699562, acc.: 61.72%] [G loss: 2.778366]\n",
            "15700 [D loss: 0.400647, acc.: 83.59%] [G loss: 3.741610]\n",
            "15720 [D loss: 0.576694, acc.: 71.09%] [G loss: 3.213967]\n",
            "15740 [D loss: 0.548076, acc.: 69.53%] [G loss: 2.722476]\n",
            "15760 [D loss: 0.462586, acc.: 78.52%] [G loss: 3.737808]\n",
            "15780 [D loss: 0.561482, acc.: 68.75%] [G loss: 3.139091]\n",
            "15800 [D loss: 0.600641, acc.: 69.14%] [G loss: 3.248711]\n",
            "15820 [D loss: 0.497622, acc.: 77.34%] [G loss: 3.602757]\n",
            "15840 [D loss: 0.526887, acc.: 75.78%] [G loss: 3.319015]\n",
            "15860 [D loss: 0.487718, acc.: 78.52%] [G loss: 3.395507]\n",
            "15880 [D loss: 0.430933, acc.: 82.42%] [G loss: 3.468445]\n",
            "15900 [D loss: 0.505391, acc.: 79.30%] [G loss: 3.322110]\n",
            "15920 [D loss: 0.372923, acc.: 85.16%] [G loss: 4.078044]\n",
            "15940 [D loss: 0.579714, acc.: 69.92%] [G loss: 3.035251]\n",
            "15960 [D loss: 0.385045, acc.: 85.16%] [G loss: 3.894274]\n",
            "15980 [D loss: 0.510723, acc.: 77.34%] [G loss: 3.351202]\n",
            "16000 [D loss: 0.430497, acc.: 78.91%] [G loss: 3.433635]\n",
            "16020 [D loss: 0.437340, acc.: 80.86%] [G loss: 3.570546]\n",
            "16040 [D loss: 0.537147, acc.: 72.66%] [G loss: 3.373109]\n",
            "16060 [D loss: 0.451432, acc.: 78.52%] [G loss: 3.612471]\n",
            "16080 [D loss: 0.507045, acc.: 75.78%] [G loss: 3.731206]\n",
            "16100 [D loss: 0.427770, acc.: 79.69%] [G loss: 3.975433]\n",
            "16120 [D loss: 0.485441, acc.: 79.30%] [G loss: 3.577917]\n",
            "16140 [D loss: 0.485838, acc.: 78.12%] [G loss: 3.831873]\n",
            "16160 [D loss: 0.413603, acc.: 81.25%] [G loss: 3.861144]\n",
            "16180 [D loss: 0.424227, acc.: 82.03%] [G loss: 3.966362]\n",
            "16200 [D loss: 0.442420, acc.: 78.12%] [G loss: 3.769581]\n",
            "16220 [D loss: 0.421285, acc.: 80.08%] [G loss: 3.908467]\n",
            "16240 [D loss: 0.458592, acc.: 80.47%] [G loss: 4.079065]\n",
            "16260 [D loss: 0.379296, acc.: 83.98%] [G loss: 4.399849]\n",
            "16280 [D loss: 0.471574, acc.: 79.69%] [G loss: 3.821191]\n",
            "16300 [D loss: 0.415152, acc.: 83.59%] [G loss: 4.287755]\n",
            "16320 [D loss: 0.441783, acc.: 81.64%] [G loss: 4.158505]\n",
            "16340 [D loss: 0.452405, acc.: 81.25%] [G loss: 3.874599]\n",
            "16360 [D loss: 0.462015, acc.: 77.73%] [G loss: 3.880376]\n",
            "16380 [D loss: 0.460527, acc.: 78.12%] [G loss: 3.978537]\n",
            "16400 [D loss: 0.445067, acc.: 80.47%] [G loss: 3.843220]\n",
            "16420 [D loss: 0.455994, acc.: 81.64%] [G loss: 4.106298]\n",
            "16440 [D loss: 0.429434, acc.: 82.81%] [G loss: 3.887859]\n",
            "16460 [D loss: 0.426455, acc.: 80.86%] [G loss: 3.881473]\n",
            "16480 [D loss: 0.407283, acc.: 83.59%] [G loss: 3.916560]\n",
            "16500 [D loss: 0.411014, acc.: 82.42%] [G loss: 4.464926]\n",
            "16520 [D loss: 0.462243, acc.: 78.12%] [G loss: 4.045566]\n",
            "16540 [D loss: 0.455365, acc.: 77.73%] [G loss: 4.301623]\n",
            "16560 [D loss: 0.439338, acc.: 82.81%] [G loss: 4.287032]\n",
            "16580 [D loss: 0.396898, acc.: 82.42%] [G loss: 4.126605]\n",
            "16600 [D loss: 0.434249, acc.: 80.08%] [G loss: 4.532569]\n",
            "16620 [D loss: 0.368517, acc.: 85.94%] [G loss: 4.419341]\n",
            "16640 [D loss: 0.496954, acc.: 75.00%] [G loss: 4.003910]\n",
            "16660 [D loss: 0.421107, acc.: 80.86%] [G loss: 4.059888]\n",
            "16680 [D loss: 0.458552, acc.: 80.47%] [G loss: 4.046149]\n",
            "16700 [D loss: 0.369781, acc.: 85.16%] [G loss: 4.430816]\n",
            "16720 [D loss: 0.478534, acc.: 75.78%] [G loss: 4.067480]\n",
            "16740 [D loss: 0.479553, acc.: 76.17%] [G loss: 3.865680]\n",
            "16760 [D loss: 0.234820, acc.: 93.36%] [G loss: 2.844849]\n",
            "16780 [D loss: 0.293754, acc.: 89.84%] [G loss: 2.876758]\n",
            "16800 [D loss: 0.747951, acc.: 62.89%] [G loss: 3.895149]\n",
            "16820 [D loss: 0.333026, acc.: 88.67%] [G loss: 5.369226]\n",
            "16840 [D loss: 0.570824, acc.: 73.05%] [G loss: 3.948549]\n",
            "16860 [D loss: 0.560720, acc.: 68.75%] [G loss: 3.948017]\n",
            "16880 [D loss: 0.642513, acc.: 67.97%] [G loss: 4.455096]\n",
            "16900 [D loss: 0.467870, acc.: 75.78%] [G loss: 3.451777]\n",
            "16920 [D loss: 0.627990, acc.: 71.88%] [G loss: 2.750352]\n",
            "16940 [D loss: 0.462832, acc.: 77.34%] [G loss: 3.493181]\n",
            "16960 [D loss: 0.474901, acc.: 76.95%] [G loss: 4.049307]\n",
            "16980 [D loss: 0.346285, acc.: 87.11%] [G loss: 4.256129]\n",
            "17000 [D loss: 0.431180, acc.: 79.69%] [G loss: 5.191535]\n",
            "17020 [D loss: 0.500355, acc.: 74.61%] [G loss: 4.458185]\n",
            "17040 [D loss: 0.371593, acc.: 83.59%] [G loss: 4.531582]\n",
            "17060 [D loss: 0.437486, acc.: 79.69%] [G loss: 4.476826]\n",
            "17080 [D loss: 0.342628, acc.: 85.16%] [G loss: 4.785570]\n",
            "17100 [D loss: 0.408244, acc.: 82.03%] [G loss: 4.818806]\n",
            "17120 [D loss: 0.383365, acc.: 83.59%] [G loss: 4.483673]\n",
            "17140 [D loss: 0.395860, acc.: 84.38%] [G loss: 4.453334]\n",
            "17160 [D loss: 0.316986, acc.: 88.28%] [G loss: 5.149656]\n",
            "17180 [D loss: 0.397209, acc.: 82.81%] [G loss: 4.389203]\n",
            "17200 [D loss: 0.376088, acc.: 82.81%] [G loss: 4.215083]\n",
            "17220 [D loss: 0.465200, acc.: 79.30%] [G loss: 4.035328]\n",
            "17240 [D loss: 0.374675, acc.: 84.77%] [G loss: 4.652866]\n",
            "17260 [D loss: 0.426710, acc.: 81.25%] [G loss: 4.409223]\n",
            "17280 [D loss: 0.382033, acc.: 82.03%] [G loss: 4.650807]\n",
            "17300 [D loss: 0.361597, acc.: 86.33%] [G loss: 4.420091]\n",
            "17320 [D loss: 0.449671, acc.: 80.08%] [G loss: 4.996961]\n",
            "17340 [D loss: 0.387513, acc.: 84.38%] [G loss: 4.653886]\n",
            "17360 [D loss: 0.367071, acc.: 84.77%] [G loss: 4.814690]\n",
            "17380 [D loss: 0.432466, acc.: 80.47%] [G loss: 4.599825]\n",
            "17400 [D loss: 0.349636, acc.: 83.98%] [G loss: 4.753123]\n",
            "17420 [D loss: 0.414075, acc.: 82.81%] [G loss: 4.963600]\n",
            "17440 [D loss: 0.435509, acc.: 79.30%] [G loss: 4.804631]\n",
            "17460 [D loss: 0.357561, acc.: 85.16%] [G loss: 4.645768]\n",
            "17480 [D loss: 0.424584, acc.: 80.08%] [G loss: 4.425593]\n",
            "17500 [D loss: 0.455252, acc.: 80.47%] [G loss: 4.464862]\n",
            "17520 [D loss: 0.433671, acc.: 80.08%] [G loss: 4.512049]\n",
            "17540 [D loss: 0.368265, acc.: 82.42%] [G loss: 4.831359]\n",
            "17560 [D loss: 0.400339, acc.: 81.25%] [G loss: 4.848986]\n",
            "17580 [D loss: 0.392389, acc.: 81.64%] [G loss: 5.247176]\n",
            "17600 [D loss: 0.343552, acc.: 87.50%] [G loss: 4.821279]\n",
            "17620 [D loss: 0.461947, acc.: 76.17%] [G loss: 5.213452]\n",
            "17640 [D loss: 0.342316, acc.: 88.28%] [G loss: 4.922108]\n",
            "17660 [D loss: 0.400624, acc.: 80.08%] [G loss: 4.694927]\n",
            "17680 [D loss: 0.475513, acc.: 78.91%] [G loss: 4.260591]\n",
            "17700 [D loss: 0.361475, acc.: 85.16%] [G loss: 4.825483]\n",
            "17720 [D loss: 0.443954, acc.: 77.73%] [G loss: 5.244626]\n",
            "17740 [D loss: 0.328617, acc.: 87.89%] [G loss: 5.087080]\n",
            "17760 [D loss: 0.388356, acc.: 82.42%] [G loss: 4.692625]\n",
            "17780 [D loss: 0.325232, acc.: 87.89%] [G loss: 4.768943]\n",
            "17800 [D loss: 0.346128, acc.: 85.55%] [G loss: 5.216579]\n",
            "17820 [D loss: 0.411791, acc.: 80.86%] [G loss: 5.443164]\n",
            "17840 [D loss: 0.371502, acc.: 85.55%] [G loss: 4.798802]\n",
            "17860 [D loss: 0.450507, acc.: 76.95%] [G loss: 4.624603]\n",
            "17880 [D loss: 0.445292, acc.: 79.69%] [G loss: 5.018198]\n",
            "17900 [D loss: 0.389189, acc.: 82.81%] [G loss: 5.061753]\n",
            "17920 [D loss: 0.382765, acc.: 84.77%] [G loss: 5.225767]\n",
            "17940 [D loss: 0.413618, acc.: 79.69%] [G loss: 5.180696]\n",
            "17960 [D loss: 0.393900, acc.: 82.81%] [G loss: 5.101843]\n",
            "17980 [D loss: 0.399921, acc.: 82.03%] [G loss: 5.323112]\n",
            "18000 [D loss: 0.398841, acc.: 80.08%] [G loss: 5.258513]\n",
            "18020 [D loss: 0.401806, acc.: 79.69%] [G loss: 4.852219]\n",
            "18040 [D loss: 0.400809, acc.: 83.59%] [G loss: 5.115755]\n",
            "18060 [D loss: 0.386314, acc.: 81.25%] [G loss: 4.963210]\n",
            "18080 [D loss: 0.323931, acc.: 86.72%] [G loss: 5.021158]\n",
            "18100 [D loss: 0.347375, acc.: 84.77%] [G loss: 4.931914]\n",
            "18120 [D loss: 0.421029, acc.: 80.47%] [G loss: 5.043447]\n",
            "18140 [D loss: 0.479037, acc.: 78.12%] [G loss: 5.303087]\n",
            "18160 [D loss: 0.298831, acc.: 89.06%] [G loss: 5.161130]\n",
            "18180 [D loss: 0.451215, acc.: 78.52%] [G loss: 5.032243]\n",
            "18200 [D loss: 0.371249, acc.: 85.55%] [G loss: 5.267118]\n",
            "18220 [D loss: 0.386838, acc.: 83.59%] [G loss: 5.145065]\n",
            "18240 [D loss: 0.374558, acc.: 84.77%] [G loss: 5.215402]\n",
            "18260 [D loss: 0.315182, acc.: 86.33%] [G loss: 6.059291]\n",
            "18280 [D loss: 0.398626, acc.: 84.38%] [G loss: 4.993536]\n",
            "18300 [D loss: 0.388323, acc.: 80.47%] [G loss: 5.231543]\n",
            "18320 [D loss: 0.347948, acc.: 83.20%] [G loss: 5.822380]\n",
            "18340 [D loss: 0.399163, acc.: 81.25%] [G loss: 5.275237]\n",
            "18360 [D loss: 0.309070, acc.: 85.55%] [G loss: 6.000675]\n",
            "18380 [D loss: 0.379023, acc.: 83.20%] [G loss: 5.150967]\n",
            "18400 [D loss: 0.363658, acc.: 83.20%] [G loss: 5.151995]\n",
            "18420 [D loss: 0.313659, acc.: 90.62%] [G loss: 5.153777]\n",
            "18440 [D loss: 0.635104, acc.: 69.92%] [G loss: 4.515091]\n",
            "18460 [D loss: 0.336439, acc.: 85.55%] [G loss: 3.821000]\n",
            "18480 [D loss: 0.636181, acc.: 71.88%] [G loss: 4.080092]\n",
            "18500 [D loss: 0.391655, acc.: 81.25%] [G loss: 4.696949]\n",
            "18520 [D loss: 0.309589, acc.: 87.50%] [G loss: 3.756393]\n",
            "18540 [D loss: 0.354302, acc.: 85.16%] [G loss: 5.924698]\n",
            "18560 [D loss: 0.379780, acc.: 82.81%] [G loss: 5.713478]\n",
            "18580 [D loss: 0.481644, acc.: 73.44%] [G loss: 5.580680]\n",
            "18600 [D loss: 0.369643, acc.: 86.72%] [G loss: 5.530689]\n",
            "18620 [D loss: 0.354964, acc.: 84.77%] [G loss: 5.812890]\n",
            "18640 [D loss: 0.321311, acc.: 84.38%] [G loss: 5.443337]\n",
            "18660 [D loss: 0.396729, acc.: 81.25%] [G loss: 5.629862]\n",
            "18680 [D loss: 0.402701, acc.: 82.03%] [G loss: 5.457888]\n",
            "18700 [D loss: 0.451873, acc.: 78.91%] [G loss: 5.338519]\n",
            "18720 [D loss: 0.307876, acc.: 89.45%] [G loss: 5.473884]\n",
            "18740 [D loss: 0.424834, acc.: 84.38%] [G loss: 5.659544]\n",
            "18760 [D loss: 0.308873, acc.: 88.28%] [G loss: 5.814860]\n",
            "18780 [D loss: 0.387460, acc.: 82.42%] [G loss: 5.410065]\n",
            "18800 [D loss: 0.322640, acc.: 86.33%] [G loss: 5.441453]\n",
            "18820 [D loss: 0.375898, acc.: 83.59%] [G loss: 6.176211]\n",
            "18840 [D loss: 0.315162, acc.: 86.72%] [G loss: 5.518354]\n",
            "18860 [D loss: 0.370008, acc.: 84.77%] [G loss: 5.921404]\n",
            "18880 [D loss: 0.374502, acc.: 85.16%] [G loss: 5.095966]\n",
            "18900 [D loss: 0.390852, acc.: 82.42%] [G loss: 5.301105]\n",
            "18920 [D loss: 0.374864, acc.: 82.42%] [G loss: 5.472234]\n",
            "18940 [D loss: 0.424230, acc.: 80.47%] [G loss: 5.405178]\n",
            "18960 [D loss: 0.374240, acc.: 84.77%] [G loss: 5.305317]\n",
            "18980 [D loss: 0.327869, acc.: 88.28%] [G loss: 5.926379]\n",
            "19000 [D loss: 0.412185, acc.: 78.12%] [G loss: 5.772942]\n",
            "19020 [D loss: 0.323344, acc.: 87.11%] [G loss: 6.187756]\n",
            "19040 [D loss: 0.377138, acc.: 81.25%] [G loss: 5.828420]\n",
            "19060 [D loss: 0.399256, acc.: 81.64%] [G loss: 5.563416]\n",
            "19080 [D loss: 0.357812, acc.: 83.59%] [G loss: 5.731555]\n",
            "19100 [D loss: 0.410280, acc.: 81.64%] [G loss: 5.712850]\n",
            "19120 [D loss: 0.306745, acc.: 87.89%] [G loss: 5.902047]\n",
            "19140 [D loss: 0.295757, acc.: 87.89%] [G loss: 5.665105]\n",
            "19160 [D loss: 0.370324, acc.: 84.77%] [G loss: 5.435636]\n",
            "19180 [D loss: 0.328810, acc.: 86.33%] [G loss: 5.719147]\n",
            "19200 [D loss: 0.411077, acc.: 80.08%] [G loss: 5.975705]\n",
            "19220 [D loss: 0.353278, acc.: 83.98%] [G loss: 5.956974]\n",
            "19240 [D loss: 0.353775, acc.: 83.59%] [G loss: 5.405715]\n",
            "19260 [D loss: 0.360401, acc.: 84.77%] [G loss: 5.861368]\n",
            "19280 [D loss: 0.401912, acc.: 83.59%] [G loss: 5.164056]\n",
            "19300 [D loss: 0.371751, acc.: 85.94%] [G loss: 5.901752]\n",
            "19320 [D loss: 0.368997, acc.: 83.20%] [G loss: 5.629539]\n",
            "19340 [D loss: 0.351589, acc.: 84.77%] [G loss: 5.814043]\n",
            "19360 [D loss: 0.345746, acc.: 87.50%] [G loss: 5.278246]\n",
            "19380 [D loss: 0.325503, acc.: 85.94%] [G loss: 6.372057]\n",
            "19400 [D loss: 0.312244, acc.: 87.50%] [G loss: 5.953518]\n",
            "19420 [D loss: 0.310416, acc.: 86.33%] [G loss: 5.886408]\n",
            "19440 [D loss: 0.254293, acc.: 89.84%] [G loss: 6.342517]\n",
            "19460 [D loss: 0.329482, acc.: 84.38%] [G loss: 6.135691]\n",
            "19480 [D loss: 0.304687, acc.: 87.50%] [G loss: 6.091099]\n",
            "19500 [D loss: 0.269963, acc.: 89.84%] [G loss: 6.139659]\n",
            "19520 [D loss: 0.337108, acc.: 85.55%] [G loss: 5.887897]\n",
            "19540 [D loss: 0.327563, acc.: 85.94%] [G loss: 6.169545]\n",
            "19560 [D loss: 0.393559, acc.: 81.64%] [G loss: 5.800520]\n",
            "19580 [D loss: 0.282047, acc.: 90.23%] [G loss: 5.734071]\n",
            "19600 [D loss: 0.290233, acc.: 88.28%] [G loss: 6.013749]\n",
            "19620 [D loss: 0.259547, acc.: 87.50%] [G loss: 6.297706]\n",
            "19640 [D loss: 0.317970, acc.: 86.72%] [G loss: 6.136525]\n",
            "19660 [D loss: 0.356514, acc.: 84.77%] [G loss: 6.105327]\n",
            "19680 [D loss: 0.281290, acc.: 88.28%] [G loss: 6.313779]\n",
            "19700 [D loss: 0.341401, acc.: 86.33%] [G loss: 5.923169]\n",
            "19720 [D loss: 0.347390, acc.: 85.55%] [G loss: 5.740863]\n",
            "19740 [D loss: 0.433944, acc.: 82.42%] [G loss: 6.298615]\n",
            "19760 [D loss: 0.276539, acc.: 91.02%] [G loss: 5.839060]\n",
            "19780 [D loss: 0.368534, acc.: 85.55%] [G loss: 6.163935]\n",
            "19800 [D loss: 0.340592, acc.: 85.55%] [G loss: 5.652044]\n",
            "19820 [D loss: 0.334232, acc.: 85.94%] [G loss: 6.200978]\n",
            "19840 [D loss: 0.282329, acc.: 88.67%] [G loss: 6.649645]\n",
            "19860 [D loss: 0.246833, acc.: 89.84%] [G loss: 6.412734]\n",
            "19880 [D loss: 0.368394, acc.: 82.42%] [G loss: 6.387113]\n",
            "19900 [D loss: 0.297359, acc.: 87.11%] [G loss: 5.798499]\n",
            "19920 [D loss: 0.098699, acc.: 97.66%] [G loss: 4.255200]\n",
            "19940 [D loss: 0.180993, acc.: 94.92%] [G loss: 3.548318]\n",
            "19960 [D loss: 0.747998, acc.: 53.91%] [G loss: 3.178943]\n",
            "19980 [D loss: 0.808360, acc.: 57.42%] [G loss: 1.974301]\n",
            "20000 [D loss: 2.194938, acc.: 25.78%] [G loss: 4.515843]\n",
            "20020 [D loss: 1.355394, acc.: 45.31%] [G loss: 3.178391]\n",
            "20040 [D loss: 0.625668, acc.: 66.80%] [G loss: 3.640019]\n",
            "20060 [D loss: 0.651373, acc.: 66.41%] [G loss: 2.961201]\n",
            "20080 [D loss: 0.623684, acc.: 68.75%] [G loss: 3.437541]\n",
            "20100 [D loss: 0.497357, acc.: 74.22%] [G loss: 3.769715]\n",
            "20120 [D loss: 0.580817, acc.: 71.48%] [G loss: 3.415902]\n",
            "20140 [D loss: 0.437279, acc.: 78.52%] [G loss: 3.550519]\n",
            "20160 [D loss: 0.561109, acc.: 72.27%] [G loss: 3.755256]\n",
            "20180 [D loss: 0.440441, acc.: 78.91%] [G loss: 4.103528]\n",
            "20200 [D loss: 0.572176, acc.: 71.48%] [G loss: 3.145607]\n",
            "20220 [D loss: 0.490679, acc.: 74.61%] [G loss: 3.878032]\n",
            "20240 [D loss: 0.537434, acc.: 75.78%] [G loss: 3.531730]\n",
            "20260 [D loss: 0.468505, acc.: 77.34%] [G loss: 3.569690]\n",
            "20280 [D loss: 0.518444, acc.: 74.61%] [G loss: 3.746999]\n",
            "20300 [D loss: 0.464402, acc.: 80.86%] [G loss: 3.771670]\n",
            "20320 [D loss: 0.440770, acc.: 79.69%] [G loss: 4.132745]\n",
            "20340 [D loss: 0.481599, acc.: 78.91%] [G loss: 3.915854]\n",
            "20360 [D loss: 0.397468, acc.: 83.20%] [G loss: 4.309699]\n",
            "20380 [D loss: 0.427270, acc.: 79.69%] [G loss: 4.039608]\n",
            "20400 [D loss: 0.454744, acc.: 77.73%] [G loss: 3.762802]\n",
            "20420 [D loss: 0.371963, acc.: 85.16%] [G loss: 4.029562]\n",
            "20440 [D loss: 0.425479, acc.: 80.86%] [G loss: 3.994753]\n",
            "20460 [D loss: 0.408978, acc.: 80.08%] [G loss: 4.145732]\n",
            "20480 [D loss: 0.438502, acc.: 81.25%] [G loss: 3.980612]\n",
            "20500 [D loss: 0.511468, acc.: 75.00%] [G loss: 3.895910]\n",
            "20520 [D loss: 0.442679, acc.: 81.25%] [G loss: 4.331522]\n",
            "20540 [D loss: 0.462514, acc.: 77.73%] [G loss: 4.213581]\n",
            "20560 [D loss: 0.396504, acc.: 82.03%] [G loss: 4.449269]\n",
            "20580 [D loss: 0.451067, acc.: 79.69%] [G loss: 4.203137]\n",
            "20600 [D loss: 0.366924, acc.: 87.11%] [G loss: 4.540938]\n",
            "20620 [D loss: 0.552924, acc.: 72.27%] [G loss: 4.245627]\n",
            "20640 [D loss: 0.346844, acc.: 85.94%] [G loss: 4.550578]\n",
            "20660 [D loss: 0.450050, acc.: 80.47%] [G loss: 4.756289]\n",
            "20680 [D loss: 0.420969, acc.: 78.12%] [G loss: 4.600702]\n",
            "20700 [D loss: 0.472722, acc.: 80.08%] [G loss: 4.384318]\n",
            "20720 [D loss: 0.322268, acc.: 86.33%] [G loss: 4.749411]\n",
            "20740 [D loss: 0.365054, acc.: 84.77%] [G loss: 4.721325]\n",
            "20760 [D loss: 0.429105, acc.: 81.25%] [G loss: 4.438404]\n",
            "20780 [D loss: 0.381821, acc.: 85.94%] [G loss: 4.725952]\n",
            "20800 [D loss: 0.390079, acc.: 82.42%] [G loss: 4.737700]\n",
            "20820 [D loss: 0.433647, acc.: 77.73%] [G loss: 4.283920]\n",
            "20840 [D loss: 0.385475, acc.: 83.59%] [G loss: 4.639791]\n",
            "20860 [D loss: 0.414414, acc.: 82.81%] [G loss: 4.697921]\n",
            "20880 [D loss: 0.425311, acc.: 80.47%] [G loss: 4.600875]\n",
            "20900 [D loss: 0.364211, acc.: 83.59%] [G loss: 5.097467]\n",
            "20920 [D loss: 0.385934, acc.: 81.64%] [G loss: 4.892012]\n",
            "20940 [D loss: 0.404417, acc.: 81.64%] [G loss: 4.530541]\n",
            "20960 [D loss: 0.365082, acc.: 83.59%] [G loss: 5.175801]\n",
            "20980 [D loss: 0.367809, acc.: 83.59%] [G loss: 4.859294]\n",
            "21000 [D loss: 0.358962, acc.: 83.59%] [G loss: 4.956438]\n",
            "21020 [D loss: 0.413543, acc.: 83.59%] [G loss: 4.856047]\n",
            "21040 [D loss: 0.382258, acc.: 85.16%] [G loss: 4.963515]\n",
            "21060 [D loss: 0.470120, acc.: 78.12%] [G loss: 4.871979]\n",
            "21080 [D loss: 0.375783, acc.: 82.42%] [G loss: 4.971638]\n",
            "21100 [D loss: 0.365545, acc.: 81.64%] [G loss: 5.219364]\n",
            "21120 [D loss: 0.314086, acc.: 87.50%] [G loss: 5.360996]\n",
            "21140 [D loss: 0.415416, acc.: 83.59%] [G loss: 5.151586]\n",
            "21160 [D loss: 0.381673, acc.: 85.16%] [G loss: 5.220392]\n",
            "21180 [D loss: 0.461498, acc.: 81.25%] [G loss: 4.803948]\n",
            "21200 [D loss: 0.372176, acc.: 83.20%] [G loss: 5.369838]\n",
            "21220 [D loss: 0.342552, acc.: 85.55%] [G loss: 5.536335]\n",
            "21240 [D loss: 0.431766, acc.: 80.08%] [G loss: 5.134056]\n",
            "21260 [D loss: 0.374606, acc.: 82.81%] [G loss: 4.909476]\n",
            "21280 [D loss: 0.373709, acc.: 83.98%] [G loss: 5.290996]\n",
            "21300 [D loss: 0.371925, acc.: 85.16%] [G loss: 5.313308]\n",
            "21320 [D loss: 0.319419, acc.: 88.28%] [G loss: 4.712466]\n",
            "21340 [D loss: 0.425170, acc.: 81.64%] [G loss: 4.969961]\n",
            "21360 [D loss: 0.360165, acc.: 83.20%] [G loss: 5.437313]\n",
            "21380 [D loss: 0.306319, acc.: 87.89%] [G loss: 5.372806]\n",
            "21400 [D loss: 0.378466, acc.: 82.81%] [G loss: 5.145426]\n",
            "21420 [D loss: 0.328276, acc.: 86.72%] [G loss: 5.478229]\n",
            "21440 [D loss: 0.359797, acc.: 80.08%] [G loss: 5.348874]\n",
            "21460 [D loss: 0.436019, acc.: 78.91%] [G loss: 5.054910]\n",
            "21480 [D loss: 0.382247, acc.: 83.98%] [G loss: 5.482703]\n",
            "21500 [D loss: 0.356769, acc.: 86.33%] [G loss: 5.413826]\n",
            "21520 [D loss: 0.373969, acc.: 85.16%] [G loss: 5.221217]\n",
            "21540 [D loss: 0.371034, acc.: 82.03%] [G loss: 5.187530]\n",
            "21560 [D loss: 0.339830, acc.: 83.20%] [G loss: 5.605153]\n",
            "21580 [D loss: 0.305767, acc.: 87.89%] [G loss: 5.359852]\n",
            "21600 [D loss: 0.344252, acc.: 84.77%] [G loss: 5.577882]\n",
            "21620 [D loss: 0.404637, acc.: 82.03%] [G loss: 5.494057]\n",
            "21640 [D loss: 0.345255, acc.: 85.94%] [G loss: 5.868481]\n",
            "21660 [D loss: 0.305913, acc.: 87.50%] [G loss: 4.227313]\n",
            "21680 [D loss: 0.260964, acc.: 89.45%] [G loss: 2.808719]\n",
            "21700 [D loss: 1.076485, acc.: 57.81%] [G loss: 5.386552]\n",
            "21720 [D loss: 0.568936, acc.: 73.83%] [G loss: 6.532316]\n",
            "21740 [D loss: 0.494964, acc.: 75.39%] [G loss: 6.931387]\n",
            "21760 [D loss: 0.473728, acc.: 79.30%] [G loss: 6.760861]\n",
            "21780 [D loss: 0.354390, acc.: 83.98%] [G loss: 5.676984]\n",
            "21800 [D loss: 0.410842, acc.: 80.86%] [G loss: 5.030004]\n",
            "21820 [D loss: 0.322930, acc.: 86.33%] [G loss: 6.028423]\n",
            "21840 [D loss: 0.629013, acc.: 70.31%] [G loss: 5.198678]\n",
            "21860 [D loss: 0.221718, acc.: 90.62%] [G loss: 6.331093]\n",
            "21880 [D loss: 0.437008, acc.: 80.08%] [G loss: 5.645777]\n",
            "21900 [D loss: 0.318447, acc.: 88.28%] [G loss: 5.804121]\n",
            "21920 [D loss: 0.376364, acc.: 82.81%] [G loss: 5.885602]\n",
            "21940 [D loss: 0.332491, acc.: 86.33%] [G loss: 5.538515]\n",
            "21960 [D loss: 0.338015, acc.: 87.11%] [G loss: 5.750942]\n",
            "21980 [D loss: 0.322396, acc.: 84.77%] [G loss: 6.243457]\n",
            "22000 [D loss: 0.310075, acc.: 89.06%] [G loss: 5.635961]\n",
            "22020 [D loss: 0.341237, acc.: 85.16%] [G loss: 6.067112]\n",
            "22040 [D loss: 0.301952, acc.: 87.11%] [G loss: 6.386141]\n",
            "22060 [D loss: 0.349621, acc.: 89.06%] [G loss: 5.734578]\n",
            "22080 [D loss: 0.292817, acc.: 87.50%] [G loss: 5.593024]\n",
            "22100 [D loss: 0.367715, acc.: 85.55%] [G loss: 5.684487]\n",
            "22120 [D loss: 0.444545, acc.: 79.69%] [G loss: 5.654233]\n",
            "22140 [D loss: 0.359865, acc.: 83.98%] [G loss: 5.936833]\n",
            "22160 [D loss: 0.392812, acc.: 82.42%] [G loss: 5.924455]\n",
            "22180 [D loss: 0.404680, acc.: 80.86%] [G loss: 5.868394]\n",
            "22200 [D loss: 0.328484, acc.: 87.11%] [G loss: 6.298607]\n",
            "22220 [D loss: 0.275099, acc.: 89.45%] [G loss: 5.992904]\n",
            "22240 [D loss: 0.313317, acc.: 83.59%] [G loss: 6.264534]\n",
            "22260 [D loss: 0.277394, acc.: 89.45%] [G loss: 5.937181]\n",
            "22280 [D loss: 0.313456, acc.: 85.94%] [G loss: 6.522655]\n",
            "22300 [D loss: 0.334999, acc.: 86.72%] [G loss: 6.043257]\n",
            "22320 [D loss: 0.241842, acc.: 91.41%] [G loss: 5.906935]\n",
            "22340 [D loss: 0.319228, acc.: 86.33%] [G loss: 6.139748]\n",
            "22360 [D loss: 0.386883, acc.: 84.77%] [G loss: 5.931978]\n",
            "22380 [D loss: 0.286137, acc.: 90.62%] [G loss: 6.148335]\n",
            "22400 [D loss: 0.388893, acc.: 84.38%] [G loss: 5.812803]\n",
            "22420 [D loss: 0.393769, acc.: 82.81%] [G loss: 5.722119]\n",
            "22440 [D loss: 0.341300, acc.: 84.77%] [G loss: 6.127864]\n",
            "22460 [D loss: 0.278294, acc.: 89.06%] [G loss: 6.338820]\n",
            "22480 [D loss: 0.376561, acc.: 82.81%] [G loss: 5.838321]\n",
            "22500 [D loss: 0.402989, acc.: 82.42%] [G loss: 6.119685]\n",
            "22520 [D loss: 0.295667, acc.: 88.67%] [G loss: 6.077122]\n",
            "22540 [D loss: 0.327066, acc.: 86.33%] [G loss: 6.139139]\n",
            "22560 [D loss: 0.280647, acc.: 89.06%] [G loss: 6.045933]\n",
            "22580 [D loss: 0.290343, acc.: 88.28%] [G loss: 6.440338]\n",
            "22600 [D loss: 0.333962, acc.: 85.55%] [G loss: 5.739193]\n",
            "22620 [D loss: 0.291858, acc.: 89.45%] [G loss: 6.413966]\n",
            "22640 [D loss: 0.263457, acc.: 87.50%] [G loss: 6.428954]\n",
            "22660 [D loss: 0.397683, acc.: 81.64%] [G loss: 6.482882]\n",
            "22680 [D loss: 0.271758, acc.: 87.89%] [G loss: 6.200916]\n",
            "22700 [D loss: 0.307805, acc.: 87.11%] [G loss: 7.279718]\n",
            "22720 [D loss: 0.331289, acc.: 84.38%] [G loss: 6.256070]\n",
            "22740 [D loss: 0.288500, acc.: 86.72%] [G loss: 6.661819]\n",
            "22760 [D loss: 0.290432, acc.: 87.89%] [G loss: 6.586983]\n",
            "22780 [D loss: 0.324721, acc.: 85.55%] [G loss: 6.742846]\n",
            "22800 [D loss: 0.387913, acc.: 85.16%] [G loss: 6.170999]\n",
            "22820 [D loss: 0.335307, acc.: 85.94%] [G loss: 6.254101]\n",
            "22840 [D loss: 0.269446, acc.: 89.06%] [G loss: 7.316420]\n",
            "22860 [D loss: 0.259169, acc.: 91.41%] [G loss: 6.778710]\n",
            "22880 [D loss: 0.296460, acc.: 88.67%] [G loss: 6.539559]\n",
            "22900 [D loss: 0.240910, acc.: 91.02%] [G loss: 6.570074]\n",
            "22920 [D loss: 0.225428, acc.: 91.80%] [G loss: 6.898039]\n",
            "22940 [D loss: 0.266497, acc.: 89.84%] [G loss: 6.161397]\n",
            "22960 [D loss: 0.419451, acc.: 82.03%] [G loss: 6.589434]\n",
            "22980 [D loss: 0.285633, acc.: 90.62%] [G loss: 6.965230]\n",
            "23000 [D loss: 0.360188, acc.: 86.33%] [G loss: 6.188745]\n",
            "23020 [D loss: 0.287662, acc.: 89.45%] [G loss: 6.424937]\n",
            "23040 [D loss: 0.312942, acc.: 87.50%] [G loss: 6.511503]\n",
            "23060 [D loss: 0.309661, acc.: 86.33%] [G loss: 6.641928]\n",
            "23080 [D loss: 0.376813, acc.: 84.38%] [G loss: 6.359273]\n",
            "23100 [D loss: 0.400345, acc.: 83.20%] [G loss: 6.399978]\n",
            "23120 [D loss: 0.314309, acc.: 84.38%] [G loss: 6.045286]\n",
            "23140 [D loss: 0.304810, acc.: 87.11%] [G loss: 6.173864]\n",
            "23160 [D loss: 0.283067, acc.: 88.28%] [G loss: 5.613777]\n",
            "23180 [D loss: 0.334934, acc.: 86.72%] [G loss: 5.952944]\n",
            "23200 [D loss: 0.320662, acc.: 87.11%] [G loss: 7.740681]\n",
            "23220 [D loss: 0.380616, acc.: 82.81%] [G loss: 6.323351]\n",
            "23240 [D loss: 0.347667, acc.: 85.94%] [G loss: 6.955351]\n",
            "23260 [D loss: 0.424921, acc.: 79.30%] [G loss: 6.906896]\n",
            "23280 [D loss: 0.326840, acc.: 85.16%] [G loss: 6.878266]\n",
            "23300 [D loss: 0.327547, acc.: 85.94%] [G loss: 6.364478]\n",
            "23320 [D loss: 0.430992, acc.: 79.30%] [G loss: 6.160678]\n",
            "23340 [D loss: 0.283794, acc.: 88.28%] [G loss: 6.583346]\n",
            "23360 [D loss: 0.353044, acc.: 84.77%] [G loss: 6.539110]\n",
            "23380 [D loss: 0.261021, acc.: 88.67%] [G loss: 7.055329]\n",
            "23400 [D loss: 0.319767, acc.: 87.50%] [G loss: 6.697513]\n",
            "23420 [D loss: 0.279705, acc.: 88.28%] [G loss: 6.994331]\n",
            "23440 [D loss: 0.299187, acc.: 87.50%] [G loss: 6.881523]\n",
            "23460 [D loss: 0.273707, acc.: 91.02%] [G loss: 6.956607]\n",
            "23480 [D loss: 0.345242, acc.: 85.94%] [G loss: 7.505045]\n",
            "23500 [D loss: 0.332751, acc.: 85.94%] [G loss: 6.911546]\n",
            "23520 [D loss: 0.252254, acc.: 90.23%] [G loss: 6.308644]\n",
            "23540 [D loss: 0.310160, acc.: 86.33%] [G loss: 6.502654]\n",
            "23560 [D loss: 0.215747, acc.: 91.41%] [G loss: 7.510768]\n",
            "23580 [D loss: 0.324824, acc.: 85.94%] [G loss: 6.726199]\n",
            "23600 [D loss: 0.221422, acc.: 91.41%] [G loss: 6.708896]\n",
            "23620 [D loss: 0.297170, acc.: 87.89%] [G loss: 7.652267]\n",
            "23640 [D loss: 0.301673, acc.: 88.28%] [G loss: 6.593806]\n",
            "23660 [D loss: 0.338032, acc.: 85.55%] [G loss: 6.900987]\n",
            "23680 [D loss: 0.305477, acc.: 87.50%] [G loss: 6.401500]\n",
            "23700 [D loss: 0.299607, acc.: 89.06%] [G loss: 7.198995]\n",
            "23720 [D loss: 0.410466, acc.: 83.20%] [G loss: 6.652088]\n",
            "23740 [D loss: 0.307921, acc.: 86.72%] [G loss: 6.846978]\n",
            "23760 [D loss: 0.274293, acc.: 88.28%] [G loss: 7.476583]\n",
            "23780 [D loss: 0.259918, acc.: 89.06%] [G loss: 7.449824]\n",
            "23800 [D loss: 0.306601, acc.: 85.94%] [G loss: 7.077966]\n",
            "23820 [D loss: 0.335468, acc.: 85.55%] [G loss: 7.186550]\n",
            "23840 [D loss: 0.283155, acc.: 87.11%] [G loss: 7.192657]\n",
            "23860 [D loss: 0.350831, acc.: 84.38%] [G loss: 6.569676]\n",
            "23880 [D loss: 0.291696, acc.: 89.45%] [G loss: 6.838398]\n",
            "23900 [D loss: 0.267496, acc.: 88.67%] [G loss: 6.935179]\n",
            "23920 [D loss: 0.267553, acc.: 88.28%] [G loss: 7.019211]\n",
            "23940 [D loss: 0.280393, acc.: 87.50%] [G loss: 7.713386]\n",
            "23960 [D loss: 0.283626, acc.: 88.28%] [G loss: 6.783951]\n",
            "23980 [D loss: 0.312198, acc.: 85.94%] [G loss: 6.929721]\n",
            "24000 [D loss: 0.241117, acc.: 91.02%] [G loss: 7.522221]\n",
            "24020 [D loss: 0.361468, acc.: 82.81%] [G loss: 6.536613]\n",
            "24040 [D loss: 0.271112, acc.: 89.06%] [G loss: 8.168806]\n",
            "24060 [D loss: 0.230844, acc.: 91.02%] [G loss: 6.917082]\n",
            "24080 [D loss: 0.363880, acc.: 82.81%] [G loss: 7.761399]\n",
            "24100 [D loss: 0.306937, acc.: 85.55%] [G loss: 6.652821]\n",
            "24120 [D loss: 0.301153, acc.: 86.33%] [G loss: 6.493802]\n",
            "24140 [D loss: 0.330884, acc.: 85.55%] [G loss: 7.338974]\n",
            "24160 [D loss: 0.350818, acc.: 84.38%] [G loss: 6.531078]\n",
            "24180 [D loss: 0.338297, acc.: 84.77%] [G loss: 7.079639]\n",
            "24200 [D loss: 0.296398, acc.: 84.77%] [G loss: 6.833627]\n",
            "24220 [D loss: 0.312302, acc.: 87.11%] [G loss: 6.500526]\n",
            "24240 [D loss: 0.218735, acc.: 91.41%] [G loss: 7.136928]\n",
            "24260 [D loss: 0.282429, acc.: 88.28%] [G loss: 7.403086]\n",
            "24280 [D loss: 0.323861, acc.: 87.50%] [G loss: 7.438334]\n",
            "24300 [D loss: 0.242791, acc.: 89.45%] [G loss: 7.234680]\n",
            "24320 [D loss: 0.201136, acc.: 94.14%] [G loss: 7.583284]\n",
            "24340 [D loss: 0.284120, acc.: 89.06%] [G loss: 7.675539]\n",
            "24360 [D loss: 0.288599, acc.: 90.23%] [G loss: 7.023813]\n",
            "24380 [D loss: 0.286808, acc.: 88.67%] [G loss: 8.265043]\n",
            "24400 [D loss: 0.275857, acc.: 89.06%] [G loss: 7.617672]\n",
            "24420 [D loss: 0.318222, acc.: 87.50%] [G loss: 6.261913]\n",
            "24440 [D loss: 0.276439, acc.: 87.11%] [G loss: 7.078958]\n",
            "24460 [D loss: 0.274860, acc.: 91.41%] [G loss: 6.454185]\n",
            "24480 [D loss: 0.293661, acc.: 88.28%] [G loss: 6.740596]\n",
            "24500 [D loss: 0.250486, acc.: 89.45%] [G loss: 6.919937]\n",
            "24520 [D loss: 0.209579, acc.: 93.75%] [G loss: 5.789324]\n",
            "24540 [D loss: 0.190770, acc.: 92.97%] [G loss: 3.651261]\n",
            "24560 [D loss: 88.647348, acc.: 0.78%] [G loss: 0.747278]\n",
            "24580 [D loss: 1.632235, acc.: 60.16%] [G loss: 12.488429]\n",
            "24600 [D loss: 0.535538, acc.: 78.52%] [G loss: 6.430773]\n",
            "24620 [D loss: 1.249864, acc.: 58.98%] [G loss: 4.602401]\n",
            "24640 [D loss: 0.282385, acc.: 89.84%] [G loss: 7.030680]\n",
            "24660 [D loss: 0.368926, acc.: 82.81%] [G loss: 6.302303]\n",
            "24680 [D loss: 0.388890, acc.: 80.47%] [G loss: 5.747590]\n",
            "24700 [D loss: 0.397819, acc.: 83.20%] [G loss: 5.846227]\n",
            "24720 [D loss: 0.310991, acc.: 86.72%] [G loss: 6.425263]\n",
            "24740 [D loss: 0.396015, acc.: 81.25%] [G loss: 5.463812]\n",
            "24760 [D loss: 0.242508, acc.: 93.36%] [G loss: 6.074323]\n",
            "24780 [D loss: 0.419814, acc.: 83.20%] [G loss: 6.013332]\n",
            "24800 [D loss: 0.348892, acc.: 85.16%] [G loss: 5.624073]\n",
            "24820 [D loss: 0.343092, acc.: 84.77%] [G loss: 6.661543]\n",
            "24840 [D loss: 0.308100, acc.: 85.94%] [G loss: 5.819884]\n",
            "24860 [D loss: 0.360158, acc.: 85.16%] [G loss: 5.679889]\n",
            "24880 [D loss: 0.332045, acc.: 87.50%] [G loss: 6.419391]\n",
            "24900 [D loss: 0.311891, acc.: 88.28%] [G loss: 6.285390]\n",
            "24920 [D loss: 0.375970, acc.: 84.77%] [G loss: 5.365633]\n",
            "24940 [D loss: 0.243281, acc.: 91.02%] [G loss: 6.790557]\n",
            "24960 [D loss: 0.277040, acc.: 89.45%] [G loss: 6.183630]\n",
            "24980 [D loss: 0.344587, acc.: 85.94%] [G loss: 6.066098]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}