{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "6_Bidirectional_Conditional_GAN_6emo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code from https://github.com/eriklindernoren/Keras-GAN/blob/master/cgan/cgan.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yxsXLHc1VFB3"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose, Concatenate\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "84df3fc3-9d78-409a-dd27-3d65160f7822"
      },
      "source": [
        "data = pd.read_csv('fer2013.csv')\n",
        "data = data[data.emotion != 1]\n",
        "data.head()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUNFMUH1kS_X",
        "outputId": "d4e77c7e-6282-46bb-ba4b-5d3fc0d82096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.Usage.value_counts()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Training       28273\n",
              "PrivateTest     3534\n",
              "PublicTest      3533\n",
              "Name: Usage, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHR9swxs5P0w"
      },
      "source": [
        "data['emotion'] = data.emotion.replace(6, 1)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQj2Szg75fm5",
        "outputId": "b6b62793-6fb7-442b-a270-4ab263e5494d"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8yxZyZWONmc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIQlzwYU1ZIU"
      },
      "source": [
        "dic = {0:'Angry', 1:'Neutral', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise'}"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "854d591b-52d8-4422-b960-6fa5e6cb673b"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "y_train = y.to_numpy().reshape(-1, 1)\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## BiCoGAN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMO6GkM-yNCl"
      },
      "source": [
        "class BiCoGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 6\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        print(self.discriminator.summary())\n",
        "        plot_model(self.discriminator, show_shapes=True)\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Build the encoder\n",
        "        self.encoder = self.build_encoder()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        label = Input(shape=(1,))\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Generate image from sampled noise\n",
        "        z = Input(shape=(self.latent_dim, ))\n",
        "        img_ = self.generator([z, label])\n",
        "\n",
        "        # Encode image\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z_ = self.encoder(img)\n",
        "\n",
        "        # Latent -> img is fake, and img -> latent is valid\n",
        "        fake = self.discriminator([z, img_, label])\n",
        "        valid = self.discriminator([z_, img, label])\n",
        "\n",
        "        # Set up and compile the combined model\n",
        "        # Trains generator to fool the discriminator\n",
        "        self.bicogan_generator = Model([z, img, label], [fake, valid])\n",
        "        self.bicogan_generator.compile(loss=['binary_crossentropy', 'binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_encoder(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "\n",
        "        model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Conv2D(512, (5,5), strides=(2,2), padding='same'))\n",
        "        model.add(BatchNormalization(momentum=0.9))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', input_shape=self.img_shape))\n",
        "        # model.add(Conv2D(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(256, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Conv2D(512, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(BatchNormalization(momentum=0.9))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        # model.add(Flatten(input_shape=self.img_shape))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(self.latent_dim))\n",
        "\n",
        "        print('encoder')\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        z = model(img)\n",
        "\n",
        "        return Model(img, z)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "        # foundation for 12x12 image\n",
        "        n_nodes = 128 * 12 * 12\n",
        "        model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        # upsample to 24x24\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # upsample to 48x48\n",
        "        model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        # generate\n",
        "        model.add(Conv2D(1, (12, 12), activation='tanh', padding='same'))\n",
        "\n",
        "        # model = Sequential()\n",
        "        # # foundation for 3x3 feature maps\n",
        "        # n_nodes = 128 * 3 * 3\n",
        "        # model.add(Dense(n_nodes, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(Reshape((3, 3, 128)))\n",
        "        # # upsample to 6x6\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 12x12\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 24x24\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # upsample to 48x48\n",
        "        # model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # # output layer 48x48x1\n",
        "        # model.add(Conv2D(1, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "        # model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(512))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(1024))\n",
        "        # model.add(LeakyReLU(alpha=0.2))\n",
        "        # model.add(BatchNormalization(momentum=0.8))\n",
        "        # model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
        "        # model.add(Reshape(self.img_shape))\n",
        "\n",
        "        print('generator')\n",
        "        model.summary()\n",
        "\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([z, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([z, label], img)\n",
        "\n",
        "    # def build_discriminator(self):\n",
        "\n",
        "    #     z = Input(shape=(self.latent_dim, ))\n",
        "    #     img = Input(shape=self.img_shape)\n",
        "    #     label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    #     label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "    #     flat_img = Flatten()(img)\n",
        "\n",
        "    #     d_in = concatenate([z, flat_img, label_embedding])\n",
        "\n",
        "    #     model = Dense(1024)(d_in)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     model = Dense(1024)(model)\n",
        "    #     model = LeakyReLU(alpha=0.2)(model)\n",
        "    #     model = Dropout(0.5)(model)\n",
        "    #     validity = Dense(1, activation=\"sigmoid\")(model)\n",
        "\n",
        "    #     return Model([z, img, label], validity, name='discriminator')\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        xi = Input(self.img_shape)\n",
        "        zi = Input(self.latent_dim)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "\n",
        "        # xn = Conv2D(df_dim, (5, 5), (2, 2), act=lrelu, W_init=w_init)(xi)\n",
        "        # xn = Conv2D(df_dim * 2, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 4, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Conv2d(df_dim * 8, (5, 5), (2, 2), W_init=w_init, b_init=None)(xn)\n",
        "        # xn = BatchNorm2d(decay=0.9, act=lrelu, gamma_init=g_init)(xn)\n",
        "        # xn = Dropout(keep=0.8)(xn)\n",
        "        # xn = Flatten()(xn)\n",
        "\n",
        "        xn = Conv2D(128, (5,5), padding='same')(xi)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 24x24\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 12x12\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 6x6\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # downsample to 3x3\n",
        "        xn = Conv2D(128, (5,5), strides=(2,2), padding='same')(xn)\n",
        "        xn = LeakyReLU(alpha=0.2)(xn)\n",
        "        # classifier\n",
        "        xn = Flatten()(xn)\n",
        "\n",
        "        zn = Flatten()(zi)\n",
        "        # zn = Dense(512, activation='relu')(zn)\n",
        "        # zn = Dropout(0.2)(zn)\n",
        "\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "\n",
        "        nn = concatenate([zn, xn, label_embedding])\n",
        "        nn = Dense(1, activation='sigmoid')(nn)\n",
        "\n",
        "        return Model([zi, xi, label], nn, name='discriminator')\n",
        "\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "        \n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images and encode\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "            z_ = self.encoder.predict(imgs)\n",
        "\n",
        "            # Sample noise and generate img\n",
        "            z = np.random.normal(0, 1, (batch_size, 100))\n",
        "            imgs_ = self.generator.predict([z, labels])\n",
        "\n",
        "            # Train the discriminator (img -> z is valid, z -> img is fake)\n",
        "            d_loss_real = self.discriminator.train_on_batch([z_, imgs, labels], valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([z, imgs_, labels], fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Condition on labels\n",
        "            sampled_labels = np.random.randint(0, 6, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.bicogan_generator.train_on_batch([z, imgs, sampled_labels], [valid, fake])\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "              print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "          r, c = 1, 6\n",
        "          noise = np.random.normal(0, 1, (r * c, 100))\n",
        "          sampled_labels = np.arange(0, 6).reshape(-1, 1)\n",
        "\n",
        "          gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "          # Rescale images 0 - 1\n",
        "          gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "          fig, axs = plt.subplots(r, c)\n",
        "          cnt = 0\n",
        "          for j in range(c):\n",
        "              axs[j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "              axs[j].set_title(\"%s\" % dic[sampled_labels[cnt][0]])\n",
        "              axs[j].axis('off')\n",
        "              cnt += 1\n",
        "          fig.savefig(r\"C:\\Users\\shir2\\Desktop\\Shir\\MSc\\Deep_generative_models\\project\\celeba\\%d.png\" % epoch)\n",
        "          plt.close()\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sONoLUCKEOZR",
        "outputId": "fde47d06-a45d-4cbf-f328-270203f1602d"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    bicogan = BiCoGAN()\n",
        "    bicogan.train(epochs=30000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 48, 48, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 48, 48, 128)  3328        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 48, 48, 128)  0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 24, 24, 128)  409728      leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 24, 24, 128)  0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 12, 12, 128)  409728      leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 12, 12, 128)  0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 6, 6, 128)    409728      leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 6, 6, 128)    0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 3, 3, 128)    409728      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 3, 3, 128)    0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 1, 2304)      13824       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 100)          0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1152)         0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 2304)         0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 3556)         0           flatten_1[0][0]                  \n",
            "                                                                 flatten[0][0]                    \n",
            "                                                                 flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            3557        concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 1,659,621\n",
            "Trainable params: 1,659,621\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "generator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 48, 48, 128)       262272    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 48, 48, 1)         18433     \n",
            "=================================================================\n",
            "Total params: 2,404,609\n",
            "Trainable params: 2,404,609\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "encoder\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 12, 12, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 6, 6, 256)         819456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 6, 6, 256)         1024      \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 3, 3, 512)         3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 3, 3, 512)         2048      \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               460900    \n",
            "=================================================================\n",
            "Total params: 4,767,844\n",
            "Trainable params: 4,766,052\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n",
            "0 [D loss: 0.687494, acc.: 71.09%] [G loss: 1.558148]\n",
            "20 [D loss: 1.295044, acc.: 14.06%] [G loss: 0.696756]\n",
            "40 [D loss: 2.092596, acc.: 23.83%] [G loss: 6.322145]\n",
            "60 [D loss: 0.710519, acc.: 50.39%] [G loss: 2.944993]\n",
            "80 [D loss: 0.658998, acc.: 58.59%] [G loss: 2.069210]\n",
            "100 [D loss: 0.391465, acc.: 97.66%] [G loss: 2.752330]\n",
            "120 [D loss: 0.597265, acc.: 83.98%] [G loss: 1.961592]\n",
            "140 [D loss: 0.519691, acc.: 76.17%] [G loss: 2.700482]\n",
            "160 [D loss: 0.207065, acc.: 99.61%] [G loss: 8.034479]\n",
            "180 [D loss: 0.667495, acc.: 55.08%] [G loss: 4.178913]\n",
            "200 [D loss: 0.552622, acc.: 86.33%] [G loss: 1.865362]\n",
            "220 [D loss: 0.642145, acc.: 66.80%] [G loss: 3.649645]\n",
            "240 [D loss: 0.513605, acc.: 92.58%] [G loss: 2.071081]\n",
            "260 [D loss: 0.518981, acc.: 81.25%] [G loss: 2.904956]\n",
            "280 [D loss: 1.686074, acc.: 44.53%] [G loss: 1.812582]\n",
            "300 [D loss: 0.892623, acc.: 33.20%] [G loss: 2.036943]\n",
            "320 [D loss: 1.146830, acc.: 48.44%] [G loss: 4.264205]\n",
            "340 [D loss: 0.421006, acc.: 85.94%] [G loss: 5.348233]\n",
            "360 [D loss: 0.671973, acc.: 60.16%] [G loss: 2.188458]\n",
            "380 [D loss: 0.330689, acc.: 92.58%] [G loss: 5.536311]\n",
            "400 [D loss: 0.594247, acc.: 69.14%] [G loss: 2.490402]\n",
            "420 [D loss: 0.434950, acc.: 84.38%] [G loss: 3.101954]\n",
            "440 [D loss: 0.716030, acc.: 54.69%] [G loss: 2.447833]\n",
            "460 [D loss: 0.466024, acc.: 79.30%] [G loss: 3.750201]\n",
            "480 [D loss: 0.365654, acc.: 89.45%] [G loss: 3.163291]\n",
            "500 [D loss: 0.543144, acc.: 70.70%] [G loss: 3.222304]\n",
            "520 [D loss: 0.407666, acc.: 82.42%] [G loss: 3.539965]\n",
            "540 [D loss: 0.681986, acc.: 57.42%] [G loss: 2.560193]\n",
            "560 [D loss: 0.536394, acc.: 76.95%] [G loss: 2.720178]\n",
            "580 [D loss: 0.579807, acc.: 69.92%] [G loss: 2.503389]\n",
            "600 [D loss: 0.511917, acc.: 79.69%] [G loss: 4.192954]\n",
            "620 [D loss: 0.670049, acc.: 61.72%] [G loss: 3.459910]\n",
            "640 [D loss: 0.486246, acc.: 80.08%] [G loss: 3.054320]\n",
            "660 [D loss: 0.614606, acc.: 67.19%] [G loss: 2.189904]\n",
            "680 [D loss: 0.563550, acc.: 71.48%] [G loss: 2.112354]\n",
            "700 [D loss: 0.582194, acc.: 69.14%] [G loss: 2.335704]\n",
            "720 [D loss: 0.561134, acc.: 70.70%] [G loss: 2.801847]\n",
            "740 [D loss: 0.681514, acc.: 62.50%] [G loss: 2.069672]\n",
            "760 [D loss: 0.682847, acc.: 65.62%] [G loss: 2.933937]\n",
            "780 [D loss: 0.702869, acc.: 56.64%] [G loss: 2.553853]\n",
            "800 [D loss: 0.497582, acc.: 76.95%] [G loss: 3.486964]\n",
            "820 [D loss: 0.610504, acc.: 68.75%] [G loss: 2.525028]\n",
            "840 [D loss: 0.440759, acc.: 83.20%] [G loss: 2.921318]\n",
            "860 [D loss: 0.578869, acc.: 71.88%] [G loss: 2.712631]\n",
            "880 [D loss: 0.515132, acc.: 73.83%] [G loss: 3.524782]\n",
            "900 [D loss: 0.571696, acc.: 73.83%] [G loss: 2.721663]\n",
            "920 [D loss: 0.603227, acc.: 68.36%] [G loss: 2.767889]\n",
            "940 [D loss: 0.451312, acc.: 82.81%] [G loss: 2.991399]\n",
            "960 [D loss: 0.501200, acc.: 80.47%] [G loss: 3.078148]\n",
            "980 [D loss: 0.640282, acc.: 64.06%] [G loss: 2.197612]\n",
            "1000 [D loss: 0.587545, acc.: 72.27%] [G loss: 2.886030]\n",
            "1020 [D loss: 0.467794, acc.: 79.30%] [G loss: 3.534456]\n",
            "1040 [D loss: 0.570899, acc.: 71.48%] [G loss: 2.954019]\n",
            "1060 [D loss: 0.601167, acc.: 65.23%] [G loss: 2.908427]\n",
            "1080 [D loss: 0.727294, acc.: 55.47%] [G loss: 2.474898]\n",
            "1100 [D loss: 0.492855, acc.: 78.12%] [G loss: 3.640531]\n",
            "1120 [D loss: 0.473104, acc.: 78.91%] [G loss: 3.527093]\n",
            "1140 [D loss: 0.593584, acc.: 63.28%] [G loss: 3.418146]\n",
            "1160 [D loss: 0.514006, acc.: 72.66%] [G loss: 3.287061]\n",
            "1180 [D loss: 0.409781, acc.: 84.38%] [G loss: 3.500905]\n",
            "1200 [D loss: 0.546634, acc.: 71.09%] [G loss: 3.059259]\n",
            "1220 [D loss: 0.462600, acc.: 79.69%] [G loss: 2.723542]\n",
            "1240 [D loss: 0.754463, acc.: 56.64%] [G loss: 2.610234]\n",
            "1260 [D loss: 0.427267, acc.: 82.42%] [G loss: 3.614255]\n",
            "1280 [D loss: 0.561852, acc.: 71.88%] [G loss: 2.753147]\n",
            "1300 [D loss: 0.484582, acc.: 79.69%] [G loss: 3.215932]\n",
            "1320 [D loss: 0.562833, acc.: 72.27%] [G loss: 3.071032]\n",
            "1340 [D loss: 0.569911, acc.: 71.88%] [G loss: 2.287495]\n",
            "1360 [D loss: 0.469422, acc.: 79.30%] [G loss: 3.495542]\n",
            "1380 [D loss: 0.571748, acc.: 71.88%] [G loss: 3.031499]\n",
            "1400 [D loss: 0.496907, acc.: 77.34%] [G loss: 3.055913]\n",
            "1420 [D loss: 0.532383, acc.: 73.05%] [G loss: 3.151849]\n",
            "1440 [D loss: 0.506859, acc.: 77.73%] [G loss: 3.322350]\n",
            "1460 [D loss: 0.595075, acc.: 67.19%] [G loss: 2.835431]\n",
            "1480 [D loss: 0.441778, acc.: 81.64%] [G loss: 3.557164]\n",
            "1500 [D loss: 0.501451, acc.: 75.00%] [G loss: 3.319012]\n",
            "1520 [D loss: 0.482874, acc.: 80.86%] [G loss: 3.551007]\n",
            "1540 [D loss: 0.584849, acc.: 67.97%] [G loss: 2.919322]\n",
            "1560 [D loss: 0.584678, acc.: 69.14%] [G loss: 2.853774]\n",
            "1580 [D loss: 0.408768, acc.: 83.98%] [G loss: 3.276071]\n",
            "1600 [D loss: 0.572316, acc.: 71.09%] [G loss: 2.780663]\n",
            "1620 [D loss: 0.506562, acc.: 75.39%] [G loss: 3.268308]\n",
            "1640 [D loss: 0.540685, acc.: 76.95%] [G loss: 2.855444]\n",
            "1660 [D loss: 0.628828, acc.: 64.84%] [G loss: 3.079456]\n",
            "1680 [D loss: 0.509575, acc.: 75.00%] [G loss: 2.961284]\n",
            "1700 [D loss: 0.504112, acc.: 77.34%] [G loss: 3.228127]\n",
            "1720 [D loss: 0.461373, acc.: 78.52%] [G loss: 3.165702]\n",
            "1740 [D loss: 0.494225, acc.: 73.44%] [G loss: 3.269253]\n",
            "1760 [D loss: 0.514358, acc.: 72.27%] [G loss: 3.127906]\n",
            "1780 [D loss: 0.485064, acc.: 77.34%] [G loss: 3.032447]\n",
            "1800 [D loss: 0.551085, acc.: 72.66%] [G loss: 2.920902]\n",
            "1820 [D loss: 0.467607, acc.: 81.25%] [G loss: 3.177521]\n",
            "1840 [D loss: 0.549025, acc.: 71.48%] [G loss: 2.879326]\n",
            "1860 [D loss: 0.530385, acc.: 74.22%] [G loss: 3.013858]\n",
            "1880 [D loss: 0.470998, acc.: 78.12%] [G loss: 3.093940]\n",
            "1900 [D loss: 0.475485, acc.: 78.52%] [G loss: 3.084584]\n",
            "1920 [D loss: 0.475430, acc.: 77.34%] [G loss: 3.114675]\n",
            "1940 [D loss: 0.567874, acc.: 72.66%] [G loss: 2.954443]\n",
            "1960 [D loss: 0.489358, acc.: 78.52%] [G loss: 3.118089]\n",
            "1980 [D loss: 0.484217, acc.: 77.73%] [G loss: 3.100550]\n",
            "2000 [D loss: 0.606862, acc.: 66.41%] [G loss: 2.740879]\n",
            "2020 [D loss: 0.463354, acc.: 79.69%] [G loss: 3.131449]\n",
            "2040 [D loss: 0.507316, acc.: 76.17%] [G loss: 2.878267]\n",
            "2060 [D loss: 0.475611, acc.: 78.52%] [G loss: 3.323338]\n",
            "2080 [D loss: 0.510136, acc.: 78.91%] [G loss: 3.065202]\n",
            "2100 [D loss: 0.482915, acc.: 76.17%] [G loss: 3.218749]\n",
            "2120 [D loss: 0.472203, acc.: 75.00%] [G loss: 3.275564]\n",
            "2140 [D loss: 0.495681, acc.: 77.73%] [G loss: 3.308692]\n",
            "2160 [D loss: 0.495357, acc.: 76.56%] [G loss: 3.179568]\n",
            "2180 [D loss: 0.483133, acc.: 79.69%] [G loss: 3.085322]\n",
            "2200 [D loss: 0.423440, acc.: 80.86%] [G loss: 3.506827]\n",
            "2220 [D loss: 0.494426, acc.: 77.73%] [G loss: 2.964869]\n",
            "2240 [D loss: 0.565738, acc.: 68.36%] [G loss: 3.182812]\n",
            "2260 [D loss: 0.520954, acc.: 76.17%] [G loss: 3.237519]\n",
            "2280 [D loss: 0.828581, acc.: 50.39%] [G loss: 2.862450]\n",
            "2300 [D loss: 0.793317, acc.: 34.77%] [G loss: 1.615490]\n",
            "2320 [D loss: 0.815089, acc.: 23.05%] [G loss: 1.365708]\n",
            "2340 [D loss: 0.737812, acc.: 46.48%] [G loss: 1.482972]\n",
            "2360 [D loss: 0.716178, acc.: 48.44%] [G loss: 1.479975]\n",
            "2380 [D loss: 0.731852, acc.: 43.75%] [G loss: 1.391917]\n",
            "2400 [D loss: 0.696745, acc.: 50.78%] [G loss: 1.515913]\n",
            "2420 [D loss: 0.694638, acc.: 48.83%] [G loss: 1.520447]\n",
            "2440 [D loss: 0.672258, acc.: 59.38%] [G loss: 1.627936]\n",
            "2460 [D loss: 0.683304, acc.: 53.12%] [G loss: 1.569602]\n",
            "2480 [D loss: 0.682887, acc.: 53.52%] [G loss: 1.606851]\n",
            "2500 [D loss: 0.636793, acc.: 64.84%] [G loss: 1.784391]\n",
            "2520 [D loss: 0.622482, acc.: 66.80%] [G loss: 2.173142]\n",
            "2540 [D loss: 0.654129, acc.: 62.11%] [G loss: 1.849116]\n",
            "2560 [D loss: 0.616803, acc.: 66.80%] [G loss: 2.064308]\n",
            "2580 [D loss: 0.608923, acc.: 68.75%] [G loss: 2.173547]\n",
            "2600 [D loss: 0.563448, acc.: 67.58%] [G loss: 2.358775]\n",
            "2620 [D loss: 0.578536, acc.: 69.53%] [G loss: 1.991301]\n",
            "2640 [D loss: 0.637841, acc.: 63.28%] [G loss: 2.214287]\n",
            "2660 [D loss: 0.532776, acc.: 71.48%] [G loss: 2.957585]\n",
            "2680 [D loss: 0.560863, acc.: 71.48%] [G loss: 2.576446]\n",
            "2700 [D loss: 0.507514, acc.: 76.17%] [G loss: 2.499295]\n",
            "2720 [D loss: 0.661012, acc.: 64.84%] [G loss: 3.150579]\n",
            "2740 [D loss: 0.603097, acc.: 69.92%] [G loss: 2.914850]\n",
            "2760 [D loss: 0.524458, acc.: 76.56%] [G loss: 3.100659]\n",
            "2780 [D loss: 0.862684, acc.: 51.17%] [G loss: 2.121030]\n",
            "2800 [D loss: 0.667562, acc.: 63.67%] [G loss: 3.083852]\n",
            "2820 [D loss: 0.848980, acc.: 51.95%] [G loss: 2.667432]\n",
            "2840 [D loss: 0.610123, acc.: 69.92%] [G loss: 2.745045]\n",
            "2860 [D loss: 0.610992, acc.: 65.23%] [G loss: 2.204350]\n",
            "2880 [D loss: 0.641135, acc.: 66.80%] [G loss: 3.037164]\n",
            "2900 [D loss: 0.640079, acc.: 68.36%] [G loss: 2.972522]\n",
            "2920 [D loss: 0.646356, acc.: 65.23%] [G loss: 2.684314]\n",
            "2940 [D loss: 0.580155, acc.: 70.31%] [G loss: 2.759623]\n",
            "2960 [D loss: 0.656980, acc.: 60.94%] [G loss: 2.495247]\n",
            "2980 [D loss: 0.545682, acc.: 73.44%] [G loss: 2.810792]\n",
            "3000 [D loss: 0.578931, acc.: 70.31%] [G loss: 2.649265]\n",
            "3020 [D loss: 0.516003, acc.: 73.83%] [G loss: 2.705455]\n",
            "3040 [D loss: 0.557036, acc.: 71.09%] [G loss: 2.716216]\n",
            "3060 [D loss: 0.514216, acc.: 79.30%] [G loss: 2.779551]\n",
            "3080 [D loss: 0.553092, acc.: 71.88%] [G loss: 2.667825]\n",
            "3100 [D loss: 0.514365, acc.: 75.00%] [G loss: 2.820505]\n",
            "3120 [D loss: 0.595000, acc.: 65.23%] [G loss: 2.914939]\n",
            "3140 [D loss: 0.475927, acc.: 78.52%] [G loss: 2.810915]\n",
            "3160 [D loss: 0.584894, acc.: 71.09%] [G loss: 2.530612]\n",
            "3180 [D loss: 0.508596, acc.: 76.95%] [G loss: 2.801579]\n",
            "3200 [D loss: 0.556528, acc.: 71.09%] [G loss: 2.837079]\n",
            "3220 [D loss: 0.591804, acc.: 71.88%] [G loss: 2.554048]\n",
            "3240 [D loss: 0.502664, acc.: 75.39%] [G loss: 2.727341]\n",
            "3260 [D loss: 0.537402, acc.: 74.61%] [G loss: 2.682852]\n",
            "3280 [D loss: 0.531443, acc.: 73.05%] [G loss: 2.625868]\n",
            "3300 [D loss: 0.523184, acc.: 74.22%] [G loss: 2.639801]\n",
            "3320 [D loss: 0.562070, acc.: 73.83%] [G loss: 2.605063]\n",
            "3340 [D loss: 0.589408, acc.: 69.92%] [G loss: 2.500610]\n",
            "3360 [D loss: 0.577611, acc.: 67.97%] [G loss: 2.538182]\n",
            "3380 [D loss: 0.494546, acc.: 75.39%] [G loss: 2.856313]\n",
            "3400 [D loss: 0.629247, acc.: 64.84%] [G loss: 2.347204]\n",
            "3420 [D loss: 0.532212, acc.: 75.00%] [G loss: 2.097294]\n",
            "3440 [D loss: 0.493081, acc.: 79.30%] [G loss: 2.587223]\n",
            "3460 [D loss: 0.611931, acc.: 67.97%] [G loss: 1.943845]\n",
            "3480 [D loss: 0.654386, acc.: 59.77%] [G loss: 3.052368]\n",
            "3500 [D loss: 0.652542, acc.: 61.72%] [G loss: 3.001362]\n",
            "3520 [D loss: 0.833918, acc.: 48.83%] [G loss: 2.677996]\n",
            "3540 [D loss: 0.597347, acc.: 71.09%] [G loss: 2.553591]\n",
            "3560 [D loss: 0.643701, acc.: 65.23%] [G loss: 2.512127]\n",
            "3580 [D loss: 0.661958, acc.: 63.28%] [G loss: 2.511653]\n",
            "3600 [D loss: 0.624727, acc.: 64.45%] [G loss: 2.456322]\n",
            "3620 [D loss: 0.568823, acc.: 72.66%] [G loss: 2.533415]\n",
            "3640 [D loss: 0.585548, acc.: 71.48%] [G loss: 2.626039]\n",
            "3660 [D loss: 0.579443, acc.: 67.97%] [G loss: 2.590189]\n",
            "3680 [D loss: 0.463772, acc.: 78.91%] [G loss: 2.845584]\n",
            "3720 [D loss: 0.556209, acc.: 69.92%] [G loss: 2.777935]\n",
            "3740 [D loss: 0.577100, acc.: 69.14%] [G loss: 2.636055]\n",
            "3760 [D loss: 0.558748, acc.: 73.05%] [G loss: 2.548458]\n",
            "3780 [D loss: 0.543947, acc.: 71.88%] [G loss: 2.637861]\n",
            "3800 [D loss: 0.530059, acc.: 73.83%] [G loss: 2.649631]\n",
            "3820 [D loss: 0.595060, acc.: 67.19%] [G loss: 2.528194]\n",
            "3840 [D loss: 0.513315, acc.: 74.61%] [G loss: 2.768539]\n",
            "3860 [D loss: 0.506360, acc.: 77.73%] [G loss: 2.842722]\n",
            "3880 [D loss: 0.530398, acc.: 74.22%] [G loss: 2.796253]\n",
            "3900 [D loss: 0.585904, acc.: 67.19%] [G loss: 2.392197]\n",
            "3920 [D loss: 0.484446, acc.: 78.91%] [G loss: 3.034143]\n",
            "3940 [D loss: 0.579035, acc.: 71.88%] [G loss: 2.590187]\n",
            "3960 [D loss: 0.603125, acc.: 68.36%] [G loss: 2.524834]\n",
            "3980 [D loss: 0.568301, acc.: 69.92%] [G loss: 2.752493]\n",
            "4000 [D loss: 0.505906, acc.: 76.95%] [G loss: 2.802422]\n",
            "4020 [D loss: 0.668505, acc.: 60.55%] [G loss: 2.540048]\n",
            "4040 [D loss: 0.533556, acc.: 71.48%] [G loss: 2.764368]\n",
            "4060 [D loss: 0.601828, acc.: 67.19%] [G loss: 2.630683]\n",
            "4080 [D loss: 0.645181, acc.: 64.06%] [G loss: 2.336078]\n",
            "4100 [D loss: 0.537589, acc.: 74.61%] [G loss: 2.652138]\n",
            "4120 [D loss: 0.628999, acc.: 64.84%] [G loss: 2.832502]\n",
            "4140 [D loss: 0.491508, acc.: 78.91%] [G loss: 2.862431]\n",
            "4160 [D loss: 0.482397, acc.: 76.95%] [G loss: 3.121657]\n",
            "4180 [D loss: 0.506607, acc.: 78.91%] [G loss: 2.915648]\n",
            "4200 [D loss: 0.577408, acc.: 70.70%] [G loss: 2.773534]\n",
            "4220 [D loss: 0.565635, acc.: 69.14%] [G loss: 2.801492]\n",
            "4240 [D loss: 0.604904, acc.: 62.50%] [G loss: 2.749093]\n",
            "4260 [D loss: 0.546328, acc.: 71.88%] [G loss: 2.736000]\n",
            "4280 [D loss: 0.594572, acc.: 68.36%] [G loss: 2.624554]\n",
            "4300 [D loss: 0.576990, acc.: 72.66%] [G loss: 2.792851]\n",
            "4320 [D loss: 0.611592, acc.: 66.80%] [G loss: 2.552830]\n",
            "4340 [D loss: 0.470601, acc.: 77.73%] [G loss: 2.853712]\n",
            "4360 [D loss: 0.623812, acc.: 65.62%] [G loss: 2.532415]\n",
            "4380 [D loss: 0.567544, acc.: 67.19%] [G loss: 2.544224]\n",
            "4400 [D loss: 0.523140, acc.: 75.39%] [G loss: 2.691802]\n",
            "4420 [D loss: 0.577920, acc.: 69.53%] [G loss: 2.647253]\n",
            "4440 [D loss: 0.489596, acc.: 75.00%] [G loss: 3.127931]\n",
            "4460 [D loss: 0.520161, acc.: 76.56%] [G loss: 2.924119]\n",
            "4480 [D loss: 0.680894, acc.: 58.98%] [G loss: 2.439308]\n",
            "4500 [D loss: 0.477368, acc.: 77.34%] [G loss: 3.096577]\n",
            "4520 [D loss: 0.540128, acc.: 74.61%] [G loss: 2.757771]\n",
            "4540 [D loss: 0.627592, acc.: 67.97%] [G loss: 2.642188]\n",
            "4560 [D loss: 0.563101, acc.: 71.09%] [G loss: 2.815250]\n",
            "4580 [D loss: 0.567055, acc.: 71.88%] [G loss: 2.577672]\n",
            "4600 [D loss: 0.473802, acc.: 79.69%] [G loss: 2.934206]\n",
            "4620 [D loss: 0.187767, acc.: 99.22%] [G loss: 2.340220]\n",
            "4640 [D loss: 3.315117, acc.: 15.23%] [G loss: 2.099011]\n",
            "4660 [D loss: 0.838721, acc.: 32.81%] [G loss: 1.487018]\n",
            "4680 [D loss: 0.902748, acc.: 23.05%] [G loss: 1.336074]\n",
            "4700 [D loss: 0.842266, acc.: 39.06%] [G loss: 1.640543]\n",
            "4720 [D loss: 0.738949, acc.: 53.12%] [G loss: 1.992098]\n",
            "4740 [D loss: 0.640668, acc.: 63.28%] [G loss: 2.070407]\n",
            "4760 [D loss: 0.641809, acc.: 64.45%] [G loss: 1.939859]\n",
            "4780 [D loss: 0.533555, acc.: 71.48%] [G loss: 2.522279]\n",
            "4800 [D loss: 0.630366, acc.: 67.58%] [G loss: 2.158423]\n",
            "4820 [D loss: 0.567448, acc.: 69.53%] [G loss: 2.371633]\n",
            "4840 [D loss: 0.580444, acc.: 69.14%] [G loss: 2.473868]\n",
            "4860 [D loss: 0.560650, acc.: 69.92%] [G loss: 2.403587]\n",
            "4880 [D loss: 0.542502, acc.: 73.05%] [G loss: 2.432054]\n",
            "4900 [D loss: 0.553431, acc.: 73.44%] [G loss: 2.275401]\n",
            "4920 [D loss: 0.603051, acc.: 67.19%] [G loss: 2.330990]\n",
            "4940 [D loss: 0.609314, acc.: 64.45%] [G loss: 2.440381]\n",
            "4960 [D loss: 0.585372, acc.: 68.75%] [G loss: 2.391497]\n",
            "4980 [D loss: 0.585755, acc.: 66.80%] [G loss: 2.339901]\n",
            "5000 [D loss: 0.528938, acc.: 75.39%] [G loss: 2.843513]\n",
            "5020 [D loss: 0.611442, acc.: 66.41%] [G loss: 2.390478]\n",
            "5040 [D loss: 0.565664, acc.: 67.19%] [G loss: 2.543785]\n",
            "5060 [D loss: 0.591386, acc.: 68.36%] [G loss: 2.366595]\n",
            "5080 [D loss: 0.593309, acc.: 69.92%] [G loss: 2.585834]\n",
            "5100 [D loss: 0.576531, acc.: 69.14%] [G loss: 2.500903]\n",
            "5120 [D loss: 0.589960, acc.: 65.62%] [G loss: 2.539204]\n",
            "5140 [D loss: 0.558075, acc.: 72.66%] [G loss: 2.605566]\n",
            "5160 [D loss: 0.633191, acc.: 66.02%] [G loss: 2.445299]\n",
            "5180 [D loss: 0.645137, acc.: 65.23%] [G loss: 2.119826]\n",
            "5200 [D loss: 0.527663, acc.: 75.00%] [G loss: 2.648471]\n",
            "5220 [D loss: 0.537251, acc.: 72.27%] [G loss: 2.571769]\n",
            "5240 [D loss: 0.600736, acc.: 64.84%] [G loss: 2.434381]\n",
            "5260 [D loss: 0.582685, acc.: 69.14%] [G loss: 2.627891]\n",
            "5280 [D loss: 0.561437, acc.: 67.19%] [G loss: 2.644783]\n",
            "5300 [D loss: 0.502056, acc.: 76.17%] [G loss: 2.721086]\n",
            "5320 [D loss: 0.596054, acc.: 71.48%] [G loss: 2.498878]\n",
            "5340 [D loss: 0.565202, acc.: 69.92%] [G loss: 2.568828]\n",
            "5360 [D loss: 0.627987, acc.: 66.41%] [G loss: 2.451079]\n",
            "5380 [D loss: 0.618743, acc.: 67.97%] [G loss: 2.509436]\n",
            "5400 [D loss: 0.553387, acc.: 72.27%] [G loss: 2.567586]\n",
            "5420 [D loss: 0.585875, acc.: 70.31%] [G loss: 2.533548]\n",
            "5440 [D loss: 0.543520, acc.: 70.70%] [G loss: 2.561718]\n",
            "5460 [D loss: 0.550836, acc.: 73.05%] [G loss: 2.620644]\n",
            "5480 [D loss: 0.506291, acc.: 75.39%] [G loss: 2.867857]\n",
            "5500 [D loss: 0.529838, acc.: 75.78%] [G loss: 2.615903]\n",
            "5520 [D loss: 0.561508, acc.: 72.66%] [G loss: 2.684486]\n",
            "5540 [D loss: 0.555130, acc.: 67.97%] [G loss: 2.808205]\n",
            "5560 [D loss: 0.561852, acc.: 70.31%] [G loss: 2.669879]\n",
            "5580 [D loss: 0.561922, acc.: 70.70%] [G loss: 2.705736]\n",
            "5600 [D loss: 0.614251, acc.: 67.97%] [G loss: 2.671430]\n",
            "5620 [D loss: 0.567654, acc.: 71.48%] [G loss: 2.687340]\n",
            "5640 [D loss: 0.571960, acc.: 71.48%] [G loss: 2.847897]\n",
            "5660 [D loss: 0.624851, acc.: 64.84%] [G loss: 2.542903]\n",
            "5680 [D loss: 0.538821, acc.: 74.22%] [G loss: 2.727855]\n",
            "5700 [D loss: 0.494653, acc.: 75.78%] [G loss: 3.039828]\n",
            "5720 [D loss: 0.543529, acc.: 69.53%] [G loss: 2.799998]\n",
            "5740 [D loss: 0.566583, acc.: 71.88%] [G loss: 2.698558]\n",
            "5760 [D loss: 0.547498, acc.: 73.44%] [G loss: 2.907209]\n",
            "5780 [D loss: 0.578729, acc.: 71.48%] [G loss: 2.573768]\n",
            "5800 [D loss: 0.503010, acc.: 77.34%] [G loss: 2.930796]\n",
            "5820 [D loss: 0.577140, acc.: 67.19%] [G loss: 2.747069]\n",
            "5840 [D loss: 0.536775, acc.: 73.44%] [G loss: 2.770498]\n",
            "5860 [D loss: 0.557970, acc.: 71.09%] [G loss: 2.522422]\n",
            "5880 [D loss: 0.516903, acc.: 75.78%] [G loss: 2.809932]\n",
            "5900 [D loss: 0.515631, acc.: 75.39%] [G loss: 2.835264]\n",
            "5920 [D loss: 0.496264, acc.: 75.00%] [G loss: 2.762408]\n",
            "5940 [D loss: 0.560933, acc.: 71.09%] [G loss: 2.900865]\n",
            "5960 [D loss: 0.552750, acc.: 74.61%] [G loss: 2.749638]\n",
            "5980 [D loss: 0.544150, acc.: 71.48%] [G loss: 2.822918]\n",
            "6000 [D loss: 0.538320, acc.: 74.22%] [G loss: 2.837431]\n",
            "6020 [D loss: 0.458740, acc.: 79.30%] [G loss: 3.268468]\n",
            "6040 [D loss: 0.496723, acc.: 75.39%] [G loss: 2.900525]\n",
            "6060 [D loss: 0.560821, acc.: 71.48%] [G loss: 2.888714]\n",
            "6080 [D loss: 0.592443, acc.: 66.02%] [G loss: 2.811832]\n",
            "6100 [D loss: 0.501518, acc.: 79.69%] [G loss: 2.925503]\n",
            "6120 [D loss: 0.567854, acc.: 72.27%] [G loss: 2.817187]\n",
            "6140 [D loss: 0.540655, acc.: 72.66%] [G loss: 2.964719]\n",
            "6160 [D loss: 0.538331, acc.: 72.66%] [G loss: 2.799343]\n",
            "6180 [D loss: 0.503824, acc.: 77.34%] [G loss: 2.898535]\n",
            "6200 [D loss: 0.503523, acc.: 75.78%] [G loss: 2.980661]\n",
            "6220 [D loss: 0.471239, acc.: 77.73%] [G loss: 3.390408]\n",
            "6240 [D loss: 0.546892, acc.: 72.66%] [G loss: 3.093330]\n",
            "6260 [D loss: 0.538466, acc.: 75.78%] [G loss: 2.919411]\n",
            "6280 [D loss: 0.579517, acc.: 67.97%] [G loss: 2.922856]\n",
            "6300 [D loss: 0.523795, acc.: 75.39%] [G loss: 2.925832]\n",
            "6320 [D loss: 0.586678, acc.: 67.97%] [G loss: 2.790796]\n",
            "6340 [D loss: 0.491602, acc.: 78.52%] [G loss: 3.249600]\n",
            "6360 [D loss: 0.498864, acc.: 77.73%] [G loss: 2.969131]\n",
            "6380 [D loss: 0.597208, acc.: 69.53%] [G loss: 2.625291]\n",
            "6400 [D loss: 0.498934, acc.: 77.73%] [G loss: 3.122131]\n",
            "6420 [D loss: 0.513948, acc.: 75.78%] [G loss: 2.911925]\n",
            "6440 [D loss: 0.479162, acc.: 80.08%] [G loss: 3.034605]\n",
            "6460 [D loss: 0.480091, acc.: 77.73%] [G loss: 3.335716]\n",
            "6480 [D loss: 0.543654, acc.: 69.92%] [G loss: 3.102069]\n",
            "6500 [D loss: 0.452109, acc.: 81.25%] [G loss: 3.040870]\n",
            "6520 [D loss: 0.536573, acc.: 74.22%] [G loss: 2.854987]\n",
            "6540 [D loss: 0.520789, acc.: 77.34%] [G loss: 2.952811]\n",
            "6560 [D loss: 0.535089, acc.: 74.22%] [G loss: 2.850359]\n",
            "6580 [D loss: 0.553296, acc.: 71.88%] [G loss: 2.867425]\n",
            "6600 [D loss: 0.465319, acc.: 80.08%] [G loss: 2.885392]\n",
            "6620 [D loss: 0.582609, acc.: 68.36%] [G loss: 3.160816]\n",
            "6640 [D loss: 0.581528, acc.: 67.58%] [G loss: 3.216930]\n",
            "6660 [D loss: 0.393512, acc.: 85.94%] [G loss: 2.789566]\n",
            "6680 [D loss: 0.537891, acc.: 73.44%] [G loss: 2.075709]\n",
            "6700 [D loss: 0.340071, acc.: 89.84%] [G loss: 1.991418]\n",
            "6720 [D loss: 0.608265, acc.: 67.58%] [G loss: 1.750961]\n",
            "6740 [D loss: 0.691524, acc.: 64.45%] [G loss: 4.321738]\n",
            "6760 [D loss: 0.501496, acc.: 78.12%] [G loss: 2.914778]\n",
            "6780 [D loss: 0.560105, acc.: 73.44%] [G loss: 2.540812]\n",
            "6800 [D loss: 0.694874, acc.: 60.55%] [G loss: 2.778667]\n",
            "6820 [D loss: 0.562007, acc.: 69.53%] [G loss: 2.358274]\n",
            "6840 [D loss: 0.990653, acc.: 44.92%] [G loss: 3.334886]\n",
            "6860 [D loss: 0.590462, acc.: 69.92%] [G loss: 3.608821]\n",
            "6880 [D loss: 0.467294, acc.: 78.52%] [G loss: 2.748956]\n",
            "6900 [D loss: 0.553787, acc.: 71.09%] [G loss: 3.332675]\n",
            "6920 [D loss: 0.451058, acc.: 78.91%] [G loss: 3.767791]\n",
            "6940 [D loss: 0.560217, acc.: 71.88%] [G loss: 2.821288]\n",
            "6960 [D loss: 0.545855, acc.: 71.48%] [G loss: 3.196025]\n",
            "6980 [D loss: 0.565628, acc.: 70.70%] [G loss: 3.316813]\n",
            "7000 [D loss: 0.547379, acc.: 73.44%] [G loss: 3.228188]\n",
            "7020 [D loss: 0.560792, acc.: 73.44%] [G loss: 3.071392]\n",
            "7040 [D loss: 0.524643, acc.: 75.00%] [G loss: 3.420484]\n",
            "7060 [D loss: 0.441414, acc.: 80.47%] [G loss: 3.327953]\n",
            "7080 [D loss: 0.570778, acc.: 70.31%] [G loss: 3.198488]\n",
            "7100 [D loss: 0.519284, acc.: 75.00%] [G loss: 3.028598]\n",
            "7120 [D loss: 0.495116, acc.: 76.17%] [G loss: 3.216324]\n",
            "7140 [D loss: 0.612917, acc.: 67.97%] [G loss: 3.003466]\n",
            "7160 [D loss: 0.553407, acc.: 72.66%] [G loss: 2.920545]\n",
            "7180 [D loss: 0.511817, acc.: 73.44%] [G loss: 3.331708]\n",
            "7200 [D loss: 0.502241, acc.: 78.12%] [G loss: 3.284549]\n",
            "7220 [D loss: 0.507571, acc.: 76.95%] [G loss: 3.417496]\n",
            "7240 [D loss: 0.498985, acc.: 75.39%] [G loss: 3.188397]\n",
            "7260 [D loss: 0.502451, acc.: 76.17%] [G loss: 3.256019]\n",
            "7280 [D loss: 0.523557, acc.: 73.83%] [G loss: 3.277529]\n",
            "7300 [D loss: 0.564447, acc.: 68.36%] [G loss: 3.036505]\n",
            "7320 [D loss: 0.513188, acc.: 72.66%] [G loss: 3.258444]\n",
            "7340 [D loss: 0.548420, acc.: 69.53%] [G loss: 3.326864]\n",
            "7360 [D loss: 0.516443, acc.: 74.61%] [G loss: 3.142176]\n",
            "7380 [D loss: 0.530230, acc.: 75.78%] [G loss: 3.192918]\n",
            "7400 [D loss: 0.511569, acc.: 72.66%] [G loss: 3.005387]\n",
            "7420 [D loss: 0.530428, acc.: 76.56%] [G loss: 3.243109]\n",
            "7440 [D loss: 0.571722, acc.: 69.92%] [G loss: 3.053074]\n",
            "7460 [D loss: 0.484155, acc.: 78.12%] [G loss: 3.356368]\n",
            "7480 [D loss: 0.545572, acc.: 75.39%] [G loss: 3.152275]\n",
            "7500 [D loss: 0.514097, acc.: 75.39%] [G loss: 3.218907]\n",
            "7520 [D loss: 0.460666, acc.: 78.12%] [G loss: 3.522406]\n",
            "7540 [D loss: 0.469032, acc.: 79.30%] [G loss: 3.549000]\n",
            "7560 [D loss: 0.488560, acc.: 76.17%] [G loss: 3.014300]\n",
            "7580 [D loss: 0.475736, acc.: 77.73%] [G loss: 3.061800]\n",
            "7600 [D loss: 0.647865, acc.: 66.41%] [G loss: 2.896288]\n",
            "7620 [D loss: 0.521262, acc.: 76.95%] [G loss: 3.250461]\n",
            "7640 [D loss: 0.571794, acc.: 69.14%] [G loss: 3.274288]\n",
            "7660 [D loss: 0.515672, acc.: 76.95%] [G loss: 3.244177]\n",
            "7680 [D loss: 0.502689, acc.: 76.95%] [G loss: 3.794326]\n",
            "7700 [D loss: 0.960841, acc.: 19.92%] [G loss: 1.486053]\n",
            "7740 [D loss: 0.828543, acc.: 30.08%] [G loss: 1.403526]\n",
            "7760 [D loss: 0.737528, acc.: 44.92%] [G loss: 1.564520]\n",
            "7780 [D loss: 0.781507, acc.: 36.33%] [G loss: 1.430133]\n",
            "7800 [D loss: 0.731113, acc.: 48.83%] [G loss: 1.510371]\n",
            "7820 [D loss: 0.715975, acc.: 50.00%] [G loss: 1.534523]\n",
            "7840 [D loss: 0.782202, acc.: 35.16%] [G loss: 1.464743]\n",
            "7860 [D loss: 0.704139, acc.: 53.52%] [G loss: 1.625154]\n",
            "7880 [D loss: 0.740075, acc.: 44.92%] [G loss: 1.621341]\n",
            "7900 [D loss: 0.695751, acc.: 53.12%] [G loss: 1.713742]\n",
            "7920 [D loss: 0.644235, acc.: 64.84%] [G loss: 1.868490]\n",
            "7940 [D loss: 0.685935, acc.: 54.69%] [G loss: 1.884399]\n",
            "7960 [D loss: 0.613369, acc.: 67.58%] [G loss: 2.075676]\n",
            "7980 [D loss: 0.661773, acc.: 62.11%] [G loss: 1.852858]\n",
            "8000 [D loss: 0.635758, acc.: 62.89%] [G loss: 1.936483]\n",
            "8020 [D loss: 0.654647, acc.: 62.89%] [G loss: 1.668345]\n",
            "8040 [D loss: 0.498730, acc.: 76.95%] [G loss: 1.850617]\n",
            "8060 [D loss: 0.749333, acc.: 53.52%] [G loss: 1.477084]\n",
            "8080 [D loss: 0.486427, acc.: 79.69%] [G loss: 1.454531]\n",
            "8100 [D loss: 0.741564, acc.: 56.64%] [G loss: 1.126595]\n",
            "8120 [D loss: 0.942247, acc.: 46.09%] [G loss: 1.171063]\n",
            "8140 [D loss: 0.836911, acc.: 50.78%] [G loss: 1.218729]\n",
            "8160 [D loss: 1.772942, acc.: 11.72%] [G loss: 1.507964]\n",
            "8180 [D loss: 0.748165, acc.: 56.25%] [G loss: 2.862547]\n",
            "8200 [D loss: 0.697031, acc.: 59.77%] [G loss: 2.640199]\n",
            "8220 [D loss: 0.597304, acc.: 64.84%] [G loss: 2.529754]\n",
            "8240 [D loss: 0.660862, acc.: 62.50%] [G loss: 2.186441]\n",
            "8260 [D loss: 0.446118, acc.: 81.64%] [G loss: 1.857585]\n",
            "8280 [D loss: 0.931177, acc.: 39.84%] [G loss: 2.224741]\n",
            "8300 [D loss: 0.640861, acc.: 66.41%] [G loss: 2.430270]\n",
            "8320 [D loss: 0.713895, acc.: 55.86%] [G loss: 2.387791]\n",
            "8340 [D loss: 0.550585, acc.: 72.27%] [G loss: 2.773180]\n",
            "8360 [D loss: 0.546875, acc.: 75.00%] [G loss: 2.489101]\n",
            "8380 [D loss: 0.604865, acc.: 67.97%] [G loss: 2.661973]\n",
            "8400 [D loss: 0.524435, acc.: 76.17%] [G loss: 2.622821]\n",
            "8420 [D loss: 0.638490, acc.: 67.19%] [G loss: 2.570458]\n",
            "8440 [D loss: 0.539606, acc.: 69.53%] [G loss: 2.740246]\n",
            "8460 [D loss: 0.596249, acc.: 69.53%] [G loss: 2.563522]\n",
            "8480 [D loss: 0.593445, acc.: 66.02%] [G loss: 2.541080]\n",
            "8500 [D loss: 0.594894, acc.: 69.14%] [G loss: 2.413967]\n",
            "8520 [D loss: 0.536373, acc.: 74.61%] [G loss: 2.808175]\n",
            "8540 [D loss: 0.527633, acc.: 73.44%] [G loss: 2.548262]\n",
            "8560 [D loss: 0.579445, acc.: 71.48%] [G loss: 2.596086]\n",
            "8580 [D loss: 0.581773, acc.: 66.41%] [G loss: 2.468611]\n",
            "8600 [D loss: 0.542145, acc.: 72.66%] [G loss: 2.565590]\n",
            "8620 [D loss: 0.630167, acc.: 63.67%] [G loss: 2.419682]\n",
            "8640 [D loss: 0.503954, acc.: 78.52%] [G loss: 2.692961]\n",
            "8660 [D loss: 0.553021, acc.: 71.09%] [G loss: 2.668393]\n",
            "8680 [D loss: 0.569784, acc.: 69.92%] [G loss: 2.629100]\n",
            "8700 [D loss: 0.522746, acc.: 75.00%] [G loss: 2.671738]\n",
            "8720 [D loss: 0.565535, acc.: 75.00%] [G loss: 2.607965]\n",
            "8740 [D loss: 0.525115, acc.: 73.83%] [G loss: 2.561700]\n",
            "8760 [D loss: 0.589115, acc.: 65.23%] [G loss: 2.677719]\n",
            "8780 [D loss: 0.549979, acc.: 73.83%] [G loss: 2.692147]\n",
            "8800 [D loss: 0.533444, acc.: 73.83%] [G loss: 2.836862]\n",
            "8820 [D loss: 0.538263, acc.: 72.27%] [G loss: 2.729428]\n",
            "8840 [D loss: 0.618039, acc.: 62.89%] [G loss: 2.594470]\n",
            "8860 [D loss: 0.512563, acc.: 74.22%] [G loss: 2.836555]\n",
            "8880 [D loss: 0.612499, acc.: 62.89%] [G loss: 2.589425]\n",
            "8900 [D loss: 0.633537, acc.: 64.45%] [G loss: 2.538355]\n",
            "8920 [D loss: 0.585088, acc.: 71.48%] [G loss: 2.719188]\n",
            "8940 [D loss: 0.549811, acc.: 71.09%] [G loss: 2.665834]\n",
            "8960 [D loss: 0.541243, acc.: 70.70%] [G loss: 2.740890]\n",
            "8980 [D loss: 0.550926, acc.: 75.39%] [G loss: 2.599428]\n",
            "9000 [D loss: 0.605013, acc.: 65.62%] [G loss: 2.337585]\n",
            "9020 [D loss: 0.570360, acc.: 70.70%] [G loss: 2.112998]\n",
            "9040 [D loss: 0.701711, acc.: 58.98%] [G loss: 2.578062]\n",
            "9060 [D loss: 0.642720, acc.: 63.67%] [G loss: 2.313200]\n",
            "9080 [D loss: 0.566772, acc.: 73.05%] [G loss: 3.309559]\n",
            "9100 [D loss: 0.654780, acc.: 63.28%] [G loss: 2.824963]\n",
            "9120 [D loss: 0.640790, acc.: 63.67%] [G loss: 2.798804]\n",
            "9140 [D loss: 0.583216, acc.: 67.97%] [G loss: 2.985188]\n",
            "9160 [D loss: 0.581715, acc.: 64.84%] [G loss: 2.936146]\n",
            "9180 [D loss: 0.601252, acc.: 66.41%] [G loss: 2.826208]\n",
            "9200 [D loss: 0.579273, acc.: 68.75%] [G loss: 2.877348]\n",
            "9220 [D loss: 0.511572, acc.: 75.78%] [G loss: 3.006902]\n",
            "9240 [D loss: 0.547170, acc.: 69.14%] [G loss: 2.859256]\n",
            "9260 [D loss: 0.479079, acc.: 78.91%] [G loss: 2.971843]\n",
            "9280 [D loss: 0.536169, acc.: 72.66%] [G loss: 2.951094]\n",
            "9300 [D loss: 0.544927, acc.: 71.48%] [G loss: 3.055049]\n",
            "9320 [D loss: 0.543814, acc.: 71.88%] [G loss: 2.955372]\n",
            "9340 [D loss: 0.596435, acc.: 67.19%] [G loss: 2.702092]\n",
            "9360 [D loss: 0.494080, acc.: 76.17%] [G loss: 2.922379]\n",
            "9380 [D loss: 0.596968, acc.: 68.36%] [G loss: 2.636365]\n",
            "9400 [D loss: 0.576159, acc.: 72.27%] [G loss: 2.951838]\n",
            "9420 [D loss: 0.575785, acc.: 67.97%] [G loss: 2.648640]\n",
            "9440 [D loss: 0.594982, acc.: 65.62%] [G loss: 2.810375]\n",
            "9460 [D loss: 0.558887, acc.: 69.92%] [G loss: 2.892263]\n",
            "9480 [D loss: 0.542523, acc.: 73.44%] [G loss: 2.845253]\n",
            "9500 [D loss: 0.591942, acc.: 69.14%] [G loss: 3.039014]\n",
            "9520 [D loss: 0.576133, acc.: 69.14%] [G loss: 2.974689]\n",
            "9540 [D loss: 0.573546, acc.: 69.92%] [G loss: 2.814201]\n",
            "9560 [D loss: 0.553689, acc.: 72.66%] [G loss: 2.818887]\n",
            "9580 [D loss: 0.523887, acc.: 76.17%] [G loss: 2.966408]\n",
            "9600 [D loss: 0.577320, acc.: 66.80%] [G loss: 2.869481]\n",
            "9620 [D loss: 0.551908, acc.: 71.09%] [G loss: 2.924954]\n",
            "9640 [D loss: 0.582251, acc.: 67.97%] [G loss: 2.693524]\n",
            "9660 [D loss: 0.561636, acc.: 75.00%] [G loss: 2.753303]\n",
            "9680 [D loss: 0.522483, acc.: 75.39%] [G loss: 2.867248]\n",
            "9700 [D loss: 0.574804, acc.: 70.31%] [G loss: 2.912661]\n",
            "9720 [D loss: 0.567928, acc.: 67.58%] [G loss: 2.757552]\n",
            "9740 [D loss: 0.488921, acc.: 75.39%] [G loss: 2.953938]\n",
            "9760 [D loss: 0.523595, acc.: 73.05%] [G loss: 2.589400]\n",
            "9780 [D loss: 0.597264, acc.: 66.41%] [G loss: 2.601096]\n",
            "9800 [D loss: 0.581603, acc.: 68.36%] [G loss: 2.241656]\n",
            "9820 [D loss: 0.494601, acc.: 77.73%] [G loss: 2.943880]\n",
            "9840 [D loss: 0.565815, acc.: 69.92%] [G loss: 3.416389]\n",
            "9860 [D loss: 0.551620, acc.: 71.09%] [G loss: 3.118381]\n",
            "9880 [D loss: 0.469296, acc.: 78.12%] [G loss: 3.212199]\n",
            "9900 [D loss: 0.549570, acc.: 71.88%] [G loss: 3.119094]\n",
            "9920 [D loss: 0.551845, acc.: 73.83%] [G loss: 3.101723]\n",
            "9940 [D loss: 0.521956, acc.: 76.56%] [G loss: 3.107090]\n",
            "9960 [D loss: 0.552979, acc.: 74.61%] [G loss: 3.066397]\n",
            "9980 [D loss: 0.568376, acc.: 72.27%] [G loss: 2.920666]\n",
            "10000 [D loss: 0.469817, acc.: 78.52%] [G loss: 3.112458]\n",
            "10020 [D loss: 0.538664, acc.: 74.61%] [G loss: 3.141667]\n",
            "10040 [D loss: 0.544717, acc.: 71.48%] [G loss: 3.237333]\n",
            "10060 [D loss: 0.515602, acc.: 72.27%] [G loss: 3.601287]\n",
            "10080 [D loss: 0.512755, acc.: 73.05%] [G loss: 3.125249]\n",
            "10100 [D loss: 0.496655, acc.: 78.52%] [G loss: 3.264734]\n",
            "10120 [D loss: 0.572427, acc.: 70.70%] [G loss: 3.259317]\n",
            "10140 [D loss: 0.543056, acc.: 71.09%] [G loss: 3.263454]\n",
            "10160 [D loss: 0.498502, acc.: 76.17%] [G loss: 3.089145]\n",
            "10180 [D loss: 0.550154, acc.: 73.83%] [G loss: 3.104313]\n",
            "10200 [D loss: 0.503011, acc.: 75.78%] [G loss: 3.022737]\n",
            "10220 [D loss: 0.501060, acc.: 75.00%] [G loss: 3.086825]\n",
            "10240 [D loss: 0.549123, acc.: 74.22%] [G loss: 2.905900]\n",
            "10260 [D loss: 0.505931, acc.: 76.95%] [G loss: 3.162741]\n",
            "10300 [D loss: 0.489102, acc.: 76.17%] [G loss: 3.094937]\n",
            "10320 [D loss: 0.510436, acc.: 74.22%] [G loss: 3.334235]\n",
            "10340 [D loss: 0.491794, acc.: 78.52%] [G loss: 3.228751]\n",
            "10360 [D loss: 0.467363, acc.: 78.12%] [G loss: 3.144713]\n",
            "10380 [D loss: 0.408233, acc.: 83.98%] [G loss: 3.261348]\n",
            "10400 [D loss: 0.442796, acc.: 82.42%] [G loss: 2.392019]\n",
            "10420 [D loss: 0.739972, acc.: 57.42%] [G loss: 2.817179]\n",
            "10440 [D loss: 0.871059, acc.: 49.22%] [G loss: 2.822335]\n",
            "10460 [D loss: 1.043549, acc.: 43.75%] [G loss: 3.104793]\n",
            "10480 [D loss: 0.673291, acc.: 60.94%] [G loss: 2.947427]\n",
            "10500 [D loss: 0.520395, acc.: 73.83%] [G loss: 3.384137]\n",
            "10520 [D loss: 0.556371, acc.: 72.66%] [G loss: 3.206419]\n",
            "10540 [D loss: 0.623883, acc.: 66.80%] [G loss: 3.153855]\n",
            "10560 [D loss: 0.531604, acc.: 74.61%] [G loss: 3.258462]\n",
            "10580 [D loss: 0.517901, acc.: 76.56%] [G loss: 3.359480]\n",
            "10600 [D loss: 0.522183, acc.: 74.61%] [G loss: 3.497176]\n",
            "10620 [D loss: 0.499737, acc.: 78.52%] [G loss: 3.169575]\n",
            "10640 [D loss: 0.638564, acc.: 65.23%] [G loss: 3.213228]\n",
            "10660 [D loss: 0.482063, acc.: 78.52%] [G loss: 3.194679]\n",
            "10680 [D loss: 0.589014, acc.: 67.97%] [G loss: 3.031755]\n",
            "10700 [D loss: 0.547460, acc.: 73.83%] [G loss: 3.337606]\n",
            "10720 [D loss: 0.603283, acc.: 69.53%] [G loss: 3.345606]\n",
            "10740 [D loss: 0.545977, acc.: 73.05%] [G loss: 3.296006]\n",
            "10760 [D loss: 0.524416, acc.: 73.05%] [G loss: 3.315976]\n",
            "10780 [D loss: 0.491167, acc.: 77.73%] [G loss: 3.341076]\n",
            "10800 [D loss: 0.447674, acc.: 81.25%] [G loss: 3.295878]\n",
            "10820 [D loss: 0.463201, acc.: 78.52%] [G loss: 3.562702]\n",
            "10840 [D loss: 0.515450, acc.: 72.66%] [G loss: 3.297668]\n",
            "10860 [D loss: 0.505068, acc.: 74.61%] [G loss: 3.317349]\n",
            "10880 [D loss: 0.529308, acc.: 75.00%] [G loss: 3.107686]\n",
            "10900 [D loss: 0.487515, acc.: 76.56%] [G loss: 3.426195]\n",
            "10920 [D loss: 0.479436, acc.: 78.12%] [G loss: 3.219428]\n",
            "10940 [D loss: 0.541018, acc.: 71.88%] [G loss: 3.377609]\n",
            "10960 [D loss: 0.532238, acc.: 72.27%] [G loss: 3.403613]\n",
            "10980 [D loss: 0.525844, acc.: 75.78%] [G loss: 3.697159]\n",
            "11000 [D loss: 0.488562, acc.: 78.12%] [G loss: 3.393361]\n",
            "11020 [D loss: 0.545516, acc.: 73.05%] [G loss: 3.340760]\n",
            "11040 [D loss: 0.449928, acc.: 80.47%] [G loss: 3.236760]\n",
            "11060 [D loss: 0.556168, acc.: 72.66%] [G loss: 3.227970]\n",
            "11080 [D loss: 0.504565, acc.: 75.78%] [G loss: 3.266570]\n",
            "11100 [D loss: 0.491085, acc.: 74.61%] [G loss: 3.423527]\n",
            "11120 [D loss: 0.537882, acc.: 72.27%] [G loss: 3.335530]\n",
            "11140 [D loss: 0.498562, acc.: 74.22%] [G loss: 3.324354]\n",
            "11160 [D loss: 0.483946, acc.: 77.73%] [G loss: 3.575799]\n",
            "11180 [D loss: 0.485210, acc.: 73.44%] [G loss: 4.010461]\n",
            "11200 [D loss: 0.479605, acc.: 75.78%] [G loss: 3.742714]\n",
            "11220 [D loss: 0.452866, acc.: 76.17%] [G loss: 3.372311]\n",
            "11240 [D loss: 0.463367, acc.: 75.78%] [G loss: 3.644572]\n",
            "11260 [D loss: 0.566187, acc.: 67.58%] [G loss: 3.348576]\n",
            "11280 [D loss: 0.453186, acc.: 79.30%] [G loss: 3.614686]\n",
            "11300 [D loss: 0.472202, acc.: 77.73%] [G loss: 3.325865]\n",
            "11320 [D loss: 0.439666, acc.: 80.86%] [G loss: 3.359422]\n",
            "11340 [D loss: 0.426272, acc.: 80.47%] [G loss: 3.937174]\n",
            "11360 [D loss: 0.429910, acc.: 81.25%] [G loss: 2.873225]\n",
            "11380 [D loss: 0.419791, acc.: 83.20%] [G loss: 2.194473]\n",
            "11400 [D loss: 0.702833, acc.: 62.11%] [G loss: 3.553001]\n",
            "11420 [D loss: 0.633453, acc.: 67.19%] [G loss: 3.319390]\n",
            "11440 [D loss: 0.566117, acc.: 69.92%] [G loss: 3.588347]\n",
            "11460 [D loss: 0.620929, acc.: 66.41%] [G loss: 3.360430]\n",
            "11480 [D loss: 0.664847, acc.: 63.28%] [G loss: 3.391920]\n",
            "11500 [D loss: 0.515279, acc.: 75.00%] [G loss: 3.753444]\n",
            "11520 [D loss: 0.490256, acc.: 76.95%] [G loss: 3.483949]\n",
            "11540 [D loss: 0.496921, acc.: 78.12%] [G loss: 3.524170]\n",
            "11560 [D loss: 0.495468, acc.: 73.44%] [G loss: 3.585512]\n",
            "11580 [D loss: 0.518311, acc.: 76.56%] [G loss: 3.672111]\n",
            "11600 [D loss: 0.443481, acc.: 80.08%] [G loss: 3.813728]\n",
            "11620 [D loss: 0.618314, acc.: 68.36%] [G loss: 3.295211]\n",
            "11640 [D loss: 0.520844, acc.: 75.00%] [G loss: 3.613941]\n",
            "11660 [D loss: 0.490226, acc.: 76.95%] [G loss: 3.412715]\n",
            "11680 [D loss: 0.530190, acc.: 75.00%] [G loss: 3.341209]\n",
            "11700 [D loss: 0.515242, acc.: 75.39%] [G loss: 3.359792]\n",
            "11720 [D loss: 0.556978, acc.: 69.92%] [G loss: 3.458259]\n",
            "11740 [D loss: 0.467623, acc.: 77.34%] [G loss: 3.864626]\n",
            "11760 [D loss: 0.527532, acc.: 72.27%] [G loss: 3.432994]\n",
            "11780 [D loss: 0.443254, acc.: 81.64%] [G loss: 3.626464]\n",
            "11800 [D loss: 0.480866, acc.: 73.44%] [G loss: 3.953909]\n",
            "11820 [D loss: 0.430228, acc.: 81.25%] [G loss: 3.725126]\n",
            "11840 [D loss: 0.474329, acc.: 77.73%] [G loss: 3.607335]\n",
            "11860 [D loss: 0.455970, acc.: 81.64%] [G loss: 3.713044]\n",
            "11880 [D loss: 0.426431, acc.: 81.64%] [G loss: 3.830365]\n",
            "11900 [D loss: 0.459128, acc.: 77.34%] [G loss: 3.724203]\n",
            "11920 [D loss: 0.414961, acc.: 81.25%] [G loss: 3.591125]\n",
            "11940 [D loss: 0.532840, acc.: 73.83%] [G loss: 3.623768]\n",
            "11960 [D loss: 0.474608, acc.: 78.52%] [G loss: 4.083823]\n",
            "11980 [D loss: 0.435692, acc.: 82.81%] [G loss: 3.610373]\n",
            "12000 [D loss: 0.480631, acc.: 79.30%] [G loss: 3.575135]\n",
            "12020 [D loss: 0.459349, acc.: 78.91%] [G loss: 3.669452]\n",
            "12040 [D loss: 0.458579, acc.: 77.73%] [G loss: 3.675065]\n",
            "12060 [D loss: 0.541093, acc.: 76.17%] [G loss: 3.649045]\n",
            "12080 [D loss: 0.484137, acc.: 76.56%] [G loss: 3.658222]\n",
            "12100 [D loss: 0.408629, acc.: 82.42%] [G loss: 4.131084]\n",
            "12120 [D loss: 0.487982, acc.: 75.39%] [G loss: 3.740709]\n",
            "12140 [D loss: 0.471978, acc.: 78.12%] [G loss: 3.907699]\n",
            "12160 [D loss: 0.530920, acc.: 74.61%] [G loss: 3.558141]\n",
            "12180 [D loss: 0.466971, acc.: 79.69%] [G loss: 3.766145]\n",
            "12200 [D loss: 0.447899, acc.: 81.64%] [G loss: 3.907881]\n",
            "12220 [D loss: 0.448014, acc.: 78.91%] [G loss: 3.907137]\n",
            "12240 [D loss: 0.450701, acc.: 81.25%] [G loss: 3.995576]\n",
            "12260 [D loss: 0.374655, acc.: 84.38%] [G loss: 4.116805]\n",
            "12280 [D loss: 0.464136, acc.: 77.34%] [G loss: 3.927111]\n",
            "12300 [D loss: 0.451162, acc.: 81.25%] [G loss: 3.926146]\n",
            "12320 [D loss: 0.521206, acc.: 74.22%] [G loss: 3.885402]\n",
            "12340 [D loss: 0.414467, acc.: 83.59%] [G loss: 4.068798]\n",
            "12360 [D loss: 0.354041, acc.: 87.11%] [G loss: 2.809319]\n",
            "12380 [D loss: 1.156304, acc.: 44.53%] [G loss: 3.204928]\n",
            "12400 [D loss: 1.322556, acc.: 33.20%] [G loss: 2.767421]\n",
            "12420 [D loss: 1.064652, acc.: 47.66%] [G loss: 4.519303]\n",
            "12440 [D loss: 0.426817, acc.: 82.03%] [G loss: 4.447984]\n",
            "12460 [D loss: 0.496733, acc.: 75.39%] [G loss: 4.418677]\n",
            "12480 [D loss: 0.560811, acc.: 73.05%] [G loss: 3.636673]\n",
            "12500 [D loss: 0.486944, acc.: 77.73%] [G loss: 3.961796]\n",
            "12520 [D loss: 0.416931, acc.: 80.47%] [G loss: 4.189286]\n",
            "12540 [D loss: 0.439120, acc.: 79.69%] [G loss: 3.758483]\n",
            "12560 [D loss: 0.458234, acc.: 77.73%] [G loss: 4.180783]\n",
            "12580 [D loss: 0.497840, acc.: 75.78%] [G loss: 3.962857]\n",
            "12600 [D loss: 0.486541, acc.: 78.12%] [G loss: 3.779204]\n",
            "12620 [D loss: 0.393862, acc.: 81.25%] [G loss: 4.234546]\n",
            "12640 [D loss: 0.415391, acc.: 82.03%] [G loss: 4.063314]\n",
            "12660 [D loss: 0.448977, acc.: 79.69%] [G loss: 3.776723]\n",
            "12680 [D loss: 0.414287, acc.: 82.03%] [G loss: 4.167212]\n",
            "12700 [D loss: 0.434608, acc.: 82.42%] [G loss: 4.001852]\n",
            "12720 [D loss: 0.488159, acc.: 75.00%] [G loss: 4.213722]\n",
            "12740 [D loss: 0.468408, acc.: 79.69%] [G loss: 4.014121]\n",
            "12760 [D loss: 0.398561, acc.: 82.81%] [G loss: 4.286667]\n",
            "12780 [D loss: 0.435383, acc.: 80.47%] [G loss: 4.110616]\n",
            "12800 [D loss: 0.540246, acc.: 73.44%] [G loss: 3.815032]\n",
            "12820 [D loss: 0.449430, acc.: 79.30%] [G loss: 3.956553]\n",
            "12840 [D loss: 0.465830, acc.: 79.69%] [G loss: 4.106258]\n",
            "12860 [D loss: 0.455211, acc.: 80.86%] [G loss: 4.233923]\n",
            "12880 [D loss: 0.442104, acc.: 79.30%] [G loss: 4.064203]\n",
            "12900 [D loss: 0.468348, acc.: 75.39%] [G loss: 4.077435]\n",
            "12920 [D loss: 0.482400, acc.: 79.69%] [G loss: 4.041441]\n",
            "12940 [D loss: 0.417659, acc.: 80.47%] [G loss: 4.562035]\n",
            "12960 [D loss: 0.504319, acc.: 72.27%] [G loss: 4.100395]\n",
            "12980 [D loss: 0.394912, acc.: 82.81%] [G loss: 4.489388]\n",
            "13000 [D loss: 0.473625, acc.: 78.91%] [G loss: 3.932952]\n",
            "13020 [D loss: 0.430805, acc.: 80.08%] [G loss: 3.993725]\n",
            "13040 [D loss: 0.363535, acc.: 84.38%] [G loss: 4.538687]\n",
            "13060 [D loss: 0.370794, acc.: 83.20%] [G loss: 4.329133]\n",
            "13080 [D loss: 0.506514, acc.: 75.00%] [G loss: 3.895709]\n",
            "13120 [D loss: 0.364999, acc.: 84.77%] [G loss: 4.362689]\n",
            "13140 [D loss: 0.498141, acc.: 76.56%] [G loss: 3.965419]\n",
            "13160 [D loss: 0.394095, acc.: 84.38%] [G loss: 4.320313]\n",
            "13180 [D loss: 0.426230, acc.: 80.86%] [G loss: 4.112617]\n",
            "13200 [D loss: 0.349637, acc.: 87.89%] [G loss: 4.850826]\n",
            "13220 [D loss: 0.364168, acc.: 84.77%] [G loss: 4.502656]\n",
            "13240 [D loss: 0.436054, acc.: 79.69%] [G loss: 4.330339]\n",
            "13260 [D loss: 0.405329, acc.: 83.20%] [G loss: 4.452048]\n",
            "13280 [D loss: 0.512366, acc.: 76.56%] [G loss: 4.060997]\n",
            "13300 [D loss: 0.406935, acc.: 81.64%] [G loss: 4.438563]\n",
            "13320 [D loss: 0.352169, acc.: 84.38%] [G loss: 4.199373]\n",
            "13340 [D loss: 0.524749, acc.: 75.00%] [G loss: 3.964799]\n",
            "13360 [D loss: 0.520030, acc.: 73.44%] [G loss: 3.874833]\n",
            "13380 [D loss: 0.532615, acc.: 71.88%] [G loss: 4.071138]\n",
            "13400 [D loss: 0.396991, acc.: 81.25%] [G loss: 3.993813]\n",
            "13420 [D loss: 0.477890, acc.: 77.34%] [G loss: 4.920828]\n",
            "13440 [D loss: 0.539181, acc.: 75.39%] [G loss: 4.188732]\n",
            "13460 [D loss: 0.503999, acc.: 75.78%] [G loss: 4.057484]\n",
            "13480 [D loss: 0.428196, acc.: 78.52%] [G loss: 4.288588]\n",
            "13500 [D loss: 0.320167, acc.: 89.45%] [G loss: 4.770140]\n",
            "13520 [D loss: 0.500479, acc.: 75.00%] [G loss: 4.177998]\n",
            "13540 [D loss: 0.358207, acc.: 86.33%] [G loss: 4.854644]\n",
            "13560 [D loss: 0.461888, acc.: 78.52%] [G loss: 4.217960]\n",
            "13580 [D loss: 0.421474, acc.: 81.25%] [G loss: 4.545017]\n",
            "13600 [D loss: 0.413726, acc.: 82.42%] [G loss: 4.579486]\n",
            "13620 [D loss: 0.467942, acc.: 80.08%] [G loss: 4.251786]\n",
            "13640 [D loss: 0.352833, acc.: 87.11%] [G loss: 5.049088]\n",
            "13660 [D loss: 0.502112, acc.: 75.39%] [G loss: 3.991843]\n",
            "13680 [D loss: 0.439334, acc.: 81.25%] [G loss: 4.651052]\n",
            "13700 [D loss: 0.352713, acc.: 86.72%] [G loss: 4.515455]\n",
            "13720 [D loss: 0.434880, acc.: 79.30%] [G loss: 4.046702]\n",
            "13740 [D loss: 0.349593, acc.: 85.16%] [G loss: 5.037921]\n",
            "13760 [D loss: 0.392640, acc.: 82.03%] [G loss: 4.465853]\n",
            "13780 [D loss: 0.422466, acc.: 79.69%] [G loss: 4.379842]\n",
            "13800 [D loss: 0.354600, acc.: 85.16%] [G loss: 4.771981]\n",
            "13820 [D loss: 0.378104, acc.: 83.20%] [G loss: 4.695835]\n",
            "13840 [D loss: 0.470331, acc.: 77.73%] [G loss: 3.972243]\n",
            "13860 [D loss: 0.349401, acc.: 85.55%] [G loss: 4.683465]\n",
            "13880 [D loss: 0.496157, acc.: 79.69%] [G loss: 4.158771]\n",
            "13900 [D loss: 0.399885, acc.: 82.03%] [G loss: 4.557848]\n",
            "13920 [D loss: 0.403803, acc.: 82.42%] [G loss: 4.665912]\n",
            "13940 [D loss: 0.327017, acc.: 90.23%] [G loss: 4.744614]\n",
            "13960 [D loss: 0.468060, acc.: 79.30%] [G loss: 4.411760]\n",
            "13980 [D loss: 0.439630, acc.: 77.73%] [G loss: 4.622922]\n",
            "14000 [D loss: 0.379123, acc.: 86.33%] [G loss: 4.410106]\n",
            "14020 [D loss: 0.366591, acc.: 80.86%] [G loss: 4.701436]\n",
            "14040 [D loss: 0.411398, acc.: 78.91%] [G loss: 4.543402]\n",
            "14060 [D loss: 0.291307, acc.: 91.41%] [G loss: 5.035880]\n",
            "14080 [D loss: 0.524448, acc.: 71.09%] [G loss: 3.997252]\n",
            "14100 [D loss: 0.401965, acc.: 80.86%] [G loss: 4.691562]\n",
            "14120 [D loss: 0.488052, acc.: 78.12%] [G loss: 4.253728]\n",
            "14140 [D loss: 0.386170, acc.: 84.77%] [G loss: 5.146982]\n",
            "14160 [D loss: 0.412614, acc.: 79.30%] [G loss: 4.469100]\n",
            "14180 [D loss: 0.419232, acc.: 80.08%] [G loss: 4.829553]\n",
            "14200 [D loss: 0.364741, acc.: 85.94%] [G loss: 4.735061]\n",
            "14220 [D loss: 0.453604, acc.: 79.30%] [G loss: 5.002831]\n",
            "14240 [D loss: 0.397269, acc.: 80.86%] [G loss: 4.533510]\n",
            "14260 [D loss: 0.339183, acc.: 87.11%] [G loss: 5.036261]\n",
            "14280 [D loss: 0.400223, acc.: 82.42%] [G loss: 4.679868]\n",
            "14300 [D loss: 0.481382, acc.: 75.78%] [G loss: 4.430243]\n",
            "14320 [D loss: 0.435744, acc.: 78.52%] [G loss: 4.515152]\n",
            "14340 [D loss: 0.332037, acc.: 88.67%] [G loss: 4.941385]\n",
            "14360 [D loss: 0.368235, acc.: 82.42%] [G loss: 4.580808]\n",
            "14380 [D loss: 0.166071, acc.: 95.70%] [G loss: 2.893756]\n",
            "14400 [D loss: 0.515664, acc.: 76.17%] [G loss: 3.129707]\n",
            "14420 [D loss: 0.405030, acc.: 81.25%] [G loss: 4.960910]\n",
            "14440 [D loss: 0.672728, acc.: 66.02%] [G loss: 4.551592]\n",
            "14460 [D loss: 0.629537, acc.: 74.61%] [G loss: 4.357634]\n",
            "14480 [D loss: 0.410806, acc.: 83.20%] [G loss: 5.162037]\n",
            "14500 [D loss: 0.357411, acc.: 87.11%] [G loss: 4.781758]\n",
            "14520 [D loss: 0.339625, acc.: 86.72%] [G loss: 5.496151]\n",
            "14540 [D loss: 0.463273, acc.: 78.91%] [G loss: 4.634079]\n",
            "14560 [D loss: 0.381357, acc.: 83.59%] [G loss: 5.034862]\n",
            "14580 [D loss: 0.394880, acc.: 83.59%] [G loss: 4.961334]\n",
            "14600 [D loss: 0.426282, acc.: 78.91%] [G loss: 4.459863]\n",
            "14620 [D loss: 0.464937, acc.: 77.34%] [G loss: 4.602026]\n",
            "14640 [D loss: 0.577348, acc.: 75.00%] [G loss: 4.812849]\n",
            "14660 [D loss: 0.433560, acc.: 80.08%] [G loss: 5.112462]\n",
            "14680 [D loss: 0.346919, acc.: 86.33%] [G loss: 5.040551]\n",
            "14700 [D loss: 0.377361, acc.: 84.38%] [G loss: 4.760823]\n",
            "14720 [D loss: 0.453682, acc.: 78.52%] [G loss: 4.455758]\n",
            "14740 [D loss: 0.407074, acc.: 79.30%] [G loss: 4.523721]\n",
            "14760 [D loss: 0.420238, acc.: 82.42%] [G loss: 4.664611]\n",
            "14780 [D loss: 0.361884, acc.: 83.59%] [G loss: 4.983669]\n",
            "14800 [D loss: 0.422414, acc.: 79.30%] [G loss: 4.721410]\n",
            "14820 [D loss: 0.366585, acc.: 86.33%] [G loss: 5.287141]\n",
            "14840 [D loss: 0.375207, acc.: 82.03%] [G loss: 5.088748]\n",
            "14860 [D loss: 0.457297, acc.: 80.08%] [G loss: 4.453184]\n",
            "14880 [D loss: 0.343905, acc.: 85.94%] [G loss: 4.998139]\n",
            "14900 [D loss: 0.390077, acc.: 82.03%] [G loss: 4.866480]\n",
            "14920 [D loss: 0.463259, acc.: 76.56%] [G loss: 4.552716]\n",
            "14940 [D loss: 0.490987, acc.: 76.95%] [G loss: 4.517418]\n",
            "14960 [D loss: 0.501806, acc.: 75.39%] [G loss: 4.740751]\n",
            "14980 [D loss: 0.335323, acc.: 87.50%] [G loss: 4.888234]\n",
            "15000 [D loss: 0.286445, acc.: 86.72%] [G loss: 5.126867]\n",
            "15020 [D loss: 0.477018, acc.: 78.12%] [G loss: 4.908254]\n",
            "15040 [D loss: 0.512222, acc.: 78.12%] [G loss: 4.591154]\n",
            "15060 [D loss: 0.376338, acc.: 83.98%] [G loss: 5.203836]\n",
            "15080 [D loss: 0.396358, acc.: 79.30%] [G loss: 4.876016]\n",
            "15100 [D loss: 0.298431, acc.: 86.72%] [G loss: 5.326966]\n",
            "15120 [D loss: 0.407118, acc.: 82.03%] [G loss: 3.412839]\n",
            "15140 [D loss: 0.346509, acc.: 84.38%] [G loss: 3.871648]\n",
            "15180 [D loss: 0.521667, acc.: 78.12%] [G loss: 4.780605]\n",
            "15200 [D loss: 0.423100, acc.: 79.30%] [G loss: 4.367449]\n",
            "15220 [D loss: 0.401806, acc.: 82.81%] [G loss: 5.186312]\n",
            "15240 [D loss: 0.398921, acc.: 83.59%] [G loss: 4.786339]\n",
            "15260 [D loss: 0.323795, acc.: 88.28%] [G loss: 6.089019]\n",
            "15280 [D loss: 0.376101, acc.: 80.86%] [G loss: 4.894403]\n",
            "15300 [D loss: 0.337721, acc.: 86.33%] [G loss: 5.464455]\n",
            "15320 [D loss: 0.340617, acc.: 80.86%] [G loss: 5.189015]\n",
            "15340 [D loss: 0.450481, acc.: 78.91%] [G loss: 5.064300]\n",
            "15360 [D loss: 0.422384, acc.: 80.86%] [G loss: 5.269908]\n",
            "15380 [D loss: 0.392085, acc.: 81.64%] [G loss: 4.975830]\n",
            "15400 [D loss: 0.361334, acc.: 83.59%] [G loss: 5.402022]\n",
            "15420 [D loss: 0.344296, acc.: 84.77%] [G loss: 5.607550]\n",
            "15440 [D loss: 0.368974, acc.: 83.59%] [G loss: 5.329936]\n",
            "15460 [D loss: 0.399588, acc.: 82.03%] [G loss: 4.928152]\n",
            "15480 [D loss: 0.335612, acc.: 85.94%] [G loss: 5.337806]\n",
            "15500 [D loss: 0.432263, acc.: 81.25%] [G loss: 5.305075]\n",
            "15520 [D loss: 0.367250, acc.: 84.77%] [G loss: 5.439445]\n",
            "15540 [D loss: 0.355835, acc.: 86.33%] [G loss: 5.442018]\n",
            "15560 [D loss: 0.300803, acc.: 87.50%] [G loss: 5.440144]\n",
            "15580 [D loss: 0.466111, acc.: 77.34%] [G loss: 5.011644]\n",
            "15600 [D loss: 0.376914, acc.: 82.42%] [G loss: 5.318049]\n",
            "15620 [D loss: 0.355192, acc.: 83.59%] [G loss: 5.987556]\n",
            "15640 [D loss: 0.325083, acc.: 87.50%] [G loss: 5.385699]\n",
            "15660 [D loss: 0.332716, acc.: 86.72%] [G loss: 5.426803]\n",
            "15680 [D loss: 0.129567, acc.: 98.44%] [G loss: 3.263879]\n",
            "15700 [D loss: 0.520223, acc.: 75.00%] [G loss: 4.640100]\n",
            "15720 [D loss: 0.545498, acc.: 73.05%] [G loss: 5.478586]\n",
            "15740 [D loss: 0.483651, acc.: 76.95%] [G loss: 4.074951]\n",
            "15760 [D loss: 0.505022, acc.: 79.30%] [G loss: 4.173003]\n",
            "15780 [D loss: 0.313856, acc.: 87.89%] [G loss: 4.108263]\n",
            "15800 [D loss: 0.320673, acc.: 88.28%] [G loss: 5.453838]\n",
            "15820 [D loss: 0.347337, acc.: 83.59%] [G loss: 6.494256]\n",
            "15840 [D loss: 0.353163, acc.: 86.33%] [G loss: 6.397019]\n",
            "15860 [D loss: 0.320142, acc.: 84.38%] [G loss: 5.509915]\n",
            "15880 [D loss: 0.365474, acc.: 85.94%] [G loss: 5.894915]\n",
            "15900 [D loss: 0.320261, acc.: 85.94%] [G loss: 5.383911]\n",
            "15920 [D loss: 0.427167, acc.: 78.91%] [G loss: 5.287035]\n",
            "15940 [D loss: 0.444468, acc.: 77.73%] [G loss: 6.036949]\n",
            "15960 [D loss: 0.256212, acc.: 90.62%] [G loss: 6.537170]\n",
            "15980 [D loss: 0.390016, acc.: 85.55%] [G loss: 5.672728]\n",
            "16000 [D loss: 0.397222, acc.: 83.59%] [G loss: 5.107855]\n",
            "16020 [D loss: 0.350463, acc.: 85.94%] [G loss: 5.730735]\n",
            "16040 [D loss: 0.358226, acc.: 85.55%] [G loss: 5.769398]\n",
            "16060 [D loss: 0.383173, acc.: 80.86%] [G loss: 5.532202]\n",
            "16080 [D loss: 0.325736, acc.: 86.33%] [G loss: 6.284734]\n",
            "16100 [D loss: 0.373880, acc.: 83.59%] [G loss: 5.451688]\n",
            "16120 [D loss: 0.328485, acc.: 86.33%] [G loss: 6.079537]\n",
            "16140 [D loss: 0.372796, acc.: 83.20%] [G loss: 5.706218]\n",
            "16160 [D loss: 0.363362, acc.: 83.98%] [G loss: 6.027742]\n",
            "16180 [D loss: 0.296662, acc.: 88.67%] [G loss: 5.878767]\n",
            "16200 [D loss: 0.279880, acc.: 88.67%] [G loss: 6.205262]\n",
            "16220 [D loss: 0.326721, acc.: 86.72%] [G loss: 6.083632]\n",
            "16240 [D loss: 0.342254, acc.: 83.20%] [G loss: 5.837392]\n",
            "16260 [D loss: 0.328163, acc.: 87.11%] [G loss: 5.837648]\n",
            "16280 [D loss: 0.325214, acc.: 84.38%] [G loss: 5.949832]\n",
            "16300 [D loss: 0.431095, acc.: 82.03%] [G loss: 5.370232]\n",
            "16320 [D loss: 0.334984, acc.: 84.77%] [G loss: 5.824499]\n",
            "16340 [D loss: 0.387562, acc.: 82.81%] [G loss: 5.942995]\n",
            "16360 [D loss: 0.425197, acc.: 76.56%] [G loss: 5.987863]\n",
            "16380 [D loss: 0.409665, acc.: 82.42%] [G loss: 5.861638]\n",
            "16400 [D loss: 0.499189, acc.: 77.73%] [G loss: 5.266264]\n",
            "16420 [D loss: 0.346824, acc.: 84.38%] [G loss: 6.074624]\n",
            "16440 [D loss: 0.349593, acc.: 86.33%] [G loss: 6.586889]\n",
            "16460 [D loss: 0.374941, acc.: 86.33%] [G loss: 5.599318]\n",
            "16480 [D loss: 0.341927, acc.: 86.72%] [G loss: 5.714206]\n",
            "16500 [D loss: 0.250436, acc.: 90.23%] [G loss: 6.019756]\n",
            "16520 [D loss: 0.386355, acc.: 83.20%] [G loss: 5.608378]\n",
            "16540 [D loss: 0.330252, acc.: 86.72%] [G loss: 5.820851]\n",
            "16560 [D loss: 0.415488, acc.: 80.08%] [G loss: 5.383107]\n",
            "16580 [D loss: 0.329930, acc.: 84.77%] [G loss: 5.568245]\n",
            "16600 [D loss: 0.249894, acc.: 90.62%] [G loss: 6.151464]\n",
            "16620 [D loss: 0.380832, acc.: 83.59%] [G loss: 5.519597]\n",
            "16640 [D loss: 0.348369, acc.: 83.98%] [G loss: 5.794480]\n",
            "16660 [D loss: 0.283457, acc.: 89.45%] [G loss: 6.582224]\n",
            "16680 [D loss: 0.375549, acc.: 83.59%] [G loss: 5.947823]\n",
            "16700 [D loss: 0.326014, acc.: 85.16%] [G loss: 5.945160]\n",
            "16720 [D loss: 0.340912, acc.: 87.11%] [G loss: 5.783365]\n",
            "16740 [D loss: 0.298136, acc.: 86.33%] [G loss: 5.936006]\n",
            "16760 [D loss: 0.333695, acc.: 87.11%] [G loss: 6.038760]\n",
            "16780 [D loss: 0.388630, acc.: 82.81%] [G loss: 5.615678]\n",
            "16800 [D loss: 0.318145, acc.: 87.11%] [G loss: 6.013947]\n",
            "16820 [D loss: 0.286647, acc.: 88.28%] [G loss: 3.493061]\n",
            "16840 [D loss: 0.659225, acc.: 69.14%] [G loss: 6.503576]\n",
            "16860 [D loss: 0.608422, acc.: 69.53%] [G loss: 5.821430]\n",
            "16880 [D loss: 0.368635, acc.: 82.81%] [G loss: 6.231730]\n",
            "16900 [D loss: 0.381390, acc.: 83.98%] [G loss: 5.763344]\n",
            "16920 [D loss: 0.426938, acc.: 79.69%] [G loss: 5.811415]\n",
            "16940 [D loss: 0.368438, acc.: 84.38%] [G loss: 5.409851]\n",
            "16960 [D loss: 0.377797, acc.: 83.59%] [G loss: 6.165015]\n",
            "16980 [D loss: 0.370070, acc.: 82.03%] [G loss: 6.027173]\n",
            "17000 [D loss: 0.342913, acc.: 84.38%] [G loss: 6.303402]\n",
            "17020 [D loss: 0.263589, acc.: 89.45%] [G loss: 6.178236]\n",
            "17040 [D loss: 0.354854, acc.: 82.42%] [G loss: 6.526241]\n",
            "17060 [D loss: 0.342473, acc.: 87.50%] [G loss: 5.862255]\n",
            "17080 [D loss: 0.307025, acc.: 87.50%] [G loss: 6.638608]\n",
            "17100 [D loss: 0.358453, acc.: 84.38%] [G loss: 6.116178]\n",
            "17120 [D loss: 0.419301, acc.: 80.86%] [G loss: 6.116621]\n",
            "17140 [D loss: 0.318401, acc.: 88.28%] [G loss: 6.861005]\n",
            "17160 [D loss: 0.363837, acc.: 84.77%] [G loss: 5.923966]\n",
            "17180 [D loss: 0.378912, acc.: 82.42%] [G loss: 5.610514]\n",
            "17200 [D loss: 0.257375, acc.: 91.80%] [G loss: 6.135838]\n",
            "17220 [D loss: 0.292882, acc.: 87.11%] [G loss: 7.056197]\n",
            "17240 [D loss: 0.298346, acc.: 89.45%] [G loss: 6.103920]\n",
            "17260 [D loss: 0.337458, acc.: 86.72%] [G loss: 6.012770]\n",
            "17280 [D loss: 0.367929, acc.: 82.81%] [G loss: 6.001101]\n",
            "17300 [D loss: 0.342705, acc.: 86.33%] [G loss: 6.017804]\n",
            "17320 [D loss: 0.352119, acc.: 85.55%] [G loss: 5.688773]\n",
            "17340 [D loss: 0.340031, acc.: 85.16%] [G loss: 6.185600]\n",
            "17360 [D loss: 0.344801, acc.: 85.16%] [G loss: 6.560829]\n",
            "17380 [D loss: 0.390938, acc.: 81.25%] [G loss: 5.715786]\n",
            "17400 [D loss: 0.359779, acc.: 83.59%] [G loss: 6.440878]\n",
            "17420 [D loss: 0.404321, acc.: 84.38%] [G loss: 5.397002]\n",
            "17440 [D loss: 0.307228, acc.: 86.33%] [G loss: 6.249482]\n",
            "17460 [D loss: 0.282421, acc.: 89.06%] [G loss: 6.770514]\n",
            "17480 [D loss: 0.294538, acc.: 87.50%] [G loss: 6.441381]\n",
            "17500 [D loss: 0.270914, acc.: 87.11%] [G loss: 6.996630]\n",
            "17520 [D loss: 0.301206, acc.: 90.62%] [G loss: 6.579168]\n",
            "17540 [D loss: 0.300572, acc.: 87.50%] [G loss: 6.349356]\n",
            "17560 [D loss: 0.333767, acc.: 85.16%] [G loss: 6.159515]\n",
            "17580 [D loss: 0.348132, acc.: 82.03%] [G loss: 5.964693]\n",
            "17620 [D loss: 0.373673, acc.: 83.59%] [G loss: 6.283666]\n",
            "17640 [D loss: 0.337426, acc.: 86.72%] [G loss: 6.227210]\n",
            "17660 [D loss: 0.324078, acc.: 85.16%] [G loss: 6.149628]\n",
            "17680 [D loss: 0.294888, acc.: 88.28%] [G loss: 6.166326]\n",
            "17700 [D loss: 0.243237, acc.: 91.80%] [G loss: 6.503705]\n",
            "17720 [D loss: 0.289123, acc.: 86.72%] [G loss: 5.968730]\n",
            "17740 [D loss: 0.345691, acc.: 84.77%] [G loss: 6.297939]\n",
            "17760 [D loss: 0.293551, acc.: 86.33%] [G loss: 6.433186]\n",
            "17780 [D loss: 0.321385, acc.: 85.94%] [G loss: 6.427052]\n",
            "17800 [D loss: 0.337335, acc.: 83.59%] [G loss: 6.429667]\n",
            "17820 [D loss: 0.252993, acc.: 90.23%] [G loss: 7.244049]\n",
            "17840 [D loss: 0.402216, acc.: 82.03%] [G loss: 6.087584]\n",
            "17860 [D loss: 0.339576, acc.: 85.94%] [G loss: 6.531042]\n",
            "17880 [D loss: 0.333293, acc.: 84.77%] [G loss: 5.839901]\n",
            "17900 [D loss: 0.234861, acc.: 90.23%] [G loss: 5.094469]\n",
            "17920 [D loss: 0.409981, acc.: 82.03%] [G loss: 4.291303]\n",
            "17940 [D loss: 0.247129, acc.: 90.23%] [G loss: 6.896796]\n",
            "17960 [D loss: 0.549884, acc.: 73.05%] [G loss: 7.134972]\n",
            "17980 [D loss: 0.506150, acc.: 81.25%] [G loss: 6.394380]\n",
            "18000 [D loss: 0.413096, acc.: 80.08%] [G loss: 6.049628]\n",
            "18020 [D loss: 0.323454, acc.: 85.55%] [G loss: 6.357910]\n",
            "18040 [D loss: 0.230099, acc.: 91.41%] [G loss: 7.013072]\n",
            "18060 [D loss: 0.284266, acc.: 87.89%] [G loss: 8.112339]\n",
            "18080 [D loss: 0.255473, acc.: 91.02%] [G loss: 6.249471]\n",
            "18100 [D loss: 0.261279, acc.: 91.41%] [G loss: 6.460731]\n",
            "18120 [D loss: 0.267764, acc.: 89.06%] [G loss: 6.411471]\n",
            "18140 [D loss: 0.292814, acc.: 87.11%] [G loss: 6.593788]\n",
            "18160 [D loss: 0.266989, acc.: 90.23%] [G loss: 7.050136]\n",
            "18180 [D loss: 0.321743, acc.: 86.33%] [G loss: 6.075394]\n",
            "18200 [D loss: 0.291499, acc.: 87.89%] [G loss: 6.707492]\n",
            "18220 [D loss: 0.308820, acc.: 88.28%] [G loss: 6.970116]\n",
            "18240 [D loss: 0.282329, acc.: 89.06%] [G loss: 7.402093]\n",
            "18260 [D loss: 0.297494, acc.: 88.67%] [G loss: 6.963879]\n",
            "18280 [D loss: 0.274287, acc.: 89.45%] [G loss: 7.003953]\n",
            "18300 [D loss: 0.400825, acc.: 82.42%] [G loss: 6.470118]\n",
            "18320 [D loss: 0.268010, acc.: 90.62%] [G loss: 6.609590]\n",
            "18340 [D loss: 0.342248, acc.: 83.98%] [G loss: 6.797677]\n",
            "18360 [D loss: 0.324003, acc.: 87.89%] [G loss: 6.183664]\n",
            "18380 [D loss: 0.257762, acc.: 90.23%] [G loss: 6.828508]\n",
            "18400 [D loss: 0.219036, acc.: 90.62%] [G loss: 6.535859]\n",
            "18420 [D loss: 0.343468, acc.: 87.50%] [G loss: 7.010303]\n",
            "18440 [D loss: 0.249653, acc.: 91.80%] [G loss: 6.728282]\n",
            "18460 [D loss: 0.359365, acc.: 85.16%] [G loss: 6.662474]\n",
            "18480 [D loss: 0.358466, acc.: 83.98%] [G loss: 6.647969]\n",
            "18500 [D loss: 0.312748, acc.: 86.72%] [G loss: 6.574895]\n",
            "18520 [D loss: 0.270020, acc.: 90.62%] [G loss: 7.438589]\n",
            "18540 [D loss: 0.252105, acc.: 90.23%] [G loss: 7.005327]\n",
            "18560 [D loss: 0.305714, acc.: 85.94%] [G loss: 6.439310]\n",
            "18580 [D loss: 0.313843, acc.: 89.45%] [G loss: 6.708886]\n",
            "18600 [D loss: 0.254411, acc.: 90.62%] [G loss: 7.352778]\n",
            "18620 [D loss: 0.318282, acc.: 86.72%] [G loss: 6.469974]\n",
            "18640 [D loss: 0.323889, acc.: 84.38%] [G loss: 6.610515]\n",
            "18660 [D loss: 0.393342, acc.: 79.69%] [G loss: 6.842946]\n",
            "18680 [D loss: 0.292163, acc.: 89.84%] [G loss: 7.363020]\n",
            "18700 [D loss: 0.290877, acc.: 87.50%] [G loss: 6.769838]\n",
            "18720 [D loss: 0.305438, acc.: 85.16%] [G loss: 7.328497]\n",
            "18740 [D loss: 0.284373, acc.: 87.11%] [G loss: 6.975817]\n",
            "18760 [D loss: 0.353611, acc.: 85.94%] [G loss: 7.056044]\n",
            "18780 [D loss: 0.357550, acc.: 85.55%] [G loss: 6.816353]\n",
            "18800 [D loss: 0.247736, acc.: 91.02%] [G loss: 7.550014]\n",
            "18820 [D loss: 0.281577, acc.: 86.33%] [G loss: 6.682001]\n",
            "18840 [D loss: 0.272868, acc.: 89.84%] [G loss: 7.157393]\n",
            "18860 [D loss: 0.269332, acc.: 88.28%] [G loss: 7.090656]\n",
            "18880 [D loss: 0.285463, acc.: 86.72%] [G loss: 6.999803]\n",
            "18900 [D loss: 0.313040, acc.: 85.55%] [G loss: 6.418520]\n",
            "18920 [D loss: 0.368498, acc.: 81.25%] [G loss: 6.521098]\n",
            "18940 [D loss: 0.304389, acc.: 87.50%] [G loss: 6.940094]\n",
            "18960 [D loss: 0.283065, acc.: 87.11%] [G loss: 7.052404]\n",
            "18980 [D loss: 0.295132, acc.: 88.67%] [G loss: 6.572736]\n",
            "19000 [D loss: 0.291899, acc.: 87.50%] [G loss: 7.087654]\n",
            "19020 [D loss: 0.356019, acc.: 82.42%] [G loss: 6.483127]\n",
            "19040 [D loss: 0.282611, acc.: 88.28%] [G loss: 7.356317]\n",
            "19060 [D loss: 0.382195, acc.: 83.98%] [G loss: 6.999265]\n",
            "19080 [D loss: 0.269497, acc.: 88.67%] [G loss: 7.954237]\n",
            "19100 [D loss: 0.282214, acc.: 86.33%] [G loss: 7.736051]\n",
            "19120 [D loss: 0.261499, acc.: 89.06%] [G loss: 7.658673]\n",
            "19140 [D loss: 0.260642, acc.: 91.02%] [G loss: 7.286853]\n",
            "19160 [D loss: 0.294859, acc.: 84.77%] [G loss: 7.167904]\n",
            "19180 [D loss: 0.289966, acc.: 89.06%] [G loss: 6.852365]\n",
            "19200 [D loss: 0.274396, acc.: 91.41%] [G loss: 6.958252]\n",
            "19220 [D loss: 0.230754, acc.: 91.41%] [G loss: 6.977283]\n",
            "19240 [D loss: 0.323417, acc.: 85.16%] [G loss: 7.249591]\n",
            "19260 [D loss: 0.286286, acc.: 88.67%] [G loss: 6.819812]\n",
            "19280 [D loss: 0.261706, acc.: 88.28%] [G loss: 7.043087]\n",
            "19300 [D loss: 0.260803, acc.: 88.67%] [G loss: 7.363311]\n",
            "19320 [D loss: 0.285221, acc.: 87.89%] [G loss: 7.056681]\n",
            "19340 [D loss: 0.341688, acc.: 86.33%] [G loss: 7.389070]\n",
            "19360 [D loss: 0.409665, acc.: 82.03%] [G loss: 7.338928]\n",
            "19380 [D loss: 0.362207, acc.: 82.03%] [G loss: 6.838099]\n",
            "19400 [D loss: 0.296624, acc.: 87.89%] [G loss: 7.098237]\n",
            "19420 [D loss: 0.298330, acc.: 86.33%] [G loss: 6.559735]\n",
            "19440 [D loss: 0.407978, acc.: 82.03%] [G loss: 6.978535]\n",
            "19460 [D loss: 0.311516, acc.: 88.67%] [G loss: 6.825434]\n",
            "19480 [D loss: 0.386622, acc.: 80.86%] [G loss: 7.167571]\n",
            "19500 [D loss: 0.290492, acc.: 90.23%] [G loss: 7.442394]\n",
            "19520 [D loss: 0.344744, acc.: 87.11%] [G loss: 7.094747]\n",
            "19540 [D loss: 0.328426, acc.: 84.38%] [G loss: 6.932734]\n",
            "19560 [D loss: 0.254648, acc.: 87.89%] [G loss: 7.027321]\n",
            "19580 [D loss: 0.290274, acc.: 89.06%] [G loss: 6.759752]\n",
            "19600 [D loss: 0.215400, acc.: 92.19%] [G loss: 7.206299]\n",
            "19620 [D loss: 0.318877, acc.: 85.55%] [G loss: 8.002436]\n",
            "19640 [D loss: 0.306502, acc.: 87.89%] [G loss: 6.409915]\n",
            "19660 [D loss: 0.327878, acc.: 87.11%] [G loss: 7.192157]\n",
            "19680 [D loss: 0.321175, acc.: 86.72%] [G loss: 6.354165]\n",
            "19700 [D loss: 0.308770, acc.: 88.28%] [G loss: 6.502063]\n",
            "19720 [D loss: 0.287045, acc.: 88.67%] [G loss: 7.606859]\n",
            "19740 [D loss: 0.317934, acc.: 85.94%] [G loss: 7.571935]\n",
            "19760 [D loss: 0.334693, acc.: 86.33%] [G loss: 7.275369]\n",
            "19780 [D loss: 0.330592, acc.: 87.11%] [G loss: 7.427366]\n",
            "19800 [D loss: 0.273407, acc.: 89.84%] [G loss: 7.433375]\n",
            "19820 [D loss: 0.261470, acc.: 90.62%] [G loss: 7.342039]\n",
            "19840 [D loss: 0.272103, acc.: 89.84%] [G loss: 7.414045]\n",
            "19860 [D loss: 0.364428, acc.: 83.98%] [G loss: 7.292259]\n",
            "19880 [D loss: 0.210982, acc.: 91.02%] [G loss: 7.878307]\n",
            "19900 [D loss: 0.363220, acc.: 84.38%] [G loss: 7.461666]\n",
            "19940 [D loss: 0.239303, acc.: 91.41%] [G loss: 7.684868]\n",
            "19960 [D loss: 0.189334, acc.: 93.75%] [G loss: 7.649074]\n",
            "19980 [D loss: 0.264161, acc.: 87.50%] [G loss: 7.787716]\n",
            "20000 [D loss: 0.261049, acc.: 89.06%] [G loss: 7.394352]\n",
            "20020 [D loss: 0.264764, acc.: 89.84%] [G loss: 7.217489]\n",
            "20040 [D loss: 0.144921, acc.: 95.70%] [G loss: 5.018394]\n",
            "20060 [D loss: 0.488740, acc.: 78.12%] [G loss: 4.976461]\n",
            "20080 [D loss: 2.385269, acc.: 8.98%] [G loss: 2.327164]\n",
            "20100 [D loss: 0.940379, acc.: 60.94%] [G loss: 4.909222]\n",
            "20120 [D loss: 0.885956, acc.: 65.23%] [G loss: 5.333086]\n",
            "20140 [D loss: 0.845304, acc.: 65.23%] [G loss: 4.656062]\n",
            "20160 [D loss: 0.328175, acc.: 84.77%] [G loss: 5.542826]\n",
            "20180 [D loss: 0.440607, acc.: 77.34%] [G loss: 4.993761]\n",
            "20200 [D loss: 0.473360, acc.: 80.08%] [G loss: 5.327459]\n",
            "20220 [D loss: 0.401172, acc.: 79.30%] [G loss: 4.965593]\n",
            "20240 [D loss: 0.455931, acc.: 75.78%] [G loss: 5.026046]\n",
            "20260 [D loss: 0.364629, acc.: 83.59%] [G loss: 5.425553]\n",
            "20280 [D loss: 0.348830, acc.: 87.50%] [G loss: 5.877284]\n",
            "20300 [D loss: 0.415880, acc.: 80.08%] [G loss: 5.588079]\n",
            "20320 [D loss: 0.384566, acc.: 83.20%] [G loss: 5.827223]\n",
            "20340 [D loss: 0.296986, acc.: 86.72%] [G loss: 5.723245]\n",
            "20360 [D loss: 0.332935, acc.: 86.72%] [G loss: 5.553025]\n",
            "20380 [D loss: 0.315538, acc.: 86.72%] [G loss: 5.244979]\n",
            "20400 [D loss: 0.391462, acc.: 84.38%] [G loss: 5.302299]\n",
            "20420 [D loss: 0.291056, acc.: 89.06%] [G loss: 6.340652]\n",
            "20440 [D loss: 0.324689, acc.: 85.16%] [G loss: 6.234472]\n",
            "20460 [D loss: 0.369106, acc.: 83.98%] [G loss: 6.290020]\n",
            "20480 [D loss: 0.340965, acc.: 84.77%] [G loss: 5.802519]\n",
            "20500 [D loss: 0.350798, acc.: 86.72%] [G loss: 6.143846]\n",
            "20520 [D loss: 0.287706, acc.: 86.72%] [G loss: 5.436300]\n",
            "20540 [D loss: 0.425310, acc.: 81.25%] [G loss: 5.290943]\n",
            "20560 [D loss: 0.272877, acc.: 90.23%] [G loss: 5.692092]\n",
            "20580 [D loss: 0.339586, acc.: 89.06%] [G loss: 5.979916]\n",
            "20600 [D loss: 0.274260, acc.: 88.67%] [G loss: 5.794527]\n",
            "20620 [D loss: 0.296651, acc.: 88.67%] [G loss: 5.731591]\n",
            "20640 [D loss: 0.314928, acc.: 88.28%] [G loss: 5.761903]\n",
            "20660 [D loss: 0.339582, acc.: 84.77%] [G loss: 5.856323]\n",
            "20680 [D loss: 0.283728, acc.: 86.72%] [G loss: 5.812775]\n",
            "20700 [D loss: 0.284448, acc.: 89.84%] [G loss: 6.359479]\n",
            "20720 [D loss: 0.327473, acc.: 86.72%] [G loss: 6.083586]\n",
            "20740 [D loss: 0.353914, acc.: 85.94%] [G loss: 6.218143]\n",
            "20760 [D loss: 0.370168, acc.: 83.98%] [G loss: 6.250820]\n",
            "20780 [D loss: 0.243306, acc.: 91.80%] [G loss: 6.416917]\n",
            "20800 [D loss: 0.267687, acc.: 90.23%] [G loss: 5.897644]\n",
            "20820 [D loss: 0.263433, acc.: 88.28%] [G loss: 6.412097]\n",
            "20840 [D loss: 0.272884, acc.: 87.89%] [G loss: 6.602036]\n",
            "20860 [D loss: 0.389168, acc.: 82.42%] [G loss: 5.859060]\n",
            "20880 [D loss: 0.300036, acc.: 87.89%] [G loss: 6.680660]\n",
            "20900 [D loss: 0.324739, acc.: 85.94%] [G loss: 6.285419]\n",
            "20920 [D loss: 0.380160, acc.: 83.59%] [G loss: 6.785130]\n",
            "20940 [D loss: 0.324008, acc.: 85.55%] [G loss: 5.972684]\n",
            "20960 [D loss: 0.306557, acc.: 86.33%] [G loss: 6.127896]\n",
            "20980 [D loss: 0.280832, acc.: 90.62%] [G loss: 6.662483]\n",
            "21000 [D loss: 0.349130, acc.: 86.33%] [G loss: 6.720431]\n",
            "21020 [D loss: 0.300368, acc.: 85.16%] [G loss: 6.607707]\n",
            "21040 [D loss: 0.323371, acc.: 83.98%] [G loss: 6.226887]\n",
            "21060 [D loss: 0.244549, acc.: 89.84%] [G loss: 6.876099]\n",
            "21080 [D loss: 0.389577, acc.: 83.59%] [G loss: 6.068287]\n",
            "21100 [D loss: 0.399380, acc.: 83.20%] [G loss: 6.157556]\n",
            "21120 [D loss: 0.366099, acc.: 83.98%] [G loss: 6.269849]\n",
            "21140 [D loss: 0.273645, acc.: 87.89%] [G loss: 6.757221]\n",
            "21160 [D loss: 0.231518, acc.: 91.41%] [G loss: 7.067590]\n",
            "21180 [D loss: 0.298612, acc.: 87.89%] [G loss: 7.636885]\n",
            "21200 [D loss: 0.348321, acc.: 85.55%] [G loss: 6.880097]\n",
            "21220 [D loss: 0.317849, acc.: 87.89%] [G loss: 6.846404]\n",
            "21240 [D loss: 0.369945, acc.: 83.20%] [G loss: 6.509558]\n",
            "21260 [D loss: 0.348829, acc.: 87.11%] [G loss: 6.496656]\n",
            "21280 [D loss: 0.263844, acc.: 90.23%] [G loss: 6.938514]\n",
            "21300 [D loss: 0.313774, acc.: 87.50%] [G loss: 6.305480]\n",
            "21320 [D loss: 0.277306, acc.: 88.28%] [G loss: 6.892214]\n",
            "21340 [D loss: 0.259235, acc.: 88.67%] [G loss: 7.312675]\n",
            "21360 [D loss: 0.247180, acc.: 90.62%] [G loss: 6.641213]\n",
            "21380 [D loss: 0.297400, acc.: 88.28%] [G loss: 7.092172]\n",
            "21400 [D loss: 0.268867, acc.: 87.89%] [G loss: 6.661583]\n",
            "21420 [D loss: 0.279677, acc.: 89.84%] [G loss: 7.528040]\n",
            "21440 [D loss: 0.245296, acc.: 90.23%] [G loss: 7.009863]\n",
            "21460 [D loss: 0.309679, acc.: 85.55%] [G loss: 6.720700]\n",
            "21480 [D loss: 0.248070, acc.: 91.02%] [G loss: 7.632874]\n",
            "21500 [D loss: 0.325015, acc.: 85.94%] [G loss: 7.165032]\n",
            "21520 [D loss: 0.340395, acc.: 81.64%] [G loss: 6.919838]\n",
            "21540 [D loss: 0.304120, acc.: 87.11%] [G loss: 6.923682]\n",
            "21560 [D loss: 0.408485, acc.: 83.98%] [G loss: 6.514157]\n",
            "21580 [D loss: 0.240157, acc.: 90.23%] [G loss: 6.888622]\n",
            "21600 [D loss: 0.282304, acc.: 88.28%] [G loss: 7.315640]\n",
            "21620 [D loss: 0.249990, acc.: 88.67%] [G loss: 6.633720]\n",
            "21640 [D loss: 0.292294, acc.: 86.33%] [G loss: 7.169778]\n",
            "21660 [D loss: 0.298486, acc.: 85.94%] [G loss: 6.944489]\n",
            "21680 [D loss: 0.322544, acc.: 85.94%] [G loss: 7.816341]\n",
            "21700 [D loss: 0.214364, acc.: 91.41%] [G loss: 5.686368]\n",
            "21720 [D loss: 0.198977, acc.: 92.19%] [G loss: 4.336667]\n",
            "21740 [D loss: 0.349159, acc.: 85.16%] [G loss: 3.480807]\n",
            "21760 [D loss: 0.551820, acc.: 75.39%] [G loss: 8.429498]\n",
            "21780 [D loss: 0.891951, acc.: 64.84%] [G loss: 7.417076]\n",
            "21800 [D loss: 0.355071, acc.: 83.98%] [G loss: 6.741021]\n",
            "21820 [D loss: 0.214960, acc.: 91.80%] [G loss: 7.646620]\n",
            "21840 [D loss: 0.276398, acc.: 87.50%] [G loss: 6.695137]\n",
            "21880 [D loss: 0.251436, acc.: 90.62%] [G loss: 7.587977]\n",
            "21900 [D loss: 0.285199, acc.: 87.89%] [G loss: 7.178042]\n",
            "21920 [D loss: 0.246087, acc.: 89.06%] [G loss: 7.457529]\n",
            "21940 [D loss: 0.258880, acc.: 88.67%] [G loss: 7.221097]\n",
            "21960 [D loss: 0.231858, acc.: 94.14%] [G loss: 7.638107]\n",
            "21980 [D loss: 0.258279, acc.: 89.45%] [G loss: 7.031470]\n",
            "22000 [D loss: 0.222432, acc.: 92.97%] [G loss: 7.525009]\n",
            "22020 [D loss: 0.281100, acc.: 88.28%] [G loss: 7.324027]\n",
            "22040 [D loss: 0.245810, acc.: 89.45%] [G loss: 6.796433]\n",
            "22060 [D loss: 0.244308, acc.: 88.67%] [G loss: 8.132950]\n",
            "22080 [D loss: 0.215846, acc.: 91.80%] [G loss: 7.701718]\n",
            "22100 [D loss: 0.246151, acc.: 89.45%] [G loss: 7.259754]\n",
            "22120 [D loss: 0.336770, acc.: 86.33%] [G loss: 7.054591]\n",
            "22140 [D loss: 0.274729, acc.: 90.23%] [G loss: 7.199208]\n",
            "22160 [D loss: 0.281878, acc.: 87.11%] [G loss: 7.373280]\n",
            "22180 [D loss: 0.226934, acc.: 90.62%] [G loss: 7.192511]\n",
            "22200 [D loss: 0.278665, acc.: 88.67%] [G loss: 7.232546]\n",
            "22220 [D loss: 0.212837, acc.: 91.80%] [G loss: 7.602685]\n",
            "22240 [D loss: 0.294814, acc.: 88.28%] [G loss: 8.049704]\n",
            "22260 [D loss: 0.274929, acc.: 88.28%] [G loss: 7.977842]\n",
            "22280 [D loss: 0.259623, acc.: 90.62%] [G loss: 7.798000]\n",
            "22300 [D loss: 0.171272, acc.: 94.53%] [G loss: 8.199299]\n",
            "22320 [D loss: 0.250308, acc.: 91.02%] [G loss: 7.918779]\n",
            "22340 [D loss: 0.225671, acc.: 91.02%] [G loss: 6.758894]\n",
            "22360 [D loss: 0.299455, acc.: 86.72%] [G loss: 7.344421]\n",
            "22380 [D loss: 0.193696, acc.: 92.19%] [G loss: 7.705620]\n",
            "22400 [D loss: 0.266014, acc.: 89.45%] [G loss: 7.830427]\n",
            "22420 [D loss: 0.207001, acc.: 92.19%] [G loss: 7.712625]\n",
            "22440 [D loss: 0.253505, acc.: 90.23%] [G loss: 7.877544]\n",
            "22460 [D loss: 0.283372, acc.: 87.89%] [G loss: 8.196737]\n",
            "22480 [D loss: 0.385191, acc.: 82.81%] [G loss: 7.486884]\n",
            "22500 [D loss: 0.307016, acc.: 87.11%] [G loss: 7.586644]\n",
            "22520 [D loss: 0.403245, acc.: 82.03%] [G loss: 7.726365]\n",
            "22540 [D loss: 0.286785, acc.: 88.67%] [G loss: 7.844892]\n",
            "22560 [D loss: 0.229887, acc.: 91.02%] [G loss: 9.187434]\n",
            "22580 [D loss: 0.342818, acc.: 83.98%] [G loss: 7.422257]\n",
            "22600 [D loss: 0.251440, acc.: 90.23%] [G loss: 7.913055]\n",
            "22620 [D loss: 0.273198, acc.: 88.28%] [G loss: 7.114897]\n",
            "22640 [D loss: 0.324042, acc.: 85.16%] [G loss: 7.565695]\n",
            "22660 [D loss: 0.222391, acc.: 91.02%] [G loss: 7.961683]\n",
            "22680 [D loss: 0.213830, acc.: 91.02%] [G loss: 8.121124]\n",
            "22700 [D loss: 0.159675, acc.: 93.75%] [G loss: 8.314659]\n",
            "22720 [D loss: 0.280332, acc.: 85.94%] [G loss: 8.228685]\n",
            "22740 [D loss: 0.268958, acc.: 90.62%] [G loss: 7.953689]\n",
            "22760 [D loss: 0.296595, acc.: 87.11%] [G loss: 8.082085]\n",
            "22780 [D loss: 0.293258, acc.: 86.72%] [G loss: 7.736402]\n",
            "22800 [D loss: 0.283422, acc.: 88.67%] [G loss: 7.498943]\n",
            "22820 [D loss: 0.237013, acc.: 91.41%] [G loss: 8.746961]\n",
            "22840 [D loss: 0.334311, acc.: 84.77%] [G loss: 7.689592]\n",
            "22860 [D loss: 0.221228, acc.: 92.19%] [G loss: 7.724787]\n",
            "22880 [D loss: 0.312439, acc.: 87.50%] [G loss: 7.937188]\n",
            "22900 [D loss: 0.235765, acc.: 91.80%] [G loss: 7.885759]\n",
            "22920 [D loss: 0.203948, acc.: 91.41%] [G loss: 7.510289]\n",
            "22940 [D loss: 0.180720, acc.: 94.14%] [G loss: 8.797070]\n",
            "22960 [D loss: 0.202826, acc.: 92.97%] [G loss: 7.545145]\n",
            "22980 [D loss: 0.195957, acc.: 91.41%] [G loss: 8.743526]\n",
            "23000 [D loss: 0.233755, acc.: 90.62%] [G loss: 8.290321]\n",
            "23020 [D loss: 0.194150, acc.: 90.62%] [G loss: 7.819876]\n",
            "23040 [D loss: 0.163489, acc.: 92.97%] [G loss: 7.873639]\n",
            "23060 [D loss: 0.336253, acc.: 83.20%] [G loss: 7.818745]\n",
            "23080 [D loss: 0.226147, acc.: 92.97%] [G loss: 7.925941]\n",
            "23100 [D loss: 0.302964, acc.: 85.94%] [G loss: 8.782681]\n",
            "23120 [D loss: 0.296658, acc.: 85.94%] [G loss: 8.758696]\n",
            "23140 [D loss: 0.202276, acc.: 93.36%] [G loss: 8.305870]\n",
            "23160 [D loss: 0.250036, acc.: 91.02%] [G loss: 7.851731]\n",
            "23180 [D loss: 0.279052, acc.: 87.11%] [G loss: 7.881948]\n",
            "23200 [D loss: 0.226329, acc.: 92.19%] [G loss: 8.044117]\n",
            "23220 [D loss: 0.271977, acc.: 88.67%] [G loss: 8.586827]\n",
            "23240 [D loss: 0.254139, acc.: 90.62%] [G loss: 8.409324]\n",
            "23260 [D loss: 0.198394, acc.: 94.53%] [G loss: 7.900352]\n",
            "23280 [D loss: 0.216118, acc.: 91.02%] [G loss: 8.398044]\n",
            "23300 [D loss: 0.134746, acc.: 94.53%] [G loss: 8.964397]\n",
            "23320 [D loss: 0.290872, acc.: 87.89%] [G loss: 8.601435]\n",
            "23340 [D loss: 0.188291, acc.: 92.58%] [G loss: 7.980968]\n",
            "23360 [D loss: 0.257472, acc.: 89.45%] [G loss: 7.721589]\n",
            "23380 [D loss: 0.328513, acc.: 82.81%] [G loss: 8.287378]\n",
            "23400 [D loss: 0.250872, acc.: 88.67%] [G loss: 7.834339]\n",
            "23420 [D loss: 0.273318, acc.: 86.72%] [G loss: 7.820124]\n",
            "23440 [D loss: 0.152569, acc.: 95.31%] [G loss: 9.371841]\n",
            "23460 [D loss: 0.235946, acc.: 88.28%] [G loss: 8.586165]\n",
            "23480 [D loss: 0.206030, acc.: 94.53%] [G loss: 8.428225]\n",
            "23500 [D loss: 0.235539, acc.: 92.19%] [G loss: 7.347260]\n",
            "23520 [D loss: 0.231601, acc.: 91.41%] [G loss: 8.055747]\n",
            "23540 [D loss: 0.204788, acc.: 92.58%] [G loss: 8.161834]\n",
            "23560 [D loss: 0.275076, acc.: 87.50%] [G loss: 8.167305]\n",
            "23580 [D loss: 0.173200, acc.: 93.36%] [G loss: 8.919985]\n",
            "23600 [D loss: 0.333698, acc.: 84.38%] [G loss: 8.170837]\n",
            "23620 [D loss: 0.207717, acc.: 91.80%] [G loss: 8.763773]\n",
            "23640 [D loss: 0.277043, acc.: 88.67%] [G loss: 7.918528]\n",
            "23660 [D loss: 0.227906, acc.: 91.41%] [G loss: 8.295185]\n",
            "23680 [D loss: 0.346558, acc.: 87.11%] [G loss: 8.802416]\n",
            "23700 [D loss: 0.288382, acc.: 89.06%] [G loss: 7.923646]\n",
            "23720 [D loss: 0.278424, acc.: 87.11%] [G loss: 7.812220]\n",
            "23740 [D loss: 0.256741, acc.: 89.84%] [G loss: 8.190028]\n",
            "23760 [D loss: 0.258511, acc.: 91.02%] [G loss: 8.508968]\n",
            "23780 [D loss: 0.300621, acc.: 85.55%] [G loss: 8.700085]\n",
            "23800 [D loss: 0.269737, acc.: 88.67%] [G loss: 8.275565]\n",
            "23820 [D loss: 0.282427, acc.: 87.89%] [G loss: 8.716127]\n",
            "23840 [D loss: 0.231444, acc.: 91.41%] [G loss: 8.751951]\n",
            "23860 [D loss: 0.280520, acc.: 91.02%] [G loss: 9.083691]\n",
            "23880 [D loss: 0.235287, acc.: 91.41%] [G loss: 8.663231]\n",
            "23900 [D loss: 0.339649, acc.: 84.38%] [G loss: 7.504812]\n",
            "23920 [D loss: 0.407126, acc.: 80.47%] [G loss: 9.312900]\n",
            "23940 [D loss: 0.187299, acc.: 92.58%] [G loss: 9.032900]\n",
            "23960 [D loss: 0.275979, acc.: 89.84%] [G loss: 8.553559]\n",
            "23980 [D loss: 0.301561, acc.: 89.06%] [G loss: 8.624828]\n",
            "24000 [D loss: 0.240710, acc.: 91.02%] [G loss: 8.322075]\n",
            "24020 [D loss: 0.222134, acc.: 89.84%] [G loss: 8.653246]\n",
            "24040 [D loss: 0.204753, acc.: 93.36%] [G loss: 8.668134]\n",
            "24060 [D loss: 0.218845, acc.: 91.80%] [G loss: 9.342379]\n",
            "24080 [D loss: 0.230190, acc.: 91.80%] [G loss: 9.534048]\n",
            "24100 [D loss: 0.310573, acc.: 87.50%] [G loss: 7.328496]\n",
            "24120 [D loss: 0.186105, acc.: 93.75%] [G loss: 8.873180]\n",
            "24140 [D loss: 0.279405, acc.: 87.50%] [G loss: 8.291639]\n",
            "24160 [D loss: 0.396138, acc.: 82.81%] [G loss: 8.606043]\n",
            "24180 [D loss: 0.353439, acc.: 86.33%] [G loss: 8.925816]\n",
            "24200 [D loss: 0.162773, acc.: 93.75%] [G loss: 9.056076]\n",
            "24220 [D loss: 0.199635, acc.: 92.58%] [G loss: 8.182907]\n",
            "24240 [D loss: 0.190879, acc.: 94.53%] [G loss: 8.079544]\n",
            "24260 [D loss: 0.298788, acc.: 86.33%] [G loss: 8.880445]\n",
            "24280 [D loss: 0.207121, acc.: 91.02%] [G loss: 8.732863]\n",
            "24300 [D loss: 0.257709, acc.: 88.67%] [G loss: 7.651498]\n",
            "24320 [D loss: 0.254015, acc.: 91.02%] [G loss: 7.516818]\n",
            "24340 [D loss: 0.313171, acc.: 87.50%] [G loss: 9.013186]\n",
            "24360 [D loss: 0.160118, acc.: 92.97%] [G loss: 4.892963]\n",
            "24380 [D loss: 0.174135, acc.: 92.58%] [G loss: 4.875359]\n",
            "24420 [D loss: 0.422685, acc.: 83.59%] [G loss: 9.367802]\n",
            "24440 [D loss: 0.265998, acc.: 88.28%] [G loss: 8.756659]\n",
            "24460 [D loss: 0.296700, acc.: 89.06%] [G loss: 7.839568]\n",
            "24480 [D loss: 0.244706, acc.: 91.02%] [G loss: 10.725361]\n",
            "24500 [D loss: 0.155725, acc.: 92.97%] [G loss: 9.262973]\n",
            "24520 [D loss: 0.209591, acc.: 91.02%] [G loss: 9.587561]\n",
            "24540 [D loss: 0.285615, acc.: 88.28%] [G loss: 8.730492]\n",
            "24560 [D loss: 0.273657, acc.: 92.19%] [G loss: 9.765064]\n",
            "24580 [D loss: 0.220102, acc.: 91.41%] [G loss: 9.159050]\n",
            "24600 [D loss: 0.214529, acc.: 92.19%] [G loss: 8.656404]\n",
            "24620 [D loss: 0.301614, acc.: 83.59%] [G loss: 9.051971]\n",
            "24640 [D loss: 0.260818, acc.: 87.89%] [G loss: 9.097897]\n",
            "24660 [D loss: 0.232316, acc.: 91.41%] [G loss: 10.250191]\n",
            "24680 [D loss: 0.222408, acc.: 91.80%] [G loss: 8.741017]\n",
            "24700 [D loss: 0.181050, acc.: 93.75%] [G loss: 8.466145]\n",
            "24720 [D loss: 0.205563, acc.: 90.62%] [G loss: 9.071373]\n",
            "24740 [D loss: 0.163998, acc.: 93.75%] [G loss: 9.211880]\n",
            "24760 [D loss: 0.229200, acc.: 91.02%] [G loss: 9.321847]\n",
            "24780 [D loss: 0.165347, acc.: 93.36%] [G loss: 9.774652]\n",
            "24800 [D loss: 0.240266, acc.: 90.62%] [G loss: 9.053348]\n",
            "24820 [D loss: 0.178227, acc.: 93.75%] [G loss: 8.597061]\n",
            "24840 [D loss: 0.179800, acc.: 94.53%] [G loss: 9.597792]\n",
            "24860 [D loss: 0.210327, acc.: 89.84%] [G loss: 9.379044]\n",
            "24880 [D loss: 0.222661, acc.: 91.80%] [G loss: 9.628002]\n",
            "24900 [D loss: 0.210630, acc.: 91.41%] [G loss: 8.760813]\n",
            "24920 [D loss: 0.305450, acc.: 87.11%] [G loss: 9.086036]\n",
            "24940 [D loss: 0.189613, acc.: 93.36%] [G loss: 8.443020]\n",
            "24960 [D loss: 0.203153, acc.: 92.58%] [G loss: 10.124025]\n",
            "24980 [D loss: 0.195432, acc.: 91.80%] [G loss: 9.211709]\n",
            "25000 [D loss: 0.260992, acc.: 90.62%] [G loss: 9.134839]\n",
            "25020 [D loss: 0.194710, acc.: 92.97%] [G loss: 9.418228]\n",
            "25040 [D loss: 0.160767, acc.: 94.53%] [G loss: 9.523191]\n",
            "25060 [D loss: 0.204916, acc.: 91.41%] [G loss: 8.532715]\n",
            "25080 [D loss: 0.213327, acc.: 91.02%] [G loss: 8.266965]\n",
            "25100 [D loss: 0.261280, acc.: 88.67%] [G loss: 8.922233]\n",
            "25120 [D loss: 0.202521, acc.: 94.53%] [G loss: 8.882904]\n",
            "25140 [D loss: 0.181125, acc.: 92.97%] [G loss: 10.187512]\n",
            "25160 [D loss: 0.222436, acc.: 90.23%] [G loss: 9.383881]\n",
            "25180 [D loss: 0.164124, acc.: 94.92%] [G loss: 9.217117]\n",
            "25200 [D loss: 0.188124, acc.: 92.58%] [G loss: 9.661703]\n",
            "25220 [D loss: 0.157766, acc.: 95.31%] [G loss: 9.713786]\n",
            "25240 [D loss: 0.152192, acc.: 93.75%] [G loss: 9.928401]\n",
            "25260 [D loss: 0.224296, acc.: 91.41%] [G loss: 8.288989]\n",
            "25280 [D loss: 0.276405, acc.: 88.67%] [G loss: 9.338710]\n",
            "25300 [D loss: 0.322050, acc.: 84.77%] [G loss: 8.613861]\n",
            "25320 [D loss: 0.221226, acc.: 92.19%] [G loss: 9.055615]\n",
            "25340 [D loss: 0.230264, acc.: 91.02%] [G loss: 8.693359]\n",
            "25360 [D loss: 0.243164, acc.: 90.62%] [G loss: 8.376949]\n",
            "25380 [D loss: 0.196150, acc.: 92.19%] [G loss: 9.312149]\n",
            "25400 [D loss: 0.217702, acc.: 90.62%] [G loss: 8.852909]\n",
            "25420 [D loss: 0.200544, acc.: 92.19%] [G loss: 8.931813]\n",
            "25440 [D loss: 0.249835, acc.: 89.45%] [G loss: 8.727542]\n",
            "25460 [D loss: 0.236226, acc.: 91.02%] [G loss: 9.374200]\n",
            "25480 [D loss: 0.183921, acc.: 92.97%] [G loss: 9.635994]\n",
            "25500 [D loss: 0.287484, acc.: 89.06%] [G loss: 9.883148]\n",
            "25520 [D loss: 0.168230, acc.: 94.53%] [G loss: 9.725224]\n",
            "25540 [D loss: 0.199279, acc.: 91.41%] [G loss: 9.771458]\n",
            "25560 [D loss: 0.239953, acc.: 91.80%] [G loss: 8.649425]\n",
            "25580 [D loss: 0.276756, acc.: 88.28%] [G loss: 9.194493]\n",
            "25600 [D loss: 0.301279, acc.: 89.06%] [G loss: 8.783655]\n",
            "25620 [D loss: 0.180440, acc.: 93.75%] [G loss: 9.727755]\n",
            "25640 [D loss: 0.287146, acc.: 86.72%] [G loss: 9.519499]\n",
            "25660 [D loss: 0.196662, acc.: 91.02%] [G loss: 10.352064]\n",
            "25680 [D loss: 0.177313, acc.: 92.97%] [G loss: 9.301270]\n",
            "25700 [D loss: 0.309551, acc.: 85.16%] [G loss: 10.021919]\n",
            "25720 [D loss: 0.222263, acc.: 91.80%] [G loss: 10.088160]\n",
            "25740 [D loss: 0.236860, acc.: 89.84%] [G loss: 9.327660]\n",
            "25760 [D loss: 0.163200, acc.: 94.14%] [G loss: 9.456661]\n",
            "25780 [D loss: 0.236560, acc.: 90.23%] [G loss: 9.421819]\n",
            "25800 [D loss: 0.266869, acc.: 89.06%] [G loss: 9.089969]\n",
            "25820 [D loss: 0.198569, acc.: 92.19%] [G loss: 9.676268]\n",
            "25840 [D loss: 0.187231, acc.: 91.02%] [G loss: 9.421854]\n",
            "25860 [D loss: 0.177796, acc.: 93.75%] [G loss: 8.995964]\n",
            "25880 [D loss: 0.192002, acc.: 93.36%] [G loss: 9.324440]\n",
            "25900 [D loss: 0.213182, acc.: 90.62%] [G loss: 9.303729]\n",
            "25920 [D loss: 0.222082, acc.: 89.45%] [G loss: 10.000862]\n",
            "25940 [D loss: 0.141468, acc.: 93.36%] [G loss: 10.695940]\n",
            "25960 [D loss: 0.172677, acc.: 94.53%] [G loss: 9.894030]\n",
            "25980 [D loss: 0.219180, acc.: 90.23%] [G loss: 9.213024]\n",
            "26000 [D loss: 0.152782, acc.: 94.14%] [G loss: 9.580263]\n",
            "26020 [D loss: 0.230521, acc.: 91.02%] [G loss: 9.520958]\n",
            "26040 [D loss: 0.210180, acc.: 91.41%] [G loss: 10.781820]\n",
            "26060 [D loss: 0.249049, acc.: 89.45%] [G loss: 9.480577]\n",
            "26080 [D loss: 0.230313, acc.: 91.41%] [G loss: 9.348693]\n",
            "26100 [D loss: 0.226671, acc.: 91.80%] [G loss: 9.565065]\n",
            "26120 [D loss: 0.200450, acc.: 92.97%] [G loss: 8.198523]\n",
            "26140 [D loss: 0.206964, acc.: 91.80%] [G loss: 10.105719]\n",
            "26160 [D loss: 0.172865, acc.: 92.19%] [G loss: 8.967367]\n",
            "26180 [D loss: 0.249648, acc.: 89.06%] [G loss: 9.910758]\n",
            "26200 [D loss: 0.223538, acc.: 92.19%] [G loss: 8.782282]\n",
            "26220 [D loss: 0.168038, acc.: 93.75%] [G loss: 9.713888]\n",
            "26240 [D loss: 0.223632, acc.: 91.41%] [G loss: 9.242504]\n",
            "26260 [D loss: 0.307698, acc.: 85.16%] [G loss: 9.790409]\n",
            "26280 [D loss: 0.244870, acc.: 90.23%] [G loss: 10.681704]\n",
            "26300 [D loss: 0.213096, acc.: 91.02%] [G loss: 8.966144]\n",
            "26320 [D loss: 0.300977, acc.: 87.11%] [G loss: 9.807781]\n",
            "26340 [D loss: 0.187055, acc.: 92.19%] [G loss: 8.817966]\n",
            "26360 [D loss: 0.156087, acc.: 94.92%] [G loss: 10.032765]\n",
            "26380 [D loss: 0.175118, acc.: 94.53%] [G loss: 9.591686]\n",
            "26400 [D loss: 0.167168, acc.: 93.75%] [G loss: 9.897590]\n",
            "26420 [D loss: 0.138314, acc.: 94.53%] [G loss: 9.604889]\n",
            "26440 [D loss: 0.264658, acc.: 89.45%] [G loss: 9.759495]\n",
            "26460 [D loss: 0.179671, acc.: 94.14%] [G loss: 9.663360]\n",
            "26480 [D loss: 0.197237, acc.: 91.02%] [G loss: 9.016273]\n",
            "26500 [D loss: 0.181467, acc.: 92.97%] [G loss: 9.682325]\n",
            "26520 [D loss: 0.285468, acc.: 88.67%] [G loss: 9.682228]\n",
            "26540 [D loss: 0.216429, acc.: 89.84%] [G loss: 10.197935]\n",
            "26560 [D loss: 0.114699, acc.: 97.27%] [G loss: 9.089302]\n",
            "26580 [D loss: 0.307304, acc.: 86.72%] [G loss: 9.725693]\n",
            "26600 [D loss: 0.161576, acc.: 93.75%] [G loss: 10.116444]\n",
            "26620 [D loss: 0.239511, acc.: 91.41%] [G loss: 9.331236]\n",
            "26640 [D loss: 0.165626, acc.: 94.14%] [G loss: 10.100239]\n",
            "26660 [D loss: 0.199263, acc.: 93.75%] [G loss: 10.316298]\n",
            "26700 [D loss: 0.220515, acc.: 91.41%] [G loss: 10.090714]\n",
            "26720 [D loss: 0.228167, acc.: 91.80%] [G loss: 9.160791]\n",
            "26740 [D loss: 0.202211, acc.: 91.80%] [G loss: 9.818628]\n",
            "26760 [D loss: 0.228996, acc.: 90.23%] [G loss: 9.795611]\n",
            "26780 [D loss: 0.261448, acc.: 87.11%] [G loss: 10.639676]\n",
            "26800 [D loss: 0.205146, acc.: 91.41%] [G loss: 9.519144]\n",
            "26820 [D loss: 0.213152, acc.: 91.02%] [G loss: 8.575230]\n",
            "26840 [D loss: 0.176037, acc.: 92.19%] [G loss: 10.105502]\n",
            "26860 [D loss: 0.305190, acc.: 85.16%] [G loss: 10.785644]\n",
            "26880 [D loss: 0.281535, acc.: 87.50%] [G loss: 10.168644]\n",
            "26900 [D loss: 0.201264, acc.: 91.41%] [G loss: 9.465640]\n",
            "26920 [D loss: 0.257259, acc.: 89.84%] [G loss: 9.566507]\n",
            "26940 [D loss: 0.143792, acc.: 94.53%] [G loss: 10.955714]\n",
            "26960 [D loss: 0.247987, acc.: 90.62%] [G loss: 9.573765]\n",
            "26980 [D loss: 0.275965, acc.: 87.50%] [G loss: 11.775373]\n",
            "27000 [D loss: 0.160484, acc.: 94.92%] [G loss: 9.245687]\n",
            "27020 [D loss: 0.162819, acc.: 93.36%] [G loss: 9.961789]\n",
            "27040 [D loss: 0.132214, acc.: 94.14%] [G loss: 10.474703]\n",
            "27060 [D loss: 0.242136, acc.: 89.84%] [G loss: 8.548691]\n",
            "27080 [D loss: 0.203016, acc.: 89.45%] [G loss: 10.779166]\n",
            "27100 [D loss: 0.305393, acc.: 87.89%] [G loss: 10.660151]\n",
            "27120 [D loss: 0.165083, acc.: 93.36%] [G loss: 9.663431]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}