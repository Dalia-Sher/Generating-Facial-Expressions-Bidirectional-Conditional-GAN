{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "9.Wasserstein_GAN_GP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOVvrUGPVFBx"
      },
      "source": [
        "code based on https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHB1NhbdvOzn"
      },
      "source": [
        "# from __future__ import print_function, division\n",
        "\n",
        "\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers import MaxPooling2D, concatenate, Conv2DTranspose \n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import losses\n",
        "from keras.utils import to_categorical\n",
        "from functools import partial\n",
        "\n",
        "import keras.backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWNlt1yg9wv"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfAYPg_71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d0b7f45b-e2e0-4059-bdcf-be2435873ee9"
      },
      "source": [
        "data = pd.read_csv('./fer2013.csv')\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "      <th>Usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "      <td>Training</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion                                             pixels     Usage\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCW5O-J-hGaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429797ad-362a-4bc5-f850-1383e44ab452"
      },
      "source": [
        "data.emotion.value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "6    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "1     547\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLasjF73EZg1",
        "outputId": "26e17c37-a1b9-4717-99e4-32a1c08308ce"
      },
      "source": [
        "data = data[data.emotion != 1]\n",
        "data['emotion'] = data.emotion.replace(6, 1)\n",
        "\n",
        "data.emotion.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    8989\n",
              "1    6198\n",
              "4    6077\n",
              "2    5121\n",
              "0    4953\n",
              "5    4002\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ggwA1k6hIns"
      },
      "source": [
        "num_classes = 6\n",
        "img_width = 48\n",
        "img_height = 48"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6b-fw-3VFB6",
        "outputId": "263a3934-f165-4fd5-a297-ec2be21a9eb2"
      },
      "source": [
        "X = data['pixels']\n",
        "y = data['emotion']\n",
        "\n",
        "X_train = []\n",
        "for i in X:\n",
        "    X_train.append([int(j) for j in i.split()])\n",
        "\n",
        "X_train = np.array(X_train)/255.0\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "\n",
        "print(X_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(35340, 48, 48, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85p02tc0KiTA"
      },
      "source": [
        "## WGAN_GP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4fAs1IqEEw"
      },
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "disable_eager_execution()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21YgioTHuW_X"
      },
      "source": [
        "class RandomWeightedAverage(Concatenate):\n",
        "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
        "    def _merge_function(self, inputs):\n",
        "        alpha = K.random_uniform((128, 48, 48, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlRf5YdauhSr"
      },
      "source": [
        "class WGANGP():\n",
        "    def __init__(self):\n",
        "        self.img_rows = 48\n",
        "        self.img_cols = 48\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        # Following parameter and optimizer set as recommended in paper\n",
        "        self.n_critic = 5\n",
        "        optimizer = RMSprop(lr=0.00005)\n",
        "\n",
        "        # Build the generator and critic\n",
        "        self.generator = self.build_generator()\n",
        "        self.critic = self.build_critic()\n",
        "\n",
        "        #-------------------------------\n",
        "        # Construct Computational Graph\n",
        "        #       for the Critic\n",
        "        #-------------------------------\n",
        "\n",
        "        # Freeze generator's layers while training critic\n",
        "        self.generator.trainable = False\n",
        "\n",
        "        # Image input (real sample)\n",
        "        real_img = Input(shape=self.img_shape)\n",
        "\n",
        "        # Noise input\n",
        "        z_disc = Input(shape=(self.latent_dim,))\n",
        "        # Generate image based of noise (fake sample)\n",
        "        fake_img = self.generator(z_disc)\n",
        "\n",
        "        # Discriminator determines validity of the real and fake images\n",
        "        fake = self.critic(fake_img)\n",
        "        valid = self.critic(real_img)\n",
        "\n",
        "        # Construct weighted average between real and fake images\n",
        "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
        "        # Determine validity of weighted sample\n",
        "        validity_interpolated = self.critic(interpolated_img)\n",
        "\n",
        "        # Use Python partial to provide loss function with additional\n",
        "        # 'averaged_samples' argument\n",
        "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
        "                          averaged_samples=interpolated_img)\n",
        "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
        "\n",
        "        self.critic_model = Model(inputs=[real_img, z_disc],\n",
        "                            outputs=[valid, fake, validity_interpolated])\n",
        "        # self.critic_model.add_loss([self.wasserstein_loss,\n",
        "        #                             self.wasserstein_loss, partial_gp_loss])\n",
        "\n",
        "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
        "                                              self.wasserstein_loss,\n",
        "                                              partial_gp_loss],\n",
        "                                        optimizer=optimizer,\n",
        "                                        loss_weights=[1, 1, 10],\n",
        "                                  experimental_run_tf_function=False)\n",
        "        #-------------------------------\n",
        "        # Construct Computational Graph\n",
        "        #         for Generator\n",
        "        #-------------------------------\n",
        "\n",
        "        # For the generator we freeze the critic's layers\n",
        "        self.critic.trainable = False\n",
        "        self.generator.trainable = True\n",
        "\n",
        "        # Sampled noise for input to generator\n",
        "        z_gen = Input(shape=(self.latent_dim,))\n",
        "        # Generate images based of noise\n",
        "        img = self.generator(z_gen)\n",
        "        # Discriminator determines validity\n",
        "        valid = self.critic(img)\n",
        "        # Defines generator model\n",
        "        self.generator_model = Model(z_gen, valid)\n",
        "        # self.generator_model.add_loss(self.wasserstein_loss)\n",
        "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
        "        \"\"\"\n",
        "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
        "        \"\"\"\n",
        "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
        "        # compute the euclidean norm by squaring ...\n",
        "        gradients_sqr = K.square(gradients)\n",
        "        #   ... summing over the rows ...\n",
        "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
        "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
        "        #   ... and sqrt\n",
        "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
        "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
        "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
        "        # return the mean as loss over all the batch samples\n",
        "        return K.mean(gradient_penalty)\n",
        "\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return K.mean(y_true * y_pred)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 12 * 12, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((12, 12, 128)))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        print(\"generator\")\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        img = model(noise)\n",
        "\n",
        "        return Model(noise, img)\n",
        "\n",
        "    def build_critic(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        print(\"critic\")\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "        validity = model(img)\n",
        "\n",
        "        return Model(img, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size, sample_interval=50):\n",
        "\n",
        "        # # Load the dataset\n",
        "        # (X_train, _), (_, _) = mnist.load_data()\n",
        "\n",
        "        # # Rescale -1 to 1\n",
        "        # X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        # X_train = np.expand_dims(X_train, axis=3)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = -np.ones((batch_size, 1))\n",
        "        fake =  np.ones((batch_size, 1))\n",
        "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            for _ in range(self.n_critic):\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                # Select a random batch of images\n",
        "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "                imgs = X_train[idx]\n",
        "                # Sample generator input\n",
        "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "                # Train the critic\n",
        "                d_loss = self.critic_model.train_on_batch([imgs, noise], [valid, fake, dummy])\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
        "\n",
        "            # Plot the progress\n",
        "            if epoch%20 == 0:\n",
        "                print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 5, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        gen_imgs = self.generator.predict(noise)\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images/epoch_%d.png\" % epoch)\n",
        "        plt.close()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEPJLd7AqMxL",
        "outputId": "3af4ca51-88ab-4e5b-ce35-793a4d5c945c"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    wgan_gp = WGANGP()\n",
        "    wgan_gp.train(epochs=10000, batch_size=128, sample_interval=200)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generator\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_14 (Dense)             (None, 18432)             1861632   \n",
            "_________________________________________________________________\n",
            "reshape_7 (Reshape)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_14 (UpSampling (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_49 (Conv2D)           (None, 24, 24, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_15 (UpSampling (None, 48, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_50 (Conv2D)           (None, 48, 48, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_36 (Batc (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_51 (Conv2D)           (None, 48, 48, 1)         1025      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 48, 48, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,256,833\n",
            "Trainable params: 2,256,449\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "critic\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_52 (Conv2D)           (None, 24, 24, 16)        160       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_28 (LeakyReLU)   (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         (None, 24, 24, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 12, 12, 32)        4640      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_7 (ZeroPaddin (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_37 (Batc (None, 13, 13, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_29 (LeakyReLU)   (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_38 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_30 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_55 (Conv2D)           (None, 7, 7, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_39 (Batc (None, 7, 7, 128)         512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_31 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 6273      \n",
            "=================================================================\n",
            "Total params: 104,321\n",
            "Trainable params: 103,873\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "0 [D loss: 9.495295] [G loss: 1.023715]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20 [D loss: 10.323456] [G loss: -3.648062]\n",
            "40 [D loss: 9.202007] [G loss: -5.434942]\n",
            "60 [D loss: 5.750023] [G loss: -4.073890]\n",
            "80 [D loss: -1.440470] [G loss: -3.245393]\n",
            "100 [D loss: -12.648176] [G loss: -2.058782]\n",
            "120 [D loss: -37.683044] [G loss: 1.311417]\n",
            "140 [D loss: -53.469151] [G loss: -1.116182]\n",
            "160 [D loss: -77.707054] [G loss: -6.786216]\n",
            "180 [D loss: -110.442947] [G loss: -6.915751]\n",
            "200 [D loss: -146.332642] [G loss: -10.820290]\n",
            "220 [D loss: -169.032028] [G loss: -5.226610]\n",
            "240 [D loss: -203.906296] [G loss: -5.610369]\n",
            "260 [D loss: -222.055893] [G loss: -3.312910]\n",
            "280 [D loss: -240.605392] [G loss: -16.274292]\n",
            "300 [D loss: -264.981384] [G loss: -21.393461]\n",
            "320 [D loss: -276.690338] [G loss: -48.723793]\n",
            "340 [D loss: -105.930267] [G loss: -116.094818]\n",
            "360 [D loss: -108.575073] [G loss: -51.671501]\n",
            "380 [D loss: -223.182785] [G loss: -44.302612]\n",
            "400 [D loss: -331.148132] [G loss: -35.816662]\n",
            "420 [D loss: -400.486084] [G loss: -49.542496]\n",
            "440 [D loss: -422.026978] [G loss: -27.809633]\n",
            "460 [D loss: -438.288177] [G loss: -49.732777]\n",
            "480 [D loss: -504.008514] [G loss: -80.786293]\n",
            "500 [D loss: -546.866150] [G loss: -64.236450]\n",
            "520 [D loss: -546.895447] [G loss: -41.904259]\n",
            "540 [D loss: -624.135010] [G loss: -92.412125]\n",
            "560 [D loss: -621.248108] [G loss: -83.379791]\n",
            "580 [D loss: -643.608032] [G loss: -41.464722]\n",
            "600 [D loss: -711.590271] [G loss: -74.687912]\n",
            "620 [D loss: -722.180298] [G loss: -95.121826]\n",
            "640 [D loss: -774.224609] [G loss: -110.326538]\n",
            "660 [D loss: -790.337952] [G loss: -47.928505]\n",
            "680 [D loss: -827.969727] [G loss: -77.611450]\n",
            "700 [D loss: -923.878540] [G loss: -77.547798]\n",
            "720 [D loss: 491569.187500] [G loss: 490.670929]\n",
            "740 [D loss: 940.629395] [G loss: 327.128784]\n",
            "760 [D loss: 126.663040] [G loss: 593.486816]\n",
            "780 [D loss: 218590448.000000] [G loss: -386.888062]\n",
            "800 [D loss: 51628780.000000] [G loss: -412.780518]\n",
            "820 [D loss: 20401944.000000] [G loss: -373.536987]\n",
            "840 [D loss: 15266016.000000] [G loss: -357.011139]\n",
            "860 [D loss: 20345662.000000] [G loss: -350.645996]\n",
            "880 [D loss: 60373200.000000] [G loss: -370.603638]\n",
            "900 [D loss: 24946758.000000] [G loss: -375.761963]\n",
            "920 [D loss: 411770528.000000] [G loss: 253.900085]\n",
            "940 [D loss: 102147960.000000] [G loss: -880.811279]\n",
            "960 [D loss: 27735656.000000] [G loss: -1001.821411]\n",
            "980 [D loss: 191563520.000000] [G loss: 386.928467]\n",
            "1000 [D loss: 4320677.500000] [G loss: 648.392944]\n",
            "1020 [D loss: 5761703.000000] [G loss: 621.635010]\n",
            "1040 [D loss: 9174200.000000] [G loss: 664.331055]\n",
            "1060 [D loss: 4043453184.000000] [G loss: -303.819183]\n",
            "1080 [D loss: 252220224.000000] [G loss: -424.133545]\n",
            "1100 [D loss: 14446460.000000] [G loss: -409.802734]\n",
            "1120 [D loss: 1092037.875000] [G loss: -320.576782]\n",
            "1140 [D loss: 93708.546875] [G loss: -270.064026]\n",
            "1160 [D loss: 234098.875000] [G loss: 459.466339]\n",
            "1180 [D loss: 98910.546875] [G loss: 486.311768]\n",
            "1200 [D loss: 9250797.000000] [G loss: -177.250732]\n",
            "1220 [D loss: 93743.929688] [G loss: -188.143005]\n",
            "1240 [D loss: 44488.406250] [G loss: 167.432190]\n",
            "1260 [D loss: 269304.812500] [G loss: -832.463928]\n",
            "1280 [D loss: 43731.042969] [G loss: -994.411804]\n",
            "1300 [D loss: 15071.848633] [G loss: -59.672504]\n",
            "1320 [D loss: 8065.059082] [G loss: -592.346375]\n",
            "1340 [D loss: 7177.429688] [G loss: 44.707443]\n",
            "1360 [D loss: 516.888062] [G loss: -305.230408]\n",
            "1380 [D loss: -565.811279] [G loss: -100.616669]\n",
            "1400 [D loss: 594.920898] [G loss: -334.036011]\n",
            "1420 [D loss: 7945.559570] [G loss: -405.015442]\n",
            "1440 [D loss: -52.770569] [G loss: -158.198792]\n",
            "1460 [D loss: 944.176575] [G loss: -143.850555]\n",
            "1480 [D loss: 331.704926] [G loss: 128.623108]\n",
            "1500 [D loss: -673.760925] [G loss: 237.374924]\n",
            "1520 [D loss: -496.040405] [G loss: 265.489838]\n",
            "1540 [D loss: -789.693420] [G loss: 470.926208]\n",
            "1560 [D loss: -251.652725] [G loss: 344.878357]\n",
            "1580 [D loss: -473.185425] [G loss: 494.876221]\n",
            "1600 [D loss: -680.712158] [G loss: 339.502838]\n",
            "1620 [D loss: -858.691589] [G loss: 387.547363]\n",
            "1640 [D loss: -748.286194] [G loss: 228.204315]\n",
            "1660 [D loss: -279.885712] [G loss: 357.790375]\n",
            "1680 [D loss: -517.787537] [G loss: 502.152313]\n",
            "1700 [D loss: -366.752747] [G loss: 396.184265]\n",
            "1720 [D loss: -501.408752] [G loss: 292.553314]\n",
            "1740 [D loss: -553.004089] [G loss: 316.573578]\n",
            "1760 [D loss: -673.592773] [G loss: 289.162262]\n",
            "1780 [D loss: -726.756531] [G loss: 254.660217]\n",
            "1800 [D loss: -885.969360] [G loss: 208.003998]\n",
            "1820 [D loss: -900.842957] [G loss: 181.635254]\n",
            "1840 [D loss: -948.014343] [G loss: 172.173523]\n",
            "1860 [D loss: -1075.334106] [G loss: 159.900970]\n",
            "1880 [D loss: -942.718933] [G loss: 177.196167]\n",
            "1900 [D loss: -1134.658325] [G loss: 173.476974]\n",
            "1920 [D loss: -1289.104614] [G loss: 165.686157]\n",
            "1940 [D loss: -1371.281982] [G loss: 127.945244]\n",
            "1960 [D loss: -1347.379028] [G loss: 106.619522]\n",
            "1980 [D loss: -1138.960083] [G loss: 60.200790]\n",
            "2000 [D loss: -367.098816] [G loss: 2.074391]\n",
            "2020 [D loss: -584.869751] [G loss: -141.670166]\n",
            "2040 [D loss: -190.264236] [G loss: -153.591812]\n",
            "2060 [D loss: -805.750488] [G loss: -162.899078]\n",
            "2080 [D loss: -844.051514] [G loss: -145.846771]\n",
            "2100 [D loss: -751.746765] [G loss: -144.497894]\n",
            "2120 [D loss: -1025.332275] [G loss: -128.019806]\n",
            "2140 [D loss: -1148.569580] [G loss: -152.394485]\n",
            "2160 [D loss: -1262.131836] [G loss: -174.601089]\n",
            "2180 [D loss: -1265.373535] [G loss: -226.924072]\n",
            "2200 [D loss: -1368.752808] [G loss: -304.823792]\n",
            "2220 [D loss: -1517.138916] [G loss: -365.531555]\n",
            "2240 [D loss: -1627.713501] [G loss: -509.531799]\n",
            "2260 [D loss: -888.600098] [G loss: -564.897095]\n",
            "2280 [D loss: -1632.704102] [G loss: -610.375244]\n",
            "2300 [D loss: -1531.974487] [G loss: -566.752319]\n",
            "2320 [D loss: -1227.831665] [G loss: -406.179443]\n",
            "2340 [D loss: -1679.395874] [G loss: -276.382507]\n",
            "2360 [D loss: -2292.781494] [G loss: -207.755981]\n",
            "2380 [D loss: -2226.429199] [G loss: -121.089195]\n",
            "2400 [D loss: -2330.481934] [G loss: -34.670887]\n",
            "2420 [D loss: -2352.275879] [G loss: -33.628544]\n",
            "2440 [D loss: -2440.370850] [G loss: -17.866512]\n",
            "2460 [D loss: -2578.929688] [G loss: 47.644718]\n",
            "2480 [D loss: -2471.001465] [G loss: 47.128288]\n",
            "2500 [D loss: -2850.785400] [G loss: 44.078426]\n",
            "2520 [D loss: -3017.309082] [G loss: 75.725449]\n",
            "2540 [D loss: -3215.865479] [G loss: 97.022850]\n",
            "2560 [D loss: -3328.376465] [G loss: 127.295761]\n",
            "2580 [D loss: -3411.409668] [G loss: 140.840057]\n",
            "2600 [D loss: -3465.468018] [G loss: 212.042755]\n",
            "2620 [D loss: -3465.412109] [G loss: 240.519745]\n",
            "2640 [D loss: -3613.156006] [G loss: 213.895111]\n",
            "2660 [D loss: -3748.603027] [G loss: 185.601929]\n",
            "2680 [D loss: -3740.641113] [G loss: 173.850830]\n",
            "2700 [D loss: -3874.042725] [G loss: 230.851379]\n",
            "2720 [D loss: -3839.133789] [G loss: 170.001526]\n",
            "2740 [D loss: -4015.964844] [G loss: 67.715393]\n",
            "2760 [D loss: -4146.060547] [G loss: 67.881866]\n",
            "2780 [D loss: -4225.451172] [G loss: 125.899399]\n",
            "2800 [D loss: -4330.088867] [G loss: 129.682587]\n",
            "2820 [D loss: -4342.279785] [G loss: 107.174438]\n",
            "2840 [D loss: -4398.552246] [G loss: 46.567917]\n",
            "2860 [D loss: -4448.593750] [G loss: 217.026520]\n",
            "2880 [D loss: -4530.033691] [G loss: 144.914459]\n",
            "2900 [D loss: -4444.923828] [G loss: 82.666550]\n",
            "2920 [D loss: -4528.600586] [G loss: 90.583328]\n",
            "2940 [D loss: -4579.132812] [G loss: 102.336464]\n",
            "2960 [D loss: -4793.870605] [G loss: 158.429352]\n",
            "2980 [D loss: -4824.733887] [G loss: 72.998756]\n",
            "3000 [D loss: -4813.313477] [G loss: -103.780777]\n",
            "3020 [D loss: -4873.145508] [G loss: -111.778618]\n",
            "3040 [D loss: -5080.308594] [G loss: -81.052689]\n",
            "3060 [D loss: -5223.725586] [G loss: -60.854393]\n",
            "3080 [D loss: -5285.072754] [G loss: -47.686115]\n",
            "3100 [D loss: -5354.894043] [G loss: -139.611237]\n",
            "3120 [D loss: -5145.850098] [G loss: -243.922241]\n",
            "3140 [D loss: -5486.820801] [G loss: -262.105011]\n",
            "3160 [D loss: -5508.097656] [G loss: -188.310501]\n",
            "3180 [D loss: -5483.301270] [G loss: -318.646484]\n",
            "3200 [D loss: -5420.474121] [G loss: -306.509216]\n",
            "3220 [D loss: -5625.201660] [G loss: -236.107086]\n",
            "3240 [D loss: -5865.822754] [G loss: -235.507263]\n",
            "3260 [D loss: -5946.157227] [G loss: -353.820129]\n",
            "3280 [D loss: -5889.299805] [G loss: -368.604370]\n",
            "3300 [D loss: -6034.864746] [G loss: -598.293884]\n",
            "3320 [D loss: -5923.503906] [G loss: -369.306641]\n",
            "3340 [D loss: -5580.628418] [G loss: -265.838623]\n",
            "3360 [D loss: -6019.697754] [G loss: -259.951233]\n",
            "3380 [D loss: -6061.474609] [G loss: -205.671936]\n",
            "3400 [D loss: -6250.312500] [G loss: -181.066620]\n",
            "3420 [D loss: -6328.944824] [G loss: -161.859955]\n",
            "3440 [D loss: -6212.416992] [G loss: -161.920044]\n",
            "3460 [D loss: -6362.764648] [G loss: -403.950928]\n",
            "3480 [D loss: -6381.901367] [G loss: -292.372833]\n",
            "3500 [D loss: -6581.176270] [G loss: -295.317749]\n",
            "3520 [D loss: -6798.052246] [G loss: -327.631775]\n",
            "3540 [D loss: -6573.835938] [G loss: -339.579956]\n",
            "3560 [D loss: -7088.914551] [G loss: -330.975403]\n",
            "3580 [D loss: -7255.722168] [G loss: -462.573486]\n",
            "3600 [D loss: -7408.704590] [G loss: -249.897308]\n",
            "3620 [D loss: -7524.075195] [G loss: -193.644165]\n",
            "3640 [D loss: -7761.454102] [G loss: -445.308167]\n",
            "3660 [D loss: -7893.429688] [G loss: -175.259384]\n",
            "3680 [D loss: -8022.115234] [G loss: -229.455597]\n",
            "3700 [D loss: -7875.863281] [G loss: -359.144989]\n",
            "3720 [D loss: -7034.219727] [G loss: 136.712158]\n",
            "3740 [D loss: -7481.516602] [G loss: -261.989777]\n",
            "3760 [D loss: -7466.771973] [G loss: -192.389282]\n",
            "3780 [D loss: -7869.475586] [G loss: -437.183594]\n",
            "3800 [D loss: -8195.239258] [G loss: -324.762695]\n",
            "3820 [D loss: -8459.426758] [G loss: -211.914719]\n",
            "3840 [D loss: -8634.263672] [G loss: -338.109924]\n",
            "3860 [D loss: -8646.451172] [G loss: -215.184784]\n",
            "3880 [D loss: -8170.644531] [G loss: 4.784103]\n",
            "3900 [D loss: -7559.125488] [G loss: -198.232025]\n",
            "3920 [D loss: -8423.835938] [G loss: -654.974487]\n",
            "3940 [D loss: -8794.348633] [G loss: -56.489819]\n",
            "3960 [D loss: -8967.501953] [G loss: -250.226624]\n",
            "3980 [D loss: -8815.262695] [G loss: 77.402878]\n",
            "4000 [D loss: -8459.395508] [G loss: -211.691101]\n",
            "4020 [D loss: -9249.752930] [G loss: -217.395737]\n",
            "4040 [D loss: -9344.322266] [G loss: -20.648674]\n",
            "4060 [D loss: -8577.634766] [G loss: -216.864197]\n",
            "4080 [D loss: -8810.344727] [G loss: -398.521362]\n",
            "4100 [D loss: -9559.478516] [G loss: -134.495697]\n",
            "4120 [D loss: -9493.411133] [G loss: -309.297424]\n",
            "4140 [D loss: -9425.291992] [G loss: -192.025299]\n",
            "4160 [D loss: -9919.347656] [G loss: -325.827911]\n",
            "4180 [D loss: -9803.953125] [G loss: -440.589966]\n",
            "4200 [D loss: -10014.070312] [G loss: -17.234936]\n",
            "4220 [D loss: -9982.155273] [G loss: 76.660263]\n",
            "4240 [D loss: -10259.605469] [G loss: 132.223022]\n",
            "4260 [D loss: -10398.409180] [G loss: -8.486595]\n",
            "4280 [D loss: -10381.305664] [G loss: 83.303238]\n",
            "4300 [D loss: -10595.567383] [G loss: 103.628563]\n",
            "4320 [D loss: -10750.360352] [G loss: -345.615295]\n",
            "4340 [D loss: -10706.800781] [G loss: 222.479950]\n",
            "4360 [D loss: -10985.738281] [G loss: 190.894028]\n",
            "4380 [D loss: -11084.708984] [G loss: 101.468506]\n",
            "4400 [D loss: -11270.568359] [G loss: -24.850903]\n",
            "4420 [D loss: -11284.726562] [G loss: 250.023300]\n",
            "4440 [D loss: -11329.625000] [G loss: 202.199493]\n",
            "4460 [D loss: -11595.921875] [G loss: 27.471107]\n",
            "4480 [D loss: -11591.686523] [G loss: 226.711182]\n",
            "4500 [D loss: -11623.741211] [G loss: 259.186554]\n",
            "4520 [D loss: -11957.937500] [G loss: -126.323044]\n",
            "4540 [D loss: -12017.160156] [G loss: 131.858139]\n",
            "4560 [D loss: -12205.213867] [G loss: -209.412125]\n",
            "4580 [D loss: -12135.256836] [G loss: 147.209534]\n",
            "4600 [D loss: -12399.822266] [G loss: 18.512924]\n",
            "4620 [D loss: -12413.742188] [G loss: 205.943146]\n",
            "4640 [D loss: -12577.421875] [G loss: 197.022537]\n",
            "4660 [D loss: -12552.197266] [G loss: 252.329636]\n",
            "4680 [D loss: -12710.472656] [G loss: 234.442627]\n",
            "4700 [D loss: -12538.974609] [G loss: 124.324692]\n",
            "4720 [D loss: -12588.132812] [G loss: -609.090759]\n",
            "4740 [D loss: -12759.107422] [G loss: 90.312798]\n",
            "4760 [D loss: -12664.886719] [G loss: -38.008858]\n",
            "4780 [D loss: -12853.132812] [G loss: -359.351624]\n",
            "4800 [D loss: -13168.304688] [G loss: -368.867432]\n",
            "4820 [D loss: -13347.217773] [G loss: -387.417725]\n",
            "4840 [D loss: -13366.891602] [G loss: -725.298828]\n",
            "4860 [D loss: -13413.545898] [G loss: -49.701054]\n",
            "4880 [D loss: -13221.676758] [G loss: 453.748596]\n",
            "4900 [D loss: -13638.264648] [G loss: 302.524872]\n",
            "4920 [D loss: -13756.631836] [G loss: 209.055084]\n",
            "4940 [D loss: -14111.465820] [G loss: -582.921509]\n",
            "4960 [D loss: -14380.298828] [G loss: -1086.170166]\n",
            "4980 [D loss: -14472.339844] [G loss: -770.876831]\n",
            "5000 [D loss: -14590.004883] [G loss: -471.872559]\n",
            "5020 [D loss: -14599.503906] [G loss: -271.257690]\n",
            "5040 [D loss: -14690.067383] [G loss: -322.847412]\n",
            "5060 [D loss: -14815.019531] [G loss: -982.701416]\n",
            "5080 [D loss: -14928.614258] [G loss: -629.081299]\n",
            "5100 [D loss: -15015.220703] [G loss: -1555.875732]\n",
            "5120 [D loss: -15372.695312] [G loss: -693.342773]\n",
            "5140 [D loss: -15449.512695] [G loss: -322.262146]\n",
            "5160 [D loss: -15449.125000] [G loss: -372.244629]\n",
            "5180 [D loss: -15651.630859] [G loss: -146.713623]\n",
            "5200 [D loss: -15877.698242] [G loss: -617.992188]\n",
            "5220 [D loss: -15838.827148] [G loss: 16.451540]\n",
            "5240 [D loss: -15893.429688] [G loss: 155.431473]\n",
            "5260 [D loss: -15962.281250] [G loss: 124.884193]\n",
            "5280 [D loss: -16069.631836] [G loss: 299.031860]\n",
            "5300 [D loss: -16363.353516] [G loss: -678.067688]\n",
            "5320 [D loss: -16513.695312] [G loss: -461.238373]\n",
            "5340 [D loss: -16659.113281] [G loss: -75.043869]\n",
            "5360 [D loss: -16674.095703] [G loss: 111.637108]\n",
            "5380 [D loss: -16912.785156] [G loss: 207.929810]\n",
            "5400 [D loss: -17016.906250] [G loss: 15.759656]\n",
            "5420 [D loss: -16996.582031] [G loss: 144.361237]\n",
            "5440 [D loss: -16356.014648] [G loss: -829.949036]\n",
            "5460 [D loss: -16014.100586] [G loss: 13.514142]\n",
            "5480 [D loss: -14868.027344] [G loss: -691.256104]\n",
            "5500 [D loss: -15405.635742] [G loss: 81.943069]\n",
            "5520 [D loss: -15673.891602] [G loss: 501.819672]\n",
            "5540 [D loss: -15962.381836] [G loss: 736.913757]\n",
            "5560 [D loss: -16217.327148] [G loss: 709.858032]\n",
            "5580 [D loss: -17080.664062] [G loss: 1039.541260]\n",
            "5600 [D loss: -17371.583984] [G loss: 1251.279785]\n",
            "5620 [D loss: -18011.861328] [G loss: 183.009369]\n",
            "5640 [D loss: -17939.054688] [G loss: -405.751923]\n",
            "5660 [D loss: -17691.470703] [G loss: -694.204102]\n",
            "5680 [D loss: -17407.548828] [G loss: -904.219116]\n",
            "5700 [D loss: -15315.501953] [G loss: -241.681259]\n",
            "5720 [D loss: -16479.302734] [G loss: 57.275715]\n",
            "5740 [D loss: -15526.567383] [G loss: -167.981888]\n",
            "5760 [D loss: -16689.203125] [G loss: 55.372047]\n",
            "5780 [D loss: -17219.689453] [G loss: 75.682434]\n",
            "5800 [D loss: -13787.897461] [G loss: 753.961548]\n",
            "5820 [D loss: -18271.927734] [G loss: -1001.921387]\n",
            "5840 [D loss: -18131.550781] [G loss: -212.187057]\n",
            "5860 [D loss: -15494.791016] [G loss: 493.358459]\n",
            "5880 [D loss: -15400.842773] [G loss: 608.530029]\n",
            "5900 [D loss: -17324.724609] [G loss: 203.326477]\n",
            "5920 [D loss: -16960.931641] [G loss: -602.365967]\n",
            "5940 [D loss: -15733.550781] [G loss: 239.058502]\n",
            "5980 [D loss: -17730.675781] [G loss: -99.560440]\n",
            "6000 [D loss: -17985.382812] [G loss: 450.416168]\n",
            "6020 [D loss: -17622.207031] [G loss: 394.382843]\n",
            "6040 [D loss: -19289.617188] [G loss: 353.058655]\n",
            "6060 [D loss: -20312.097656] [G loss: 531.136719]\n",
            "6080 [D loss: -19990.150391] [G loss: -588.615662]\n",
            "6100 [D loss: -19084.056641] [G loss: -751.279846]\n",
            "6120 [D loss: -16993.427734] [G loss: -160.984055]\n",
            "6140 [D loss: -18485.867188] [G loss: -493.247650]\n",
            "6160 [D loss: -18611.490234] [G loss: -579.341248]\n",
            "6180 [D loss: -18487.958984] [G loss: 510.249390]\n",
            "6200 [D loss: -19260.615234] [G loss: -8.888317]\n",
            "6220 [D loss: -20814.347656] [G loss: -277.435608]\n",
            "6240 [D loss: -21150.628906] [G loss: -112.488602]\n",
            "6260 [D loss: -21268.792969] [G loss: -316.008636]\n",
            "6280 [D loss: -21417.935547] [G loss: -801.018921]\n",
            "6300 [D loss: -21535.337891] [G loss: -1150.855469]\n",
            "6320 [D loss: -21593.000000] [G loss: -1215.178345]\n",
            "6340 [D loss: -21763.851562] [G loss: -1367.234985]\n",
            "6360 [D loss: -21585.595703] [G loss: -1191.456909]\n",
            "6380 [D loss: -21722.699219] [G loss: -1234.750244]\n",
            "6400 [D loss: -21334.031250] [G loss: -1103.200195]\n",
            "6420 [D loss: -21459.787109] [G loss: -439.660767]\n",
            "6440 [D loss: -18913.986328] [G loss: 151.019470]\n",
            "6460 [D loss: -20938.242188] [G loss: -77.004120]\n",
            "6480 [D loss: -21801.763672] [G loss: 486.607574]\n",
            "6500 [D loss: -21831.148438] [G loss: 1106.295410]\n",
            "6520 [D loss: -22120.939453] [G loss: -114.819962]\n",
            "6540 [D loss: -22398.906250] [G loss: -1050.865479]\n",
            "6560 [D loss: -22655.515625] [G loss: -560.931885]\n",
            "6580 [D loss: -23057.033203] [G loss: -780.043579]\n",
            "6600 [D loss: -22756.712891] [G loss: -1092.972900]\n",
            "6620 [D loss: -23008.703125] [G loss: -1295.798462]\n",
            "6640 [D loss: -23124.404297] [G loss: -797.554565]\n",
            "6660 [D loss: -23349.013672] [G loss: -717.613708]\n",
            "6680 [D loss: -23584.248047] [G loss: -505.204834]\n",
            "6700 [D loss: -23766.115234] [G loss: -771.201965]\n",
            "6720 [D loss: -23646.941406] [G loss: -553.388855]\n",
            "6740 [D loss: -23577.228516] [G loss: -1678.940430]\n",
            "6760 [D loss: -23837.146484] [G loss: -1099.496460]\n",
            "6780 [D loss: -23952.478516] [G loss: -845.410828]\n",
            "6800 [D loss: -23604.929688] [G loss: -259.766388]\n",
            "6820 [D loss: -23549.591797] [G loss: -916.253235]\n",
            "6840 [D loss: -24210.736328] [G loss: -251.584381]\n",
            "6860 [D loss: -24188.027344] [G loss: -362.190399]\n",
            "6880 [D loss: -24136.675781] [G loss: -76.472626]\n",
            "6900 [D loss: -23991.949219] [G loss: -1361.307617]\n",
            "6920 [D loss: -24623.042969] [G loss: -328.767365]\n",
            "6940 [D loss: -24622.634766] [G loss: -318.190643]\n",
            "6960 [D loss: -25045.060547] [G loss: -22.716501]\n",
            "6980 [D loss: -25275.599609] [G loss: -154.735535]\n",
            "7000 [D loss: -25317.882812] [G loss: -726.652832]\n",
            "7020 [D loss: -25627.431641] [G loss: -1638.510986]\n",
            "7040 [D loss: -25762.394531] [G loss: -11.223442]\n",
            "7060 [D loss: -25922.244141] [G loss: 335.795746]\n",
            "7080 [D loss: -25555.876953] [G loss: -431.329102]\n",
            "7100 [D loss: -26285.630859] [G loss: -28.276701]\n",
            "7120 [D loss: -26278.533203] [G loss: -1393.191162]\n",
            "7140 [D loss: -26685.498047] [G loss: 299.190948]\n",
            "7160 [D loss: -26601.029297] [G loss: -1343.098755]\n",
            "7180 [D loss: -27041.291016] [G loss: -63.656948]\n",
            "7200 [D loss: -27159.250000] [G loss: -422.643982]\n",
            "7220 [D loss: -27165.193359] [G loss: 56.426231]\n",
            "7240 [D loss: -27163.265625] [G loss: 469.908844]\n",
            "7260 [D loss: -25140.466797] [G loss: -976.417786]\n",
            "7280 [D loss: -26535.349609] [G loss: -476.319427]\n",
            "7300 [D loss: -27025.640625] [G loss: 600.423584]\n",
            "7320 [D loss: -26541.287109] [G loss: -674.601074]\n",
            "7340 [D loss: -26425.041016] [G loss: -625.077515]\n",
            "7360 [D loss: -26141.863281] [G loss: -565.829712]\n",
            "7380 [D loss: -26345.302734] [G loss: -294.981689]\n",
            "7400 [D loss: -26888.048828] [G loss: -742.077393]\n",
            "7420 [D loss: -27361.224609] [G loss: -3.201262]\n",
            "7440 [D loss: -27589.417969] [G loss: -142.920044]\n",
            "7460 [D loss: -27566.320312] [G loss: -1211.467529]\n",
            "7480 [D loss: -27656.535156] [G loss: 73.252884]\n",
            "7500 [D loss: -28018.291016] [G loss: -276.891907]\n",
            "7520 [D loss: -27958.527344] [G loss: -1306.073608]\n",
            "7540 [D loss: -27637.900391] [G loss: 68.715958]\n",
            "7560 [D loss: -27963.724609] [G loss: -488.147491]\n",
            "7580 [D loss: -28008.234375] [G loss: -16.073002]\n",
            "7600 [D loss: -27635.271484] [G loss: 648.487549]\n",
            "7620 [D loss: -28215.556641] [G loss: -183.808121]\n",
            "7640 [D loss: -28132.474609] [G loss: -243.629395]\n",
            "7660 [D loss: -27451.615234] [G loss: 389.207336]\n",
            "7680 [D loss: -28408.556641] [G loss: -996.824097]\n",
            "7700 [D loss: -27955.353516] [G loss: -464.254211]\n",
            "7720 [D loss: -27942.373047] [G loss: -79.176125]\n",
            "7740 [D loss: -28362.294922] [G loss: -9.127842]\n",
            "7760 [D loss: -27378.904297] [G loss: -394.248413]\n",
            "7780 [D loss: -23064.111328] [G loss: 873.099182]\n",
            "7800 [D loss: -24753.476562] [G loss: 455.942413]\n",
            "7820 [D loss: -24618.761719] [G loss: 695.857361]\n",
            "7840 [D loss: -26184.033203] [G loss: -492.065277]\n",
            "7860 [D loss: -28307.185547] [G loss: -2290.699219]\n",
            "7880 [D loss: -27952.156250] [G loss: -1337.076172]\n",
            "7900 [D loss: -27987.425781] [G loss: -653.618225]\n",
            "7920 [D loss: -26505.978516] [G loss: 241.709579]\n",
            "7940 [D loss: -26364.326172] [G loss: -413.225525]\n",
            "7960 [D loss: -28723.814453] [G loss: -912.037842]\n",
            "7980 [D loss: -28657.115234] [G loss: -1188.336426]\n",
            "8000 [D loss: -27500.732422] [G loss: -1589.752563]\n",
            "8020 [D loss: -25508.982422] [G loss: -346.435760]\n",
            "8040 [D loss: -27700.449219] [G loss: -242.196899]\n",
            "8060 [D loss: -28866.132812] [G loss: -869.157104]\n",
            "8080 [D loss: -28981.521484] [G loss: -1019.440674]\n",
            "8100 [D loss: -29151.197266] [G loss: -1152.474365]\n",
            "8120 [D loss: -27933.970703] [G loss: -677.811707]\n",
            "8140 [D loss: -29006.789062] [G loss: -1018.472778]\n",
            "8160 [D loss: -30385.429688] [G loss: -908.503723]\n",
            "8180 [D loss: -29943.888672] [G loss: -746.831543]\n",
            "8200 [D loss: -29992.859375] [G loss: -830.707886]\n",
            "8220 [D loss: -29336.238281] [G loss: -2433.034180]\n",
            "8240 [D loss: -27542.855469] [G loss: -1134.975952]\n",
            "8260 [D loss: -30452.451172] [G loss: -1549.699097]\n",
            "8280 [D loss: -30805.753906] [G loss: -1516.319946]\n",
            "8300 [D loss: -30729.396484] [G loss: -1644.657349]\n",
            "8320 [D loss: -31196.421875] [G loss: -2287.062988]\n",
            "8340 [D loss: -31109.582031] [G loss: -1106.423828]\n",
            "8360 [D loss: -30260.187500] [G loss: -1064.521484]\n",
            "8380 [D loss: -29625.712891] [G loss: -1858.459229]\n",
            "8400 [D loss: -29793.998047] [G loss: -1793.312622]\n",
            "8420 [D loss: -29995.693359] [G loss: -1793.633057]\n",
            "8440 [D loss: -29280.439453] [G loss: -290.794067]\n",
            "8460 [D loss: -28220.703125] [G loss: -668.493652]\n",
            "8480 [D loss: -29468.623047] [G loss: -1725.194336]\n",
            "8500 [D loss: -29385.509766] [G loss: -2905.813477]\n",
            "8520 [D loss: -28628.986328] [G loss: -2001.775391]\n",
            "8540 [D loss: -28895.011719] [G loss: -3218.122803]\n",
            "8560 [D loss: -31880.238281] [G loss: -3872.626465]\n",
            "8580 [D loss: -33067.378906] [G loss: -2192.684814]\n",
            "8600 [D loss: -33828.183594] [G loss: -4628.072754]\n",
            "8620 [D loss: -34339.097656] [G loss: -4625.079590]\n",
            "8640 [D loss: -34669.035156] [G loss: -5505.208008]\n",
            "8660 [D loss: -34957.679688] [G loss: -6236.488281]\n",
            "8680 [D loss: -35511.113281] [G loss: -3605.704834]\n",
            "8700 [D loss: -35579.335938] [G loss: -4732.814941]\n",
            "8720 [D loss: -35245.968750] [G loss: -5726.247559]\n",
            "8740 [D loss: -35148.792969] [G loss: -6723.958008]\n",
            "8760 [D loss: -35590.605469] [G loss: -4278.845215]\n",
            "8780 [D loss: -34939.859375] [G loss: -4476.178711]\n",
            "8800 [D loss: -34940.839844] [G loss: -5802.224609]\n",
            "8820 [D loss: -35554.976562] [G loss: -4420.103516]\n",
            "8840 [D loss: -35050.839844] [G loss: -6738.332031]\n",
            "8860 [D loss: -34219.015625] [G loss: -5696.984375]\n",
            "8880 [D loss: -34761.562500] [G loss: -6506.246094]\n",
            "8900 [D loss: -35339.375000] [G loss: -4504.238281]\n",
            "8920 [D loss: -34757.250000] [G loss: -7032.431641]\n",
            "8940 [D loss: -32095.294922] [G loss: -9247.812500]\n",
            "8960 [D loss: -27204.921875] [G loss: -12633.066406]\n",
            "8980 [D loss: -26523.160156] [G loss: -18159.070312]\n",
            "9000 [D loss: -25643.072266] [G loss: -20230.484375]\n",
            "9020 [D loss: -24815.218750] [G loss: -17186.531250]\n",
            "9040 [D loss: -26068.585938] [G loss: -23816.277344]\n",
            "9060 [D loss: -28969.042969] [G loss: -30955.792969]\n",
            "9080 [D loss: -30616.558594] [G loss: -23712.857422]\n",
            "9100 [D loss: -31777.519531] [G loss: -11372.224609]\n",
            "9120 [D loss: -26774.343750] [G loss: 309.111542]\n",
            "9140 [D loss: -20250.283203] [G loss: 4053.949219]\n",
            "9160 [D loss: 13815.958984] [G loss: -19888.015625]\n",
            "9180 [D loss: -8029.177734] [G loss: -7169.021484]\n",
            "9200 [D loss: -13449.919922] [G loss: -9336.447266]\n",
            "9220 [D loss: -16538.527344] [G loss: -11029.264648]\n",
            "9240 [D loss: -13313.789062] [G loss: -16289.437500]\n",
            "9260 [D loss: -14579.511719] [G loss: -10830.728516]\n",
            "9280 [D loss: -14158.050781] [G loss: -3075.418945]\n",
            "9300 [D loss: -5943.235352] [G loss: -17515.500000]\n",
            "9320 [D loss: -15733.548828] [G loss: -8356.171875]\n",
            "9340 [D loss: -15861.900391] [G loss: -2551.034424]\n",
            "9360 [D loss: -17742.894531] [G loss: -12809.517578]\n",
            "9380 [D loss: -13323.071289] [G loss: 4065.299316]\n",
            "9400 [D loss: -674.108887] [G loss: 1938.530762]\n",
            "9420 [D loss: -8928.645508] [G loss: -12079.164062]\n",
            "9440 [D loss: -20661.986328] [G loss: -6087.772461]\n",
            "9460 [D loss: -21275.601562] [G loss: -4625.750977]\n",
            "9480 [D loss: -20165.892578] [G loss: 1642.761963]\n",
            "9500 [D loss: -16519.078125] [G loss: 2569.814209]\n",
            "9520 [D loss: -16585.974609] [G loss: 624.904663]\n",
            "9540 [D loss: -19211.914062] [G loss: -10196.319336]\n",
            "9560 [D loss: -18055.470703] [G loss: 5314.857422]\n",
            "9580 [D loss: -23356.642578] [G loss: -4419.066406]\n",
            "9600 [D loss: -20474.742188] [G loss: 7282.698730]\n",
            "9620 [D loss: -21250.828125] [G loss: -3093.766113]\n",
            "9640 [D loss: -24845.007812] [G loss: 381.829773]\n",
            "9660 [D loss: -11088.348633] [G loss: 6574.230957]\n",
            "9680 [D loss: -18221.121094] [G loss: 6721.637695]\n",
            "9700 [D loss: -19977.392578] [G loss: 4058.306152]\n",
            "9720 [D loss: -16378.384766] [G loss: -5335.498535]\n",
            "9740 [D loss: 1688.499023] [G loss: 8654.065430]\n",
            "9760 [D loss: -18460.669922] [G loss: 3536.824219]\n",
            "9780 [D loss: -21954.736328] [G loss: 455.045502]\n",
            "9800 [D loss: -13862.268555] [G loss: -9091.333984]\n",
            "9820 [D loss: 1916.611328] [G loss: 11389.860352]\n",
            "9840 [D loss: -21541.156250] [G loss: 1279.835205]\n",
            "9860 [D loss: -20453.392578] [G loss: -1705.669922]\n",
            "9880 [D loss: -8290.115234] [G loss: 770.576843]\n",
            "9900 [D loss: -20205.636719] [G loss: -5906.473145]\n",
            "9920 [D loss: -20222.695312] [G loss: 4197.506836]\n",
            "9940 [D loss: -15797.991211] [G loss: -1909.923828]\n",
            "9960 [D loss: -8406.517578] [G loss: 4548.875977]\n",
            "9980 [D loss: -15367.373047] [G loss: 3495.791992]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
